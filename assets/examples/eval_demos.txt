-Demonstrations-
The following are examples of other paper evaluations with their novelty ratings and rationales:

--------------------------------------------------
Title: Creating Training Corpora for NLG Micro-Planning
Abstract: In this paper, we focus on how to create data-to-text corpora which can support the learning of wide-coverage microplanners i.e., generation systems that handle lexicalisation, aggregation, surface realisation, sentence segmentation and referring expression generation. We start by reviewing common practice in designing training benchmarks for Natural Language Generation. We then present a novel framework for semi-automatically creating linguistically challenging NLG corpora from existing Knowledge Bases. We apply our framework to DBpedia data and compare the resulting dataset with (Wen et al., 2016)’s dataset. We show that while (Wen et al., 2016)’s dataset is more than twice larger than ours, it is less diverse both in terms of input and in terms of text. We thus propose our corpus generation framework as a novel method for creating challenging data sets from which NLG models can be learned which are capable of generating text from KB data.
Novelty rating: 1
Rationale: This paper presents a step in the direction of developing more challenging corpora for training sentence planners in data-to-text NLG important and timely direction.
It is unclear whether the work reported in this paper represents a substantial advance over Perez-Beltrachini et al.'s (2016) method for selecting content. The authors do not directly compare the present paper to that one. It appears that the main novelty of this paper is the additional analysis, which is however rather superficial.
It is good that the authors report a comparison of how an NNLG baseline fares on this corpus in comparison to that of Wen et al. (2016).  However, the BLEU scores in Wen et al.'s paper appear to be much much higher, suggesting that this NNLG baseline is not sufficient for an informative comparison.
The authors need to more clearly articulate why this paper should count as a substantial advance over what has been published already by Perez-Beltrachini et al, and why the NNLG baseline should be taken seriously.  In contrast to LREC, it is not so common for ACL to publish a main session paper on a corpus development methodology in the absence of some new results of a system making use of the corpus.

--------------------------------------------------
Title: LIE-ACCESS NEURAL TURING MACHINES
Abstract: External neural memory structures have recently become a popular tool for algorithmic deep learning (Graves et al., 2014; Weston et al., 2014). These models generally utilize differentiable versions of traditional discrete memory-access structures (random access, stacks, tapes) to provide the storage necessary for computational tasks. In this work, we argue that these neural memory systems lack specific structure important for relative indexing, and propose an alternative model, Lieaccess memory, that is explicitly designed for the neural setting. In this paradigm, memory is accessed using a continuous head in a key-space manifold. The head is moved via Lie group actions, such as shifts or rotations, generated by a controller, and memory access is performed by linear smoothing in key space. We argue that Lie groups provide a natural generalization of discrete memory structures, such as Turing machines, as they provide inverse and identity operators while maintaining differentiability. To experiment with this approach, we implement a simplified Lie-access neural Turing machine (LANTM) with different Lie groups. We find that this approach is able to perform well on a range of algorithmic tasks.
Novelty rating: 2
Rationale: The Neural Turing Machine and related “external memory models” have demonstrated an ability to learn algorithmic solutions by utilizing differentiable analogues of conventional memory structures. The NTM, which is the most relevant to this work, uses a differentiable version of a Turing machine tape. The controller outputs a kernel which “softly” shifts the head, allowing the machine to read and write sequences. Since this soft shift typically “smears” the focus of the head, the controller also outputs a sharpening parameter which compensates by refocusing the distribution.
The premise of this work is to notice that while the NTM emulates a differentiable version of a Turing tape, there is no particular reason that one is constrained to follow the topology of a Turing tape. Instead they propose memory stored at a set of points on a manifold and shift actions which form a Lie group. In this way, memory points can have have different relationships to one another, rather than being constrained to Z.
Overall, the paper is well communicated and a novel idea.
The primary limitation of this paper is its limited impact. While this approach is certainly mathematically elegant, even likely beneficial for some specific problems where the problem structure matches the group structure, it is not clear that this significantly contributes to building models capable of more general program learning. Instead, it is likely to make an already complex and slow model such as the NTM even slower. In general, it would seem memory topology is problem specific and should therefore be learned rather than specified.
The baseline used for comparison is a very simple model, which does not even having the sharpening (the NTM approach to solving the problem of head distributions becoming ‘smeared’). There is also no comparison with the successor to the NTM, the DNC, which provides a more general approach to linking memories based on prior memory accesses.

--------------------------------------------------
Title: INTROSPECTION:ACCELERATING NEURAL NETWORK TRAINING BY LEARNING WEIGHT EVOLUTION
Abstract: Neural Networks are function approximators that have achieved state-of-the-art accuracy in numerous machine learning tasks. In spite of their great success in terms of accuracy, their large training time makes it difficult to use them for various tasks. In this paper, we explore the idea of learning weight evolution pattern from a simple network for accelerating training of novel neural networks. We use a neural network to learn the training pattern from MNIST classification and utilize it to accelerate training of neural networks used for CIFAR-10 and ImageNet classification. Our method has a low memory footprint and is computationally efficient. This method can also be used with other optimizers to give faster convergence. The results indicate a general trend in the weight evolution during training of neural networks.
Novelty rating: 3
Rationale: In this paper, the authors use a separate introspection neural network to predict the future value of the weights directly from their past history. The introspection network is trained on the parameter progressions collected from training separate set of meta learning models using a typical optimizer, e.g. SGD.
Meta-learning approach is different than the previous learning to learn approach
- The paper will benefit from more thorough experiments on other neural network architectures where the geometry of the parameter space are sufficiently different than CNNs such as fully connected and recurrent neural networks.
Neither MNIST nor CIFAR experimental section explained the architectural details
Comparison with different baseline optimizer such as Adam would be a strong addition or at least explain how the hyper-parameters, such as learning rate and momentum, are chosen for the baseline SGD method.
Overall, due to the omission of the experimental details in the current revision, it is hard to draw any conclusive insight about the proposed method.

--------------------------------------------------
Title: OTHER MODIFICATIONS
Abstract: PixelCNNs are a recently proposed class of powerful generative models with tractable likelihood. Here we discuss our implementation of PixelCNNs which we make available at https://github.com/openai/pixel-cnn. Our implementation contains a number of modifications to the original model that both simplify its structure and improve its performance. 1) We use a discretized logistic mixture likelihood on the pixels, rather than a 256-way softmax, which we find to speed up training. 2) We condition on whole pixels, rather than R/G/B sub-pixels, simplifying the model structure. 3) We use downsampling to efficiently capture structure at multiple resolutions. 4) We introduce additional short-cut connections to further speed up optimization. 5) We regularize the model using dropout. Finally, we present state-of-the-art log likelihood results on CIFAR-10 to demonstrate the usefulness of these modifications.
Novelty rating: 4
Rationale: This paper proposes five modifications to improve PixelCNN, a generative model with tractable likelihood. The authors empirically showed the impact of each of their proposed modifications using a series of ablation experiments. They also reported a new state-of-the-art result on CIFAR-10. Improving generative models, especially for images, is an active research area and this paper definitely contributes to it.
The authors motivate each modification well they proposed. They also used ablation experiments to show each of them is important.
The authors use a discretized mixture of logistic distributions to model the conditional distribution of a sub-pixel instead of a 256-way softmax. This allows to have a lower output dimension and to be better suited at learning ordinal relationships between sub-pixel values. The authors also mentioned it speeded up training time (less computation) as well as the convergence during the optimization of the model (as shown in Fig.6).
The authors make an interesting remark about how the dependencies between the color channels of a pixel are likely to be relatively simple and do not require a deep network to model. This allows them to have a simplified architecture where you don't have to separate out all feature maps in 3 groups depending on whether or not they can see the R/G/B sub-pixel of the current location.
Overall, the work is perhaps a bit incremental, but it seems to be well-executed. The results are convincing, even if they aren't particularly ground-breaking.

--------------------------------------------------
Title: Handling Cold-Start Problem in Review Spam Detection by Jointly Embedding Texts and Behaviors
Abstract: Solving cold-start problem in review spam detection is an urgent and significant task. It can help the on-line review websites to relieve the damage of spammers in time, but has never been investigated by previous work. This paper proposes a novel neural network model to detect review spam for cold-start problem, by learning to represent the new reviewers’ review with jointly embedded textual and behavioral information. Experimental results prove the proposed model achieves an effective performance and possesses preferable domain-adaptability. It is also applicable to a large scale dataset in an unsupervised way.
Novelty rating: 5
Rationale: This paper investigates the cold-start problem in review spam detection. The authors first qualitatively and quantitatively analyze the cold-start problem.
They observe that there is no enough prior data from a new user in this realistic scenario. The traditional features fail to help to identify review spam. Instead, they turn to rely on the abundant textual and behavioral information of the existing reviewer to augment the information of a new user.
In specific, they propose a neural network to represent the review of the new reviewer with the learnt word embedding and jointly encoded behavioral information. In the experiments, the authors make comparisons with traditional methods, and show the effectiveness of their model.
The idea of jointly encoding texts and behaviors is interesting. The cold-start problem is actually an urgent problem to several online review analysis applications. In my knowledge, the previous work has not yet attempted to tackle this problem. This paper is meaningful and presents a reasonable analysis. And the results of the proposed model can also be available for downstream detection models.
It is a good paper and should be accepted by ACL.
