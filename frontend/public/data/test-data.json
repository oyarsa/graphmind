[{"graph":{"title":"LanguageMPC: Large Language Models as Decision Makers for Autonomous Driving","abstract":"Existing learning-based autonomous driving (AD) systems face challenges in comprehending high-level information, generalizing to rare events, and providing interpretability. To address these problems, this work employs Large Language Models (LLMs) as a decision-making component for complex AD scenarios that require human commonsense understanding. We devise cognitive pathways to enable comprehensive reasoning with LLMs, and develop algorithms for translating LLM decisions into actionable driving commands. Through this approach, LLM decisions are seamlessly integrated with low-level controllers by guided parameter matrix adaptation. Extensive experiments demonstrate that our proposed method not only consistently surpasses baseline approaches in single-vehicle tasks, but also helps handle complex driving behaviors even multi-vehicle coordination, thanks to the commonsense reasoning capabilities of LLMs. This paper presents an initial step toward leveraging LLMs as effective decision-makers for intricate AD scenarios in terms of safety, efficiency, generalizability, and interoperability. We aspire for it to serve as inspiration for future research in this field.","entities":[{"label":"LanguageMPC: Large Language Models as Decision Makers for Autonomous Driving","type":"title","detail":null,"excerpts":null},{"label":"applications to robotics, autonomy, planning","type":"primary_area","detail":null,"excerpts":null},{"label":"Large Language Models","type":"keyword","detail":null,"excerpts":null},{"label":"Autonomous Driving","type":"keyword","detail":null,"excerpts":null},{"label":"Decision Making","type":"keyword","detail":null,"excerpts":null},{"label":"Model Predictive Control","type":"keyword","detail":null,"excerpts":null},{"label":"Commonsense Reasoning","type":"keyword","detail":null,"excerpts":null},{"label":"This paper presents a method that employs Large Language Models as decision-makers in autonomous driving, demonstrating improved performance in complex driving scenarios through enhanced reasoning and interpretability.","type":"tldr","detail":null,"excerpts":null},{"label":"Integration of LLMs in Autonomous Driving","type":"claim","detail":"The paper claims that integrating Large Language Models (LLMs) into autonomous driving systems enhances decision-making capabilities, particularly in complex scenarios requiring commonsense reasoning.","excerpts":null},{"label":"Performance Improvement Over Baselines","type":"claim","detail":"The proposed method consistently outperforms baseline approaches in both single-vehicle tasks and multi-vehicle coordination, showcasing the effectiveness of LLMs in driving scenarios.","excerpts":null},{"label":"Cognitive Pathways for LLMs","type":"method","detail":"This method involves devising cognitive pathways that enable LLMs to engage in comprehensive reasoning for driving scenarios, translating high-level decisions into actionable commands for low-level controllers.","excerpts":null},{"label":"Guided Parameter Matrix Adaptation","type":"method","detail":"This method integrates LLM decisions with low-level controllers through guided parameter matrix adaptation, allowing for seamless execution of driving commands based on LLM outputs.","excerpts":null},{"label":"Single-Vehicle Decision-Making","type":"experiment","detail":"Experiments conducted in various driving scenarios, including signalized intersections and emergency avoidance, to evaluate the performance of the proposed LLM-based decision-making system against traditional methods.","excerpts":null},{"label":"Multi-Vehicle Coordination","type":"experiment","detail":"Evaluation of the proposed method in complex multi-vehicle scenarios to assess its adaptability and effectiveness in real-world driving situations.","excerpts":null}],"relationships":[{"source":"LanguageMPC: Large Language Models as Decision Makers for Autonomous Driving","target":"applications to robotics, autonomy, planning"},{"source":"LanguageMPC: Large Language Models as Decision Makers for Autonomous Driving","target":"Large Language Models"},{"source":"LanguageMPC: Large Language Models as Decision Makers for Autonomous Driving","target":"Autonomous Driving"},{"source":"LanguageMPC: Large Language Models as Decision Makers for Autonomous Driving","target":"Decision Making"},{"source":"LanguageMPC: Large Language Models as Decision Makers for Autonomous Driving","target":"Model Predictive Control"},{"source":"LanguageMPC: Large Language Models as Decision Makers for Autonomous Driving","target":"Commonsense Reasoning"},{"source":"LanguageMPC: Large Language Models as Decision Makers for Autonomous Driving","target":"This paper presents a method that employs Large Language Models as decision-makers in autonomous driving, demonstrating improved performance in complex driving scenarios through enhanced reasoning and interpretability."},{"source":"This paper presents a method that employs Large Language Models as decision-makers in autonomous driving, demonstrating improved performance in complex driving scenarios through enhanced reasoning and interpretability.","target":"Integration of LLMs in Autonomous Driving"},{"source":"This paper presents a method that employs Large Language Models as decision-makers in autonomous driving, demonstrating improved performance in complex driving scenarios through enhanced reasoning and interpretability.","target":"Performance Improvement Over Baselines"},{"source":"Integration of LLMs in Autonomous Driving","target":"Cognitive Pathways for LLMs"},{"source":"Performance Improvement Over Baselines","target":"Guided Parameter Matrix Adaptation"},{"source":"Cognitive Pathways for LLMs","target":"Single-Vehicle Decision-Making"},{"source":"Cognitive Pathways for LLMs","target":"Multi-Vehicle Coordination"},{"source":"Guided Parameter Matrix Adaptation","target":"Single-Vehicle Decision-Making"},{"source":"Guided Parameter Matrix Adaptation","target":"Multi-Vehicle Coordination"}],"valid_status":"Valid","valid_status_all":["Valid"]},"related":[{"summary":"The Related Paper, 'KoMA: Knowledge-driven Multi-agent Framework for Autonomous Driving with Large Language Models,' supports the Main Paper by emphasizing the advantages of using Large Language Models (LLMs) in autonomous driving, particularly in enhancing generalization and interpretability. It builds on the Main Paper's premise by introducing a multi-agent framework that facilitates cooperative knowledge sharing and cognitive synergy among LLM-driven agents, thereby addressing the complexities of driving tasks. Both papers highlight the effectiveness of LLMs in improving decision-making in intricate driving scenarios, with the Related Paper providing additional methodologies that enhance robustness and adaptability, further validating the Main Paper's claims.","paper_id":"126061d85cf25e37b74f2a4eb84578c3fae609c5","title":"KoMA: Knowledge-driven Multi-agent Framework for Autonomous Driving with Large Language Models","abstract":"Large language models (LLMs) as autonomous agents offer a novel avenue for tackling real-world challenges through a knowledge-driven manner. These LLM-enhanced methodologies excel in generalization and interpretability. However, the complexity of driving tasks often necessitates the collaboration of multiple, heterogeneous agents, underscoring the need for such LLM-driven agents to engage in cooperative knowledge sharing and cognitive synergy. Despite the promise of LLMs, current applications predominantly center around single agent scenarios. To broaden the horizons of knowledge-driven strategies and bolster the generalization capabilities of autonomous agents, we propose the KoMA framework consisting of multi-agent interaction, multi-step planning, shared-memory, and ranking-based reflection modules to enhance multi-agents' decision-making in complex driving scenarios. Based on the framework's generated text descriptions of driving scenarios, the multi-agent interaction module enables LLM agents to analyze and infer the intentions of surrounding vehicles, akin to human cognition. The multi-step planning module enables LLM agents to analyze and obtain final action decisions layer by layer to ensure consistent goals for short-term action decisions. The shared memory module can accumulate collective experience to make superior decisions, and the ranking-based reflection module can evaluate and improve agent behavior with the aim of enhancing driving safety and efficiency. The KoMA framework not only enhances the robustness and adaptability of autonomous driving agents but also significantly elevates their generalization capabilities across diverse scenarios. Empirical results demonstrate the superiority of our approach over traditional methods, particularly in its ability to handle complex, unpredictable driving environments without extensive retraining.","score":0.7316346168518066,"polarity":"positive","source":"semantic","year":2024,"authors":["Kemou Jiang","Xuan Cai","Zhiyong Cui","Aoyong Li","Yilong Ren","Haiyang Yu","Hao Yang","Daocheng Fu","Licheng Wen","Pinlong Cai"],"venue":null,"citation_count":4,"reference_count":46,"influential_citation_count":0,"corpus_id":271310037,"url":"https://www.semanticscholar.org/paper/126061d85cf25e37b74f2a4eb84578c3fae609c5","arxiv_id":null,"contexts":null,"background":"Large language models (LLMs) as autonomous agents offer a novel avenue for tackling real-world challenges through a knowledge-driven manner. These LLM-enhanced methodologies excel in generalization and interpretability. However, the complexity of driving tasks often necessitates the collaboration of multiple, heterogeneous agents, underscoring the need for such LLM-driven agents to engage in cooperative knowledge sharing and cognitive synergy. Despite the promise of LLMs, current applications predominantly center around single agent scenarios.","target":"To broaden the horizons of knowledge-driven strategies and bolster the generalization capabilities of autonomous agents, we propose the KoMA framework consisting of multi-agent interaction, multi-step planning, shared-memory, and ranking-based reflection modules to enhance multi-agents' decision-making in complex driving scenarios. Based on the framework's generated text descriptions of driving scenarios, the multi-agent interaction module enables LLM agents to analyze and infer the intentions of surrounding vehicles, akin to human cognition. The multi-step planning module enables LLM agents to analyze and obtain final action decisions layer by layer to ensure consistent goals for short-term action decisions. The shared memory module can accumulate collective experience to make superior decisions, and the ranking-based reflection module can evaluate and improve agent behavior with the aim of enhancing driving safety and efficiency. The KoMA framework not only enhances the robustness and adaptability of autonomous driving agents but also significantly elevates their generalization capabilities across diverse scenarios. Empirical results demonstrate the superiority of our approach over traditional methods, particularly in its ability to handle complex, unpredictable driving environments without extensive retraining."},{"summary":"The Related Paper, \"AD-H: Autonomous Driving with Hierarchical Agents,\" supports the Main Paper by emphasizing the importance of leveraging multimodal large language models (MLLMs) for high-level reasoning in autonomous driving. It highlights the limitations of directly translating high-level instructions into low-level control signals, which aligns with the Main Paper's focus on using Large Language Models (LLMs) for decision-making in complex driving scenarios. Both papers advocate for a structured approach to integrate high-level reasoning with actionable commands, with the Related Paper providing additional insights on hierarchical systems that enhance generalizability and performance, thus reinforcing the Main Paper's claims about the effectiveness of LLMs in autonomous driving.","paper_id":"6e0c98f0e7a992102e682dbe2a598d21936e2a11","title":"AD-H: Autonomous Driving with Hierarchical Agents","abstract":"Due to the impressive capabilities of multimodal large language models (MLLMs), recent works have focused on employing MLLM-based agents for autonomous driving in large-scale and dynamic environments. However, prevalent approaches often directly translate high-level instructions into low-level vehicle control signals, which deviates from the inherent language generation paradigm of MLLMs and fails to fully harness their emergent powers. As a result, the generalizability of these methods is highly restricted by autonomous driving datasets used during fine-tuning. To tackle this challenge, we propose to connect high-level instructions and low-level control signals with mid-level language-driven commands, which are more fine-grained than high-level instructions but more universal and explainable than control signals, and thus can effectively bridge the gap in between. We implement this idea through a hierarchical multi-agent driving system named AD-H, including a MLLM planner for high-level reasoning and a lightweight controller for low-level execution. The hierarchical design liberates the MLLM from low-level control signal decoding and therefore fully releases their emergent capability in high-level perception, reasoning, and planning. We build a new dataset with action hierarchy annotations. Comprehensive closed-loop evaluations demonstrate several key advantages of our proposed AD-H system. First, AD-H can notably outperform state-of-the-art methods in achieving exceptional driving performance, even exhibiting self-correction capabilities during vehicle operation, a scenario not encountered in the training dataset. Second, AD-H demonstrates superior generalization under long-horizon instructions and novel environmental conditions, significantly surpassing current state-of-the-art methods. We will make our data and code publicly accessible at https://github.com/zhangzaibin/AD-H","score":0.709808886051178,"polarity":"positive","source":"semantic","year":2024,"authors":["Zaibin Zhang","Shiyu Tang","Yuanhang Zhang","Talas Fu","Yifan Wang","Yang Liu","Dong Wang","Jing Shao","Lijun Wang","Huchuan Lu"],"venue":null,"citation_count":3,"reference_count":93,"influential_citation_count":1,"corpus_id":270258429,"url":"https://www.semanticscholar.org/paper/6e0c98f0e7a992102e682dbe2a598d21936e2a11","arxiv_id":null,"contexts":null,"background":"Due to the impressive capabilities of multimodal large language models (MLLMs), recent works have focused on employing MLLM-based agents for autonomous driving in large-scale and dynamic environments. However, prevalent approaches often directly translate high-level instructions into low-level vehicle control signals, which deviates from the inherent language generation paradigm of MLLMs and fails to fully harness their emergent powers. As a result, the generalizability of these methods is highly restricted by autonomous driving datasets used during fine-tuning.","target":"To tackle this challenge, we propose to connect high-level instructions and low-level control signals with mid-level language-driven commands, which are more fine-grained than high-level instructions but more universal and explainable than control signals, and thus can effectively bridge the gap in between. We implement this idea through a hierarchical multi-agent driving system named AD-H, including a MLLM planner for high-level reasoning and a lightweight controller for low-level execution. The hierarchical design liberates the MLLM from low-level control signal decoding and therefore fully releases their emergent capability in high-level perception, reasoning, and planning. We build a new dataset with action hierarchy annotations. Comprehensive closed-loop evaluations demonstrate several key advantages of our proposed AD-H system. First, AD-H can notably outperform state-of-the-art methods in achieving exceptional driving performance, even exhibiting self-correction capabilities during vehicle operation, a scenario not encountered in the training dataset. Second, AD-H demonstrates superior generalization under long-horizon instructions and novel environmental conditions, significantly surpassing current state-of-the-art methods. We will make our data and code publicly accessible at https://github.com/zhangzaibin/AD-H."},{"summary":"The Related Paper supports the Main Paper by emphasizing the integration of Large Language Models (LLMs) in enhancing safety and decision-making in autonomous driving systems. Both papers highlight the challenges of non-interpretability and poor generalization in traditional deep learning approaches, advocating for LLMs' commonsense reasoning capabilities. The Related Paper's focus on safety verification and adaptive control complements the Main Paper's cognitive pathways and decision-making algorithms, reinforcing the argument that LLMs can significantly improve performance and safety in complex driving scenarios.","paper_id":"c579ab910bd0ef8d6e06fc1b3557c16068af4fe5","title":"Empowering Autonomous Driving with Large Language Models: A Safety Perspective","abstract":"Autonomous Driving (AD) encounters significant safety hurdles in long-tail unforeseen driving scenarios, largely stemming from the non-interpretability and poor generalization of the deep neural networks within the AD system, particularly in out-of-distribution and uncertain data. To this end, this paper explores the integration of Large Language Models (LLMs) into AD systems, leveraging their robust common-sense knowledge and reasoning abilities. The proposed methodologies employ LLMs as intelligent decision-makers in behavioral planning, augmented with a safety verifier shield for contextual safety learning, for enhancing driving performance and safety. We present two key studies in a simulated environment: an adaptive LLM-conditioned Model Predictive Control (MPC) and an LLM-enabled interactive behavior planning scheme with a state machine. Demonstrating superior performance and safety metrics compared to state-of-the-art approaches, our approach shows the promising potential for using LLMs for autonomous vehicles.","score":0.693374752998352,"polarity":"positive","source":"semantic","year":2023,"authors":["Yixuan Wang","Ruochen Jiao","Chengtian Lang","Sinong Simon Zhan","Chao Huang","Zhaoran Wang","Zhuoran Yang","Qi Zhu"],"venue":null,"citation_count":21,"reference_count":56,"influential_citation_count":0,"corpus_id":265609613,"url":"https://www.semanticscholar.org/paper/c579ab910bd0ef8d6e06fc1b3557c16068af4fe5","arxiv_id":null,"contexts":null,"background":"Autonomous Driving (AD) encounters significant safety hurdles in long-tail unforeseen driving scenarios, largely stemming from the non-interpretability and poor generalization of the deep neural networks within the AD system, particularly in out-of-distribution and uncertain data.","target":"To this end, this paper explores the integration of Large Language Models (LLMs) into AD systems, leveraging their robust common-sense knowledge and reasoning abilities. The proposed methodologies employ LLMs as intelligent decision-makers in behavioral planning, augmented with a safety verifier shield for contextual safety learning, for enhancing driving performance and safety. We present two key studies in a simulated environment: an adaptive LLM-conditioned Model Predictive Control (MPC) and an LLM-enabled interactive behavior planning scheme with a state machine. Demonstrating superior performance and safety metrics compared to state-of-the-art approaches, our approach shows the promising potential for using LLMs for autonomous vehicles."},{"summary":"The Related Paper, 'LLM-Assist: Enhancing Closed-Loop Planning with Language-Based Reasoning', supports the Main Paper by demonstrating the effectiveness of Large Language Models (LLMs) in enhancing planning algorithms for autonomous driving. Both papers emphasize the importance of commonsense reasoning in addressing the limitations of existing planning methods, with the Related Paper showcasing a hybrid approach that combines rule-based and LLM-based planning to achieve superior performance. This aligns with the Main Paper's claim that LLMs can improve decision-making in complex driving scenarios, reinforcing the potential of LLMs as effective tools in autonomous driving systems.","paper_id":"237b0cf9d78f4a52274b868656ad011f599aeb26","title":"LLM-Assist: Enhancing Closed-Loop Planning with Language-Based Reasoning","abstract":"Although planning is a crucial component of the autonomous driving stack, researchers have yet to develop robust planning algorithms that are capable of safely handling the diverse range of possible driving scenarios. Learning-based planners suffer from overfitting and poor long-tail performance. On the other hand, rule-based planners generalize well, but might fail to handle scenarios that require complex driving maneuvers. To address these limitations, we investigate the possibility of leveraging the common-sense reasoning capabilities of Large Language Models (LLMs) such as GPT4 and Llama2 to generate plans for self-driving vehicles. In particular, we develop a novel hybrid planner that leverages a conventional rule-based planner in conjunction with an LLM-based planner. Guided by commonsense reasoning abilities of LLMs, our approach navigates complex scenarios which existing planners struggle with, produces well-reasoned outputs while also remaining grounded through working alongside the rule-based approach. Through extensive evaluation on the nuPlan benchmark, we achieve state-of-the-art performance, outperforming all existing pure learning- and rule-based methods across most metrics. Our code will be available at https://llmassist.github.io.","score":0.6924854516983032,"polarity":"positive","source":"semantic","year":2023,"authors":["S. P. Sharan","F. Pittaluga","G. VijayKumarB.","M. Chandraker"],"venue":null,"citation_count":43,"reference_count":37,"influential_citation_count":3,"corpus_id":266693892,"url":"https://www.semanticscholar.org/paper/237b0cf9d78f4a52274b868656ad011f599aeb26","arxiv_id":null,"contexts":null,"background":"Although planning is a crucial component of the autonomous driving stack, researchers have yet to develop robust planning algorithms that are capable of safely handling the diverse range of possible driving scenarios. Learning-based planners suffer from overfitting and poor long-tail performance. On the other hand, rule-based planners generalize well, but might fail to handle scenarios that require complex driving maneuvers.","target":"To address these limitations, we investigate the possibility of leveraging the common-sense reasoning capabilities of Large Language Models (LLMs) such as GPT4 and Llama2 to generate plans for self-driving vehicles. In particular, we develop a novel hybrid planner that leverages a conventional rule-based planner in conjunction with an LLM-based planner. Guided by commonsense reasoning abilities of LLMs, our approach navigates complex scenarios which existing planners struggle with, produces well-reasoned outputs while also remaining grounded through working alongside the rule-based approach. Through extensive evaluation on the nuPlan benchmark, we achieve state-of-the-art performance, outperforming all existing pure learning- and rule-based methods across most metrics. Our code will be available at https://llmassist.github.io."},{"summary":"The Related Paper contrasts with the Main Paper by emphasizing the limitations of using LLMs directly for decision-making in autonomous driving, highlighting potential inaccuracies and inconsistencies in LLM outputs. While the Main Paper advocates for LLMs as effective decision-makers through cognitive pathways and integration with low-level controllers, the Related Paper suggests a more cautious approach that combines LLM knowledge with statistical learning to create a robust framework for decision-making. This divergence underscores the need for a structured learning process rather than relying solely on LLMs for complex driving tasks.","paper_id":"0df4a0bee3cf7bc3cb7e0264ccf8963034850481","title":"Facilitating Autonomous Driving Tasks With Large Language Models","abstract":"We explore how large language models (LLMs) can expedite and automate the learning process for autonomous driving tasks. This involves harnessing LLM knowledge to shape a learning framework and utilizing LLMs to guide the learning process. We conduct a case study to demonstrate LLMs’ ability to export driving rules. LLM outputs may not be entirely reliable for the direct handling of driving decisions due to potential inaccuracies and inconsistencies. To address these issues, we propose integrating LLM knowledge with statistical learning. This enables LLMs to export task-specific knowledge as symbolic rules, forming the initial learning structure. Rule weights are calculated based on statistical salience derived from training data, resulting in a set of weighted rules for robust decision making. Furthermore, this set of weighted rules preserves strong semantics, allowing LLMs to comprehend and make modifications based on varying needs. Simulations using a highway driving simulator validate the effectiveness of our approach.","score":0.8126449584960938,"polarity":"negative","source":"semantic","year":2025,"authors":["Mengyao Wu","F. Yu","P. X. Liu","Ying He"],"venue":null,"citation_count":0,"reference_count":18,"influential_citation_count":0,"corpus_id":272876917,"url":"https://www.semanticscholar.org/paper/0df4a0bee3cf7bc3cb7e0264ccf8963034850481","arxiv_id":null,"contexts":null,"background":"We explore how large language models (LLMs) can expedite and automate the learning process for autonomous driving tasks. LLM outputs may not be entirely reliable for the direct handling of driving decisions due to potential inaccuracies and inconsistencies.","target":"This involves harnessing LLM knowledge to shape a learning framework and utilizing LLMs to guide the learning process. We conduct a case study to demonstrate LLMs’ ability to export driving rules. To address these issues, we propose integrating LLM knowledge with statistical learning. This enables LLMs to export task-specific knowledge as symbolic rules, forming the initial learning structure. Rule weights are calculated based on statistical salience derived from training data, resulting in a set of weighted rules for robust decision making. Furthermore, this set of weighted rules preserves strong semantics, allowing LLMs to comprehend and make modifications based on varying needs. Simulations using a highway driving simulator validate the effectiveness of our approach."},{"summary":"The Related Paper, LMDrive, contrasts with the Main Paper by emphasizing a closed-loop, end-to-end approach to autonomous driving that integrates multimodal sensor data with natural language instructions, allowing for real-time human interaction. While the Main Paper focuses on using LLMs for decision-making in complex scenarios, LMDrive critiques the reliance on limited-format inputs in existing systems and highlights the challenges of handling unforeseen events in urban environments. Additionally, LMDrive claims to be the first to implement LLMs in a closed-loop driving framework, suggesting a different methodological approach compared to the Main Paper's cognitive pathways and parameter adaptation techniques.","paper_id":"e0b05e314372ed580d9612ef5f0ee672b17ad2e4","title":"LMDrive: Closed-Loop End-to-End Driving with Large Language Models","abstract":"Despite significant recent progress in the field of autonomous driving, modern methods still struggle and can incur serious accidents when encountering long-tail unfore-seen events and challenging urban scenarios. On the one hand, large language models (LLM) have shown impres-sive reasoning capabilities that approach “Artificial Gen-eral Intelligence”. On the other hand, previous autonomous driving methods tend to rely on limited-format inputs (e.g., sensor data and navigation waypoints), restricting the vehi-cle's ability to understand language information and inter-act with humans. To this end, this paper introduces LM-Drive, a novel language-guided, end-to-end, closed-loop autonomous driving framework. LMDrive uniquely processes and integrates multimodal sensor data with naturallanguage instructions, enabling interaction with humans and navigation software in realistic instructional settings. To facilitate research in language-based closed-loop autonomous driving, we also publicly release the corresponding dataset which includes approximately 64K instruction-following data clips, and the LangAuto benchmark that tests the system's ability to handle complex instructions and challenging driving scenarios. Extensive closed-loop experiments are conducted to demonstrate LMDrive's effectiveness. To the best of our knowledge, we're the very first work to leverage LLMs for closed-loop end-to-end autonomous driving. Code is available on our webpage.","score":0.7793165445327759,"polarity":"negative","source":"semantic","year":2023,"authors":["Hao Shao","Yuxuan Hu","Letian Wang","Steven L. Waslander","Yu Liu","Hongsheng Li"],"venue":null,"citation_count":79,"reference_count":56,"influential_citation_count":8,"corpus_id":266174267,"url":"https://www.semanticscholar.org/paper/e0b05e314372ed580d9612ef5f0ee672b17ad2e4","arxiv_id":null,"contexts":null,"background":"Despite significant recent progress in the field of autonomous driving, modern methods still struggle and can incur serious accidents when encountering long-tail unforeseen events and challenging urban scenarios. On the one hand, large language models (LLM) have shown impressive reasoning capabilities that approach 'Artificial General Intelligence'. On the other hand, previous autonomous driving methods tend to rely on limited-format inputs (e.g., sensor data and navigation waypoints), restricting the vehicle's ability to understand language information and interact with humans.","target":"To this end, this paper introduces LM-Drive, a novel language-guided, end-to-end, closed-loop autonomous driving framework. LM-Drive uniquely processes and integrates multimodal sensor data with natural language instructions, enabling interaction with humans and navigation software in realistic instructional settings. To facilitate research in language-based closed-loop autonomous driving, we also publicly release the corresponding dataset which includes approximately 64K instruction-following data clips, and the LangAuto benchmark that tests the system's ability to handle complex instructions and challenging driving scenarios. Extensive closed-loop experiments are conducted to demonstrate LM-Drive's effectiveness. To the best of our knowledge, we're the very first work to leverage LLMs for closed-loop end-to-end autonomous driving. Code is available on our webpage."},{"summary":"The Related Paper contrasts with the Main Paper by emphasizing the limitations of LLMs in real-time autonomous driving environments, particularly their lengthy inference times. While the Main Paper advocates for LLMs as effective decision-makers, the Related Paper introduces the RAPID framework, which focuses on leveraging LLM knowledge to enhance Reinforcement Learning (RL) agents, thereby addressing efficiency and robustness concerns. This highlights a critical gap in the Main Paper's approach, suggesting that while LLMs have potential, their integration into real-time systems requires further adaptation and optimization.","paper_id":"e7d5f13c250a40ecb6d58a3119a19acd74e86933","title":"Robust RL with LLM-Driven Data Synthesis and Policy Adaptation for Autonomous Driving","abstract":"The integration of Large Language Models (LLMs) into autonomous driving systems demonstrates strong common sense and reasoning abilities, effectively addressing the pitfalls of purely data-driven methods. Current LLM-based agents require lengthy inference times and face challenges in interacting with real-time autonomous driving environments. A key open question is whether we can effectively leverage the knowledge from LLMs to train an efficient and robust Reinforcement Learning (RL) agent. This paper introduces RAPID, a novel \\underline{\\textbf{R}}obust \\underline{\\textbf{A}}daptive \\underline{\\textbf{P}}olicy \\underline{\\textbf{I}}nfusion and \\underline{\\textbf{D}}istillation framework, which trains specialized mix-of-policy RL agents using data synthesized by an LLM-based driving agent and online adaptation. RAPID features three key designs: 1) utilization of offline data collected from an LLM agent to distil expert knowledge into RL policies for faster real-time inference; 2) introduction of robust distillation in RL to inherit both performance and robustness from LLM-based teacher; and 3) employment of a mix-of-policy approach for joint decision decoding with a policy adapter. Through fine-tuning via online environment interaction, RAPID reduces the forgetting of LLM knowledge while maintaining adaptability to different tasks. Extensive experiments demonstrate RAPID's capability to effectively integrate LLM knowledge into scaled-down RL policies in an efficient, adaptable, and robust way. Code and checkpoints will be made publicly available upon acceptance.","score":0.7761332988739014,"polarity":"negative","source":"semantic","year":2024,"authors":["Sihao Wu","Jiaxu Liu","Xiangyu Yin","Guangliang Cheng","Meng Fang","Xingyu Zhao","Xinping Yi","Xiaowei Huang"],"venue":null,"citation_count":0,"reference_count":56,"influential_citation_count":0,"corpus_id":273375416,"url":"https://www.semanticscholar.org/paper/e7d5f13c250a40ecb6d58a3119a19acd74e86933","arxiv_id":null,"contexts":null,"background":"The integration of Large Language Models (LLMs) into autonomous driving systems demonstrates strong common sense and reasoning abilities, effectively addressing the pitfalls of purely data-driven methods. Current LLM-based agents require lengthy inference times and face challenges in interacting with real-time autonomous driving environments. A key open question is whether we can effectively leverage the knowledge from LLMs to train an efficient and robust Reinforcement Learning (RL) agent.","target":"This paper introduces RAPID, a novel Robust Adaptive Policy Infusion and Distillation framework, which trains specialized mix-of-policy RL agents using data synthesized by an LLM-based driving agent and online adaptation. RAPID features three key designs: 1) utilization of offline data collected from an LLM agent to distil expert knowledge into RL policies for faster real-time inference; 2) introduction of robust distillation in RL to inherit both performance and robustness from LLM-based teacher; and 3) employment of a mix-of-policy approach for joint decision decoding with a policy adapter. Through fine-tuning via online environment interaction, RAPID reduces the forgetting of LLM knowledge while maintaining adaptability to different tasks. Extensive experiments demonstrate RAPID's capability to effectively integrate LLM knowledge into scaled-down RL policies in an efficient, adaptable, and robust way. Code and checkpoints will be made publicly available upon acceptance."},{"summary":"The Related Paper critically examines the role of Large Language Models (LLMs) in autonomous driving, contrasting the Main Paper's optimistic view. While the Main Paper posits that LLMs can enhance decision-making in complex driving scenarios, the Related Paper highlights the limitations of LLMs, questioning their ability to effectively integrate with existing modular and end-to-end approaches. It emphasizes the challenges LLMs face in handling unpredictable events and suggests that their application may not be a panacea for the inherent issues in autonomous driving technology.","paper_id":"cb3b42133b4a4d36a63106b8ea4e8f363bc51268","title":"Will Large Language Models be a Panacea to Autonomous Driving?","abstract":"Artificial intelligence (AI) plays a crucial role in autonomous driving (AD) research, propelling its development towards intelligence and efficiency. Currently, the development of AD technology follows two main technical paths: modularization and end-to-end. Modularization decompose the driving task into modules such as perception, prediction, planning, and control, and train them separately. Due to the inconsistency of training objectives between modules, the integrated effect suffers from bias. End-to-end attempts to address this issue by utilizing a single model that directly maps from sensor data to control signals. This path has limited learning capabilities in a comprehensive set of features and struggles to handle unpredictable long-tail events and complex urban traffic scenarios. In the face of challenges encountered in both paths, many researchers believe that large language models (LLMs) with powerful reasoning capabilities and extensive knowledge understanding may be the solution, expecting LLMs to provide AD systems with deeper levels of understanding and decision-making capabilities. In light of the challenges faced by both paths, many researchers believe that LLMs, with their powerful reasoning abilities and extensive knowledge, could offer a solution. To understand if LLMs could enhance AD, this paper conducts a thorough analysis of the potential applications of LLMs in AD systems, including exploring their optimization strategies in both modular and end-to-end approaches, with a particular focus on how LLMs can tackle the problems and challenges present in current solutions. Furthermore, we discuss an important question: Can LLM-based artificial general intelligence (AGI) be a key to achieve high-level AD? We further analyze the potential limitations and challenges that LLMs may encounter in promoting the development of AD technology.","score":0.7722053527832031,"polarity":"negative","source":"semantic","year":2024,"authors":["Yuxuan Zhu","Shiyi Wang","Wenqing Zhong","Nianchen Shen","Yunqi Li","Siqi Wang","Zhiheng Li","Cathy Wu","Zhengbing He","Li Li"],"venue":null,"citation_count":0,"reference_count":115,"influential_citation_count":0,"corpus_id":272827362,"url":"https://www.semanticscholar.org/paper/cb3b42133b4a4d36a63106b8ea4e8f363bc51268","arxiv_id":null,"contexts":null,"background":"Artificial intelligence (AI) plays a crucial role in autonomous driving (AD) research, propelling its development towards intelligence and efficiency. Currently, the development of AD technology follows two main technical paths: modularization and end-to-end. Modularization decompose the driving task into modules such as perception, prediction, planning, and control, and train them separately. Due to the inconsistency of training objectives between modules, the integrated effect suffers from bias. End-to-end attempts to address this issue by utilizing a single model that directly maps from sensor data to control signals. This path has limited learning capabilities in a comprehensive set of features and struggles to handle unpredictable long-tail events and complex urban traffic scenarios. In the face of challenges encountered in both paths, many researchers believe that large language models (LLMs) with powerful reasoning capabilities and extensive knowledge understanding may be the solution, expecting LLMs to provide AD systems with deeper levels of understanding and decision-making capabilities.","target":"To understand if LLMs could enhance AD, this paper conducts a thorough analysis of the potential applications of LLMs in AD systems, including exploring their optimization strategies in both modular and end-to-end approaches, with a particular focus on how LLMs can tackle the problems and challenges present in current solutions. Furthermore, we discuss an important question: Can LLM-based artificial general intelligence (AGI) be a key to achieve high-level AD? We further analyze the potential limitations and challenges that LLMs may encounter in promoting the development of AD technology."},{"summary":"The Related Paper contrasts the Main Paper by emphasizing the challenges of integrating Large Language Models (LLMs) into autonomous driving systems, such as real-time inference and safety assurance, which the Main Paper does not address in detail. While the Main Paper presents LLMs as effective decision-makers for complex driving scenarios, the Related Paper critiques this approach by highlighting the need for a more comprehensive understanding of the limitations and challenges faced in achieving human-like autonomous driving. Additionally, the Related Paper focuses on a broader survey of advancements and challenges in the field, rather than proposing a specific method as the Main Paper does.","paper_id":"8aaf6ad1a2fe2e732f23a7a9ee54d02ca7658fa3","title":"Large Language Models for Human-like Autonomous Driving: A Survey","abstract":"Large Language Models (LLMs), AI models trained on massive text corpora with remarkable language understanding and generation capabilities, are transforming the field of Autonomous Driving (AD). As AD systems evolve from rule-based and optimization-based methods to learning-based techniques like deep reinforcement learning, they are now poised to embrace a third and more advanced category: knowledge-based AD empowered by LLMs. This shift promises to bring AD closer to human-like AD. However, integrating LLMs into AD systems poses challenges in real-time inference, safety assurance, and deployment costs. This survey provides a comprehensive and critical review of recent progress in leveraging LLMs for AD, focusing on their applications in modular AD pipelines and end-to-end AD systems. We highlight key advancements, identify pressing challenges, and propose promising research directions to bridge the gap between LLMs and AD, thereby facilitating the development of more human-like AD systems. The survey first introduces LLMs' key features and common training schemes, then delves into their applications in modular AD pipelines and end-to-end AD, respectively, followed by discussions on open challenges and future directions. Through this in-depth analysis, we aim to provide insights and inspiration for researchers and practitioners working at the intersection of AI and autonomous vehicles, ultimately contributing to safer, smarter, and more human-centric AD technologies.","score":0.769720733165741,"polarity":"negative","source":"semantic","year":2024,"authors":["Yun Li","Kai Katsumata","Ehsan Javanmardi","Manabu Tsukada"],"venue":null,"citation_count":1,"reference_count":56,"influential_citation_count":0,"corpus_id":271533538,"url":"https://www.semanticscholar.org/paper/8aaf6ad1a2fe2e732f23a7a9ee54d02ca7658fa3","arxiv_id":null,"contexts":null,"background":"Large Language Models (LLMs), AI models trained on massive text corpora with remarkable language understanding and generation capabilities, are transforming the field of Autonomous Driving (AD). As AD systems evolve from rule-based and optimization-based methods to learning-based techniques like deep reinforcement learning, they are now poised to embrace a third and more advanced category: knowledge-based AD empowered by LLMs. This shift promises to bring AD closer to human-like AD. However, integrating LLMs into AD systems poses challenges in real-time inference, safety assurance, and deployment costs.","target":"This survey provides a comprehensive and critical review of recent progress in leveraging LLMs for AD, focusing on their applications in modular AD pipelines and end-to-end AD systems. We highlight key advancements, identify pressing challenges, and propose promising research directions to bridge the gap between LLMs and AD, thereby facilitating the development of more human-like AD systems. The survey first introduces LLMs' key features and common training schemes, then delves into their applications in modular AD pipelines and end-to-end AD, respectively, followed by discussions on open challenges and future directions. Through this in-depth analysis, we aim to provide insights and inspiration for researchers and practitioners working at the intersection of AI and autonomous vehicles, ultimately contributing to safer, smarter, and more human-centric AD technologies."},{"summary":"The Related Paper supports the Main Paper by emphasizing the advantages of using Large Language Models (LLMs) in autonomous driving, particularly in reasoning and interpreting complex scenarios. Both papers highlight the limitations of traditional systems in handling rare events and propose LLMs as a solution, with the Related Paper providing additional insights into the human-like capabilities of LLMs, such as memorization and continuous learning. This alignment reinforces the Main Paper's claims about the effectiveness of LLMs in enhancing decision-making for autonomous driving.","paper_id":"11bca2cafe89e14dc733504f97e2489de697ceab","title":"Drive Like a Human: Rethinking Autonomous Driving with Large Language Models","abstract":"In this paper, we explore the potential of using a large language model (LLM) to understand the driving environment in a human-like manner and analyze its ability to reason, interpret, and memorize when facing complex scenarios. We argue that traditional optimization-based and modular autonomous driving (AD) systems face inherent performance limitations when dealing with long-tail corner cases. To address this problem, we propose that an ideal AD system should drive like a human, accumulating experience through continuous driving and using common sense to solve problems. To achieve this goal, we identify three key abilities necessary for an AD system: reasoning, interpretation, and memorization. We demonstrate the feasibility of employing an LLM in driving scenarios by building a closed-loop system to showcase its comprehension and environment-interaction abilities. Our extensive experiments show that the LLM exhibits the impressive ability to reason and solve long-tailed cases, providing valuable insights for the development of human-like autonomous driving. The related code are available at https:/ithub.om/PJLab-ADG/DriveLikeAHuman.","score":0.7331142425537109,"polarity":"positive","source":"citations","year":2023,"authors":["Daocheng Fu","Xin Li","Licheng Wen","Min Dou","Pinlong Cai","Botian Shi","Y. Qiao"],"venue":null,"citation_count":128,"reference_count":50,"influential_citation_count":8,"corpus_id":259924488,"url":"https://www.semanticscholar.org/paper/11bca2cafe89e14dc733504f97e2489de697ceab","arxiv_id":null,"contexts":[{"sentence":"LLMs can think like humans~\\citep{fu2023drive}, and reason about new scenarios by combining common sense, and the visible thinking process makes them strongly interpretable.","polarity":"positive"},{"sentence":"Recent research has integrated LLMs and their strong reasoning capabilities into AD systems~\\citep{fu2023drive,chen2023driving} to solve the interpretability problem and complex interaction scenarios.","polarity":"positive"}],"background":null,"target":null},{"summary":"The Related Paper supports the Main Paper by demonstrating the feasibility of translating structured natural language into actionable commands for robotic systems, which aligns with the Main Paper's approach of using Large Language Models (LLMs) for decision-making in autonomous driving. Both papers emphasize the importance of bridging high-level human language understanding with low-level control mechanisms, showcasing how natural language can enhance the interpretability and effectiveness of decision-making processes in complex scenarios.","paper_id":"be36a57c1873f5ff8ec7cf0b5da6a018f2550e0b","title":"Translating Structured English to Robot Controllers","abstract":"Recently, Linear Temporal Logic (LTL) has been successfully applied to high-level task and motion planning problems for mobile robots. One of the main attributes of LTL is its close relationship with fragments of natural language. In this paper, we take the first steps toward building a natural language interface for LTL planning methods with mobile robots as the application domain. For this purpose, we built a structured English language which maps directly to a fragment of LTL.","score":0.5248690843582153,"polarity":"positive","source":"citations","year":2008,"authors":["H. Kress-Gazit","Georgios Fainekos","George Pappas"],"venue":null,"citation_count":167,"reference_count":25,"influential_citation_count":8,"corpus_id":7457220,"url":"https://www.semanticscholar.org/paper/be36a57c1873f5ff8ec7cf0b5da6a018f2550e0b","arxiv_id":null,"contexts":[{"sentence":"One prominent task is language-to-actions mapping, which has seen early approaches leveraging frameworks like temporal logic~\\citep{kress2008translating} and motion primitive learning~\\citep{matuszek2013learning}, evolving towards more recent end-to-end mo","polarity":"positive"}],"background":null,"target":null},{"summary":"The Related Paper, 'ProgPrompt', supports the Main Paper by demonstrating the effective use of Large Language Models (LLMs) in generating actionable plans for robots in complex environments, similar to how the Main Paper employs LLMs for decision-making in autonomous driving. Both papers highlight the ability of LLMs to enhance task execution through commonsense reasoning and structured prompts, thereby addressing challenges in generalization and interpretability. The insights from 'ProgPrompt' regarding programmatic prompt structures can further inform the integration of LLMs in the Main Paper's approach, reinforcing the potential of LLMs in diverse autonomous applications.","paper_id":"c03fa01fbb9c77fe3d10609ba5f1dee33a723867","title":"ProgPrompt: Generating Situated Robot Task Plans using Large Language Models","abstract":"Task planning can require defining myriad domain knowledge about the world in which a robot needs to act. To ameliorate that effort, large language models (LLMs) can be used to score potential next actions during task planning, and even generate action sequences directly, given an instruction in natural language with no additional domain information. However, such methods either require enumerating all possible next steps for scoring, or generate free-form text that may contain actions not possible on a given robot in its current context. We present a programmatic LLM prompt structure that enables plan generation functional across situated environments, robot capabilities, and tasks. Our key insight is to prompt the LLM with program-like specifications of the available actions and objects in an environment, as well as with example programs that can be executed. We make concrete recommendations about prompt structure and generation constraints through ablation experiments, demonstrate state of the art success rates in VirtualHome household tasks, and deploy our method on a physical robot arm for tabletop tasks. Website at progprompt.github.io","score":0.5156584978103638,"polarity":"positive","source":"citations","year":2022,"authors":["Ishika Singh","Valts Blukis","A. Mousavian","Ankit Goyal","Danfei Xu","Jonathan Tremblay","D. Fox","Jesse Thomason","Animesh Garg"],"venue":null,"citation_count":551,"reference_count":40,"influential_citation_count":40,"corpus_id":252519594,"url":"https://www.semanticscholar.org/paper/c03fa01fbb9c77fe3d10609ba5f1dee33a723867","arxiv_id":null,"contexts":[{"sentence":"Moreover, exiting works~\\cite{ahn2022saycan,huang2022inner,liang2022code,singh2022progprompt,brohan2023rt1,vemprala2023chatgpt,bucker2022latte} connect LLMs to robot commands and translate natural language instructions into domain-specific reward models~\\c","polarity":"positive"}],"background":null,"target":null},{"summary":"The Related Paper discusses the emergent abilities of large language models (LLMs), highlighting how scaling these models can lead to unpredictable improvements in performance and capabilities. This supports the Main Paper's claim that LLMs can enhance decision-making in autonomous driving by leveraging their commonsense reasoning and adaptability to complex scenarios. The findings in the Related Paper reinforce the notion that larger LLMs can provide significant advantages in understanding and generalizing high-level information, which is crucial for the challenges addressed in the Main Paper.","paper_id":"dac3a172b504f4e33c029655e9befb3386e5f63a","title":"Emergent Abilities of Large Language Models","abstract":"Scaling up language models has been shown to predictably improve performance and sample efficiency on a wide range of downstream tasks. This paper instead discusses an unpredictable phenomenon that we refer to as emergent abilities of large language models. We consider an ability to be emergent if it is not present in smaller models but is present in larger models. Thus, emergent abilities cannot be predicted simply by extrapolating the performance of smaller models. The existence of such emergence implies that additional scaling could further expand the range of capabilities of language models.","score":0.49614500999450684,"polarity":"positive","source":"citations","year":2022,"authors":["Jason Wei","Yi Tay","Rishi Bommasani","Colin Raffel","Barret Zoph","Sebastian Borgeaud","Dani Yogatama","Maarten Bosma","Denny Zhou","Donald Metzler","Ed H. Chi","Tatsunori Hashimoto","O. Vinyals","P. Liang","J. Dean","W. Fedus"],"venue":null,"citation_count":2141,"reference_count":107,"influential_citation_count":123,"corpus_id":249674500,"url":"https://www.semanticscholar.org/paper/dac3a172b504f4e33c029655e9befb3386e5f63a","arxiv_id":null,"contexts":[{"sentence":"Recent introductions of models like ChatGPT~\\citep{chatgpt}, have positioned LLMs as early versions of Artificial General Intelligence (AGI)~\\citep{bubeck2023sparks}, owing to their remarkable emergent abilities~\\citep{wei2022emergent} and innovative techn","polarity":"positive"}],"background":null,"target":null},{"summary":"The Related Paper supports the Main Paper by demonstrating the effectiveness of natural language as a tool for human-in-the-loop corrections in robotic planning, which parallels the Main Paper's use of Large Language Models (LLMs) for decision-making in autonomous driving. Both papers emphasize the importance of human commonsense understanding and the ability to generalize to complex scenarios, showcasing how LLMs and natural language can enhance system performance and adaptability. The success rates achieved in the Related Paper further validate the potential of integrating language-based approaches in autonomous systems, reinforcing the Main Paper's claims about the advantages of LLMs in handling intricate driving behaviors.","paper_id":"a58b3f2ab75fdbda082e684d027ab4f552b0b5d3","title":"Correcting Robot Plans with Natural Language Feedback","abstract":"When humans design cost or goal specifications for robots, they often produce specifications that are ambiguous, underspecified, or beyond planners' ability to solve. In these cases, corrections provide a valuable tool for human-in-the-loop robot control. Corrections might take the form of new goal specifications, new constraints (e.g. to avoid specific objects), or hints for planning algorithms (e.g. to visit specific waypoints). Existing correction methods (e.g. using a joystick or direct manipulation of an end effector) require full teleoperation or real-time interaction. In this paper, we explore natural language as an expressive and flexible tool for robot correction. We describe how to map from natural language sentences to transformations of cost functions. We show that these transformations enable users to correct goals, update robot motions to accommodate additional user preferences, and recover from planning errors. These corrections can be leveraged to get 81% and 93% success rates on tasks where the original planner failed, with either one or two language corrections. Our method makes it possible to compose multiple constraints and generalizes to unseen scenes, objects, and sentences in simulated environments and real-world environments.","score":0.47119301557540894,"polarity":"positive","source":"citations","year":2022,"authors":["Pratyusha Sharma","Balakumar Sundaralingam","Valts Blukis","Chris Paxton","Tucker Hermans","A. Torralba","Jacob Andreas","D. Fox"],"venue":null,"citation_count":87,"reference_count":44,"influential_citation_count":7,"corpus_id":248085271,"url":"https://www.semanticscholar.org/paper/a58b3f2ab75fdbda082e684d027ab4f552b0b5d3","arxiv_id":null,"contexts":[{"sentence":"Additionally, incorporating iterative human feedback has been explored in correcting plans, with approaches employing semantic parsers~\\citep{broad2017realtime} or trajectory optimization methods~\\citep{sharma2022correcting}.","polarity":"positive"}],"background":null,"target":null},{"summary":"The Related Paper contrasts the Main Paper by focusing on deep reinforcement learning (DRL) as a primary framework for autonomous driving, emphasizing its computational challenges and the need for robust validation methods. While the Main Paper advocates for the use of large language models (LLMs) to enhance decision-making through commonsense reasoning, the Related Paper highlights the limitations of RL approaches and the necessity of exploring alternative learning paradigms, suggesting that LLMs may not address the foundational issues faced by DRL in real-world applications.","paper_id":"129983331ca874142a3e8eb2d93d820bdf1f9aca","title":"Deep Reinforcement Learning for Autonomous Driving: A Survey","abstract":"With the development of deep representation learning, the domain of reinforcement learning (RL) has become a powerful learning framework now capable of learning complex policies in high dimensional environments. This review summarises deep reinforcement learning (DRL) algorithms and provides a taxonomy of automated driving tasks where (D)RL methods have been employed, while addressing key computational challenges in real world deployment of autonomous driving agents. It also delineates adjacent domains such as behavior cloning, imitation learning, inverse reinforcement learning that are related but are not classical RL algorithms. The role of simulators in training agents, methods to validate, test and robustify existing solutions in RL are discussed.","score":0.43713822960853577,"polarity":"negative","source":"citations","year":2020,"authors":["B. R. Kiran","Ibrahim Sobh","V. Talpaert","P. Mannion","A. A. Sallab","S. Yogamani","P. P'erez"],"venue":null,"citation_count":1495,"reference_count":157,"influential_citation_count":36,"corpus_id":211011033,"url":"https://www.semanticscholar.org/paper/129983331ca874142a3e8eb2d93d820bdf1f9aca","arxiv_id":null,"contexts":[{"sentence":"However, existing advanced learning-based Autonomous Driving (AD) systems typically require complex rules or reward function designs to handle such scenarios effectively~\\citep{10138317,9351818}.","polarity":"negative"}],"background":null,"target":null},{"summary":"The Related Paper contrasts with the Main Paper by emphasizing the necessity of explainability in autonomous driving systems, arguing that current AI decision-making lacks transparency and hinders societal acceptance. While the Main Paper focuses on leveraging Large Language Models for decision-making in complex driving scenarios, the Related Paper highlights the importance of making these decisions understandable to humans to ensure regulatory compliance and public trust. This divergence underscores a critical gap in the Main Paper's approach, as it prioritizes performance over the explainability of AI-driven decisions.","paper_id":"d513cf54ee71edfff356a1545d4a0ed31d11652e","title":"Explainable Artificial Intelligence for Autonomous Driving: A Comprehensive Overview and Field Guide for Future Research Directions","abstract":"Autonomous driving has achieved significant milestones in research and development over the last two decades. There is increasing interest in the field as the deployment of autonomous vehicles (AVs) promises safer and more ecologically friendly transportation systems. With the rapid progress in computationally powerful artificial intelligence (AI) techniques, AVs can sense their environment with high precision, make safe real-time decisions, and operate reliably without human intervention. However, intelligent decision-making in such vehicles is not generally understandable by humans in the current state of the art, and such deficiency hinders this technology from being socially acceptable. Hence, aside from making safe real-time decisions, AVs must also explain their AI-guided decision-making process in order to be regulatory-compliant across many jurisdictions. Our study sheds comprehensive light on the development of explainable artificial intelligence (XAI) approaches for AVs. In particular, we make the following contributions. First, we provide a thorough overview of the state-of-the-art and emerging approaches for XAI-based autonomous driving. We then propose a conceptual framework considering the essential elements for explainable end-to-end autonomous driving. Finally, we present XAI-based prospective directions and emerging paradigms for future directions that hold promise for enhancing transparency, trustworthiness, and societal acceptance of AVs.","score":0.42810261249542236,"polarity":"negative","source":"citations","year":2021,"authors":["Shahin Atakishiyev","Mohammad Salameh","Hengshuai Yao","R. Goebel"],"venue":null,"citation_count":109,"reference_count":359,"influential_citation_count":5,"corpus_id":245385604,"url":"https://www.semanticscholar.org/paper/d513cf54ee71edfff356a1545d4a0ed31d11652e","arxiv_id":null,"contexts":[{"sentence":"Both limited datasets and sampling efficiency~\\citep{atakishiyev2023explainable} can present challenges for existing learning-based AD systems when making decisions in rare real-world driving scenarios.","polarity":"negative"},{"sentence":"Nevertheless, existing learning-based AD systems inherently resemble black boxes, making it challenging to discern their decision-making processes or understand the rationale behind their actions~\\citep{atakishiyev2023explainable}.","polarity":"negative"},{"sentence":"At the same time, limitations in data and sampling efficiency~\\citep{atakishiyev2023explainable} make it vulnerable to dealing with long-tail situations, especially interaction scenarios, in real-world environments~\\citep{kong2023robo3d}.","polarity":"negative"}],"background":null,"target":null},{"summary":"The Related Paper contrasts the Main Paper by focusing on a sampling-based nonlinear model predictive control (NMPC) approach for autonomous vehicle motion planning, emphasizing optimization and Bayesian state estimation rather than the commonsense reasoning capabilities of large language models (LLMs) proposed in the Main Paper. While the Main Paper highlights the integration of LLMs for decision-making in complex driving scenarios, the Related Paper addresses the challenges of controlling neural network dynamics without relying on high-level reasoning, suggesting a more traditional control methodology that may not leverage the interpretability and generalization claims made by the Main Paper.","paper_id":"82542d436cbef448adc4e0f116355060809d4374","title":"Sampling-Based Nonlinear MPC of Neural Network Dynamics with Application to Autonomous Vehicle Motion Planning","abstract":"Control of machine learning models has emerged as an important paradigm for a broad range of robotics applications. In this paper, we present a sampling-based nonlinear model predictive control (NMPC) approach for control of neural network dynamics. We show its design in two parts: 1) formulating conventional optimization-based NMPC as a Bayesian state estimation problem, and 2) using particle filtering/smoothing to achieve the estimation. Through a principled sampling-based implementation, this approach can potentially make effective searches in the control action space for optimal control and also facilitate computation toward overcoming the challenges caused by neural network dynamics. We apply the proposed NMPC approach to motion planning for autonomous vehicles. The specific problem considers nonlinear unknown vehicle dynamics modeled as neural networks as well as dynamic on-road driving scenarios. The approach shows significant effectiveness in successful motion planning in case studies.","score":0.39376211166381836,"polarity":"negative","source":"citations","year":2022,"authors":["Iman Askari","Babak Badnava","Thomas Woodruff","Shen Zeng","H. Fang"],"venue":null,"citation_count":16,"reference_count":29,"influential_citation_count":1,"corpus_id":248665644,"url":"https://www.semanticscholar.org/paper/82542d436cbef448adc4e0f116355060809d4374","arxiv_id":null,"contexts":[{"sentence":"Due to the complexity of driving scenarios, designing a set of weights and residual terms that are applicable to all driving scenarios is almost impossible~\\citep{unknown}.","polarity":"negative"}],"background":null,"target":null},{"summary":"The Related Paper, DeeP-LCC, contrasts with the Main Paper by emphasizing a data-driven, nonparametric approach to control connected and autonomous vehicles (CAVs) in mixed traffic, rather than relying on large language models (LLMs) for decision-making. While the Main Paper focuses on leveraging LLMs for high-level reasoning and commonsense understanding in autonomous driving, the Related Paper critiques the need for explicit modeling of human-driven vehicle dynamics, proposing a method that prioritizes data-centric strategies for safety and efficiency. This highlights a fundamental difference in methodology and underlying assumptions about vehicle control in complex environments.","paper_id":"8eeb15f9e959e5656530180c00e54ef7d88eeea4","title":"DeeP-LCC: Data-EnablEd Predictive Leading Cruise Control in Mixed Traffic Flow","abstract":"For the control of connected and autonomous vehicles (CAVs), most existing methods focus on model-based strategies. They require explicit knowledge of car-following dynamics of human-driven vehicles that are nontrivial to identify accurately. In this article, instead of relying on a parametric car-following model, we introduce a data-driven nonparametric strategy, called Data-EnablEd Predictive Leading Cruise Control (DeeP-LCC), to achieve safe and optimal control of CAVs in mixed traffic. We first utilize Willems’ fundamental lemma to obtain a data-centric representation of mixed traffic behavior. This is justified by rigorous analysis on controllability and observability properties of mixed traffic. We then employ a receding horizon strategy to solve a finite-horizon optimal control problem at each time step, in which input–output constraints are incorporated for collision-free guarantees. Numerical experiments validate the performance of DeeP-LCC compared to a standard predictive controller that requires an accurate model. Multiple nonlinear traffic simulations further confirm its great potential on improving traffic efficiency, driving safety, and fuel economy.","score":0.3700202405452728,"polarity":"negative","source":"citations","year":2022,"authors":["Jiawei Wang","Yang Zheng","Keqiang Li","Qing Xu"],"venue":null,"citation_count":50,"reference_count":54,"influential_citation_count":5,"corpus_id":247594633,"url":"https://www.semanticscholar.org/paper/8eeb15f9e959e5656530180c00e54ef7d88eeea4","arxiv_id":null,"contexts":[{"sentence":"However, both centralized and distributed approaches are often overly reliant on environmental prior and fail to exhibit good performance when the traffic model is unknown~\\citep{10172912}.","polarity":"negative"}],"background":null,"target":null},{"summary":"The Related Paper, 'ChauffeurNet', contrasts with the Main Paper by emphasizing the limitations of imitation learning in autonomous driving, arguing that merely imitating expert behavior is insufficient for handling complex scenarios. While the Main Paper advocates for using Large Language Models (LLMs) to enhance decision-making through commonsense reasoning, the Related Paper focuses on augmenting imitation learning with synthesized data to improve robustness against undesirable events. This highlights a fundamental difference in approach: the Main Paper seeks to leverage LLMs for high-level decision-making, whereas the Related Paper aims to refine traditional learning methods to better cope with real-world driving challenges.","paper_id":"8e8bc8a9ac9df65c6c2a45efc639baf1ba431fbc","title":"ChauffeurNet: Learning to Drive by Imitating the Best and Synthesizing the Worst","abstract":"Our goal is to train a policy for autonomous driving via imitation learning that is robust enough to drive a real vehicle. We find that standard behavior cloning is insufficient for handling complex driving scenarios, even when we leverage a perception system for preprocessing the input and a controller for executing the output on the car: 30 million examples are still not enough. We propose exposing the learner to synthesized data in the form of perturbations to the expert's driving, which creates interesting situations such as collisions and/or going off the road. Rather than purely imitating all data, we augment the imitation loss with additional losses that penalize undesirable events and encourage progress -- the perturbations then provide an important signal for these losses and lead to robustness of the learned model. We show that the ChauffeurNet model can handle complex situations in simulation, and present ablation experiments that emphasize the importance of each of our proposed changes and show that the model is responding to the appropriate causal factors. Finally, we demonstrate the model driving a car in the real world.","score":0.3318139612674713,"polarity":"negative","source":"citations","year":2018,"authors":["Mayank Bansal","A. Krizhevsky","A. Ogale"],"venue":null,"citation_count":716,"reference_count":26,"influential_citation_count":68,"corpus_id":54457648,"url":"https://www.semanticscholar.org/paper/8e8bc8a9ac9df65c6c2a45efc639baf1ba431fbc","arxiv_id":null,"contexts":[{"sentence":"Chauffeurnet~\\citep{bansal2018chauffeurnet} demonstrated such limits where even 30 million state-action samples were insufficient to learn an optimal policy that mapped bird's-eye view images (states) to control (action).","polarity":"negative"}],"background":null,"target":null}],"paper":{"title":"LanguageMPC: Large Language Models as Decision Makers for Autonomous Driving","abstract":"Existing learning-based autonomous driving (AD) systems face challenges in comprehending high-level information, generalizing to rare events, and providing interpretability. To address these problems, this work employs Large Language Models (LLMs) as a decision-making component for complex AD scenarios that require human commonsense understanding. We devise cognitive pathways to enable comprehensive reasoning with LLMs, and develop algorithms for translating LLM decisions into actionable driving commands. Through this approach, LLM decisions are seamlessly integrated with low-level controllers by guided parameter matrix adaptation. Extensive experiments demonstrate that our proposed method not only consistently surpasses baseline approaches in single-vehicle tasks, but also helps handle complex driving behaviors even multi-vehicle coordination, thanks to the commonsense reasoning capabilities of LLMs. This paper presents an initial step toward leveraging LLMs as effective decision-makers for intricate AD scenarios in terms of safety, efficiency, generalizability, and interoperability. We aspire for it to serve as inspiration for future research in this field.","authors":["Hao Sha","Yao Mu","Yuxuan Jiang","Guojian Zhan","Li Chen","Chenfeng Xu","Ping Luo","Shengbo Eben Li","Masayoshi Tomizuka","Wei Zhan","Mingyu Ding"],"sections":[{"heading":"Introduction","text":"# Introduction\n\nImagine you are behind the wheel, approaching an unsignalized intersection and planning to turn left, with an oncoming vehicle straight ahead. Human drivers intuitively know that according to traffic rules, they should slow down and yield, even if it is technically possible to speed through. However, existing advanced learning-based Autonomous Driving (AD) systems typically require complex rules or reward function designs to handle such scenarios effectively [@10138317; @9351818]. This reliance on predefined rule bases often limits their ability to generalize to various situations.\n\nAnother challenge facing existing learning-based AD systems is the long-tail problem [@9022290]. Both limited datasets and sampling efficiency [@atakishiyev2023explainable] can present challenges for existing learning-based AD systems when making decisions in rare real-world driving scenarios. Chauffeurnet [@bansal2018chauffeurnet] demonstrated such limits where even 30 million state-action samples were insufficient to learn an optimal policy that mapped bird's-eye view images (states) to control (action).\n\nFurthermore, the lack of interpretability [@gohel2021explainable] is a pressing issue for existing learning-based AD systems. A mature AD system must possess interpretability to gain recognition within society and regulatory entities, allowing it to be subject to targeted optimization and iterative improvements. Nevertheless, existing learning-based AD systems inherently resemble black boxes, making it challenging to discern their decision-making processes or understand the rationale behind their actions [@atakishiyev2023explainable]. This lack of transparency can pose obstacles to the practical implementation of AD systems.\n\nConsidering the aforementioned challenges, a fundamental question arises: *Can we equip AD systems with the capability to think and drive like humans?* Our proposed solution involves employing a Large Language Model (LLM) to serve as the \\\"brain\\\" of the AD system. Recent introductions of models like ChatGPT [@chatgpt], have positioned LLMs as early versions of Artificial General Intelligence (AGI) [@bubeck2023sparks], owing to their remarkable emergent abilities [@wei2022emergent] and innovative techniques such as Instruct Following and In-Context Learning (ICL) [@dong2023survey]. LLMs can think like humans [@fu2023drive], and reason about new scenarios by combining common sense, and the visible thinking process makes them strongly interpretable. These features make LLMs a powerful solution to the problems faced by AD systems described above.\n\n![Faced with the rare situation where a breakdown vehicle is stopped in the middle of the road in a roundabout, LLM makes a decision that complies with traffic rules through common sense reasoning and understanding of high-level information.](figure/fig1.png){#fig:fig1}\n\nIn this paper, we leverage LLMs to analyze and reason about various scenarios, enabling it to provide high-level decisions, and by tuning parameter matrix, we convert high-level decisions into mathematical representations to guide the bottom-level controller, Model Predictive Control (MPC). Fig. [1](#fig:fig1){reference-type=\"ref\" reference=\"fig:fig1\"} illustrates the powerful reasoning capabilities of our system for rare and complex scenarios, demonstrating its superiority in understanding high-level information, commonsense reasoning, and interpretability. Through quantitative experiments, we showcase that our system significantly surpasses existing learning-based and optimization-based methods for single-vehicle decision-making tasks, with Overall Cost decreasing by **18.1%** and **16.4%**. Additionally, through qualitative experiments, we demonstrate the impressive capabilities of our system by effectively addressing intricate tasks, such as multi-vehicle joint control and driving behavior modulation guided by textual input.\n\nThe main contributions of this paper are as follows:\n\n(1) We have devised a dedicated chain-of-thought framework for LLMs for driving scenarios, which divides the analysis and decision-making process into numerous sub-problems, enabling LLMs to comprehensively engage in logical reasoning and arrive at informed decisions.\n\n(2) We have developed techniques for directing the bottom-level controller using high-level textual decisions provided by LLMs. This has enabled us to construct a comprehensive AD system that gives precise driving actions directly based on observational data.\n\n(3) In a groundbreaking achievement, we have conducted quantitative experiments that conclusively showcase the substantial performance superiority of the AD system enhanced by LLMs over existing methods. Additionally, we showcase our system's success in complex tasks, including coordinating multiple vehicles and regulating driving behavior with text-based input."},{"heading":"Related Work","text":"# Related Work\n\n**Large Language Models for Planning and Decision-Making.** The remarkable achievements of LLMs are undeniably captivating, demonstrating LLM's human-like reasoning skills and generalization of human commonsense [@bian2023chatgpt; @nay2022law; @chowdhery2022palm; @ouyang2022training; @chung2022scaling]. In advanced tasks with LLMs, the translation of natural language input into actionable results is crucial. One prominent task is language-to-actions mapping, which has seen early approaches leveraging frameworks like temporal logic [@kress2008translating] and motion primitive learning [@matuszek2013learning], evolving towards more recent end-to-end models for instruction-following in navigation [@ku2020room; @kamath2023new] and manipulation tasks, employing latent embeddings of language commands [@jang2021bc; @mees23hulc2; @lynch2022interactive]. Another critical dimension is language-to-code generation, extensively explored in contexts ranging from coding competitions [@li2022competition] to instruction-following tasks [@liang2022code]. Moreover, exiting works [@ahn2022saycan; @huang2022inner; @liang2022code; @singh2022progprompt; @brohan2023rt1; @vemprala2023chatgpt; @bucker2022latte] connect LLMs to robot commands and translate natural language instructions into domain-specific reward models [@lin2022inferring; @goyal2019using; @pmlr-v164-nair22a] for robotics. [@kwon2023reward; @hu2023language] propose the use of LLMs for assigning reward values during Reinforcement Learning (RL) training. Additionally, incorporating iterative human feedback has been explored in correcting plans, with approaches employing semantic parsers [@broad2017realtime] or trajectory optimization methods [@sharma2022correcting]. These various dimensions underscore the versatility and growing importance of LLMs in bridging the gap between natural language understanding and actionable outcomes in a wide range of applications.\n\n**Autonomous Driving Autonomy.** Though autonomous driving systems have achieved remarkable successes in planning and decision-making [@kelly2003reactive; @zhang2022rethinking], there are still problems in terms of interpretability [@gohel2021explainable; @arrieta2019explainable; @article; @chib2023recent]. At the same time, limitations in data and sampling efficiency [@atakishiyev2023explainable] make it vulnerable to dealing with long-tail situations, especially interaction scenarios, in real-world environments [@kong2023robo3d]. Recent research has integrated LLMs and their strong reasoning capabilities into AD systems [@fu2023drive; @chen2023driving] to solve the interpretability problem and complex interaction scenarios. However, [@chen2023driving] lacks the capability to translate reasoning into actionable driving maneuvers. [@fu2023drive] solves this issue, but the high-level safety judgments and decision-making processes rely on fixed rules rather than harnessing the full potential of LLMs' advanced reasoning capabilities. In this work, we aim to develop an AD system where LLMs play a central role in high-level decision-making. We extend their application to more intricate scenarios, such as navigating intersections and roundabouts, providing an initial step towards unlocking LLMs' effectiveness as decision-makers for complex AD scenarios."},{"heading":"Method","text":"# Method\n\nWe develop an AD system with LLM as the core of high-level decision-making, as shown in Fig. [\\[fig:chain\\]](#fig:chain){reference-type=\"ref\" reference=\"fig:chain\"}. The LLM initiates a dialogue based on the provided prompt, continuously gathering information from the environment, engaging in reasoning, and rendering judgments. As illustrated in the center of Fig. [\\[fig:chain\\]](#fig:chain){reference-type=\"ref\" reference=\"fig:chain\"}, from left to right, the LLM proceeds sequentially: 1) identifies the vehicles requiring attention, 2)evaluates the situation, and 3) offers action guidance. Then the system transforms these three high-level textual decisions into mathematical representations, namely the observation matrix, weight matrix, and action bias. These elements serve as directives for the bottom-level controller, the MPC, instructing it on specific driving actions to be taken.\n\nTaking the case of a left turn at an intersection as an example, in Fig. [\\[fig:matrix\\]](#fig:matrix){reference-type=\"ref\" reference=\"fig:matrix\"} we show how the three high-level textual decisions described above can be converted into the mathematical representations needed for MPC. The LLM selects 'vehicle_26', and we create a corresponding vector using the MPC's observation operator and zero out other elements in the observation matrix to focus solely on 'vehicle_26'. According to the waiting situation at the intersection that the LLM signals, we adjust the weight matrix to prioritize deceleration instructions over trajectory following, which prompts the MPC to decelerate promptly in alignment with the LLM's directive. We directly convert LLM's action guidance into action bias through predefined rules. Guided by the mathematical form of the above three aspects, the MPC completes the driving action of stopping and yielding.\n\n## Background\n\nThe MPC solves a finite-time open-loop optimization problem online at each moment, based on the current measurement information obtained, and applies the first element of the resulting control sequence with the lowest cost to the controlled vehicle.\n\nIn this work, we define the cost function of MPC in the context of Markov Decision Process (MDP), which is commonly used to formulate vehicle control problems: $(S, A, C, P, p_0)$, where $S$ is the state space, $A$ is the action space, $C: S \\times A \\mapsto \\mathbb{R}$ is the cost function, $P: S \\times A \\mapsto S$ is the dynamics equation, and $p_0$ is the initial state distribution. Given a cost function $C$, the MPC finds a sequence of actions $\\mathbf{a}_{1:H} ={ \\mathbf{a}_1, \\dots, \\mathbf{a}_H}$ that minimizes the expected accumulated cost $\\small J(\\mathbf{a}_{1:H}) = \\sum_{t=1}^{H} C(\\mathbf{s}_t, \\mathbf{a}_t)$. The cost function takes the following form: $$\\begin{gathered}\n\\small\nC(\\mathbf{s}, \\mathbf{a}) = \\sum_{i = 0}^M w_i \\cdot \\textrm{n}_i\\big(r_i(\\mathbf{s}, \\mathbf{a}, \\psi_i)\\big),\n\\end{gathered}$$ where $w \\in \\mathbb{R}{+}$ is a non-negative weight, $\\textrm{n}(\\cdot) : \\mathbb{R} \\rightarrow \\mathbb{R}+$ is a twice-differentiable norm that takes its minimum at 0, $r \\in \\mathbb{R}$ is a residual term that achieves optimality when $r=0$, and $\\psi_i$ is the parameters of the $i^\\text{th}$ residual term. For example, if we want the vehicle to adopt the desired acceleration, we may design a residual term $r_{acc}(acc, \\psi) = acc-\\psi$, where the cost parameter $\\psi$ denotes the desired acceleration, and use the $\\ell_2$ norm to construct the final reward function: $C_{acc} = w |r_{acc}|_2$. Due to the complexity of driving scenarios, designing a set of weights and residual terms that are applicable to all driving scenarios is almost impossible [@unknown]. In this work, we use a generic and simple set of residual terms that include action biases to tune the control behavior, and design multiple sets of weight matrixes based on the certainty that the MPC should perform the action biases. We use the power of LLM to give action bias and select weight matrices for driving in complex scenarios.\n\n<figure id=\"fig:pipeline\">\n\n<figcaption>(a) Pipeline of our system with LLM as the core of high-level decision-making. (b) The LLM textual high-level decisions are converted into mathematical representations that guide the MPC to give specific driving actions. Take the case of a left turn at an intersection as an example.</figcaption>\n</figure>\n\n## Chain-of-thought\n\n![Prompts for the three core tools we have defined. Each tool's prompt contains both information and guidelines to help LLM complete reasoning and judgment and begin the next step.](figure/prompt.png){#fig:prompt}\n\nWe employ LangChain [@langchain] as a framework to manage the LLM and establish a structured thought process for the LLM. This is achieved by defining a set of tools and specifying the sequence in which they should be utilized. To begin, we introduce these designated tools in a prompt at the outset of the conversation. Subsequently, during the course of the dialogue, the LLM actively invokes these tools to acquire pertinent information and guidance for its ongoing decision-making process. The LLM follows these guidelines to determine its next course of action until it successfully addresses the entire problem.\n\nAs an illustrative example, let's consider the three core tools depicted in Fig. [3](#fig:prompt){reference-type=\"ref\" reference=\"fig:prompt\"}. Each of these tools serves the dual purpose of providing the LLM with the relevant information and reasoning guidelines necessary to complete a specific reasoning step, while also directing the LLM on what actions it should take next.\n\nMoreover, these tools enable us to revamp the way we deliver scenario information. Instead of overwhelming the LLM with all the scenario details at once, we provide only the relevant information needed for each decision step in the LLM's thought process, as depicted in Fig. [3](#fig:prompt){reference-type=\"ref\" reference=\"fig:prompt\"}. This approach represents a strategic shift that tackles a significant challenge faced by the LLM when dealing with complex and extensive data. It ensures that information is organized for simplicity and necessity, leading to a substantial improvement in the LLM's ability to reason and exercise judgment.\n\n## Attention Allocation {#sec:attention}\n\nThe ability to effectively distribute attention while driving reflects a human-like thought process. In this context, we task the LLM with systematically assessing information pertaining to surrounding vehicles, one at a time. Its objective is to discern the intentions of these vehicles and, ultimately, determine if they pose any conflicts with the movements of the ego vehicle. Specifically, At time $t$, for each element of surrounding vehicles $\\mathcal{V}^t=\\left\\{V_1^t,\\ V_2^t,\\ \\ldots\\right\\}$, we have: $$\\begin{gathered}\nI_i^t = {\\rm LLM}(S_i^t,\\ S_i^{t-1},\\ \\ldots,\\ S_i^{t-10},\\ env^t),\\\\\nM_i^t = {\\rm LLM}(S_i^t,\\ I_i^t,\\ env^t),\n\\end{gathered}$$ where $env^t$ is the scene road information, $S_i^t$ is the state of $V_i^t$ obtained from the environment, $I_i^t$ is the intention of $V_i^t$, and $M_i^t$ is 0 or 1, indicating whether the $V_i^t$ is considered by the LLM to be of concern. Subsequently, we create an observation matrix for the MPC exclusively based on the vehicles identified by the LLM: $$\\begin{gathered}\nobs_i^t = {\\rm MPC_{obs}}(S_i^t,\\ I_i^t,\\ env^t) * M_i^t,\n\\end{gathered}$$ where $obs_i^t$ is row $i$ of the MPC observation matrix, $\\rm MPC_{obs}$ is the operator for MPC to compute the observation matrix. This ensures that the MPC focuses solely on these selected vehicles.\n\n## Situation Awareness and Action Guidance\n\nSituation awareness stands as a pivotal high-level decision-making process in driving, encompassing a profound understanding of the scenario at hand, along with common-sense reasoning. Within this framework, we tasked the LLM with the responsibility of selecting one specific situation from among several options, leveraging the information gleaned during the attention allocation process outlined in Section [3.3](#sec:attention){reference-type=\"ref\" reference=\"sec:attention\"}, as well as the results of the LLM's reasoned judgments. We define the feature $F_i^t=\\left\\{S_i^t,\\ I_i^t,\\ M_i^t\\right\\}$ to characterize the information associated with $V_i^t$, then we have: $$\\begin{gathered}\nX^t = {\\rm LLM}(F_1^t,\\ \\ldots,\\ F_k^t,\\ env^t),\n\\end{gathered}$$ where $k$ is the number of surrounding vehicles selected by LLM, and $X^t$ is the selected situation. The judgment of $X^t$ serves as a mechanism to tune the weight matrix of the MPC. For each predefined situation, we have established a corresponding weight matrix $W^t$. Subsequently, the LLM provided guidance concerning acceleration and steering based on its chosen situation: $$\\begin{gathered}\nA^t = {\\rm LLM}(F_1^t,\\ \\ldots,\\ F_k^t,\\ env^t,\\ X^t),\n\\end{gathered}$$ where $A^t$ is action guidance. The $A^t$ influences the adjustment of the MPC's action bias, with costs decreasing as the vehicle's actions align more closely with the provided guidance: $$\\begin{gathered}\nr_{bias}(bias,\\ \\psi) = bias-\\psi, \\\\\nC_{bias} = w_{bias} |r_{bias}|_2,\n\\end{gathered}$$ where $bias$ is acceleration or steering. It's worth noting that our set of predefined situations, though limited in number, are abstract and broad enough to encompass a wide range of driving scenarios, as they do not represent specific scenarios, but rather the certainty that the MPC should perform the action biases. We substantiate the effectiveness of this approach through experimental validation in Section [4.1](#sec:single){reference-type=\"ref\" reference=\"sec:single\"}.\n\n## Multi-vehicle Joint Control\n\nMulti-vehicle joint control is an important solution for improving transportation efficiency and safety. However, both centralized and distributed approaches are often overly reliant on environmental prior and fail to exhibit good performance when the traffic model is unknown [@10172912]. To address this problem, we propose a solution. Each vehicle is individually controlled by a distributed LLM, with one central LLM acting as the \\\"brain\\\" of the fleet for multi-vehicle communication and coordination. Each distributed LLM reports the situation it is into the central LLM and receives commands to control the ego vehicle; the central LLM judges and gives the coordination commands based on the environmental information and the reports from the distributed LLMs.\n\n[]{#tab:t1 label=\"tab:t1\"}"},{"heading":"Experiments","text":"# Experiments\n\nOur approach was applied to both single-vehicle decision-making and multi-vehicle joint control tasks. The scenario maps and traffic flows were generated using the IdSim [@2021arXiv210910557L].\n\nIn the context of single-vehicle decision-making, we conducted evaluations over three different approaches: Reinforcement Learning-Based Planning (RL) [@9760270; @9857655], Model Predictive Control (MPC) [@9760270], and our system, MPC with LLM's High-Level Decision-Making Guidelines (LLM+MPC). Among them, RL was trained and validated in a wide range of complex traffic scenarios, and MPC was finely tuned and validated in real-vehicle experiments. Each of these approaches was tested across diverse scenarios, including signalized intersections, unsignalized intersections, driveways, emergency avoidance, and roundabouts. We selected 25 complex and challenging cases for each scenario type to comprehensively assess system performance. Evaluation metrics are detailed in Appendix [6](#app:metrics){reference-type=\"ref\" reference=\"app:metrics\"}. Furthermore, We have also demonstrated the great potential of our system in driving behavior modulation guided by textual input, demonstrating its excellent understanding of high-level information.\n\nIn the realm of multi-vehicle joint control, our method's capabilities were put to the test in intricate gaming scenarios. Notably, we evaluated its performance in complex situations like narrow lane meetings, showcasing its adaptability and effectiveness in challenging environments.\n\n## Single-vehicle Decision-making {#sec:single}\n\nThe quantitative results of single-vehicle decision-making are shown in Table [\\[tab:t1\\]](#tab:t1){reference-type=\"ref\" reference=\"tab:t1\"}. It's important to highlight that our system achieves overall cost reductions across four scenario types, reflecting improved driving behavior. In non-emergency situations, we observe minimal fail occurrences and no collisions, underlining the safety of our approach. Additionally, in emergency scenarios, our method significantly lowers the accident rate, indicating its effectiveness in obstacle avoidance.\n\nSpecifically, in intersections, our focus is primarily on left-turn situations, where ego vehicle inherently possesses a lower right-of-way status, necessitating a deliberate choice to slow down and yield in accordance with established traffic rules. In left-turning within the intersections scenarios and roundabouts scenarios, although our approach may result in a slight increase in elapsed time, it yields substantial benefits in terms of enhanced traffic flow efficiency and reduced safety penalties. This outcome underscores our method's commitment to adopting safer and more reasonable driving behaviors that align with the principles of traffic regulations. In lanes, our approach excels in all metrics, indicating a more sensible approach to overtaking and lane changes. Finally, in emergency situations, our method demonstrates its effectiveness by reducing accident rates and enhancing overall performance.\n\n![The ego vehicle is traveling straight through an unsignalized intersection. The red vehicle(s) in the last row is the one(s) selected by the LLM as needing attention. This example demonstrates LLM's understanding and reasoning about high-level information, proving the validity of the chain-of-thought we devised.](figure/single.png){#fig:cross_straight}\n\n**Attention Allocation.** Fig. [4](#fig:cross_straight){reference-type=\"ref\" reference=\"fig:cross_straight\"} illustrates a scenario where the ego vehicle is proceeding straight through an unsignalized intersection. In this situation, the MPC incorporates all surrounding vehicles into its observation matrix for prediction and trajectory planning. However, it becomes evident from the outcomes that the MPC fails to accurately discern that \\\"vehicle_32\\\" has a lower right-of-way priority, erroneously decelerates and maneuvers to avoid it. In contrast, our approach, which employs LLM's reasoning, effectively comprehends the intention of \\\"vehicle_32.\\\" As a result, it concentrates its attention solely on the foremost vehicle that could impact the ego vehicle's motion and appropriately follows it as it navigates the intersection. This example serves as a compelling demonstration of how the LLM in our approach adeptly comprehends the traffic scenario and discerns the intentions of other vehicles. This enables it to make more informed decisions about allocating attention and, consequently, facilitates more efficient and rational driving behavior.\n\n**Situation Awareness and Action Guidance.** In the scenario depicted in Fig. [4](#fig:cross_straight){reference-type=\"ref\" reference=\"fig:cross_straight\"}, a crucial factor to consider is that the ego vehicle, despite being hindered by the slower-moving vehicle ahead, is obligated by traffic regulations not to perform overtaking maneuvers within the intersection. However, the RL opts to overtake the leading vehicle from the right side immediately within the intersection. This decision stems from the inherent limitations of learning-based approaches, which often struggle to grasp high-level information such as traffic regulations. Their primary focus tends to be on achieving a certain level of efficiency and safety in driving. In contrast, our approach, as indicated in the dialogue box on the right, showcases the LLM's capacity to accurately comprehend the ego vehicle's situation. It makes a reasoned choice to follow the vehicle ahead through the intersection before considering an overtaking maneuver, which aligns with traffic regulations. This example underscores the LLM's proficiency in reasoning about complex traffic scenarios and assessing its own circumstances. As a result, it produces decisions that closely mimic human thinking, guided by higher-level information like traffic regulations and common sense.\n\n**Ablations.** We conducted ablation experiments in two typical driving scenarios, unsignalized intersections, and roundabouts, and the outcomes are presented in Table [\\[tab:t2\\]](#tab:t2){reference-type=\"ref\" reference=\"tab:t2\"}. When solely employing the LLM's Attention Allocation capability, nearly all metrics exhibited improvement compared to the baseline MPC. This improvement stemmed from the system's ability to disregard irrelevant surrounding vehicles, leading to more effective decision-making. On the other hand, when utilizing only the LLM Situation Awareness and Action Guidance features, all metrics, except for time, demonstrated substantial enhancement compared to the baseline MPC. This highlights the pivotal role played by our system's comprehension of high-level information in decision-making.\n\n[]{#tab:t2 label=\"tab:t2\"}\n\n## Multi-vehicle Joint Control\n\n<figure>\n\n<figcaption>(a) The convoy is in a meeting situation. This example demonstrates that the high-level decision-making of the central LLM and the fine-grained control of the distributed LLMs collaborate to accomplish high-quality multi-vehicle cooperative control. (b) LLM changes lanes to avoid road construction under textual guidance.</figcaption>\n</figure>\n\nWe have assessed the effectiveness of our approach through a specific scenario involving a narrow road encounter, as depicted in Fig. [\\[fig:multi\\]](#fig:multi){reference-type=\"ref\" reference=\"fig:multi\"}. In this scenario, two distributed LLMs concurrently report that they are in a meeting situation. The central LLM, informed by the scenario details and the self-assessment of each distributed LLM, issues instructions. One vehicle is instructed to decelerate and wait, while the other is advised to slow down and proceed, thus facilitating communication and coordination within the convoy. Subsequently, each distributed LLM adjusts the control of its respective vehicle in accordance with this convoy-level decision. This example vividly illustrates how our system combines the strengths of both centralized and distributed methodologies. The central LLM acts as the \\\"brain\\\" for convoy communication and coordination, while distributed LLMs can intelligently manage their respective vehicles based on decisions made at the convoy level.\n\n## Text-modulated Driving Behavior\n\n![The ego vehicle is traveling in lane, the front vehicle is slow, and the ego vehicle overtaking would be low risk. This example demonstrates the ease and effectiveness of LLM in driving style adjustment.](figure/style.png){#fig:style}\n\n**Driving style adjustment.** In real-world driving scenarios, users often desire the ability to effortlessly customize the driving behavior of AD systems to align with their preferences for efficiency and comfort. However, for learning-based or optimization-based AD systems, achieving this level of intuitive and reliable customization requires complex rule or reward function designs [@chang2023editing]. In contrast, our approach simplifies the process by merely providing textual descriptions to the LLM through a dedicated interface. Fig. [5](#fig:style){reference-type=\"ref\" reference=\"fig:style\"} exemplifies this feature. When there is low risk of overtaking, LLM instructed to drive aggressively will make reasonable overtaking decisions, while those directed to drive conservatively will opt to slow down and follow the vehicle in front of it. This example effectively illustrates how our approach excels at comprehending the user's abstract and non-intuitive requirements, easily delivering the expected driving behavior.\n\n**Textual guidance for complex scenarios.** Certain complex transportation scenarios, such as road construction and other uncommon situations, pose significant challenges for many existing AD systems [@10138317]. However, these scenarios are typically straightforward for humans to identify and understand. To address this issue, our approach enables users or utilizes high-precision maps to provide textual instructions that guide the AD system's decision-making process. As depicted in Fig. [\\[fig:txt\\]](#fig:txt){reference-type=\"ref\" reference=\"fig:txt\"}, we conducted an experiment involving a road construction scenario. Upon receiving textual guidelines, our approach successfully recognized the situation and gave appropriate driving behavior."},{"heading":"Conclusion","text":"# Conclusion\n\nThis paper demonstrates that LLMs can effectively serve as the core high-level decision-making component of AD systems. Our approach combining LLMs and MPC substantially outperforms existing methods on key metrics and handles complex real-world driving scenarios. The reasoning skills and interpretability of LLMs help overcome the limitations of current learning-based AD systems regarding adaptability and transparency. This paper makes a compelling case for LLMs as a transformative solution to enable human-like performance in diverse driving scenarios. Our approach provides an initial step for developing safe, efficient, generalizable, and interpretable LLM-based AD systems. We aspire for it to serve as inspiration for future research in this domain."},{"heading":"Metrics","text":"# Metrics {#app:metrics}\n\nWe use the metrics below to measure the safety and efficiency of driving behavior.\n\n**Number of failure/collision cases:** We keep a tally of failure cases. A case is marked as a failure if the ego vehicle cannot reach its target area within a designated 200-second time frame. For clarity, when the ego vehicle travels within a lane, the target area is defined as the end of that lane. When it navigates through an intersection or roundabout, the target area is set as the beginning of the target lane. Collision cases are tracked separately.\n\n**Inefficiency:** To gauge the efficiency of traffic flow, we calculate the average difference between the ego vehicle's maximum and current speeds: $$\\begin{gathered}\n\\xi = \\frac{1}{N} \\sum_{i = 1}^N (v_i^{max}-v_i).\n\\end{gathered}$$ Our assessment focuses solely on the lead vehicle within each convoy, as it is the one directly influenced by the ego vehicle's driving behavior. Vehicles affected by red lights are excluded from this calculation.\n\n**Time:** We directly record the time the ego vehicle takes to reach its target area. This metric serves as an indicator of the ego vehicle's driving efficiency.\n\n**Penalty:** The penalty metric is employed to assess the safety of the ego vehicle's driving behavior. It's computed based on the distance between the target vehicle and the ego vehicle, as well as the deceleration of the target vehicle. Smaller distances and higher decelerations indicate more unsafe driving behavior, thus contributing to a higher penalty: $$P_{dec} = w_{dec}\\sum_{i = 1}^N {\\rm f_{dec}}(dec_i - dec_0),$$ $$P_{dist} = w_{dist}\\sum_{i = 1}^N {\\rm f_{dist}}(dist_0 - dist_i,\\ dec_i - dec_0),$$ $$\\ {\\rm f_{dec}}(x) =\n\\begin{cases}\n0& \\text{if } x \\leq 0,\\\\\nx& \\text{if } x > 0,\n\\end{cases}\n\\;\n{\\rm f_{dist}}(x, y) =\n\\begin{cases}\n0& \\text{if } y \\leq 0,\\\\\nx& \\text{if } y > 0,\n\\end{cases}$$ where $dec$ refers to decceleration, $dist$ refers to distance, $w_{dec}=100$, $w_{dist}=1$, $dec_0=1m/s^2$, and $dist_0=50m$.This penalty is calculated for each vehicle, similar to the Efficiency metric, and then aggregated to derive the final penalty score.\n\n**Cost:** To provide an overall evaluation, we apply weighted values to the above metrics so that the values of each item are close to each other: $$\\begin{gathered}\nCost = 30 * \\xi + t + 50 * P_{dec} + 5 * P_{dist}.\n\\end{gathered}$$ This comprehensive cost assessment enables us to holistically evaluate the performance of our autonomous vehicle system."},{"heading":"Decision-making Process","text":"# Decision-making Process\n\n<figure>\n\n<figcaption>Pink one is the ego vehicle, green ones are the vehicles within the perception range of the ego vehicle, blue ones are the vehicles outside the perception range of the ego vehicle, and red one is the vehicle chosen by the LLM.</figcaption>\n</figure>\n\nThe complete decision-making process of LLM is shown in the text box below. Faced with the scenario in Fig. [\\[fig:1\\]](#fig:1){reference-type=\"ref\" reference=\"fig:1\"}, LLM+MPC gives the driving actions shown in Fig. [\\[fig:2\\]](#fig:2){reference-type=\"ref\" reference=\"fig:2\"}.\n\n[^1]: $\\dag$ Corresponding author."}],"approval":false,"conference":"iclr","rating":2,"year":2024,"id":"606c6f29c74e126528b2276c4d6b17c6f0b7eb87540acde3b9387a5e6e788269","y_true":0,"y_pred":0,"rationale_true":"Summary: Current autonomous driving algorithms still face some challenges such as comprehending high-level information, generalizing to rare events, and providing interpretability. Large language models (LLMs) show great potential in solving problems with human common sense and providing human-understandable explanations. Therefore, this paper proposes to use LLM as a high-level planner and use MPC as a low-level controller. The authors evaluate their driving pipeline in both single-vehicle tasks and multi-vehicle coordination. Results show that their method consistently surpasses baselines.\n\nStrengths: 1. Combining LLM and MPC is a smart way to leverage the high-level reasoning capability of LLM. Since LLM is not good at low-level control as shown in previous literature, using MPC to output low-level actions helps incorporate strong prior knowledge into the whole pipeline.\n\n2. Rare and long-tail events put great challenges to data-driven algorithms. Using common sense to solve such long-tail problems is a promising way. Since LLM already contains a lot of common sense, this paper is a good attempt to explore this direction.\n\n3. The paper is well-written and easy to follow. Figure 2 provides a clear illustration of the entire pipeline, which makes me easy to understand the method proposed in this paper.\n\nWeaknesses: 1. A common question for LLM-based agents is the trustworthiness. When used for safety-critical applications like autonomous driving, the requirements for safety are much higher than QA or code generation tasks. As far as I know, GPT3.5 and GPT4 have uncontrollable randomness even with temperature = 0. Does this randomness influence the running results? How to ensure that LLM always outputs reasonable results. Are the results consistent if the version of the ChatGPT model changes?\n\n2. How about the running frequency of the entire system? Since driving requires a high frequency to make decisions, the speed of processing the information with language may not be fast enough. This is one of the biggest challenges of using LLM for driving, which may be unaffordable and unsolvable with a general-purpose LLM. Using LLM which can only communicate with downstream modules with language wastes a lot of time on extracting information.\n\n3. In the first paragraph of the introduction, the authors provide an example “Human drivers intuitively know that according to traffic rules, they should slow down and yield, even if it is technically possible to speed through”. This example is confusing to me as it just shows two types of common driving styles of human drivers (aggressive and conservative styles). I am not aware of any complex rules or reward functions here since it is easy to force the driving algorithm to be conservative with some simple rules. Actually, considering the complexity of the pipeline designed in Figure 2, I am not sure which one is more complex. Maybe several simple cost functions in an MPC can already solve the scenarios in the experiment part.\n\n4. It seems that one important motivation for using LLM for driving is solving rare and long-tail events. However, I only find Figure 1 discusses one specific case where a breakdown vehicle is stopped in the middle of the road in a roundabout. I can’t find any evidence or experiment results showing that LLM broadly solves long-tail problems. \n\n5. In the evaluation part, only one RL algorithm (not mentioned which algorithm exactly) and one MPC are compared. Since modern AV algorithms are usually complex, I don’t think such a comparison is not convincing enough to demonstrate the advantage of the LLM method.\n\nQuestions: 1.\tCould the authors report the running time of all methods in the experiments? \n2.\tThere are 5 scenarios mentioned in Table 1 but no detailed explanation of them. I cannot evaluate the realism and difficulty of them.\n3.\tCould the authors explain more about the interpretability of the LLM method? Does this interpretability mean the reason for decision-making? Why is such interpretability useful and can we always trust such interpretability? If the interpretability is something shown in Figure 1, I think all collision avoidance algorithms can provide the reason for the decision.","rationale_pred":"Paper Summary: The paper 'LanguageMPC: Large Language Models as Decision Makers for Autonomous Driving' presents a novel approach to integrating Large Language Models (LLMs) into autonomous driving systems. It addresses challenges such as high-level information comprehension, generalization to rare events, and interpretability by employing LLMs for decision-making in complex driving scenarios. The authors develop cognitive pathways for reasoning and algorithms for translating LLM decisions into actionable driving commands, demonstrating improved performance in both single-vehicle and multi-vehicle tasks.\n\nSupporting Evidence:\n- The paper 'KoMA: Knowledge-driven Multi-agent Framework for Autonomous Driving with Large Language Models' emphasizes the advantages of using LLMs in enhancing generalization and interpretability in autonomous driving, supporting the Main Paper's claims about LLMs' effectiveness in complex scenarios. (from: KoMA: Knowledge-driven Multi-agent Framework for Autonomous Driving with Large Language Models)\n- The paper 'AD-H: Autonomous Driving with Hierarchical Agents' highlights the importance of multimodal large language models for high-level reasoning, aligning with the Main Paper's focus on LLMs for decision-making in complex driving scenarios. (from: AD-H: Autonomous Driving with Hierarchical Agents)\n- The paper 'Empowering Autonomous Driving with Large Language Models: A Safety Perspective' supports the Main Paper by emphasizing LLMs' commonsense reasoning capabilities, which can significantly improve performance and safety in complex driving scenarios. (from: Empowering Autonomous Driving with Large Language Models: A Safety Perspective)\n- The paper 'Drive Like a Human: Rethinking Autonomous Driving with Large Language Models' reinforces the Main Paper's claims about LLMs' effectiveness in enhancing decision-making for autonomous driving by providing insights into human-like capabilities of LLMs. (from: Drive Like a Human: Rethinking Autonomous Driving with Large Language Models)\n- The paper 'Emergent Abilities of Large Language Models' discusses how scaling LLMs can lead to unpredictable improvements in performance, supporting the Main Paper's claim that LLMs can enhance decision-making in autonomous driving. (from: Emergent Abilities of Large Language Models)\n\nContradictory Evidence:\n- The paper 'Facilitating Autonomous Driving Tasks With Large Language Models' emphasizes the limitations of using LLMs directly for decision-making, suggesting a more cautious approach that combines LLM knowledge with statistical learning, contrasting the Main Paper's optimistic view. (from: Facilitating Autonomous Driving Tasks With Large Language Models)\n- The paper 'LMDrive: Closed-Loop End-to-End Driving with Large Language Models' critiques the reliance on LLMs for decision-making, highlighting challenges in handling unforeseen events and suggesting a different methodological approach. (from: LMDrive: Closed-Loop End-to-End Driving with Large Language Models)\n- The paper 'Will Large Language Models be a Panacea to Autonomous Driving?' questions the effectiveness of LLMs in integrating with existing approaches, emphasizing their limitations in unpredictable scenarios, which contrasts with the Main Paper's claims. (from: Will Large Language Models be a Panacea to Autonomous Driving?)\n- The paper 'Large Language Models for Human-like Autonomous Driving: A Survey' critiques the Main Paper's approach by highlighting the challenges of integrating LLMs into autonomous driving systems, suggesting a need for a more comprehensive understanding of limitations. (from: Large Language Models for Human-like Autonomous Driving: A Survey)\n- The paper 'Explainable Artificial Intelligence for Autonomous Driving: A Comprehensive Overview and Field Guide for Future Research Directions' emphasizes the necessity of explainability in AI decision-making, contrasting with the Main Paper's focus on performance over transparency. (from: Explainable Artificial Intelligence for Autonomous Driving: A Comprehensive Overview and Field Guide for Future Research Directions)\n\nKey Comparisons:\n- The Main Paper's approach relies heavily on LLMs for decision-making, while contrasting papers emphasize the limitations and potential inaccuracies of LLM outputs.\n- Supporting papers highlight the advantages of LLMs in enhancing interpretability and generalization, whereas contrasting papers question their effectiveness in real-time applications.\n- The Main Paper proposes cognitive pathways for LLMs, while contrasting papers suggest alternative methods that combine LLMs with traditional learning frameworks.\n\nConclusion: Despite the supporting evidence from related papers that highlight the potential of LLMs in enhancing decision-making for autonomous driving, the contrasting papers raise significant concerns about the limitations and challenges of relying solely on LLMs. The critiques regarding the integration of LLMs into real-time systems and the need for explainability suggest that the Main Paper's claims may not be sufficiently novel, as similar concerns have been raised in existing literature. Therefore, the paper does not present a sufficiently unique contribution to be considered novel.","structured_evaluation":{"paper_summary":"The paper 'LanguageMPC: Large Language Models as Decision Makers for Autonomous Driving' presents a novel approach to integrating Large Language Models (LLMs) into autonomous driving systems. It addresses challenges such as high-level information comprehension, generalization to rare events, and interpretability by employing LLMs for decision-making in complex driving scenarios. The authors develop cognitive pathways for reasoning and algorithms for translating LLM decisions into actionable driving commands, demonstrating improved performance in both single-vehicle and multi-vehicle tasks.","supporting_evidence":[{"text":"The paper 'KoMA: Knowledge-driven Multi-agent Framework for Autonomous Driving with Large Language Models' emphasizes the advantages of using LLMs in enhancing generalization and interpretability in autonomous driving, supporting the Main Paper's claims about LLMs' effectiveness in complex scenarios.","paper_id":"S1","paper_title":"KoMA: Knowledge-driven Multi-agent Framework for Autonomous Driving with Large Language Models","source":"citations"},{"text":"The paper 'AD-H: Autonomous Driving with Hierarchical Agents' highlights the importance of multimodal large language models for high-level reasoning, aligning with the Main Paper's focus on LLMs for decision-making in complex driving scenarios.","paper_id":"S2","paper_title":"AD-H: Autonomous Driving with Hierarchical Agents","source":"citations"},{"text":"The paper 'Empowering Autonomous Driving with Large Language Models: A Safety Perspective' supports the Main Paper by emphasizing LLMs' commonsense reasoning capabilities, which can significantly improve performance and safety in complex driving scenarios.","paper_id":"S3","paper_title":"Empowering Autonomous Driving with Large Language Models: A Safety Perspective","source":"citations"},{"text":"The paper 'Drive Like a Human: Rethinking Autonomous Driving with Large Language Models' reinforces the Main Paper's claims about LLMs' effectiveness in enhancing decision-making for autonomous driving by providing insights into human-like capabilities of LLMs.","paper_id":"S4","paper_title":"Drive Like a Human: Rethinking Autonomous Driving with Large Language Models","source":"citations"},{"text":"The paper 'Emergent Abilities of Large Language Models' discusses how scaling LLMs can lead to unpredictable improvements in performance, supporting the Main Paper's claim that LLMs can enhance decision-making in autonomous driving.","paper_id":"S5","paper_title":"Emergent Abilities of Large Language Models","source":"citations"}],"contradictory_evidence":[{"text":"The paper 'Facilitating Autonomous Driving Tasks With Large Language Models' emphasizes the limitations of using LLMs directly for decision-making, suggesting a more cautious approach that combines LLM knowledge with statistical learning, contrasting the Main Paper's optimistic view.","paper_id":"C1","paper_title":"Facilitating Autonomous Driving Tasks With Large Language Models","source":"citations"},{"text":"The paper 'LMDrive: Closed-Loop End-to-End Driving with Large Language Models' critiques the reliance on LLMs for decision-making, highlighting challenges in handling unforeseen events and suggesting a different methodological approach.","paper_id":"C2","paper_title":"LMDrive: Closed-Loop End-to-End Driving with Large Language Models","source":"citations"},{"text":"The paper 'Will Large Language Models be a Panacea to Autonomous Driving?' questions the effectiveness of LLMs in integrating with existing approaches, emphasizing their limitations in unpredictable scenarios, which contrasts with the Main Paper's claims.","paper_id":"C3","paper_title":"Will Large Language Models be a Panacea to Autonomous Driving?","source":"citations"},{"text":"The paper 'Large Language Models for Human-like Autonomous Driving: A Survey' critiques the Main Paper's approach by highlighting the challenges of integrating LLMs into autonomous driving systems, suggesting a need for a more comprehensive understanding of limitations.","paper_id":"C4","paper_title":"Large Language Models for Human-like Autonomous Driving: A Survey","source":"citations"},{"text":"The paper 'Explainable Artificial Intelligence for Autonomous Driving: A Comprehensive Overview and Field Guide for Future Research Directions' emphasizes the necessity of explainability in AI decision-making, contrasting with the Main Paper's focus on performance over transparency.","paper_id":"C5","paper_title":"Explainable Artificial Intelligence for Autonomous Driving: A Comprehensive Overview and Field Guide for Future Research Directions","source":"citations"}],"key_comparisons":["The Main Paper's approach relies heavily on LLMs for decision-making, while contrasting papers emphasize the limitations and potential inaccuracies of LLM outputs.","Supporting papers highlight the advantages of LLMs in enhancing interpretability and generalization, whereas contrasting papers question their effectiveness in real-time applications.","The Main Paper proposes cognitive pathways for LLMs, while contrasting papers suggest alternative methods that combine LLMs with traditional learning frameworks."],"conclusion":"Despite the supporting evidence from related papers that highlight the potential of LLMs in enhancing decision-making for autonomous driving, the contrasting papers raise significant concerns about the limitations and challenges of relying solely on LLMs. The critiques regarding the integration of LLMs into real-time systems and the need for explainability suggest that the Main Paper's claims may not be sufficiently novel, as similar concerns have been raised in existing literature. Therefore, the paper does not present a sufficiently unique contribution to be considered novel.","label":0,"novel":"no","probability":9.237449576640118e-9,"rationale":"Paper Summary: The paper 'LanguageMPC: Large Language Models as Decision Makers for Autonomous Driving' presents a novel approach to integrating Large Language Models (LLMs) into autonomous driving systems. It addresses challenges such as high-level information comprehension, generalization to rare events, and interpretability by employing LLMs for decision-making in complex driving scenarios. The authors develop cognitive pathways for reasoning and algorithms for translating LLM decisions into actionable driving commands, demonstrating improved performance in both single-vehicle and multi-vehicle tasks.\n\nSupporting Evidence:\n- The paper 'KoMA: Knowledge-driven Multi-agent Framework for Autonomous Driving with Large Language Models' emphasizes the advantages of using LLMs in enhancing generalization and interpretability in autonomous driving, supporting the Main Paper's claims about LLMs' effectiveness in complex scenarios. (from: KoMA: Knowledge-driven Multi-agent Framework for Autonomous Driving with Large Language Models)\n- The paper 'AD-H: Autonomous Driving with Hierarchical Agents' highlights the importance of multimodal large language models for high-level reasoning, aligning with the Main Paper's focus on LLMs for decision-making in complex driving scenarios. (from: AD-H: Autonomous Driving with Hierarchical Agents)\n- The paper 'Empowering Autonomous Driving with Large Language Models: A Safety Perspective' supports the Main Paper by emphasizing LLMs' commonsense reasoning capabilities, which can significantly improve performance and safety in complex driving scenarios. (from: Empowering Autonomous Driving with Large Language Models: A Safety Perspective)\n- The paper 'Drive Like a Human: Rethinking Autonomous Driving with Large Language Models' reinforces the Main Paper's claims about LLMs' effectiveness in enhancing decision-making for autonomous driving by providing insights into human-like capabilities of LLMs. (from: Drive Like a Human: Rethinking Autonomous Driving with Large Language Models)\n- The paper 'Emergent Abilities of Large Language Models' discusses how scaling LLMs can lead to unpredictable improvements in performance, supporting the Main Paper's claim that LLMs can enhance decision-making in autonomous driving. (from: Emergent Abilities of Large Language Models)\n\nContradictory Evidence:\n- The paper 'Facilitating Autonomous Driving Tasks With Large Language Models' emphasizes the limitations of using LLMs directly for decision-making, suggesting a more cautious approach that combines LLM knowledge with statistical learning, contrasting the Main Paper's optimistic view. (from: Facilitating Autonomous Driving Tasks With Large Language Models)\n- The paper 'LMDrive: Closed-Loop End-to-End Driving with Large Language Models' critiques the reliance on LLMs for decision-making, highlighting challenges in handling unforeseen events and suggesting a different methodological approach. (from: LMDrive: Closed-Loop End-to-End Driving with Large Language Models)\n- The paper 'Will Large Language Models be a Panacea to Autonomous Driving?' questions the effectiveness of LLMs in integrating with existing approaches, emphasizing their limitations in unpredictable scenarios, which contrasts with the Main Paper's claims. (from: Will Large Language Models be a Panacea to Autonomous Driving?)\n- The paper 'Large Language Models for Human-like Autonomous Driving: A Survey' critiques the Main Paper's approach by highlighting the challenges of integrating LLMs into autonomous driving systems, suggesting a need for a more comprehensive understanding of limitations. (from: Large Language Models for Human-like Autonomous Driving: A Survey)\n- The paper 'Explainable Artificial Intelligence for Autonomous Driving: A Comprehensive Overview and Field Guide for Future Research Directions' emphasizes the necessity of explainability in AI decision-making, contrasting with the Main Paper's focus on performance over transparency. (from: Explainable Artificial Intelligence for Autonomous Driving: A Comprehensive Overview and Field Guide for Future Research Directions)\n\nKey Comparisons:\n- The Main Paper's approach relies heavily on LLMs for decision-making, while contrasting papers emphasize the limitations and potential inaccuracies of LLM outputs.\n- Supporting papers highlight the advantages of LLMs in enhancing interpretability and generalization, whereas contrasting papers question their effectiveness in real-time applications.\n- The Main Paper proposes cognitive pathways for LLMs, while contrasting papers suggest alternative methods that combine LLMs with traditional learning frameworks.\n\nConclusion: Despite the supporting evidence from related papers that highlight the potential of LLMs in enhancing decision-making for autonomous driving, the contrasting papers raise significant concerns about the limitations and challenges of relying solely on LLMs. The critiques regarding the integration of LLMs into real-time systems and the need for explainability suggest that the Main Paper's claims may not be sufficiently novel, as similar concerns have been raised in existing literature. Therefore, the paper does not present a sufficiently unique contribution to be considered novel."},"arxiv_id":"2310.03026"},"terms":{"tasks":["comprehending high-level information","generalizing to rare events","providing interpretability","decision-making for complex autonomous driving scenarios","translating Large Language Model decisions into actionable driving commands","handling complex driving behaviors","multi-vehicle coordination","safety","efficiency","generalizability","interoperability"],"methods":["Large Language Models","cognitive pathways","algorithms for translating LLM decisions","guided parameter matrix adaptation"],"metrics":["baseline approaches"],"resources":[],"relations":[{"head":"Large Language Models","tail":"decision-making for complex autonomous driving scenarios"},{"head":"Large Language Models","tail":"safety"},{"head":"Large Language Models","tail":"efficiency"},{"head":"Large Language Models","tail":"generalizability"},{"head":"Large Language Models","tail":"interoperability"},{"head":"cognitive pathways","tail":"comprehending high-level information"},{"head":"cognitive pathways","tail":"generalizing to rare events"},{"head":"cognitive pathways","tail":"providing interpretability"},{"head":"algorithms for translating LLM decisions","tail":"translating Large Language Model decisions into actionable driving commands"},{"head":"guided parameter matrix adaptation","tail":"integrating LLM decisions with low-level controllers"},{"head":"baseline approaches","tail":"single-vehicle tasks"}]},"background":"Existing learning-based autonomous driving (AD) systems face challenges in comprehending high-level information, generalizing to rare events, and providing interpretability. To address these problems, this work employs Large Language Models (LLMs) as a decision-making component for complex AD scenarios that require human commonsense understanding.","target":"We devise cognitive pathways to enable comprehensive reasoning with LLMs, and develop algorithms for translating LLM decisions into actionable driving commands. Through this approach, LLM decisions are seamlessly integrated with low-level controllers by guided parameter matrix adaptation. Extensive experiments demonstrate that our proposed method not only consistently surpasses baseline approaches in single-vehicle tasks, but also helps handle complex driving behaviors even multi-vehicle coordination, thanks to the commonsense reasoning capabilities of LLMs. This paper presents an initial step toward leveraging LLMs as effective decision-makers for intricate AD scenarios in terms of safety, efficiency, generalizability, and interoperability. We aspire for it to serve as inspiration for future research in this field."},{"graph":{"title":"SelfVC: Voice Conversion With Iterative Refinement using Self Transformations","abstract":"We propose SelfVC, a training strategy to iteratively improve a voice conversion model with self-synthesized examples. Previous efforts on voice conversion focus on factorizing speech into explicitly disentangled representations that separately encode speaker characteristics and linguistic content. However, disentangling speech representations to capture such attributes using task-specific loss terms can lead to information loss by discarding finer nuances such as accent and emotion of the original signal. In this work, instead of explicitly disentangling attributes with loss terms, we present a framework to train a controllable voice conversion model on entangled speech representations derived from self-supervised learning (SSL) and speaker verification models. First, we develop techniques to derive prosodic information from the audio signal and SSL representations to train predictive submodules in the synthesis model. Next, we propose a training strategy to iteratively improve the synthesis model for voice conversion, by creating a challenging training objective using self-synthesized examples. In this training approach, the current state of the synthesis model is used to generate voice-converted variations of an utterance, which serve as inputs for the reconstruction task, ensuring a continuous and purposeful refinement of the model. We demonstrate that incorporating such self-synthesized examples during training improves the speaker similarity of generated speech as compared to a baseline voice conversion model trained solely on heuristically perturbed inputs. Our framework is trained without any text and is applicable to a range of tasks such as zero-shot voice conversion, voice conversion across different languages, and controllable speech synthesis with pitch and pace modifications. We conduct extensive comparisons against prior work and find that SelfVC achieves state-of-the-art results in zero-shot voice conversion on metrics evaluating naturalness, speaker similarity, and intelligibility of synthesized audio.","entities":[{"label":"SelfVC: Voice Conversion With Iterative Refinement using Self Transformations","type":"title","detail":null,"excerpts":null},{"label":"self-supervised representation learning","type":"primary_area","detail":null,"excerpts":null},{"label":"voice conversion","type":"keyword","detail":null,"excerpts":null},{"label":"self-synthesized examples","type":"keyword","detail":null,"excerpts":null},{"label":"self-supervised learning","type":"keyword","detail":null,"excerpts":null},{"label":"speaker similarity","type":"keyword","detail":null,"excerpts":null},{"label":"naturalness","type":"keyword","detail":null,"excerpts":null},{"label":"This paper proposes SelfVC, a novel training strategy for voice conversion that iteratively improves models using self-synthesized examples, achieving state-of-the-art results in zero-shot voice conversion.","type":"tldr","detail":null,"excerpts":null},{"label":"Improved Speaker Similarity","type":"claim","detail":"Incorporating self-synthesized examples during training significantly improves the speaker similarity of generated speech compared to baseline models.","excerpts":null},{"label":"Controllable Speech Synthesis","type":"claim","detail":"SelfVC allows for controllable speech synthesis from imperfectly disentangled representations, enabling tasks like zero-shot voice conversion and multilingual synthesis.","excerpts":null},{"label":"State-of-the-Art Results","type":"claim","detail":"SelfVC achieves state-of-the-art results in zero-shot voice conversion on metrics evaluating naturalness, speaker similarity, and intelligibility of synthesized audio.","excerpts":null},{"label":"SelfVC Training Strategy","type":"method","detail":"A training strategy that utilizes self-synthesized examples to iteratively improve the voice conversion model, enhancing speaker similarity metrics.","excerpts":null},{"label":"Feature Extraction Techniques","type":"method","detail":"Techniques to derive prosodic information from audio signals and SSL representations to train predictive submodules in the synthesis model.","excerpts":null},{"label":"Synthesis Model Design","type":"method","detail":"A synthesis model that reconstructs audio from SSL features and speaker embeddings, incorporating pitch and duration predictors for controllable synthesis.","excerpts":null},{"label":"LibriTTS Dataset Experiments","type":"experiment","detail":"Experiments conducted on the LibriTTS dataset to evaluate the performance of SelfVC against baseline models using metrics like SV-EER and MOS.","excerpts":null},{"label":"Cross-lingual Voice Conversion","type":"experiment","detail":"Evaluation of SelfVC's performance in cross-lingual voice conversion tasks using the CSS10 dataset, demonstrating its adaptability to different languages.","excerpts":null}],"relationships":[{"source":"SelfVC: Voice Conversion With Iterative Refinement using Self Transformations","target":"self-supervised representation learning"},{"source":"SelfVC: Voice Conversion With Iterative Refinement using Self Transformations","target":"voice conversion"},{"source":"SelfVC: Voice Conversion With Iterative Refinement using Self Transformations","target":"self-synthesized examples"},{"source":"SelfVC: Voice Conversion With Iterative Refinement using Self Transformations","target":"self-supervised learning"},{"source":"SelfVC: Voice Conversion With Iterative Refinement using Self Transformations","target":"speaker similarity"},{"source":"SelfVC: Voice Conversion With Iterative Refinement using Self Transformations","target":"naturalness"},{"source":"SelfVC: Voice Conversion With Iterative Refinement using Self Transformations","target":"This paper proposes SelfVC, a novel training strategy for voice conversion that iteratively improves models using self-synthesized examples, achieving state-of-the-art results in zero-shot voice conversion."},{"source":"This paper proposes SelfVC, a novel training strategy for voice conversion that iteratively improves models using self-synthesized examples, achieving state-of-the-art results in zero-shot voice conversion.","target":"Improved Speaker Similarity"},{"source":"This paper proposes SelfVC, a novel training strategy for voice conversion that iteratively improves models using self-synthesized examples, achieving state-of-the-art results in zero-shot voice conversion.","target":"Controllable Speech Synthesis"},{"source":"This paper proposes SelfVC, a novel training strategy for voice conversion that iteratively improves models using self-synthesized examples, achieving state-of-the-art results in zero-shot voice conversion.","target":"State-of-the-Art Results"},{"source":"Improved Speaker Similarity","target":"SelfVC Training Strategy"},{"source":"Controllable Speech Synthesis","target":"Feature Extraction Techniques"},{"source":"Controllable Speech Synthesis","target":"Synthesis Model Design"},{"source":"State-of-the-Art Results","target":"SelfVC Training Strategy"},{"source":"State-of-the-Art Results","target":"Feature Extraction Techniques"},{"source":"State-of-the-Art Results","target":"Synthesis Model Design"},{"source":"SelfVC Training Strategy","target":"LibriTTS Dataset Experiments"},{"source":"SelfVC Training Strategy","target":"Cross-lingual Voice Conversion"},{"source":"Feature Extraction Techniques","target":"LibriTTS Dataset Experiments"},{"source":"Feature Extraction Techniques","target":"Cross-lingual Voice Conversion"},{"source":"Synthesis Model Design","target":"LibriTTS Dataset Experiments"},{"source":"Synthesis Model Design","target":"Cross-lingual Voice Conversion"}],"valid_status":"Valid","valid_status_all":["Valid"]},"related":[{"summary":"The Related Paper supports the Main Paper by highlighting the importance of controllable speech representation learning, which aligns with the Main Paper's focus on improving voice conversion through self-synthesized examples. Both papers emphasize the significance of maintaining the integrity of speech attributes, such as speaker identity and prosody, during the conversion process. While the Main Paper proposes an iterative refinement approach using self-supervised learning, the Related Paper introduces a method for invertible representations that allows for independent control over speech features, thus complementing the Main Paper's findings on enhancing speaker similarity and synthesis quality.","paper_id":"69e69e106f22e3a53ea5b1c51fdcfe57e555dc6d","title":"Controllable Speech Representation Learning Via Voice Conversion and AIC Loss","abstract":"Speech representation learning transforms speech into features that are suitable for downstream tasks, e.g. speech recognition, phoneme classification, or speaker identification. For such recognition tasks, a representation can be lossy (non-invertible), which is typical of BERT-like self-supervised models. However, when used for synthesis tasks, we find these lossy representations prove to be insufficient to plausibly reconstruct the input signal. This paper introduces a method for invertible and controllable speech representation learning based on disentanglement. The representation can be decoded into a signal perceptually identical to the original. Moreover, its disentangled components (content, pitch, speaker identity, and energy) can be controlled independently to alter the synthesis result. Our model builds upon a zero-shot voice conversion model AutoVC-F0, in which we introduce alteration invariant content loss (AIC loss) and adversarial training (GAN). Through objective measures and subjective tests, we show that our formulation offers significant improvement in voice conversion sound quality as well as more precise control over the disentangled features.","score":0.7706788778305054,"polarity":"positive","source":"semantic","year":2022,"authors":["Yunyun Wang","Jiaqi Su","Adam Finkelstein","Zeyu Jin"],"venue":null,"citation_count":5,"reference_count":0,"influential_citation_count":1,"corpus_id":249436449,"url":"https://www.semanticscholar.org/paper/69e69e106f22e3a53ea5b1c51fdcfe57e555dc6d","arxiv_id":null,"contexts":null,"background":"Speech representation learning transforms speech into features that are suitable for downstream tasks, e.g. speech recognition, phoneme classification, or speaker identification. For such recognition tasks, a representation can be lossy (non-invertible), which is typical of BERT-like self-supervised models. However, when used for synthesis tasks, we find these lossy representations prove to be insufficient to plausibly reconstruct the input signal.","target":"This paper introduces a method for invertible and controllable speech representation learning based on disentanglement. The representation can be decoded into a signal perceptually identical to the original. Moreover, its disentangled components (content, pitch, speaker identity, and energy) can be controlled independently to alter the synthesis result. Our model builds upon a zero-shot voice conversion model AutoVC-F0, in which we introduce alteration invariant content loss (AIC loss) and adversarial training (GAN). Through objective measures and subjective tests, we show that our formulation offers significant improvement in voice conversion sound quality as well as more precise control over the disentangled features."},{"summary":"The Related Paper supports the Main Paper by demonstrating the effectiveness of self-supervised representations in enhancing voice conversion systems. While the Main Paper focuses on iteratively refining voice conversion models using entangled representations, the Related Paper emphasizes the benefits of speaker-disentangled representations for improving speaker similarity and naturalness in speech generation. Both papers highlight the importance of self-supervised learning, but the Related Paper provides additional evidence of how disentangled features can lead to improved performance in large language models, thereby reinforcing the Main Paper's claims about the advantages of self-synthesized examples in voice conversion.","paper_id":"527d11bae71c4a91f2e66637476e991f4a1d309b","title":"Enhancing the Stability of LLM-based Speech Generation Systems through Self-Supervised Representations","abstract":"Large Language Models (LLMs) are one of the most promising technologies for the next era of speech generation systems, due to their scalability and in-context learning capabilities. Nevertheless, they suffer from multiple stability issues at inference time, such as hallucinations, content skipping or speech repetitions. In this work, we introduce a new self-supervised Voice Conversion (VC) architecture which can be used to learn to encode transitory features, such as content, separately from stationary ones, such as speaker ID or recording conditions, creating speaker-disentangled representations. Using speaker-disentangled codes to train LLMs for text-to-speech (TTS) allows the LLM to generate the content and the style of the speech only from the text, similarly to humans, while the speaker identity is provided by the decoder of the VC model. Results show that LLMs trained over speaker-disentangled self-supervised representations provide an improvement of 4.7pp in speaker similarity over SOTA entangled representations, and a word error rate (WER) 5.4pp lower. Furthermore, they achieve higher naturalness than human recordings of the LibriTTS test-other dataset. Finally, we show that using explicit reference embedding negatively impacts intelligibility (stability), with WER increasing by 14pp compared to the model that only uses text to infer the style.","score":0.7706497311592102,"polarity":"positive","source":"semantic","year":2024,"authors":["Álvaro Martín-Cortinas","Daniel Sáez-Trigueros","Iv'an Vall'es-P'erez","Biel Tura Vecino","Piotr Bilinski","Mateusz Lajszczak","Grzegorz Beringer","R. Barra-Chicote","Jaime Lorenzo-Trueba"],"venue":null,"citation_count":4,"reference_count":37,"influential_citation_count":0,"corpus_id":267500193,"url":"https://www.semanticscholar.org/paper/527d11bae71c4a91f2e66637476e991f4a1d309b","arxiv_id":null,"contexts":null,"background":"Large Language Models (LLMs) are one of the most promising technologies for the next era of speech generation systems, due to their scalability and in-context learning capabilities. Nevertheless, they suffer from multiple stability issues at inference time, such as hallucinations, content skipping or speech repetitions.","target":"In this work, we introduce a new self-supervised Voice Conversion (VC) architecture which can be used to learn to encode transitory features, such as content, separately from stationary ones, such as speaker ID or recording conditions, creating speaker-disentangled representations. Using speaker-disentangled codes to train LLMs for text-to-speech (TTS) allows the LLM to generate the content and the style of the speech only from the text, similarly to humans, while the speaker identity is provided by the decoder of the VC model. Results show that LLMs trained over speaker-disentangled self-supervised representations provide an improvement of 4.7pp in speaker similarity over SOTA entangled representations, and a word error rate (WER) 5.4pp lower. Furthermore, they achieve higher naturalness than human recordings of the LibriTTS test-other dataset. Finally, we show that using explicit reference embedding negatively impacts intelligibility (stability), with WER increasing by 14pp compared to the model that only uses text to infer the style."},{"summary":"The Related Paper, Make-A-Voice, supports the Main Paper, SelfVC, by emphasizing the importance of self-supervised learning in voice synthesis, which aligns with SelfVC's approach of using entangled speech representations. Both papers advocate for the scalability of voice models without relying on annotated data, enhancing the ability to capture diverse acoustic variations. Additionally, Make-A-Voice's focus on controllability and conditioning flexibility complements SelfVC's iterative refinement strategy, suggesting that both frameworks aim to improve voice conversion quality and adaptability across various applications.","paper_id":"c0045bc9eb96efd4ac5e97742aa5cb4bddb55506","title":"Make-A-Voice: Unified Voice Synthesis With Discrete Representation","abstract":"Various applications of voice synthesis have been developed independently despite the fact that they generate\"voice\"as output in common. In addition, the majority of voice synthesis models currently rely on annotated audio data, but it is crucial to scale them to self-supervised datasets in order to effectively capture the wide range of acoustic variations present in human voice, including speaker identity, emotion, and prosody. In this work, we propose Make-A-Voice, a unified framework for synthesizing and manipulating voice signals from discrete representations. Make-A-Voice leverages a\"coarse-to-fine\"approach to model the human voice, which involves three stages: 1) semantic stage: model high-level transformation between linguistic content and self-supervised semantic tokens, 2) acoustic stage: introduce varying control signals as acoustic conditions for semantic-to-acoustic modeling, and 3) generation stage: synthesize high-fidelity waveforms from acoustic tokens. Make-A-Voice offers notable benefits as a unified voice synthesis framework: 1) Data scalability: the major backbone (i.e., acoustic and generation stage) does not require any annotations, and thus the training data could be scaled up. 2) Controllability and conditioning flexibility: we investigate different conditioning mechanisms and effectively handle three voice synthesis applications, including text-to-speech (TTS), voice conversion (VC), and singing voice synthesis (SVS) by re-synthesizing the discrete voice representations with prompt guidance. Experimental results demonstrate that Make-A-Voice exhibits superior audio quality and style similarity compared with competitive baseline models. Audio samples are available at https://Make-A-Voice.github.io","score":0.7428005933761597,"polarity":"positive","source":"semantic","year":2023,"authors":["Rongjie Huang","Chunlei Zhang","Yongqiang Wang","Dongchao Yang","Lu Liu","Zhenhui Ye","Ziyue Jiang","Chao Weng","Zhou Zhao","Dong Yu"],"venue":null,"citation_count":25,"reference_count":47,"influential_citation_count":2,"corpus_id":258967930,"url":"https://www.semanticscholar.org/paper/c0045bc9eb96efd4ac5e97742aa5cb4bddb55506","arxiv_id":null,"contexts":null,"background":"Various applications of voice synthesis have been developed independently despite the fact that they generate 'voice' as output in common. In addition, the majority of voice synthesis models currently rely on annotated audio data, but it is crucial to scale them to self-supervised datasets in order to effectively capture the wide range of acoustic variations present in human voice, including speaker identity, emotion, and prosody.","target":"In this work, we propose Make-A-Voice, a unified framework for synthesizing and manipulating voice signals from discrete representations. Make-A-Voice leverages a 'coarse-to-fine' approach to model the human voice, which involves three stages: 1) semantic stage: model high-level transformation between linguistic content and self-supervised semantic tokens, 2) acoustic stage: introduce varying control signals as acoustic conditions for semantic-to-acoustic modeling, and 3) generation stage: synthesize high-fidelity waveforms from acoustic tokens. Make-A-Voice offers notable benefits as a unified voice synthesis framework: 1) Data scalability: the major backbone (i.e., acoustic and generation stage) does not require any annotations, and thus the training data could be scaled up. 2) Controllability and conditioning flexibility: we investigate different conditioning mechanisms and effectively handle three voice synthesis applications, including text-to-speech (TTS), voice conversion (VC), and singing voice synthesis (SVS) by re-synthesizing the discrete voice representations with prompt guidance. Experimental results demonstrate that Make-A-Voice exhibits superior audio quality and style similarity compared with competitive baseline models. Audio samples are available at https://Make-A-Voice.github.io."},{"summary":"The Related Paper supports the Main Paper by addressing the challenges of zero-shot voice conversion, particularly in preserving prosodic features like pitch and volume, which are crucial for maintaining speaker identity. While the Main Paper introduces SelfVC, a method that leverages self-synthesized examples to enhance voice conversion without explicit disentanglement, the Related Paper complements this by proposing a self-supervised approach to learn and utilize prosodic representations effectively. Both papers emphasize the importance of maintaining speaker characteristics in voice conversion, with the Related Paper providing additional evidence that enhancing prosody can lead to improved performance, thereby reinforcing the claims made in the Main Paper.","paper_id":"3608311276e2a773da02722f8af799305522248b","title":"Zero-shot Voice Conversion via Self-supervised Prosody Representation Learning","abstract":"Voice Conversion (VC) for unseen speakers, also known as zero-shot VC, is an attractive research topic as it enables a range of applications like voice customizing, animation production, and others. Recent work in this area made progress with disentanglement methods that separate utterance content and speaker characteristics from speech audio recordings. However, many of these methods are subject to the leakage of prosody (e.g., pitch, volume), causing the speaker voice in the synthesized speech to be different from the desired target speakers. To prevent this issue, we propose a novel self-supervised approach that effectively learns disentangled pitch and volume representations that can represent the prosody styles of different speakers. We then use the learned prosodic representations as conditional information to train and enhance our VC model for zero-shot conversion. In our experiments, we show that our prosody representations are disentangled and rich in prosody information. Moreover, we demonstrate that the addition of our prosody representations improves our VC performance and surpasses state-of-the-art zero-shot VC performances.","score":0.7322424650192261,"polarity":"positive","source":"semantic","year":2021,"authors":["Shijun Wang","Dimche Kostadinov","Damian Borth"],"venue":null,"citation_count":10,"reference_count":36,"influential_citation_count":1,"corpus_id":239998459,"url":"https://www.semanticscholar.org/paper/3608311276e2a773da02722f8af799305522248b","arxiv_id":null,"contexts":null,"background":"Voice Conversion (VC) for unseen speakers, also known as zero-shot VC, is an attractive research topic as it enables a range of applications like voice customizing, animation production, and others. Recent work in this area made progress with disentanglement methods that separate utterance content and speaker characteristics from speech audio recordings. However, many of these methods are subject to the leakage of prosody (e.g., pitch, volume), causing the speaker voice in the synthesized speech to be different from the desired target speakers.","target":"To prevent this issue, we propose a novel self-supervised approach that effectively learns disentangled pitch and volume representations that can represent the prosody styles of different speakers. We then use the learned prosodic representations as conditional information to train and enhance our VC model for zero-shot conversion. In our experiments, we show that our prosody representations are disentangled and rich in prosody information. Moreover, we demonstrate that the addition of our prosody representations improves our VC performance and surpasses state-of-the-art zero-shot VC performances."},{"summary":"The Related Paper, 'Unsupervised Speech Decomposition via Triple Information Bottleneck', contrasts with the Main Paper by emphasizing the need for disentangled speech representations across multiple components (language content, timbre, pitch, and rhythm), while the Main Paper, 'SelfVC', focuses on using entangled representations for voice conversion. The Related Paper critiques existing systems for only partially disentangling timbre, highlighting the challenges of achieving full decomposition without explicit annotations, which the Main Paper does not address. Thus, while the Main Paper promotes a self-supervised approach to improve voice conversion, the Related Paper advocates for a more comprehensive disentangling of speech components.","paper_id":"9b0fffdb696614b4e917edfb166733ecdae2c7e9","title":"Unsupervised Speech Decomposition via Triple Information Bottleneck","abstract":"Speech information can be roughly decomposed into four components: language content, timbre, pitch, and rhythm. Obtaining disentangled representations of these components is useful in many speech analysis and generation applications. Recently, state-of-the-art voice conversion systems have led to speech representations that can disentangle speaker-dependent and independent information. However, these systems can only disentangle timbre, while information about pitch, rhythm and content is still mixed together. Further disentangling the remaining speech components is an under-determined problem in the absence of explicit annotations for each component, which are difficult and expensive to obtain. In this paper, we propose SpeechSplit, which can blindly decompose speech into its four components by introducing three carefully designed information bottlenecks. SpeechSplit is among the first algorithms that can separately perform style transfer on timbre, pitch and rhythm without text labels. Our code is publicly available at this https URL.","score":0.7918528318405151,"polarity":"negative","source":"semantic","year":2020,"authors":["Kaizhi Qian","Yang Zhang","Shiyu Chang","David Cox","M. Hasegawa-Johnson"],"venue":null,"citation_count":177,"reference_count":38,"influential_citation_count":21,"corpus_id":216080584,"url":"https://www.semanticscholar.org/paper/9b0fffdb696614b4e917edfb166733ecdae2c7e9","arxiv_id":null,"contexts":null,"background":"Speech information can be roughly decomposed into four components: language content, timbre, pitch, and rhythm. Obtaining disentangled representations of these components is useful in many speech analysis and generation applications. Recently, state-of-the-art voice conversion systems have led to speech representations that can disentangle speaker-dependent and independent information. However, these systems can only disentangle timbre, while information about pitch, rhythm and content is still mixed together. Further disentangling the remaining speech components is an under-determined problem in the absence of explicit annotations for each component, which are difficult and expensive to obtain.","target":"In this paper, we propose SpeechSplit, which can blindly decompose speech into its four components by introducing three carefully designed information bottlenecks. SpeechSplit is among the first algorithms that can separately perform style transfer on timbre, pitch and rhythm without text labels. Our code is publicly available at this https URL."},{"summary":"The Related Paper, MoCoVC, contrasts with the Main Paper, SelfVC, by emphasizing the need for explicit constraints in voice conversion through momentum contrastive representation learning, while the Main Paper advocates for a self-supervised approach that avoids explicit disentanglement of speaker and content attributes. MoCoVC relies on external models and transformations to maintain utterance content, whereas SelfVC focuses on iterative refinement using self-synthesized examples without requiring text data. Additionally, while both papers claim high performance in voice conversion, MoCoVC's reliance on explicit constraints highlights a fundamental difference in methodology compared to the Main Paper's more flexible, entangled representation approach.","paper_id":"ea5c6cfe0116b65598d66c852b151404d126ca57","title":"MoCoVC: Non-parallel Voice Conversion with Momentum Contrastive Representation Learning","abstract":"Non-parallel voice conversion with deep neural net-works often disentangle speaker individuality and speech content. However, these methods rely on external models, text data, or implicit constraints for ways to disentangle. They may require learning other models or annotating text, or may not understand how latent representations are acquired. Therefore, we pro-pose voice conversion with momentum contrastive representation learning (MoCo V C), a method of explicitly adding constraints to intermediate features using contrastive representation learning, which is a self-supervised learning method. Using contrastive rep-resentation learning with transformations that preserve utterance content allows us to explicitly constrain the intermediate features to preserve utterance content. We present transformations used for contrastive representation learning that could be used for voice conversion and verify the effectiveness of each in an exper-iment. Moreover, MoCoVC demonstrates a high or comparable performance to the vector quantization constrained method in terms of both naturalness and speaker individuality in subjective evaluation experiments.","score":0.7480452060699463,"polarity":"negative","source":"semantic","year":2022,"authors":["Kotaro Onishi","Toru Nakashika"],"venue":null,"citation_count":0,"reference_count":31,"influential_citation_count":0,"corpus_id":254930411,"url":"https://www.semanticscholar.org/paper/ea5c6cfe0116b65598d66c852b151404d126ca57","arxiv_id":null,"contexts":null,"background":"Non-parallel voice conversion with deep neural networks often disentangle speaker individuality and speech content. However, these methods rely on external models, text data, or implicit constraints for ways to disentangle. They may require learning other models or annotating text, or may not understand how latent representations are acquired.","target":"Therefore, we propose voice conversion with momentum contrastive representation learning (MoCo VC), a method of explicitly adding constraints to intermediate features using contrastive representation learning, which is a self-supervised learning method. Using contrastive representation learning with transformations that preserve utterance content allows us to explicitly constrain the intermediate features to preserve utterance content. We present transformations used for contrastive representation learning that could be used for voice conversion and verify the effectiveness of each in an experiment. Moreover, MoCoVC demonstrates a high or comparable performance to the vector quantization constrained method in terms of both naturalness and speaker individuality in subjective evaluation experiments."},{"summary":"The Related Paper contrasts the Main Paper by emphasizing the importance of disentangled speech representations for achieving controllable style transfer in voice conversion, while the Main Paper advocates for using entangled representations derived from self-supervised learning. The Related Paper critiques the Main Paper's approach, arguing that conventional methods lack robustness and controllability over multiple prosody-related factors, which their adversarial learning framework aims to address. Additionally, the Related Paper presents empirical results showing improved performance metrics, suggesting that their disentangled approach may be more effective than the iterative refinement strategy proposed in the Main Paper.","paper_id":"607c18c160aa66c13314b9da1e89639b79f67bca","title":"Adversarially learning disentangled speech representations for robust multi-factor voice conversion","abstract":"Factorizing speech as disentangled speech representations is vital to achieve highly controllable style transfer in voice conversion (VC). Conventional speech representation learning methods in VC only factorize speech as speaker and content, lacking controllability on other prosody-related factors. State-of-the-art speech representation learning methods for more speechfactors are using primary disentangle algorithms such as random resampling and ad-hoc bottleneck layer size adjustment,which however is hard to ensure robust speech representationdisentanglement. To increase the robustness of highly controllable style transfer on multiple factors in VC, we propose a disentangled speech representation learning framework based on adversarial learning. Four speech representations characterizing content, timbre, rhythm and pitch are extracted, and further disentangled by an adversarial Mask-And-Predict (MAP)network inspired by BERT. The adversarial network is used tominimize the correlations between the speech representations,by randomly masking and predicting one of the representationsfrom the others. Experimental results show that the proposedframework significantly improves the robustness of VC on multiple factors by increasing the speech quality MOS from 2.79 to3.30 and decreasing the MCD from 3.89 to 3.58.","score":0.7475903630256653,"polarity":"negative","source":"semantic","year":2021,"authors":["Jie Wang","Jingbei Li","Xintao Zhao","Zhiyong Wu","H. Meng"],"venue":null,"citation_count":29,"reference_count":51,"influential_citation_count":1,"corpus_id":231740427,"url":"https://www.semanticscholar.org/paper/607c18c160aa66c13314b9da1e89639b79f67bca","arxiv_id":null,"contexts":null,"background":"Factorizing speech as disentangled speech representations is vital to achieve highly controllable style transfer in voice conversion (VC). Conventional speech representation learning methods in VC only factorize speech as speaker and content, lacking controllability on other prosody-related factors. State-of-the-art speech representation learning methods for more speech factors are using primary disentangle algorithms such as random resampling and ad-hoc bottleneck layer size adjustment, which however is hard to ensure robust speech representation disentanglement.","target":"To increase the robustness of highly controllable style transfer on multiple factors in VC, we propose a disentangled speech representation learning framework based on adversarial learning. Four speech representations characterizing content, timbre, rhythm and pitch are extracted, and further disentangled by an adversarial Mask-And-Predict (MAP) network inspired by BERT. The adversarial network is used to minimize the correlations between the speech representations, by randomly masking and predicting one of the representations from the others. Experimental results show that the proposed framework significantly improves the robustness of VC on multiple factors by increasing the speech quality MOS from 2.79 to 3.30 and decreasing the MCD from 3.89 to 3.58."},{"summary":"The Related Paper, SpeechTripleNet, contrasts with the Main Paper, SelfVC, by advocating for a disentangled approach to speech representation, focusing on separating spoken content, speaker timbre, and prosody. While SelfVC emphasizes the benefits of using entangled representations and self-synthesized examples to improve voice conversion, SpeechTripleNet argues that explicit disentanglement is necessary to avoid issues like prosody leakage and degraded performance. This fundamental difference in approach highlights the Main Paper's innovative strategy of leveraging entangled representations against the Related Paper's focus on strict separation of speech factors.","paper_id":"d975f3db785ad488afbe349f6b858f53d5ae5e5b","title":"SpeechTripleNet: End-to-End Disentangled Speech Representation Learning for Content, Timbre and Prosody","abstract":"Disentangled speech representation learning aims to separate different factors of variation from speech into disjoint representations. This paper focuses on disentangling speech into representations for three factors: spoken content, speaker timbre, and speech prosody. Many previous methods for speech disentanglement have focused on separating spoken content and speaker timbre. However, the lack of explicit modeling of prosodic information leads to degraded speech generation performance and uncontrollable prosody leakage into content and/or speaker representations. While some recent methods have utilized explicit speaker labels or pre-trained models to facilitate triple-factor disentanglement, there are no end-to-end methods to simultaneously disentangle three factors using only unsupervised or self-supervised learning objectives. This paper introduces SpeechTripleNet, an end-to-end method to disentangle speech into representations for content, timbre, and prosody. Based on VAE, SpeechTripleNet restricts the structures of the latent variables and the amount of information captured in them to induce disentanglement. It is a pure unsupervised/self-supervised learning method that only requires speech data and no additional labels. Our qualitative and quantitative results demonstrate that SpeechTripleNet is effective in achieving triple-factor speech disentanglement, as well as controllable speech editing concerning different factors.","score":0.7430967092514038,"polarity":"negative","source":"semantic","year":2023,"authors":["Hui Lu","Xixin Wu","Zhiyong Wu","Helen Meng"],"venue":null,"citation_count":4,"reference_count":23,"influential_citation_count":1,"corpus_id":264492731,"url":"https://www.semanticscholar.org/paper/d975f3db785ad488afbe349f6b858f53d5ae5e5b","arxiv_id":null,"contexts":null,"background":"Disentangled speech representation learning aims to separate different factors of variation from speech into disjoint representations. This paper focuses on disentangling speech into representations for three factors: spoken content, speaker timbre, and speech prosody. Many previous methods for speech disentanglement have focused on separating spoken content and speaker timbre. However, the lack of explicit modeling of prosodic information leads to degraded speech generation performance and uncontrollable prosody leakage into content and/or speaker representations. While some recent methods have utilized explicit speaker labels or pre-trained models to facilitate triple-factor disentanglement, there are no end-to-end methods to simultaneously disentangle three factors using only unsupervised or self-supervised learning objectives.","target":"This paper introduces SpeechTripleNet, an end-to-end method to disentangle speech into representations for content, timbre, and prosody. Based on VAE, SpeechTripleNet restricts the structures of the latent variables and the amount of information captured in them to induce disentanglement. It is a pure unsupervised/self-supervised learning method that only requires speech data and no additional labels. Our qualitative and quantitative results demonstrate that SpeechTripleNet is effective in achieving triple-factor speech disentanglement, as well as controllable speech editing concerning different factors."},{"summary":"The Related Paper, AdaptVC, contrasts with the Main Paper, SelfVC, by emphasizing the importance of disentangling linguistic content and speaker characteristics for effective voice conversion. While SelfVC advocates for using entangled representations and self-synthesized examples to improve model performance, AdaptVC focuses on achieving high-quality voice conversion through adaptive learning and dynamic encoding of features, highlighting a more traditional approach to isolating content and style. Additionally, AdaptVC claims superior performance in zero-shot scenarios, suggesting that its method may be more robust compared to the iterative refinement strategy proposed in SelfVC.","paper_id":"b9c9951f5669343381b70471f86051e3ed69c5bd","title":"AdaptVC: High Quality Voice Conversion with Adaptive Learning","abstract":"The goal of voice conversion is to transform the speech of a source speaker to sound like that of a reference speaker while preserving the original content. A key challenge is to extract disentangled linguistic content from the source and voice style from the reference. While existing approaches leverage various methods to isolate the two, a generalization still requires further attention, especially for robustness in zero-shot scenarios. In this paper, we achieve successful disentanglement of content and speaker features by tuning self-supervised speech features with adapters. The adapters are trained to dynamically encode nuanced features from rich self-supervised features, and the decoder fuses them to produce speech that accurately resembles the reference with minimal loss of content. Moreover, we leverage a conditional flow matching decoder with cross-attention speaker conditioning to further boost the synthesis quality and efficiency. Subjective and objective evaluations in a zero-shot scenario demonstrate that the proposed method outperforms existing models in speech quality and similarity to the reference speech.","score":0.7290732264518738,"polarity":"negative","source":"semantic","year":2025,"authors":["Jaehun Kim","Ji-Hoon Kim","Yeunju Choi","Tan Dat Nguyen","Seongkyu Mun","Joon Son Chung"],"venue":null,"citation_count":0,"reference_count":0,"influential_citation_count":0,"corpus_id":275212001,"url":"https://www.semanticscholar.org/paper/b9c9951f5669343381b70471f86051e3ed69c5bd","arxiv_id":null,"contexts":null,"background":"The goal of voice conversion is to transform the speech of a source speaker to sound like that of a reference speaker while preserving the original content. A key challenge is to extract disentangled linguistic content from the source and voice style from the reference. While existing approaches leverage various methods to isolate the two, a generalization still requires further attention, especially for robustness in zero-shot scenarios.","target":"In this paper, we achieve successful disentanglement of content and speaker features by tuning self-supervised speech features with adapters. The adapters are trained to dynamically encode nuanced features from rich self-supervised features, and the decoder fuses them to produce speech that accurately resembles the reference with minimal loss of content. Moreover, we leverage a conditional flow matching decoder with cross-attention speaker conditioning to further boost the synthesis quality and efficiency. Subjective and objective evaluations in a zero-shot scenario demonstrate that the proposed method outperforms existing models in speech quality and similarity to the reference speech."},{"summary":"The Related Paper, S3PRL-VC, supports the Main Paper by demonstrating the effectiveness of self-supervised speech representations (S3R) in voice conversion tasks, which aligns with the Main Paper's focus on using self-synthesized examples for model improvement. Both papers emphasize the advantages of avoiding explicit disentanglement of speech attributes, with the Related Paper providing empirical evidence that S3R can achieve state-of-the-art results in voice conversion, thereby reinforcing the Main Paper's claims about the benefits of entangled representations and iterative refinement in voice conversion models.","paper_id":"2c5a410b781f90c145efac05fea235c5c3e44861","title":"S3PRL-VC: Open-Source Voice Conversion Framework with Self-Supervised Speech Representations","abstract":"This paper introduces S3PRL-VC, an open-source voice conversion (VC) framework based on the S3PRL toolkit. In the context of recognition-synthesis VC, self-supervised speech representation (S3R) is valuable in its potential to replace the expensive supervised representation adopted by state-of-the-art VC systems. Moreover, we claim that VC is a good probing task for S3R analysis. In this work, we provide a series of in-depth analyses by benchmarking on the two tasks in VCC2020, namely intra-/cross-lingual any-to-one (A2O) VC, as well as an any-to-any (A2A) setting. We also provide comparisons between not only different S3Rs but also top systems in VCC2020 with supervised representations. Systematic objective and subjective evaluation were conducted, and we show that S3R is comparable with VCC2020 top systems in the A2O setting in terms of similarity, and achieves state-of-the-art in S3R-based A2A VC. We believe the extensive analysis, as well as the toolkit itself, contribute to not only the S3R community but also the VC community. The codebase is now open-sourced1.","score":0.7603261470794678,"polarity":"positive","source":"citations","year":2021,"authors":["Wen-Chin Huang","Shu-Wen Yang","Tomoki Hayashi","Hung-yi Lee","Shinji Watanabe","T. Toda"],"venue":null,"citation_count":40,"reference_count":30,"influential_citation_count":3,"corpus_id":238743820,"url":"https://www.semanticscholar.org/paper/2c5a410b781f90c145efac05fea235c5c3e44861","arxiv_id":null,"contexts":[{"sentence":"To derive disentangled speech representations in a text-free manner, recent methods~\\citep{lakhotia2021generative,polyak2021speech,lin2021fragmentvc,huang2022s3prl,choi2021neural} have proposed to obtain speaker information from a speaker verification mode","polarity":"positive"},{"sentence":"While the representations obtained from SSL models are highly correlated with phonetic information, they also contain speaker information~\\citep{huang2022s3prl,hussain2022multi}.","polarity":"positive"},{"sentence":"}{ {l|rrr|rr|r} {c}{} & {c}{ } & {c}{ } & {c}{ } \\\\ Technique & SV-EER MATH_PLACEHOLDER & SV-Sim MATH_PLACEHOLDER & Sim-MOS MATH_PLACEHOLDER & PER MATH_PLACEHOLDER & CER MATH_PLACEHOLDER & MOS MATH_PLACEHOLDER \\\\ Real Data & MATH_PLACEHOLDER & MATH_PLACEHO","polarity":"positive"}],"background":null,"target":null},{"summary":"The Related Paper supports the Main Paper by addressing the limitations of conventional voice conversion methods, such as the degradation of speech quality and loss of spectral details. It highlights the importance of using deep neural networks (DNNs) for modeling non-linear relationships between speakers, which aligns with the Main Paper's focus on improving voice conversion through innovative training strategies. Both papers emphasize the significance of advanced modeling techniques to enhance speaker similarity and naturalness in synthesized speech, reinforcing the Main Paper's claims about the effectiveness of its SelfVC framework.","paper_id":"5cb6307f25a8af6e8081405db88dc9c837cf5b27","title":"Voice Conversion Using Deep Neural Networks With Layer-Wise Generative Training","abstract":"This paper presents a new spectral envelope conversion method using deep neural networks (DNNs). The conventional joint density Gaussian mixture model (JDGMM) based spectral conversion methods perform stably and effectively. However, the speech generated by these methods suffer severe quality degradation due to the following two factors: 1) inadequacy of JDGMM in modeling the distribution of spectral features as well as the non-linear mapping relationship between the source and target speakers, 2) spectral detail loss caused by the use of high-level spectral features such as mel-cepstra. Previously, we have proposed to use the mixture of restricted Boltzmann machines (MoRBM) and the mixture of Gaussian bidirectional associative memories (MoGBAM) to cope with these problems. In this paper, we propose to use a DNN to construct a global non-linear mapping relationship between the spectral envelopes of two speakers. The proposed DNN is generatively trained by cascading two RBMs, which model the distributions of spectral envelopes of source and target speakers respectively, using a Bernoulli BAM (BBAM). Therefore, the proposed training method takes the advantage of the strong modeling ability of RBMs in modeling the distribution of spectral envelopes and the superiority of BAMs in deriving the conditional distributions for conversion. Careful comparisons and analysis among the proposed method and some conventional methods are presented in this paper. The subjective results show that the proposed method can significantly improve the performance in terms of both similarity and naturalness compared to conventional methods.","score":0.6460428833961487,"polarity":"positive","source":"citations","year":2014,"authors":["Linghui Chen","Zhenhua Ling","Li-Juan Liu","Lirong Dai"],"venue":null,"citation_count":228,"reference_count":47,"influential_citation_count":8,"corpus_id":11313515,"url":"https://www.semanticscholar.org/paper/5cb6307f25a8af6e8081405db88dc9c837cf5b27","arxiv_id":null,"contexts":[{"sentence":"Traditionally, voice conversion models were trained as a speech-to-speech translation system on a parallel dataset containing multiple speakers saying the same utterance~\\citep{sun2015dblstm, chen2014dnn}.","polarity":"positive"}],"background":null,"target":null},{"summary":"The Related Paper supports the Main Paper by addressing the limitations of traditional voice conversion methods that rely on parallel data and explicit disentanglement of speaker and content representations. While the Main Paper introduces SelfVC, which utilizes self-synthesized examples to enhance voice conversion without losing nuanced attributes, the Related Paper presents a one-shot voice conversion approach that also operates without the need for extensive training data. Both papers emphasize the importance of effective representation learning in voice conversion, with the Related Paper's findings on meaningful speaker representations complementing the Main Paper's focus on iterative refinement and self-supervised learning.","paper_id":"c77fa76a857051a6c7deb135a45af8d4a5f32f0f","title":"One-shot Voice Conversion by Separating Speaker and Content Representations with Instance Normalization","abstract":"Recently, voice conversion (VC) without parallel data has been successfully adapted to multi-target scenario in which a single model is trained to convert the input voice to many different speakers. However, such model suffers from the limitation that it can only convert the voice to the speakers in the training data, which narrows down the applicable scenario of VC. In this paper, we proposed a novel one-shot VC approach which is able to perform VC by only an example utterance from source and target speaker respectively, and the source and target speaker do not even need to be seen during training. This is achieved by disentangling speaker and content representations with instance normalization (IN). Objective and subjective evaluation shows that our model is able to generate the voice similar to target speaker. In addition to the performance measurement, we also demonstrate that this model is able to learn meaningful speaker representations without any supervision.","score":0.6438446044921875,"polarity":"positive","source":"citations","year":2019,"authors":["Ju-Chieh Chou","Cheng-chieh Yeh","Hung-yi Lee"],"venue":null,"citation_count":230,"reference_count":36,"influential_citation_count":37,"corpus_id":119304586,"url":"https://www.semanticscholar.org/paper/c77fa76a857051a6c7deb135a45af8d4a5f32f0f","arxiv_id":null,"contexts":[{"sentence":"More recently, voice conversion systems have been developed by training neural synthesizers to reconstruct speech from disentangled representations describing linguistic content and speaker characteristics~\\citep{qian2019autovc,chou2019one}.","polarity":"positive"},{"sentence":"}{ {l|rrr|rr|r} {c}{} & {c}{ } & {c}{ } & {c}{ } \\\\ Technique & SV-EER MATH_PLACEHOLDER & SV-Sim MATH_PLACEHOLDER & Sim-MOS MATH_PLACEHOLDER & PER MATH_PLACEHOLDER & CER MATH_PLACEHOLDER & MOS MATH_PLACEHOLDER \\\\ Real Data & MATH_PLACEHOLDER & MATH_PLACEHO","polarity":"positive"}],"background":null,"target":null},{"summary":"The Related Paper, MediumVC, supports the Main Paper, SelfVC, by addressing similar challenges in voice conversion, particularly the need for effective feature decoupling. While SelfVC utilizes self-synthesized examples for iterative refinement, MediumVC proposes a two-stage reconstruction task that enhances speaker similarity and naturalness through synthetic specific-speaker speeches. Both papers emphasize the importance of maintaining the nuances of speech, such as emotion and accent, while improving conversion quality, thus reinforcing the claims made in SelfVC about the limitations of traditional disentangling methods.","paper_id":"3cdbe7e9cfc5b2b9fcfcc5cb729b40b94829df8e","title":"MediumVC: Any-to-any voice conversion using synthetic specific-speaker speeches as intermedium features","abstract":"To realize any-to-any (A2A) voice conversion (VC), most methods are to perform symmetric self-supervised reconstruction tasks (Xi to Xi), which usually results in inefficient performances due to inadequate feature decoupling, especially for unseen speakers. We propose a two-stage reconstruction task (Xi to Yi to Xi) using synthetic specific-speaker speeches as intermedium features, where A2A VC is divided into two stages: any-to-one (A2O) and one-to-Any (O2A). In the A2O stage, we propose a new A2O method: SingleVC, by employing a noval data augment strategy(pitch-shifted and duration-remained, PSDR) to accomplish Xi to Yi. In the O2A stage, MediumVC is proposed based on pre-trained SingleVC to conduct Yi to Xi. Through such asymmetrical reconstruction tasks (Xi to Yi in SingleVC and Yi to Xi in MediumVC), the models are to capture robust disentangled features purposefully. Experiments indicate MediumVC can enhance the similarity of converted speeches while maintaining a high degree of naturalness.","score":0.5942878723144531,"polarity":"positive","source":"citations","year":2021,"authors":["Yewei Gu","Zhenyu Zhang","Xiaowei Yi","Xianfeng Zhao"],"venue":null,"citation_count":15,"reference_count":22,"influential_citation_count":2,"corpus_id":238408297,"url":"https://www.semanticscholar.org/paper/3cdbe7e9cfc5b2b9fcfcc5cb729b40b94829df8e","arxiv_id":null,"contexts":[{"sentence":"To remove speaker information from the SSL model outputs, some techniques utilize an information bottleneck approach such as quantization~\\citep{polyak2021speech,lakhotia2021generative,gu2021mediumvc}.","polarity":"positive"},{"sentence":"One line of research~\\citep{polyak2021speech,lee2021voicemixer,lakhotia2021generative,gu2021mediumvc} aiming to disentangle the speaker and content representations, proposes an information bottleneck approach to quantize SSL model outputs thereby limiting ","polarity":"positive"},{"sentence":"}{ {l|rrr|rr|r} {c}{} & {c}{ } & {c}{ } & {c}{ } \\\\ Technique & SV-EER MATH_PLACEHOLDER & SV-Sim MATH_PLACEHOLDER & Sim-MOS MATH_PLACEHOLDER & PER MATH_PLACEHOLDER & CER MATH_PLACEHOLDER & MOS MATH_PLACEHOLDER \\\\ Real Data & MATH_PLACEHOLDER & MATH_PLACEHO","polarity":"positive"}],"background":null,"target":null},{"summary":"The Related Paper, FragmentVC, supports the Main Paper, SelfVC, by demonstrating a similar approach to voice conversion that avoids explicit disentanglement of speaker and content attributes, thereby minimizing information loss. Both papers emphasize the use of self-supervised learning and reconstruction loss without requiring parallel data, showcasing innovative training strategies that enhance speaker similarity and naturalness in voice conversion. Additionally, FragmentVC's success in any-to-any voice conversion complements SelfVC's achievements in zero-shot scenarios, reinforcing the effectiveness of their respective methodologies.","paper_id":"0202e13a9d49b40b934e16ae6a095d0f0ecbc99f","title":"Fragmentvc: Any-To-Any Voice Conversion by End-To-End Extracting and Fusing Fine-Grained Voice Fragments with Attention","abstract":"Any-to-any voice conversion aims to convert the voice from and to any speakers even unseen during training, which is much more challenging compared to one-to-one or many-to-many tasks, but much more attractive in real-world scenarios. In this paper we proposed FragmentVC, in which the latent phonetic structure of the utterance from the source speaker is obtained from Wav2Vec 2.0, while the spectral features of the utterance(s) from the target speaker are obtained from log mel-spectrograms. By aligning the hidden structures of the two different feature spaces with a two-stage training process, FragmentVC is able to extract fine-grained voice fragments from the target speaker utterance(s) and fuse them into the desired utterance, all based on the attention mechanism of Transformer as verified with analysis on attention maps, and is accomplished end-to-end. This approach is trained with reconstruction loss only without any disentanglement considerations between content and speaker information and doesn't require parallel data. Objective evaluation based on speaker verification and subjective evaluation with MOS both showed that this approach outperformed SOTA approaches, such as AdaIN-VC and AutoVC.","score":0.5874308347702026,"polarity":"positive","source":"citations","year":2020,"authors":["Yist Y. Lin","C. Chien","Jheng-hao Lin","Hung-yi Lee","Lin-Shan Lee"],"venue":null,"citation_count":77,"reference_count":28,"influential_citation_count":15,"corpus_id":225076127,"url":"https://www.semanticscholar.org/paper/0202e13a9d49b40b934e16ae6a095d0f0ecbc99f","arxiv_id":null,"contexts":[{"sentence":"To derive disentangled speech representations in a text-free manner, recent methods~\\citep{lakhotia2021generative,polyak2021speech,lin2021fragmentvc,huang2022s3prl,choi2021neural} have proposed to obtain speaker information from a speaker verification mode","polarity":"positive"},{"sentence":"}{ {l|rrr|rr|r} {c}{} & {c}{ } & {c}{ } & {c}{ } \\\\ Technique & SV-EER MATH_PLACEHOLDER & SV-Sim MATH_PLACEHOLDER & Sim-MOS MATH_PLACEHOLDER & PER MATH_PLACEHOLDER & CER MATH_PLACEHOLDER & MOS MATH_PLACEHOLDER \\\\ Real Data & MATH_PLACEHOLDER & MATH_PLACEHO","polarity":"positive"}],"background":null,"target":null},{"summary":"The Related Paper, ACE-VC, contrasts with the Main Paper, SelfVC, by emphasizing the importance of explicitly disentangled speech representations for voice conversion, while SelfVC advocates for using entangled representations to avoid information loss. ACE-VC employs a multi-task model and Siamese networks to decompose speech into distinct features, whereas SelfVC focuses on iterative refinement using self-synthesized examples without explicit disentanglement. Additionally, ACE-VC claims state-of-the-art results through its controlled synthesis approach, which differs from SelfVC's methodology that relies on self-supervised learning for continuous model improvement.","paper_id":"5c767b6b026405ee8e342983ee613989494ff97f","title":"ACE-VC: Adaptive and Controllable Voice Conversion Using Explicitly Disentangled Self-Supervised Speech Representations","abstract":"In this work, we propose a zero-shot voice conversion method using speech representations trained with self-supervised learning. First, we develop a multi-task model to decompose a speech utterance into features such as linguistic content, speaker characteristics, and speaking style. To disentangle content and speaker representations, we propose a training strategy based on Siamese networks that encourages similarity between the content representations of the original and pitch-shifted audio. Next, we develop a synthesis model with pitch and duration predictors that can effectively reconstruct the speech signal from its decomposed representation. Our framework allows controllable and speaker-adaptive synthesis to perform zero-shot any-to-any voice conversion achieving state-of-the-art results on metrics evaluating speaker similarity, intelligibility, and naturalness. Using just 10 seconds of data for a target speaker, our framework can perform voice swapping and achieves a speaker verification EER of 5.5% for seen speakers and 8.4% for unseen speakers. 1","score":0.7647701501846313,"polarity":"negative","source":"citations","year":2023,"authors":["Shehzeen Samarah Hussain","Paarth Neekhara","Jocelyn Huang","Jason Li","Boris Ginsburg"],"venue":null,"citation_count":19,"reference_count":28,"influential_citation_count":0,"corpus_id":256900878,"url":"https://www.semanticscholar.org/paper/5c767b6b026405ee8e342983ee613989494ff97f","arxiv_id":null,"contexts":[{"sentence":"While some techniques~\\citep{defossez2022highfi,eloff2019unsupervised,singingspeechcodec,kumar2023highfidelity} aim to compress speech into a data-efficient codec, another line of research has focused on disentangling the learned features into components s","polarity":"negative"},{"sentence":"Alternatively, several researchers have proposed training strategies that employ an information perturbation technique to eliminate speaker information without quantization~\\citep{qian2022contentvec,choi2021neural,choi2023nansy,hussain2023ace}.","polarity":"negative"},{"sentence":"}{ {l|rrr|rr|r} {c}{} & {c}{ } & {c}{ } & {c}{ } \\\\ Technique & SV-EER MATH_PLACEHOLDER & SV-Sim MATH_PLACEHOLDER & Sim-MOS MATH_PLACEHOLDER & PER MATH_PLACEHOLDER & CER MATH_PLACEHOLDER & MOS MATH_PLACEHOLDER \\\\ Real Data & MATH_PLACEHOLDER & MATH_PLACEHO","polarity":"negative"},{"sentence":"}{ {l|rr|rr|rr} {c}{} & {c}{ } & {c}{ } & {c}{ } \\\\ Technique & SV-EER MATH_PLACEHOLDER & PER MATH_PLACEHOLDER & SV-EER MATH_PLACEHOLDER & PER MATH_PLACEHOLDER & SV-EER MATH_PLACEHOLDER & PER MATH_PLACEHOLDER \\\\ Real Data & MATH_PLACEHOLDER & MATH_PLACEHOL","polarity":"negative"},{"sentence":"[h] } {l|rrr|rrr} {c}{} & {c}{ } & {c}{ } \\\\ Technique & SV-EER MATH_PLACEHOLDER & PER MATH_PLACEHOLDER & CER MATH_PLACEHOLDER & SV-EER MATH_PLACEHOLDER & PER MATH_PLACEHOLDER & CER MATH_PLACEHOLDER \\\\ Real Data & MATH_PLACEHOLDER & MATH_PLACEHOLDER & MATH","polarity":"negative"}],"background":null,"target":null},{"summary":"The Related Paper, AGAIN-VC, contrasts with the Main Paper, SelfVC, by emphasizing the use of disentangled representations to separate speaker and linguistic content information, which the Main Paper argues can lead to information loss. While SelfVC advocates for a framework that utilizes entangled speech representations and self-synthesized examples for iterative refinement, AGAIN-VC focuses on a one-shot voice conversion approach that employs Activation Guidance and Adaptive Instance Normalization to enhance synthesis quality without compromising speaker similarity. This fundamental difference in methodology highlights the contrasting philosophies regarding the balance between representation disentanglement and synthesis quality in voice conversion.","paper_id":"ac38d44780aea27b41b041f8557a4b7f95490f12","title":"Again-VC: A One-Shot Voice Conversion Using Activation Guidance and Adaptive Instance Normalization","abstract":"Recently, voice conversion (VC) has been widely studied. Many VC systems use disentangle-based learning techniques to separate the speaker and the linguistic content information from a speech signal. Subsequently, they convert the voice by changing the speaker information to that of the target speaker. To prevent the speaker information from leaking into the content embeddings, previous works either reduce the dimension or quantize the content embedding as a strong information bottleneck. These mechanisms somehow hurt the synthesis quality. In this work, we propose AGAIN-VC, an innovative VC system using Activation Guidance and Adaptive Instance Normalization. AGAIN-VC is an auto-encoder-based model, comprising of a single encoder and a decoder. With a proper activation as an information bottleneck on content embeddings, the trade-off between the synthesis quality and the speaker similarity of the converted speech is improved drastically. This one-shot VC system obtains the best performance regardless of the subjective or objective evaluations.","score":0.744779646396637,"polarity":"negative","source":"citations","year":2020,"authors":["Yen-Hao Chen","Da-Yi Wu","Tsung-Han Wu","Hung-yi Lee"],"venue":null,"citation_count":105,"reference_count":27,"influential_citation_count":21,"corpus_id":226226852,"url":"https://www.semanticscholar.org/paper/ac38d44780aea27b41b041f8557a4b7f95490f12","arxiv_id":null,"contexts":[{"sentence":"While some techniques~\\citep{defossez2022highfi,eloff2019unsupervised,singingspeechcodec,kumar2023highfidelity} aim to compress speech into a data-efficient codec, another line of research has focused on disentangling the learned features into components s","polarity":"negative"}],"background":null,"target":null},{"summary":"The Related Paper contrasts the Main Paper by proposing a one-shot voice conversion method that relies on vector quantization without supervision on speaker labels, emphasizing a strong ability to disentangle content and speaker information solely through reconstruction loss. In contrast, the Main Paper focuses on an iterative refinement approach using self-synthesized examples, which intentionally avoids explicit disentanglement of speech attributes to preserve nuances like accent and emotion. This fundamental difference highlights the Main Paper's innovative strategy of leveraging entangled representations versus the Related Paper's reliance on discrete coding for voice conversion.","paper_id":"097067541e0d5c140104ce21a8bb187c73b41b10","title":"One-Shot Voice Conversion by Vector Quantization","abstract":"In this paper, we propose a vector quantization (VQ) based one-shot voice conversion (VC) approach without any supervision on speaker label. We model the content embedding as a series of discrete codes and take the difference between quantize-before and quantize-after vector as the speaker embedding. We show that this approach has a strong ability to disentangle the content and speaker information with reconstruction loss only, and one-shot VC is thus achieved.","score":0.6658097505569458,"polarity":"negative","source":"citations","year":2020,"authors":["Da-Yi Wu","Hung-yi Lee"],"venue":null,"citation_count":82,"reference_count":18,"influential_citation_count":1,"corpus_id":216481162,"url":"https://www.semanticscholar.org/paper/097067541e0d5c140104ce21a8bb187c73b41b10","arxiv_id":null,"contexts":[{"sentence":"While some techniques~\\citep{defossez2022highfi,eloff2019unsupervised,singingspeechcodec,kumar2023highfidelity} aim to compress speech into a data-efficient codec, another line of research has focused on disentangling the learned features into components s","polarity":"negative"}],"background":null,"target":null},{"summary":"The Related Paper, KaraSinger, contrasts with the Main Paper, SelfVC, by focusing on singing voice synthesis rather than voice conversion. While SelfVC emphasizes iterative refinement using self-synthesized examples to improve speaker similarity in voice conversion, KaraSinger employs a vector-quantized variational autoencoder to synthesize singing voices without explicit score input, relying instead on a language model for alignment. Additionally, KaraSinger's approach to prosody and melody generation is spontaneous and machine-driven, differing from SelfVC's method of refining entangled speech representations.","paper_id":"14f57efb445571f1bb16fb4e685eb69788f80fb9","title":"KaraSinger: Score-Free Singing Voice Synthesis with VQ-VAE Using Mel-Spectrograms","abstract":"In this paper, we propose a novel neural network model called KaraSinger for a less-studied singing voice synthesis (SVS) task named score-free SVS, in which the prosody and melody are spontaneously decided by machine. KaraSinger comprises a vector-quantized variational autoencoder (VQ-VAE) that compresses the Mel-spectrograms of singing audio to sequences of discrete codes, and a language model (LM) that learns to predict the discrete codes given the corresponding lyrics. For the VQ-VAE part, we employ a Connectionist Temporal Classification (CTC) loss to encourage the discrete codes to carry phoneme-related information. For the LM part, we use location-sensitive attention for learning a robust alignment between the input phoneme sequence and the output discrete code. We keep the architecture of both the VQ-VAE and LM light-weight for fast training and inference speed. We validate the effectiveness of the proposed design choices using a proprietary collection of 550 English pop songs sung by multiple amateur singers. The result of a listening test shows that KaraSinger achieves high scores in intelligibility, musicality, and the overall quality.","score":0.5212312340736389,"polarity":"negative","source":"citations","year":2021,"authors":["Chien-Feng Liao","Jen-Yu Liu","Yi-Hsuan Yang"],"venue":null,"citation_count":4,"reference_count":37,"influential_citation_count":0,"corpus_id":238531555,"url":"https://www.semanticscholar.org/paper/14f57efb445571f1bb16fb4e685eb69788f80fb9","arxiv_id":null,"contexts":[{"sentence":"While some techniques~\\citep{defossez2022highfi,eloff2019unsupervised,singingspeechcodec,kumar2023highfidelity} aim to compress speech into a data-efficient codec, another line of research has focused on disentangling the learned features into components s","polarity":"negative"}],"background":null,"target":null},{"summary":"The Related Paper contrasts with the Main Paper by focusing on high-fidelity audio compression using a neural codec, rather than voice conversion. While the Main Paper emphasizes iterative refinement of voice conversion models through self-synthesized examples, the Related Paper highlights a novel loss balancing mechanism and a streaming architecture for audio encoding. Additionally, the Related Paper's approach is centered on reducing artifacts and improving audio quality across various domains, which diverges from the Main Paper's goal of enhancing speaker similarity and naturalness in voice conversion.","paper_id":"cdcfeb447fa8554c131c0a13a7ffcba30c0381e1","title":"High Fidelity Neural Audio Compression","abstract":"We introduce a state-of-the-art real-time, high-fidelity, audio codec leveraging neural networks. It consists in a streaming encoder-decoder architecture with quantized latent space trained in an end-to-end fashion. We simplify and speed-up the training by using a single multiscale spectrogram adversary that efficiently reduces artifacts and produce high-quality samples. We introduce a novel loss balancer mechanism to stabilize training: the weight of a loss now defines the fraction of the overall gradient it should represent, thus decoupling the choice of this hyper-parameter from the typical scale of the loss. Finally, we study how lightweight Transformer models can be used to further compress the obtained representation by up to 40%, while staying faster than real time. We provide a detailed description of the key design choices of the proposed model including: training objective, architectural changes and a study of various perceptual loss functions. We present an extensive subjective evaluation (MUSHRA tests) together with an ablation study for a range of bandwidths and audio domains, including speech, noisy-reverberant speech, and music. Our approach is superior to the baselines methods across all evaluated settings, considering both 24 kHz monophonic and 48 kHz stereophonic audio. Code and models are available at github.com/facebookresearch/encodec.","score":0.4175361692905426,"polarity":"negative","source":"citations","year":2022,"authors":["Alexandre D'efossez","Jade Copet","Gabriel Synnaeve","Yossi Adi"],"venue":null,"citation_count":546,"reference_count":76,"influential_citation_count":98,"corpus_id":253097788,"url":"https://www.semanticscholar.org/paper/cdcfeb447fa8554c131c0a13a7ffcba30c0381e1","arxiv_id":null,"contexts":[{"sentence":"While some techniques~\\citep{defossez2022highfi,eloff2019unsupervised,singingspeechcodec,kumar2023highfidelity} aim to compress speech into a data-efficient codec, another line of research has focused on disentangling the learned features into components s","polarity":"negative"}],"background":null,"target":null}],"paper":{"title":"SelfVC: Voice Conversion With Iterative Refinement using Self Transformations","abstract":"We propose SelfVC, a training strategy to iteratively improve a voice conversion model with self-synthesized examples. Previous efforts on voice conversion focus on factorizing speech into explicitly disentangled representations that separately encode speaker characteristics and linguistic content. However, disentangling speech representations to capture such attributes using task-specific loss terms can lead to information loss by discarding finer nuances such as accent and emotion of the original signal. In this work, instead of explicitly disentangling attributes with loss terms, we present a framework to train a controllable voice conversion model on entangled speech representations derived from self-supervised learning (SSL) and speaker verification models. First, we develop techniques to derive prosodic information from the audio signal and SSL representations to train predictive submodules in the synthesis model. Next, we propose a training strategy to iteratively improve the synthesis model for voice conversion, by creating a challenging training objective using self-synthesized examples. In this training approach, the current state of the synthesis model is used to generate voice-converted variations of an utterance, which serve as inputs for the reconstruction task, ensuring a continuous and purposeful refinement of the model. We demonstrate that incorporating such self-synthesized examples during training improves the speaker similarity of generated speech as compared to a baseline voice conversion model trained solely on heuristically perturbed inputs. Our framework is trained without any text and is applicable to a range of tasks such as zero-shot voice conversion, voice conversion across different languages, and controllable speech synthesis with pitch and pace modifications. We conduct extensive comparisons against prior work and find that SelfVC achieves state-of-the-art results in zero-shot voice conversion on metrics evaluating naturalness, speaker similarity, and intelligibility of synthesized audio.","authors":["Paarth Neekhara","Shehzeen Samarah Hussain","Rafael Valle","Boris Ginsburg","Rishabh Ranjan","Shlomo Dubnov","Farinaz Koushanfar","Julian McAuley"],"sections":[{"heading":"Introduction","text":"# Introduction {#sec:introduction}\n\nDeriving meaningful representations from speech has been a topic of significant interest because such representations can be useful for both downstream recognition and upstream speech generation tasks. While some techniques [@defossez2022highfi; @eloff2019unsupervised; @singingspeechcodec; @kumar2023highfidelity] aim to compress speech into a data-efficient codec, another line of research has focused on disentangling the learned features into components such as speaker characteristics (voice or timbre), linguistic content (phonetic information) and prosodic information (pitch modulation and speaking rate)  [@chou2019one; @qian2019autovc; @wu2020one; @chen2021again; @qian2022contentvec; @hussain2023ace]. Representation disentanglement allows controllable speech synthesis by training a model to reconstruct the audio from the disentangled features. During inference, the relevant disentangled representations can be modified for performing tasks like voice conversion (changing the speaker of an utterance) or changing the prosody.\n\nTo derive disentangled speech representations in a text-free manner, recent methods [@lakhotia2021generative; @polyak2021speech; @lin2021fragmentvc; @huang2022s3prl; @choi2021neural] have proposed to obtain speaker information from a speaker verification model and linguistic content information from the output of models trained using self-supervised learning (SSL) [@wav2vec2; @gulati2020conformer]. While the representations obtained from SSL models are highly correlated with phonetic information, they also contain speaker information [@huang2022s3prl; @hussain2022multi]. To remove speaker information from the SSL model outputs, some techniques utilize an information bottleneck approach such as quantization [@polyak2021speech; @lakhotia2021generative; @gu2021mediumvc]. Alternatively, several researchers have proposed training strategies that employ an information perturbation technique to eliminate speaker information without quantization [@qian2022contentvec; @choi2021neural; @choi2023nansy; @hussain2023ace]. Notably, for training synthesizers, NANSY [@choi2021neural] and NANSY++ [@choi2023nansy] propose to heuristically perturb the voice of a given utterance with hand-engineered data augmentations, before obtaining the output from the SSL model. To reconstruct the original audio accurately, the synthesizer is forced to derive the speaker characteristics from the speaker embedding since the speaker information in the SSL model's output is perturbed. While such techniques are effective, heuristic voice perturbation algorithms based on pitch randomization and formant shifting represent a very limited set of transformations. We hypothesize that such training strategies can be improved by utilizing neural network-generated augmentations.\n\nWhile SSL based VC models do not require transcriptions during training, they lack the ability to explicitly control prosody due to the challenge of estimating durations from SSL features. Conversely, models that have the ability to explicitly control prosody lack the ability to use SSL, making it extremely hard to support multiple languages. What if we could combine the benefits of each approach to circumvent their weakness, and use iterative refinement to obtain better results?\n\nIn this work, we propose SelfVC, a learning framework to automatically generate diverse data transformations during training and enable controllable speech synthesis from imperfectly disentangled but uncompressed speech representations. First, we propose a feature extraction pipeline to derive SSL representations, speaker embeddings and prosodic information from a given audio signal. Next, we design a synthesis model to reconstruct a given utterance from the SSL features and speaker embedding, while using the fundamental frequency contour and duration as targets for training intermediate submodules. Finally, to train an effective voice conversion model, we propose a training strategy that utilizes the synthesis model itself to create challenging voice-converted transformations of a given speech utterance. At any given training iteration, the current state of the synthesis model is used to transform the input SSL features and the model is updated to minimize the reconstruction error of the original utterance.\n\nAll the components in our framework are trained in a text-free manner requiring only audio data. Once trained, our framework can be used for tasks such as zero-shot voice conversion, audio reconstruction with pitch and duration modulation as well as multilingual voice conversion across languages outside of the training set. On metrics evaluating speaker similarity, intelligibility and naturalness of synthesized speech we demonstrate that our model outperforms previously proposed zero-shot voice conversion methods. The main contributions of our work are:\n\n1.  We develop a training strategy using self transformations to train a voice conversion model on imperfectly disentangled representations, resulting in considerable improvement in speaker similarity metrics as compared to a model trained only with heuristic transformations.\n\n2.  We propose techniques to derive prosodic information from uncompressed SSL feature vectors and use the derived information to train a controllable synthesizer that can either mimic the prosody of a source utterance or adapt the prosody given a target speaker.\n\n3.  Our models are trained in a text-free manner and independent of phonetic posteriograms, hence making it simple and efficient to scale up the training data, including other languages.\n\n4.  SelfVC achieves state-of-the-art results in zero-shot any-to-any voice conversion in English. When fine-tuned on a few hours of multi-lingual data, SelfVC outperforms prior voice conversion methods on the cross-lingual voice conversion task."},{"heading":"Related work","text":"# Related work {#sec:related_work}\n\n**Voice conversion:** Voice conversion is the task of modifying an utterance of a source speaker to match the vocal qualities of a target speaker. Traditionally, voice conversion models were trained as a speech-to-speech translation system on a parallel dataset containing multiple speakers saying the same utterance [@sun2015dblstm; @chen2014dnn]. More recently, voice conversion systems have been developed by training neural synthesizers to reconstruct speech from disentangled representations describing linguistic content and speaker characteristics [@qian2019autovc; @chou2019one]. For example,  [@sun2016phonetic; @tian2018average] have utilized pre-trained automatic speech recognition (ASR) and speaker verification (SV) models to disentangle content and speaker information respectively. The predicted text or phonetic posteriogram (PPG) obtained from the ASR model is taken as the content representation. However, such voice conversion systems have limitations: 1) Training such systems requires transcribed speech data and the synthesis is limited to the language the model is trained on. 2) Text and PPG do not capture all linguistic features such as accent, expressions, emotions or speaker-independent style resulting in neutral-sounding synthesized speech.\n\nTo derive linguistic content in a text-free manner, some prior works have utilized SSL based models. However, as noted by prior work [@polyak2021speech; @huang2022s3prl], SSL model outputs do not necessarily separate speaker and content information. One line of research [@polyak2021speech; @lee2021voicemixer; @lakhotia2021generative; @gu2021mediumvc] aiming to disentangle the speaker and content representations, proposes an information bottleneck approach to quantize SSL model outputs thereby limiting the information to only capture the content or pseudo-text of the audio. However, the loss of information during such a quantization approach leads to sub-optimal reconstruction quality. Moreover, information bottleneck by itself does not guarantee disentanglement.\n\nAddressing the limitations of information bottleneck approaches, researchers have proposed training strategies based on heuristic transformations. For example, in ContentVec [@qian2022contentvec] and ACE-VC [@hussain2023ace], while training the SSL-based feature extractor model, the audio is transformed using pitch-shift transformation and the SSL model is trained to output similar representations for the original and transformed audio. Alternatively, in NANSY [@choi2021neural], the transformations are applied while training the synthesizer, i.e. the synthesizer is tasked to reconstruct the original audio from the speaker embedding of the original audio and the SSL features of audio perturbed using transforms such as formant-shift, pitch-randomization and randomized frequency shaping. Although these heuristic transformations serve as a reasonable proxy for voice conversion methods, we hypothesize such methods can be greatly improved by utilizing the voice conversion system itself to generate more diverse input transformations.\n\n**Transformation invariant representation learning:** In unsupervised representation learning, prior work has investigated methods to learn representations that are invariant to various input transformations [@bachman2019learning; @misra2020self]. Several techniques addressing this challenge utilize domain-specific and hand-engineered data augmentation methods for training transformation invariant representation encoders [@chen2020simple; @caron2020unsupervised; @tian2020makes; @grill2020bootstrap; @misra2020self]. More recently, [@tamkinviewmaker] proposed to train generative models to produce diverse views from a given input by adding a bounded perturbation. Their results demonstrate that neural generative models can produce a more diverse set of input distortions (compared to hand-engineered augmentations) without requiring domain-specific knowledge. While these techniques have proven valuable for learning transformation-invariant representations in downstream recognition tasks, their applicability in upstream generative tasks remains unexplored. In contrast, we develop a novel framework for training a controllable synthesis model using self-generated input transformations, without the need for additional networks for data augmentation."},{"heading":"Method","text":"# Method {#sec:methodology}\n\nOur goal is to design a voice conversion framework that can modify the voice of a given utterance, while also providing control over the prosody of the synthesized speech. To this end, our framework consists of two main components: (1) A feature extractor that derives content (linguistic features), speaker embedding and prosody information from a given speech utterance (Section [3.1](#sec:featureextraction){reference-type=\"ref\" reference=\"sec:featureextraction\"}); and (2) A synthesizer model that reconstructs the audio from the derived representations (Section [3.2](#sec:synth){reference-type=\"ref\" reference=\"sec:synth\"}). To allow controllable synthesis from imperfectly disentangled representations, we propose a training strategy that challenges the model to reconstruct the audio from self-generated perturbations of the content representation (Section [3.3](#sec:selfrefine){reference-type=\"ref\" reference=\"sec:selfrefine\"}). Specifically, we train the model to reconstruct the audio from the content representation of a heuristically modified or self transformed audio, while preserving the speaker and style representations. The content and speaker encoder networks remain frozen during synthesis model training. Figure [1](#figs:maindiag){reference-type=\"ref\" reference=\"figs:maindiag\"} provides an overview of our voice conversion framework and the synthesizer training procedure.\n\n## Feature Extraction {#sec:featureextraction}\n\nThe overview of the feature extraction pipeline is shown in Figure [2](#figs:extractorsynthesizer){reference-type=\"ref\" reference=\"figs:extractorsynthesizer\"} (a). We derive the following features from an audio signal to train our synthesis models.\n\n**Content Embedding:** We define content as a temporal feature that encodes the linguistic information of a given speech utterance. We use the output of the Conformer-SSL [@gulati2020conformer] model ($G_c$) as the content representation of speech ($z$). The Conformer-SSL model is a convolution-augmented transformer architecture that is trained to reconstruct the masked areas of the mel-spectrogram on English speech data, using contrastive and masked language modelling (MLM) losses (Refer to Appendix [7](#sec:modelarch){reference-type=\"ref\" reference=\"sec:modelarch\"} for model details). Given a speech utterance as a sequence of mel-spectrogram frames $x=x_1 \\dots x_T$, the Conformer-SSL model outputs a temporally downsampled sequence of feature vectors $z=G_c(x)=z_1 \\dots z_{T'}$. While $z$ primarily encodes phonetic information, it also encompasses speaker and prosodic information. We explain our approach to address this challenge for training a voice conversion model in Section [3.3](#sec:selfrefine){reference-type=\"ref\" reference=\"sec:selfrefine\"}.\n\n![SelfVC Overview: The synthesizer $G_{\\textit{synth}}$ is trained to reconstruct the mel-spectrogram from SSL-based content representation of a transformed audio and speaker embedding of the original audio. The transformation function is either a heuristic transform or a voice-converted audio generated using self-synthesis with a different speaker embedding. ](figures/NeuripsMainDiagram_ver3.pdf){#figs:maindiag width=\"80%\"}\n\n**Duration:** Duration or rhythm characterizes the speaking rate at a granular level, that is, how long the speaker vocalizes each phoneme of a given utterance. Accurate modelling of rhythm during synthesis is important to capture the nuances between the different speakers, accents and emotions. Since SSL representations have a high correlation with phonemes [@wav2vec2; @gulati2020conformer], we conjecture that if a phoneme is emphasized in an utterance, the consecutive content vectors at the corresponding timesteps will have high similarity. Therefore, we group together consecutive content vectors with cosine similarity higher than a threshold $\\tau$, and set the target duration for the averaged vector as the number of grouped vectors multiplied by the duration of a single vector. That is, we process the content representation $z=z_1 \\dots z_{T'}$ into a duration-augmented content representation $z'=z'_1 \\dots z'_{\\hat{T}}$ and $d'=d'_1 \\dots d'_{\\hat{T}}$ where $\\hat{T} \\leq T'$ and $d'_{t}$ represents the duration of $z'_{t}$. This similarity based grouping approach is analogous to prior approaches [@lee2021voicemixer; @qian2021global]. We refer readers to Algorithm [\\[alg:grouping\\]](#alg:grouping){reference-type=\"ref\" reference=\"alg:grouping\"} in the Appendix which details our approach to obtain $z', d'$ and highlights key differences with prior methods.\n\n**Speaker Embedding:** The speaker embeddings in our setup are derived from the TitaNet [@titanet22] speaker verification model ($G_s$). The speaker verification model is trained to differentiate speakers and generate similar embeddings for utterances from the same speaker. The output from the TitaNet speaker verification model is a $192$ dimensional speaker embedding $s=G_s(x)$. We provide more details on this model in the Appendix [7](#sec:modelarch){reference-type=\"ref\" reference=\"sec:modelarch\"}.\n\n**Pitch Contour:** The pitch contour $p$ is derived from the fundamental frequency $f_0$ contour of the speech signal that represents the prosodic modulations over time. The raw values in the fundamental frequency contour (derived from PYin algorithm [@pyin]) are speaker-dependent, therefore $f_0$ is not strictly disentangled from the speaker information. To ensure that the pitch contour only encodes the intonation and not the speaker identity, we normalize $f_0$ using the mean ($f_{\\textit{mean}}$) and standard deviation ($f_{\\textit{std}}$) of all pitch contours of the given speaker. That is, $p=(f_0 - f_{\\textit{mean}})/f_{\\textit{std}}$.\n\n## Synthesizer {#sec:synth}\n\nThe task of the synthesizer is to first reconstruct the ground-truth mel-spectrogram from the extracted speech representations and then vocode the mel-spectrogram into a listenable audio waveform. For vocoding, we use a HiFiGAN [@kong2020hifi] vocoder, which is trained separately on spectrogram and waveform pairs from a multi-speaker dataset.\n\nOur mel-spectrogram synthesizer $G_{\\textit{synth}}$ is composed of two feed-forward transformers $F_e$ and $F_d$ and intermediate modules to predict the duration and pitch contour similar to [@lancucki2021fastpitch] but operates on the grouped content representation $z'=z'_1 \\dots z'_{T'}$ instead of text. The speaker embedding $s$ is repeated across all time-steps and concatenated with each $z'_t$ to be fed as input to the first feed-forward transformer $F_e$. The hidden representation from $F_e$ is then used to predict the duration and pitch contour, that is: $h=F_e(z', s)$; $\\hat{y_d} = \\textit{DurationPredictor}(h)$, $\\hat{y_p} = \\textit{PitchPredictor}(h)$. The pitch contour is projected and averaged over each time-step of the hidden representation $h$ and added to $h$ to get $k = h + \\textit{PitchEmbedding}(p)$. Finally, $k$ is discretely upsampled as per the ground-truth duration $d'$ and fed as input to the second transformer $F_d$ to get the predicted mel-spectrogram $\\hat{y} = F_d( \\textit{DurationRegulation}(k, d') )$. Our model is trained to optimize three losses --- mel-reconstruction error, pitch prediction error and duration prediction error such that $$L_\\textit{synth} = \\lVert \\hat{{y}} - {y}\\rVert^2_2 + \n    \\lambda_1      \\lVert \\hat{{y_p}} - {p}\\rVert^2_2 + \n    \\lambda_2      \\lVert \\hat{{y_d}} - {d'}\\rVert^2_2 \n\\label{eq:trainingobjective}$$\n\nDuring inference, we can use either the predicted pitch and duration, in which case the prosody is derived from both the content and speaker embeddings; or we can mimic the prosody and speaking rate of the source utterance by using ground-truth duration and pitch.\n\n![(a) Feature Extraction: The feature extractor derives the duration augmented content information from an SSL model, pitch contour using PYin algorithm and speaker embedding from a speaker verification model. (b) Mel Spectrogram Synthesizer: reconstructs the mel-spectrogram from the derived features. ](figures/NeuripsFeatureExtractor_horiz.pdf){#figs:extractorsynthesizer width=\"80%\"}\n\n## Synthesizer Training: Iterative Refinement using Self Transformations {#sec:selfrefine}\n\nWhile the mel-spectrogram can be accurately reconstructed from a synthesizer trained using the objective given by Equation [\\[eq:trainingobjective\\]](#eq:trainingobjective){reference-type=\"ref\" reference=\"eq:trainingobjective\"}, during inference, we cannot effectively modify the voice of a given utterance. This is because the content representation $z'$ is not strictly disentangled from the speaker information. To address this challenge, past works [@choi2021neural; @choi2023nansy], have proposed an information perturbation based training strategy as follows: Instead of feeding the content embedding of the original audio as the input, the audio is perturbed to synthetically modify the speaker characteristics using formant-shifting, pitch-randomization and randomized frequency shaping transforms to obtain $x_{p}=g_{\\textit{heuristic}}(x)$. Next, the content embedding is derived from the perturbed audio $z'=G_{c}(x_{p})$, while the speaker embedding is still derived from the original audio $s=G_s(x)$. The network is then tasked to reconstruct the original audio from $z'$ and $s$. While heuristically perturbed content representations play a crucial role in enhancing the synthesizer model's attention towards the speaker embedding, they are limited in terms of the range of transformations they can introduce. Heuristic transformations represent only a subset of the potential natural variations that can occur during voice conversion.\n\nTo expand on the heuristic set of transforms, we propose to utilize the synthesizer model itself to generate a voice-converted variation of a given utterance $x$. That is, given a synthesizer model $G_{\\textit{synth}}^i$ trained until training iteration $i$, we obtain a self transformed audio for iteration $i+1$ as: $$\\begin{aligned}\n& x_p=g_{\\textit{self}}(x) = G_{\\textit{synth}}^i( G_{c}(x), s') \n\\label{eq:selftransform}\n\\end{aligned}$$ where $G_{c}(x)$ is the content embedding of the original audio $x$ and $s'$ is the speaker embedding obtained from an utterance $x'$ of a different randomly selected speaker, that is, $s'=G_s(x')$. The content embedding input for the training step $i+1$ is then derived as $z'=G_{c}(x_{p})$.\n\nSelf transformations not only provide a more diverse set of transformations but also present an increasingly challenging reconstruction task for the synthesizer, as its voice conversion capabilities improve with each training iteration. Figure [1](#figs:maindiag){reference-type=\"ref\" reference=\"figs:maindiag\"} demonstrates the proposed self transformation training strategy. In our experiments, we begin self transformations after $100k$ mini-batch iterations of training with heuristically modified audio. Thereafter, we get a reasonable initialization for a voice conversion model, and we start using self transformations to obtain $x_p$ as per Equation [\\[eq:selftransform\\]](#eq:selftransform){reference-type=\"ref\" reference=\"eq:selftransform\"}."},{"heading":"Experiments","text":"# Experiments {#sec:experiments}\n\n::: table*\n[]{#tab:reconstruction label=\"tab:reconstruction\"}\n:::\n\n## Dataset and Training {#datasettraining}\n\nThe Conformer-SSL model used as the content encoder is pretrained on $56k$ hours of unlabelled English speech from the LibriLight [@librilight] corpus sampled at $16$ KHz. We fine-tune the Conformer-SSL model (using self-supervision with contrastive and MLM loss) on the *train-clean-360* subset of LibriTTS [@zen2019libritts] dataset with audio sampled at $22050\\mathit{Hz}$ to make the model compatible with the mel-spectrogram representation of the synthesizer. For both the content encoder and synthesizer, we use $80$ bands for mel spectrogram with the FFT, window, and hop size set to $1024$, $1024$, and $256$ respectively. We fine-tune the Conformer-SSL on this revised spectrogram representation for $50$ epochs with a batch size of $32$ using the AdamW optimizer with a fixed learning rate of $5e-5$ and $\\beta_1=0.9, \\beta_2=0.99$. Fine-tuning takes around $50$ hours on a single NVIDIA RTX A6000 GPU.\n\nFor our primary experiments, the mel-spectrogram synthesizer and the HifiGAN vocoder are also trained on the train-clean-360 subset of the LibriTTS dataset which contains $360$ hours of speech from $904$ speakers. We train three variants of the mel-spectrogram synthesizer:\\\n**1. Baseline--NoTransform** is trained to simply reconstruct the mel-spectrogram from the embeddings of the given utterance without any information perturbation procedure.\\\n**2. Baseline--Heuristic** is trained to reconstruct the mel-spectrogram from the content embedding of the heuristically perturbed utterance and the speaker embedding of the original utterance. We employ two transforms $g_1, g_2$ proposed in [@choi2021neural]. $g_1$ perturbs formant, pitch, and frequency response and $g_2$ perturbs formant and frequency response while preserving pitch. The hyperparameter details of these transformations are provided in the Appendix [8](#sec:heuristicfunctions){reference-type=\"ref\" reference=\"sec:heuristicfunctions\"}.\\\n**3. SelfVC** is first trained in the same way as Baseline--Heuristic for the first $100k$ mini batch iterations. Thereafter, we use the $g_\\textit{self}$ transformation procedure given by Equation [\\[eq:selftransform\\]](#eq:selftransform){reference-type=\"ref\" reference=\"eq:selftransform\"}.\n\nWe point readers to Appendix [7](#sec:modelarch){reference-type=\"ref\" reference=\"sec:modelarch\"} for detailed architectures and training details of various components.\n\n::: table*\n[]{#tab:voiceconversion label=\"tab:voiceconversion\"}\n:::\n\n## Evaluation Metrics {#sec:metrics}\n\nWe encourage readers to listen to our audio examples linked in the footnote on the first page. Quantitatively, we evaluate the synthesized audio on the following aspects:\n\n**Intelligibility:** For intelligibility, we transcribe the synthesized and original through and ASR and compute two error metrics between the transcriptions --- Character Error Rate **(CER)** and Phoneme Error Rate **(PER)**. For CER, we transcribe the audio using the Quartznet [@kriman2020quartznet] ASR model. For multilingual evaluation, we compute the PER on the transcriptions obtained from the pre-trained wav2vec2-Large-XLSR-53 ASR model which has been trained to recognize phonetic labels in multiple languages. [@Xu2021SimpleAE]. We also report the CER and PER between the predicted and ground truth transcripts of real data for reference in our Results.\n\n**Speaker Similarity Metrics:** To evaluate speaker similarity to our target speaker, we compute the speaker embeddings of synthesized and real utterances using a separate pre-trained speaker verification model [@koluguri2020speakernet]. Then we pair the synthesized and real utterances to create an equal number of positive (same-speaker) and negative (alternate-speaker) pairs for each target speaker to compute the Equal Error Rate **(SV-EER)**. We also report the mean cosine similarity between the positive pairs **(SV-SIM)**. Finally, we also ask human listeners to rate the speaker similarity of the generated and real utterance from the target speaker on a 5-point scale to obtain **Sim-MOS**.\n\n**Naturalness (MOS):** We ask human listeners to rate the naturalness of each utterance on a $1$ to $5$ scale with $1$ point increments. We include details of *MOS* and *SIM-MOS* evaluations in Appendix [11](#sec:humaneval){reference-type=\"ref\" reference=\"sec:humaneval\"}\n\n**Prosodic Similarity (GPE):** To evaluate prosodic similarity for the reconstruction task (Section [4.3](#sec:Reconstruction){reference-type=\"ref\" reference=\"sec:Reconstruction\"}), we compute error between the fundamental frequency contours of the original and synthesized audio. Specifically, we use Gross Pitch Error (GPE) [@chu2009reducing] to evaluate prosodic similarity.\n\n## Reconstruction {#sec:Reconstruction}\n\nFirst, we evaluate how effectively our setup can reconstruct audio from the extracted representations for unseen utterances and speakers. Our synthesizers can operate in two modes during inference --- *1) **Guided:*** In this scenario, we use ground truth pitch and duration information derived from the source utterance. *2) **Predictive:*** In this case, we use the predicted pitch and duration for synthesis. We conduct the reconstruction test on two unseen datasets --- 1) We choose $200$ utterances from the VCTK [@yamagishi2019vctk] dataset (English) with $20$ random utterances from each of the $10$ speakers ($5$ random male and $5$ random female speakers); 2) To evaluate performance on unseen languages, we choose $200$ utterances from the CSS10 [@park2019css10] dataset with $20$ random utterances from each of the $10$ unseen languages. The CSS10 dataset has a single speaker per language and contains at least $4$ hours of speech per language. For both of these evaluations, we use the synthesizer models trained on the same dataset, i.e. train-clean-360 subset of LibriTTS (English). The synthesized speech is evaluated on the intelligibility, speaker similarity and prosodic similarity metrics.\n\nAs indicated by the results in Table [\\[tab:reconstruction\\]](#tab:reconstruction){reference-type=\"ref\" reference=\"tab:reconstruction\"}, all three synthesizers achieve similar performance on the above metrics. This is expected since the speaker and content embedding are derived from the same utterance and all three synthesizers are trained for the reconstruction task. However, for controllable synthesis tasks such as voice conversion, we demonstrate that SelfVC considerably outperforms these baselines (Section [4.4](#sec:vcresults){reference-type=\"ref\" reference=\"sec:vcresults\"}). Since our model is trained in a text-free manner, we also see a promising generalization to unseen languages. The PER on CSS10 is higher than VCTK due to the larger phonetic vocabulary in non-English languages and the PER of the wav2vec2 model [@Xu2021SimpleAE] being higher even on real data. For unseen languages, our synthesizers produce more intelligible speech in the guided mode, where the duration information of the source utterance is kept intact.\n\n## Voice Conversion {#sec:vcresults}\n\nTo convert the voice of a given source utterance to a target speaker, we derive the content embedding from the source utterance and estimate the speaker embedding from the target speaker's audio and feed both as input to the synthesizer. To compare our zero-shot voice conversion method against prior work, we choose utterances from the LibriTTS test-clean subset since it is an unseen dataset across all voice conversion methods. We randomly choose $10$ target speakers ($5$ male and $5$ female) and $20$ source utterances from the remaining speakers to create $200$ voice conversion trials for each technique and report the results in Table [\\[tab:voiceconversion\\]](#tab:voiceconversion){reference-type=\"ref\" reference=\"tab:voiceconversion\"}. For our primary evaluation, we use $10$ seconds of speech from each target speaker to derive the speaker embedding. We split the $10$ second target-speaker utterance into $2$ second segments and estimate the speaker embedding as the mean speaker embedding across the segments. To be consistent with past work, we keep the duration of the source utterance unchanged during synthesis using duration guided mode and use predictive mode for pitch. We evaluate speaker-similarity across varying target speaker data amounts, with results detailed in Figure [3](#figs:speakergraphs){reference-type=\"ref\" reference=\"figs:speakergraphs\"} of the Appendix. Additionally, we include voice conversion results on seen speakers and out-of-domain VCTK dataset in Appendix [9](#sec:additionalvc){reference-type=\"ref\" reference=\"sec:additionalvc\"}.\n\n**Effectiveness of Self Transformations:** We perform ablations to compare effectiveness of different input transformation techniques. As reported in Table [\\[tab:voiceconversion\\]](#tab:voiceconversion){reference-type=\"ref\" reference=\"tab:voiceconversion\"}, incorporating heuristic transformations during training (Baseline--Heuristic) improves speaker similarity of generated audio over a baseline that does not use any transformations (Baseline--NoTransform). The speaker similarity metrics (SV-EER, SV-Sim and Sim-MOS) further improve in SelfVC when we incorporate the self transformation based iterative refinement procedure (Section [3.3](#sec:selfrefine){reference-type=\"ref\" reference=\"sec:selfrefine\"}). Note that both the baseline techniques and the SelfVC approach use identical neural architectures and undergo training for the same number of epochs with consistent optimizer hyperparameters. While Baseline-NoTransform produces intelligible and natural sounding audio, it clearly lacks in speaker similarity metrics, emphasizing the significance of input transformation methods in voice conversion.\n\n::: table*\n[]{#tab:crosslingual label=\"tab:crosslingual\"}\n:::\n\n**Comparison against Prior Work:** Although we have conducted controlled experiments by varying input transformation techniques in our models, it is challenging to make similar comparisons with prior research due to disparities in vocoders, datasets, and compatibility of model architectures between synthesizers and vocoders. We use the official open-source implementations and model checkpoints of six previously proposed techniques. For a fair comparison, we evaluate all prior techniques on the same voice conversion trial pairs as our methods, using the same ASR and SV models for calculating CER, PER and SV metrics. While NANSY [@choi2023nansy] is not officially open-sourced, our Baseline--Heuristic method closely follows the training strategy proposed in NANSY using the same hyperparameters for heuristic functions (Appendix [8](#sec:heuristicfunctions){reference-type=\"ref\" reference=\"sec:heuristicfunctions\"}), incorporating more recent neural architectures for the synthesizer and feature extractors. As shown in Table [\\[tab:voiceconversion\\]](#tab:voiceconversion){reference-type=\"ref\" reference=\"tab:voiceconversion\"}, SelfVC outperforms previously proposed voice conversion models on all quantitative metrics. It is interesting to note that SelfVC trained on just the train-clean-360 subset of LibriTTS outperforms YourTTS which is trained on a much larger dataset comprising LibirTTS (train-clean-360, train-clean-100), VCTK and two additional languages (French and Portugese).\n\n**Cross-lingual Voice Conversion:** For Cross-lingual voice conversion, we use the CSS10 dataset that contains speech utterances from $10$ different languages. We consider three voice conversion scenarios: 1) **English to CSS10:** Source utterance is from the test-clean subset of LibriTTS (English) and target speaker is from the CSS10 dataset 2) **CSS10 to CSS10:** Source utterance from a language in the CSS10 dataset and target speaker is from another language of CSS10. 3) **CSS10 to English:** Source utterance from a language in the CSS10 dataset and target speaker is from LibriTTS (English).\n\nFor *English to CSS10* we create $200$ voice conversion trials considering $20$ source utterances and $10$ target speakers in CSS10. For *CSS10 to CSS10* and *CSS10 to English*, we generate $500$ voice conversion trials each, considering $50$ source utterances ($5$ each from the $10$ languages) and $10$ target speakers. We use $10$ seconds of target speaker data across all experiments. We compare different voice conversion techniques on these trial pairs and present the results in Table [\\[tab:crosslingual\\]](#tab:crosslingual){reference-type=\"ref\" reference=\"tab:crosslingual\"}.\n\nIn the *English to CSS10* experiments, SelfVC (LibriTTS), which is trained solely on train-clean-360 LibriTTS subset, outperforms baseline methods and prior work, achieving lower SV-EER and PER. It is interesting to note that SelfVC (LibriTTS) outperforms YourTTS, which is trained on a more extensive trilingual dataset as discussed above. For *CSS10 to CSS10* voice conversion, we observe a higher SV-EER and PER for SelfVC (LibriTTS) as compared to YourTTS. This is not very surprising, since YourTTS model was trained on multilingual speech data while SelfVC (LibriTTS) has only been trained on English speech. For *CSS10 to English* voice conversion, SelfVC (LibriTTS) outperforms all baselines and prior work. Interestingly, ACE-VC, which uses similar model architectures and the same training data as our setup, does not generate intelligible speech when the source utterance is from CSS10. This result indicates that the text-free nature of our model allows generalization to unseen languages.\n\nTo adapt SelfVC for new languages, we conduct fine-tuning of only the synthesis model on both LibriTTS (train-clean-360) and CSS10 utterances (using data other than the test trial pairs), which considerably improves SV-EER and PER for the SelfVC (LibriTTS + CSS10) model. The improvement in SV-EER is significant but not surprising since the $10$ CSS10 speakers are now seen during training in the SelfVC (LibriTTS + CSS10) model. The improvement in PER is promising and demonstrates the effective adaptability of our model to different languages. We delve into details of the finetuning process and report the phoneme error rates for each of the $10$ CSS10 languages in Appendix [10](#sec:languagewiseper){reference-type=\"ref\" reference=\"sec:languagewiseper\"}."},{"heading":"Conclusion","text":"# Conclusion {#sec:discussion}\n\nWe introduce a novel training strategy, SelfVC, that utilizes self transformations to train controllable synthesis models on imperfectly disentangled representations. Our results indicate a clear benefit of incorporating self-synthesized examples while training a voice conversion model, as shown by a significant improvement in speaker similarity metrics while keeping the model architecture unchanged. By deriving and modelling prosodic information during training, SelfVC allows for both fine-grained and high-level control over the prosody of the synthesized speech. SelfVC achieves SOTA results in zero-shot voice conversion for English and can be easily scaled to multiple languages in a text-free manner, outperforming prior approaches in cross-lingual voice conversion. We recommend future work to apply our training strategy in other data domains for creating controllable synthesis models.\n\n[]{#sec:references label=\"sec:references\"}"},{"heading":"Deriving Duration-augmented Content Embeddings","text":"# Deriving Duration-augmented Content Embeddings\n\nGiven the output $z = G_c(x) = z_1 \\dots z_T$ from the Conformer-SSL model, we group together consecutive feature vectors with high cosine similarity. That is, we maintain a running average of consecutive vectors with cosine similarity greater than a threshold $\\tau$ and obtain the target duration for the averaged vector as the product of the number of grouped vectors and the duration of a single vector. The original duration $\\delta$ of a single vector is $4$ mel-spectrogram frames or $46$ms or raw audio. This procedure differs slightly from previous work [@lee2021voicemixer] in that, instead of computing similarities between consecutive pairs of the original vectors, we now compare the average embedding of the current group with the next original embedding. Our temporal downsampling procedure is similar to [@qian2021global] but we additionally maintain the durations of the grouped vectors to be used as targets for the duration predictor in our synthesizer. Our technique also differs from prior work [@kreuk-etal-2022-textless; @maimon2022speaking] that obtains duration/rhythm information from discrete SSL representations instead of the continuous vectors. Algorithm [\\[alg:grouping\\]](#alg:grouping){reference-type=\"ref\" reference=\"alg:grouping\"} details our grouping procedure to obtain duration-augmented content embeddings.\n\n:::: algorithm\n::: algorithmic\n$z' \\gets [z_1]$ $d' \\gets [\\delta]$ $num\\_grouped \\gets 1$ $z'[-1] \\gets (z_t + num\\_grouped * z'[-1])/(num\\_grouped + 1)$ $d'[-1] \\gets \\delta * (num\\_grouped + 1)$ $num\\_grouped \\gets num\\_grouped + 1$ $z'.append(z_t)$ $d'.append(\\delta)$ $num\\_grouped \\gets 1$ **return** $z', d'$\n:::\n\n[]{#alg:grouping label=\"alg:grouping\"}\n::::"},{"heading":"Model Architecture and Implementation Details","text":"# Model Architecture and Implementation Details {#sec:modelarch}\n\nOur voice conversion comprises the following neural networks. Total number of parameters and inference latency for each model are listed in Table [1](#tab:modelsize){reference-type=\"ref\" reference=\"tab:modelsize\"}\n\n**Conformer-SSL Model:** The Conformer-SSL model [@gulati2020conformer] used in this work is a convolution-augmented transformer architecture that is trained to reconstruct the masked areas of the mel-spectrogram on English speech data, using contrastive and masked language modelling (MLM) losses. It is pre-trained on the LibriLight corpus which consists of $56$k hrs of unlabeled English speech. The model consists of $18$ layers, $8$ attention heads and a hidden dimension of $512$. The output head of the Conformer model gives a $256$ dimensional encoding per timestep. The model temporally downsamples the input mel-spectrogram by a factor of $4$. With the STFT parameters used in our setup, each vector from the Conformer-SSL model corresponds to a contextualized representation of $46$ms of audio.\n\n**Speaker Verification TitaNet Model:** TitaNet [@titanet22] is based on a 1-D depthwise separable convolution architecture with Squeeze and Excitation layers that provide global context, followed by channel attention-based statistics pooling layer to map variable-length utterances to a fixed-length embedding. The TitaNet speaker verification model is trained using additive angular margin loss [@Liu_2017_CVPR] on $3373$ hours of speech from multiple datasets that span $16681$ speakers. Comprising of $25.3$ million parameters, the TitaNet model is designed to be parameter-efficient and achieves state-of-the-art results on the VoxCeleb-1 speaker verification benchmark with an EER of $0.68\\%$. The output from this speaker verification model is a $192$ dimensional speaker embedding.\n\n**Mel-spectrogram Synthesizer:** The spectrogram synthesizer takes as input the content and speaker embeddings and predicts the mel-spectrogram. The speaker and content embeddings derived from the Conformer-SSL and TitaNet models respectively are first projected to $256$ dimensions each using a learnable linear layer. The projected speaker embedding is then repeated across all time-steps and concatenated with the projected content embeddings. The synthesizer is a FastPitch [@lancucki2021fastpitch] based model that contains two feed forward transformer networks (encoder and decoder) that follow an identical architecture. Each transformer network contains $6$ layers, with a hidden dimension of $1536$. Each layer is composed of a single-headed attention module with an attention head of size $64$ followed by a 1-d convolutional block. Each convolutional block is a sequential operation of Conv1d, ReLU, Conv1d, Dropout and Layer Normalization. The kernel size for the convolution is $3$ and dropout probability is $0.1$.\n\nThe mel-spectrogram synthesizer also contains two submodules for predicting pitch and duration. The pitch and duration predictors take as input the output of the encoder network and predict a sequence of scalar values for duration or pitch (speaker normalized $F_0$ contour). Duration is used to regulate the length of the encoder output and pitch is embedded and concatenated with the encoder's output to be fed as input to the decoder. Both the pitch and duration predictor follow the same architecture --- Each network contains two convolutional blocks. Each convolutional block is a serial composition of Conv1d, ReLU and layer normalization with a kernel size of $3$ and hidden dimension of $256$, followed by a linear layer that maps the hidden dimension to a scalar value for duration or pitch.\n\nAll three variants of the synthesizer listed in Section [4.1](#datasettraining){reference-type=\"ref\" reference=\"datasettraining\"} are optimized using an AdamW optimizer [@loshchilov2018decoupled] with a fixed learning rate of $1e-4$ and $\\beta_1=0.8, \\beta_2=0.99$ for $500$ epochs with a batch size of $32$. The threshold $\\tau$ for duration extraction is set as $0.925$. The loss coefficients for the duration and pitch loss are set as $\\lambda_1=\\lambda_2=0.1$. The training time for Synth (SelfTransform) model is around $5$ days on $4$ NVIDIA RTX A6000 GPUs.\n\n**HiFiGAN Vocoder:** The HiFi-GAN [@kong2020hifi] vocoder used in this work consists of one generator and two discriminators: multi-scale and multi-period discriminators. In the generator network, consists of $4$ upsampling blocks with an upsampling factor of $8$, $8$, $2$, $2$ with kernel sizes $16$, $16$, $4$, $4$ respectively. The model outputs audio at $22050$Hz. The HiFiGAN vocoder is trained for $350$ epochs on train-clean-360 subset of LibriTTS. Thereafter, the vocoder is additionally fine-tuned on synthetic mel-spectrograms, generated by the three mel-spectrogram synthesizers (Baseline-NoTransform, Baseline-Heuristic and SelfVC) for the same dataset for $5$ epochs.\n\n::: {#tab:modelsize}\n  ----------------------------- --------------- ------------------ ------\n                                                  *Inference Time* \n                                                       *(Seconds)* \n  Model                           \\# Parameters                CPU    GPU\n  Speaker Encoder TiTaNet                  25 M               0.13   0.05\n  Conformer-SSL                           121 M               0.44   0.10\n  Mel-Spectrogram Synthesizer              59 M               0.15   0.01\n  HiFiGAN Vocoder                          85 M                2.1   0.08\n  ----------------------------- --------------- ------------------ ------\n\n  : Model size and wall clock inference time for a speech utterance of length $10$ seconds using a batch size of $1$ on CPU and NVIDIA RTX A6000 GPU.\n:::\n\n[]{#tab:modelsize label=\"tab:modelsize\"}"},{"heading":"Heuristic transformation functions","text":"# Heuristic transformation functions {#sec:heuristicfunctions}\n\nFor heuristic transformations, we follow the perturbation functions and hyperparameters proposed in [@choi2023nansy]. The three fundamental perturbation functions used are 1) Formant Shifting (fs) 2) Pitch Randomization (pr) and 3) Random Frequency Shaping (peq).\n\nDuring training, the source utterance is perturbed by randomly choosing a transformation function $g_1$ or $g_2$ --- Transformation function $g_1$ is a serial composition of *peq* and *fs*; And $g_2$ is a serial composition of *peq*, *pr* and *fs*.\n\nFor *pr*, pitch shift ratio is sampled uniformly from $U(1, 2)$ and pitch range ratio is sampled from $U(1, 1.5)$. Random frequency shaping (*peq*) is serial composition of low-shelfing, peaking and high-shelfing filters. Following NANSY, we use one low-shelving $H^\\text{LS}$, one high-shelving $H^\\text{HS}$, and eight peaking filters $H^\\text{Peak}_1, \\cdots, H^\\text{Peak}_8$.\n\n$$H^\\text{PEQ}(z) = H^\\text{LS}(z)H^\\text{HS}(z)\\prod_{i=1}^8H^\\text{Peak}_i(z).$$\n\nEach component is a second-order IIR filter parameterized by a cutoff/center frequency, quality factor, and gain parameter. The cutoff frequencies for $H^\\text{LS}$ and $H^\\text{HS}$ are set at $60 Hz$ and $10 kHz$, respectively. Center frequencies of $H^\\text{Peak}_1, \\cdots, H^\\text{Peak}_8$ are uniformly spaced in between the shelving filters on a logarithmic scale. The quality factor of each component is randomly sampled as $Q = Q_\\text{min}(Q_\\text{max}/Q_\\text{min})^z$ where $Q_\\text{min} = 2$, $Q_\\text{max} = 5$, and $z \\sim U(0, 1)$. The gain (in decibel) of each component is randomly sampled from $U(-12, 12)$.\n\nWe refer the readers to the link in the footnote (an unofficial open-source implementation of NANSY) for the precise implementation of transformation functions used in our work. [^2]"},{"heading":"Voice Conversion on Seen Speakers and VCTK Datasets","text":"# Voice Conversion on Seen Speakers and VCTK Datasets {#sec:additionalvc}\n\nWe present results for additional experiments on speen speakers from train-clean-360 (using utterances from the hold out set) and unseen speakers from VCTK dataset in Table [2](#tab:seenvctkresults){reference-type=\"ref\" reference=\"tab:seenvctkresults\"}. We choose VCTK because it is an out-of-domain test set of unseen speakers for our models trained on LibriTTS. Similar to our primary experiments, we consider $20$ source utterances, each from a different speaker and $10$ target speakers resulting in $200$ voice conversion trials. We compare against one prior work ACE-VC [@hussain2023ace], since ACE-VC is trained on the same dataset and VCTK dataset is not used during training. Other prior techniques considered in our main experiments conduct training on the VCTK dataset.\n\nOn the VCTK dataset, we find that SelfVC significantly outperforms the baselines and ACE-VC on the SV-EER metric. We also present the t-SNE plots for speaker embeddings of generated and real utterances in Figure [3](#figs:speakergraphs){reference-type=\"ref\" reference=\"figs:speakergraphs\"}. It can be observed that the embeddings of generated audio are closely clustered with the real embeddings of the target speaker for both seen and unseen speakers. We study the effect using different amounts of target speaker data when deriving speaker embedding for voice conversion in Figure [3](#figs:speakergraphs){reference-type=\"ref\" reference=\"figs:speakergraphs\"}. While the SV-EER improves as we incorporate more data from the target speaker, we observe marginal improvement beyond $16$ seconds of target speaker data. In this graph, *seen speakers* refers to LibriTTS train-clean-360 and *unseen speakers* refers to VCTK.\n\n![**Left**: SV-EER of voice converted speech generated by SelfVC using different amounts of target speaker data for estimating the speaker embedding. **Right:** t-SNE visualization of speaker embeddings of SelfVC synthesized and ground-truth audio for $10$ target speakers. Each color represents a different speaker.](figures/SelfVCGraphs.pdf){#figs:speakergraphs width=\"100%\"}\n\n::: {#tab:seenvctkresults}\n                               *LibriTTS (train-clean-360)*                                                      *VCTK*                    \n  -------------------------- ------------------------------ ------------------ ------------------ --------------------- ------------------ ------------------\n  Technique                             SV-EER $\\downarrow$   PER $\\downarrow$   CER $\\downarrow$   SV-EER $\\downarrow$   PER $\\downarrow$   CER $\\downarrow$\n  Real Data                                         $2.9\\%$            $8.7\\%$            $6.3\\%$               $3.1\\%$            $9.8\\%$            $5.1\\%$\n  ACE-VC [@hussain2023ace]                          $5.3\\%$            $8.8\\%$            $3.7\\%$               $9.2\\%$           $22.1\\%$            $8.2\\%$\n  Baseline--NoTransform                            $19.1\\%$            $5.5\\%$            $2.6\\%$              $25.2\\%$            $7.6\\%$            $3.8\\%$\n  Baseline--Heuristic                               $4.4\\%$            $5.5\\%$            $2.3\\%$               $8.5\\%$            $7.6\\%$            $3.1\\%$\n  SelfVC                                   $\\mathbf{3.0\\%}$   $\\mathbf{5.4\\%}$   $\\mathbf{2.2\\%}$      $\\mathbf{4.3\\%}$            $7.4\\%$   $\\mathbf{3.8\\%}$\n\n  : Voice Conversion experiments on seen speakers (LibriTTS train-clean-360) and out-of-domain unseen speakers (VCTK). We compare against one prior work trained on the same dataset as ours.\n:::\n\n[]{#tab:seenvctkresults label=\"tab:seenvctkresults\"}"},{"heading":"Multilingual Phoneme Error Rate","text":"# Multilingual Phoneme Error Rate {#sec:languagewiseper}\n\nIn Figure [4](#figs:languagepers){reference-type=\"ref\" reference=\"figs:languagepers\"} we present phoneme error rate on individual languages for *CSS10 to CSS10* and *CSS10 to LibriTTS* cross-lingual voice conversion experiments respectively. We also compare against the YourTTS model, which has the lowest average PER amongst the prior work considered in our work. As evident from the graphs, PER across all languages improve when SelfVC is fine-tuned on the LibriTTS train-clean-360 and CSS10 dataset (SelfVC (LibriTTS + CSS10) ). The fine-tuning is conducted for $10$ epochs with a fixed learning rate of $1e-4$ on the combined LibriTTS and CSS10 dataset and takes around $5$ hours on a single NVIDIA RTX A6000 GPU. Certain languages such as Chinese, Russian and Japanese have higher PER across all methods. This is because of the large phonetic vocabulary of such languages which results in a higher PER from the wav2vec2 model even on real utterances [@Xu2021SimpleAE].\n\n![Phoneme Error Rate on Individual Languages of the CSS10 dataset for voice conversion experiments when the source utterance is from CSS10 and the target speaker is from another language in CSS10 or the LibriTTS test-clean dataset.](figures/PERHorizontal.png){#figs:languagepers width=\"1.\\\\linewidth\"}"},{"heading":"MOS and Sim-MOS Evaluation","text":"# MOS and Sim-MOS Evaluation {#sec:humaneval}\n\n**Naturalness MOS Evaluation:** We ask human listeners to rate the audio on a scale of $1$ to $5$ point naturalness scale with $1$ point increments. We present $200$ audio examples of each technique and each audio is independently rated by at least $4$ listeners. This results in a total of at least $800$ evaluations per technique. The template used for the Naturalness human study is shown in Figure [\\[figs:naturalnesstemplate\\]](#figs:naturalnesstemplate){reference-type=\"ref\" reference=\"figs:naturalnesstemplate\"}. We report the MOS with $95\\%$ confidence intervals in Table [\\[tab:voiceconversion\\]](#tab:voiceconversion){reference-type=\"ref\" reference=\"tab:voiceconversion\"} of the paper.\n\n**Speaker Similarity MOS (Sim-MOS):** For Sim-MOS evaluation, we ask human listeners to rate the speaker similarity of a given pair of utterances. For this evaluation, each synthetic utterance is paired with a real utterance of the target speaker. We create pairs for all of the $200$ synthesized utterances of each technique. Each pair is rated by at least $4$ independent listeners resulting in at least $800$ speaker similarity evaluations of each technique. We ask the listeners to judge only the voice/speaker of the utterances and ignore the accent, content, grammar and expressiveness of speech following past work [@transferspeakerverification; @casanova2022yourtts]. The template used for this user study is shown in Figure [5](#figs:spksimmos){reference-type=\"ref\" reference=\"figs:spksimmos\"}. The Sim-MOS with $95\\%$ confidence intervals in Table [\\[tab:voiceconversion\\]](#tab:voiceconversion){reference-type=\"ref\" reference=\"tab:voiceconversion\"} of the paper. For reference, the reported Sim-MOS for same-speaker ground truth pairs is $4.36 \\pm 0.08$ and different-speaker ground truth pairs is $1.77 \\pm 0.10$.\n\n<figure id=\"figs:spksimmos\">\n<div class=\"minipage\">\n<img src=\"figures/naturalness.png\" />\n</div>\n<div class=\"minipage\">\n<img src=\"figures/speakersimilarity.png\" />\n</div>\n<figcaption><span>User Study template used for Speaker Similarity MOS evaluation</span></figcaption>\n</figure>\n\n[]{#sef:appendix label=\"sef:appendix\"}\n\n[^1]:\n\n[^2]: <https://github.com/dhchoi99/NANSY/blob/master/datasets/functional.py>"}],"approval":false,"conference":"iclr","rating":2,"year":2024,"id":"1c7cda09d2e6cf1181186ba285f9e636627d837149711f6ec53a564970122e1b","y_true":0,"y_pred":0,"rationale_true":"Summary: The paper introduces SelfVC, a novel training strategy aimed at enhancing voice conversion models using self-synthesized examples. The proposed model integrates prosodic information from the audio signal for predictive training and uses a unique iterative training approach with self-synthesized examples for continuous model refinement. Compared to previous methods, SelfVC sets new SOTA in zero-shot voice conversion regarding naturalness, speaker similarity, and audio intelligibility.\n\nStrengths: 1. The paper is well-composed, presenting its methodology with clarity. \n2. The extensive experiments support the presented claims. \n3. The demo provided by the author indicates the method's effectiveness.\n\nWeaknesses: Self-VC is similar to recent VC work (NANSY), except it uses pitch and duration predictors like ACE-VC. Also, as for the proposed training strategy (self transformations), random speaker embedding are commonly used for training a voice conversion model (e.g., https://arxiv.org/pdf/1806.02169.pdf, https://arxiv.org/pdf/2305.15816.pdf, https://arxiv.org/pdf/2305.07204.pdf,https://proceedings.neurips.cc/paper/2021/file/0266e33d3f546cb5436a10798e657d97-Paper.pdf). The fundamental idea seems the same. This point needs to be discussed more carefully.\n\nQuestions: /","rationale_pred":"Paper Summary: The paper proposes SelfVC, a novel training strategy for voice conversion that iteratively improves models using self-synthesized examples. It challenges traditional methods that focus on disentangling speech representations by instead using entangled representations derived from self-supervised learning. The framework aims to enhance speaker similarity and controllable speech synthesis, achieving state-of-the-art results in zero-shot voice conversion.\n\nSupporting Evidence:\n- The importance of controllable speech representation learning aligns with the Main Paper's focus on improving voice conversion through self-synthesized examples. (from: Controllable Speech Representation Learning Via Voice Conversion and AIC Loss)\n- Demonstrates the effectiveness of self-supervised representations in enhancing voice conversion systems, supporting the Main Paper's claims about the advantages of self-synthesized examples. (from: Enhancing the Stability of LLM-based Speech Generation Systems through Self-Supervised Representations)\n- Emphasizes the importance of self-supervised learning in voice synthesis, which aligns with SelfVC's approach of using entangled speech representations. (from: Make-A-Voice: Unified Voice Synthesis With Discrete Representation)\n- Addresses the challenges of zero-shot voice conversion, reinforcing the claims made in the Main Paper about the importance of maintaining speaker characteristics. (from: Zero-shot Voice Conversion via Self-supervised Prosody Representation Learning)\n- Demonstrates the effectiveness of self-supervised speech representations in voice conversion tasks, reinforcing the Main Paper's claims about the benefits of entangled representations. (from: S3PRL-VC: Open-Source Voice Conversion Framework with Self-Supervised Speech Representations)\n- Addresses the limitations of conventional voice conversion methods, reinforcing the Main Paper's claims about the effectiveness of its SelfVC framework. (from: Voice Conversion Using Deep Neural Networks With Layer-Wise Generative Training)\n- Highlights the importance of effective representation learning in voice conversion, complementing the Main Paper's focus on iterative refinement and self-supervised learning. (from: One-shot Voice Conversion by Separating Speaker and Content Representations with Instance Normalization)\n- Both papers emphasize the importance of maintaining the nuances of speech, thus reinforcing the claims made in SelfVC about the limitations of traditional disentangling methods. (from: MediumVC: Any-to-any voice conversion using synthetic specific-speaker speeches as intermedium features)\n- Demonstrates a similar approach to voice conversion that avoids explicit disentanglement of speaker and content attributes, reinforcing the effectiveness of their respective methodologies. (from: Fragmentvc: Any-To-Any Voice Conversion by End-To-End Extracting and Fusing Fine-Grained Voice Fragments with Attention)\n\nContradictory Evidence:\n- Emphasizes the need for disentangled speech representations across multiple components, critiquing the Main Paper's focus on entangled representations. (from: Unsupervised Speech Decomposition via Triple Information Bottleneck)\n- Contrasts with the Main Paper by emphasizing the need for explicit constraints in voice conversion, highlighting a fundamental difference in methodology. (from: MoCoVC: Non-parallel Voice Conversion with Momentum Contrastive Representation Learning)\n- Critiques the Main Paper's approach, arguing that conventional methods lack robustness and controllability over multiple prosody-related factors. (from: Adversarially learning disentangled speech representations for robust multi-factor voice conversion)\n- Argues that explicit disentanglement is necessary to avoid issues like prosody leakage and degraded performance, contrasting with the Main Paper's methodology. (from: SpeechTripleNet: End-to-End Disentangled Speech Representation Learning for Content, Timbre and Prosody)\n- Highlights a more traditional approach to isolating content and style, suggesting that its method may be more robust compared to the iterative refinement strategy proposed in SelfVC. (from: AdaptVC: High Quality Voice Conversion with Adaptive Learning)\n- Emphasizes the importance of explicitly disentangled speech representations for voice conversion, contrasting with the Main Paper's focus on entangled representations. (from: ACE-VC: Adaptive and Controllable Voice Conversion Using Explicitly Disentangled Self-Supervised Speech Representations)\n- Focuses on a one-shot voice conversion approach that employs disentangled representations, highlighting the contrasting philosophies regarding representation disentanglement. (from: Again-VC: A One-Shot Voice Conversion Using Activation Guidance and Adaptive Instance Normalization)\n- Proposes a one-shot voice conversion method that emphasizes a strong ability to disentangle content and speaker information, contrasting with the Main Paper's approach. (from: One-Shot Voice Conversion by Vector Quantization)\n- Focuses on singing voice synthesis rather than voice conversion, diverging from the Main Paper's goal of enhancing speaker similarity and naturalness in voice conversion. (from: KaraSinger: Score-Free Singing Voice Synthesis with VQ-VAE Using Mel-Spectrograms)\n- Highlights a novel loss balancing mechanism for audio encoding, diverging from the Main Paper's focus on voice conversion. (from: High Fidelity Neural Audio Compression)\n\nKey Comparisons:\n- SelfVC's use of self-synthesized examples vs. traditional disentangling methods\n- The effectiveness of entangled representations vs. explicit disentanglement\n- Performance metrics in zero-shot voice conversion between SelfVC and contrasting papers\n\nConclusion: Despite the innovative approach of SelfVC, the presence of multiple contrasting papers that advocate for explicit disentanglement and highlight potential shortcomings of entangled representations suggests that the paper does not present sufficiently novel ideas. The claims made in SelfVC are supported by existing literature, indicating that the contributions may not be unique enough to warrant a novel label.","structured_evaluation":{"paper_summary":"The paper proposes SelfVC, a novel training strategy for voice conversion that iteratively improves models using self-synthesized examples. It challenges traditional methods that focus on disentangling speech representations by instead using entangled representations derived from self-supervised learning. The framework aims to enhance speaker similarity and controllable speech synthesis, achieving state-of-the-art results in zero-shot voice conversion.","supporting_evidence":[{"text":"The importance of controllable speech representation learning aligns with the Main Paper's focus on improving voice conversion through self-synthesized examples.","paper_id":null,"paper_title":"Controllable Speech Representation Learning Via Voice Conversion and AIC Loss","source":"semantic"},{"text":"Demonstrates the effectiveness of self-supervised representations in enhancing voice conversion systems, supporting the Main Paper's claims about the advantages of self-synthesized examples.","paper_id":null,"paper_title":"Enhancing the Stability of LLM-based Speech Generation Systems through Self-Supervised Representations","source":"semantic"},{"text":"Emphasizes the importance of self-supervised learning in voice synthesis, which aligns with SelfVC's approach of using entangled speech representations.","paper_id":null,"paper_title":"Make-A-Voice: Unified Voice Synthesis With Discrete Representation","source":"semantic"},{"text":"Addresses the challenges of zero-shot voice conversion, reinforcing the claims made in the Main Paper about the importance of maintaining speaker characteristics.","paper_id":null,"paper_title":"Zero-shot Voice Conversion via Self-supervised Prosody Representation Learning","source":"semantic"},{"text":"Demonstrates the effectiveness of self-supervised speech representations in voice conversion tasks, reinforcing the Main Paper's claims about the benefits of entangled representations.","paper_id":null,"paper_title":"S3PRL-VC: Open-Source Voice Conversion Framework with Self-Supervised Speech Representations","source":"semantic"},{"text":"Addresses the limitations of conventional voice conversion methods, reinforcing the Main Paper's claims about the effectiveness of its SelfVC framework.","paper_id":null,"paper_title":"Voice Conversion Using Deep Neural Networks With Layer-Wise Generative Training","source":"semantic"},{"text":"Highlights the importance of effective representation learning in voice conversion, complementing the Main Paper's focus on iterative refinement and self-supervised learning.","paper_id":null,"paper_title":"One-shot Voice Conversion by Separating Speaker and Content Representations with Instance Normalization","source":"semantic"},{"text":"Both papers emphasize the importance of maintaining the nuances of speech, thus reinforcing the claims made in SelfVC about the limitations of traditional disentangling methods.","paper_id":null,"paper_title":"MediumVC: Any-to-any voice conversion using synthetic specific-speaker speeches as intermedium features","source":"semantic"},{"text":"Demonstrates a similar approach to voice conversion that avoids explicit disentanglement of speaker and content attributes, reinforcing the effectiveness of their respective methodologies.","paper_id":null,"paper_title":"Fragmentvc: Any-To-Any Voice Conversion by End-To-End Extracting and Fusing Fine-Grained Voice Fragments with Attention","source":"semantic"}],"contradictory_evidence":[{"text":"Emphasizes the need for disentangled speech representations across multiple components, critiquing the Main Paper's focus on entangled representations.","paper_id":null,"paper_title":"Unsupervised Speech Decomposition via Triple Information Bottleneck","source":"semantic"},{"text":"Contrasts with the Main Paper by emphasizing the need for explicit constraints in voice conversion, highlighting a fundamental difference in methodology.","paper_id":null,"paper_title":"MoCoVC: Non-parallel Voice Conversion with Momentum Contrastive Representation Learning","source":"semantic"},{"text":"Critiques the Main Paper's approach, arguing that conventional methods lack robustness and controllability over multiple prosody-related factors.","paper_id":null,"paper_title":"Adversarially learning disentangled speech representations for robust multi-factor voice conversion","source":"semantic"},{"text":"Argues that explicit disentanglement is necessary to avoid issues like prosody leakage and degraded performance, contrasting with the Main Paper's methodology.","paper_id":null,"paper_title":"SpeechTripleNet: End-to-End Disentangled Speech Representation Learning for Content, Timbre and Prosody","source":"semantic"},{"text":"Highlights a more traditional approach to isolating content and style, suggesting that its method may be more robust compared to the iterative refinement strategy proposed in SelfVC.","paper_id":null,"paper_title":"AdaptVC: High Quality Voice Conversion with Adaptive Learning","source":"semantic"},{"text":"Emphasizes the importance of explicitly disentangled speech representations for voice conversion, contrasting with the Main Paper's focus on entangled representations.","paper_id":null,"paper_title":"ACE-VC: Adaptive and Controllable Voice Conversion Using Explicitly Disentangled Self-Supervised Speech Representations","source":"semantic"},{"text":"Focuses on a one-shot voice conversion approach that employs disentangled representations, highlighting the contrasting philosophies regarding representation disentanglement.","paper_id":null,"paper_title":"Again-VC: A One-Shot Voice Conversion Using Activation Guidance and Adaptive Instance Normalization","source":"semantic"},{"text":"Proposes a one-shot voice conversion method that emphasizes a strong ability to disentangle content and speaker information, contrasting with the Main Paper's approach.","paper_id":null,"paper_title":"One-Shot Voice Conversion by Vector Quantization","source":"semantic"},{"text":"Focuses on singing voice synthesis rather than voice conversion, diverging from the Main Paper's goal of enhancing speaker similarity and naturalness in voice conversion.","paper_id":null,"paper_title":"KaraSinger: Score-Free Singing Voice Synthesis with VQ-VAE Using Mel-Spectrograms","source":"semantic"},{"text":"Highlights a novel loss balancing mechanism for audio encoding, diverging from the Main Paper's focus on voice conversion.","paper_id":null,"paper_title":"High Fidelity Neural Audio Compression","source":"semantic"}],"key_comparisons":["SelfVC's use of self-synthesized examples vs. traditional disentangling methods","The effectiveness of entangled representations vs. explicit disentanglement","Performance metrics in zero-shot voice conversion between SelfVC and contrasting papers"],"conclusion":"Despite the innovative approach of SelfVC, the presence of multiple contrasting papers that advocate for explicit disentanglement and highlight potential shortcomings of entangled representations suggests that the paper does not present sufficiently novel ideas. The claims made in SelfVC are supported by existing literature, indicating that the contributions may not be unique enough to warrant a novel label.","label":0,"novel":"no","probability":1.3440409770490404e-8,"rationale":"Paper Summary: The paper proposes SelfVC, a novel training strategy for voice conversion that iteratively improves models using self-synthesized examples. It challenges traditional methods that focus on disentangling speech representations by instead using entangled representations derived from self-supervised learning. The framework aims to enhance speaker similarity and controllable speech synthesis, achieving state-of-the-art results in zero-shot voice conversion.\n\nSupporting Evidence:\n- The importance of controllable speech representation learning aligns with the Main Paper's focus on improving voice conversion through self-synthesized examples. (from: Controllable Speech Representation Learning Via Voice Conversion and AIC Loss)\n- Demonstrates the effectiveness of self-supervised representations in enhancing voice conversion systems, supporting the Main Paper's claims about the advantages of self-synthesized examples. (from: Enhancing the Stability of LLM-based Speech Generation Systems through Self-Supervised Representations)\n- Emphasizes the importance of self-supervised learning in voice synthesis, which aligns with SelfVC's approach of using entangled speech representations. (from: Make-A-Voice: Unified Voice Synthesis With Discrete Representation)\n- Addresses the challenges of zero-shot voice conversion, reinforcing the claims made in the Main Paper about the importance of maintaining speaker characteristics. (from: Zero-shot Voice Conversion via Self-supervised Prosody Representation Learning)\n- Demonstrates the effectiveness of self-supervised speech representations in voice conversion tasks, reinforcing the Main Paper's claims about the benefits of entangled representations. (from: S3PRL-VC: Open-Source Voice Conversion Framework with Self-Supervised Speech Representations)\n- Addresses the limitations of conventional voice conversion methods, reinforcing the Main Paper's claims about the effectiveness of its SelfVC framework. (from: Voice Conversion Using Deep Neural Networks With Layer-Wise Generative Training)\n- Highlights the importance of effective representation learning in voice conversion, complementing the Main Paper's focus on iterative refinement and self-supervised learning. (from: One-shot Voice Conversion by Separating Speaker and Content Representations with Instance Normalization)\n- Both papers emphasize the importance of maintaining the nuances of speech, thus reinforcing the claims made in SelfVC about the limitations of traditional disentangling methods. (from: MediumVC: Any-to-any voice conversion using synthetic specific-speaker speeches as intermedium features)\n- Demonstrates a similar approach to voice conversion that avoids explicit disentanglement of speaker and content attributes, reinforcing the effectiveness of their respective methodologies. (from: Fragmentvc: Any-To-Any Voice Conversion by End-To-End Extracting and Fusing Fine-Grained Voice Fragments with Attention)\n\nContradictory Evidence:\n- Emphasizes the need for disentangled speech representations across multiple components, critiquing the Main Paper's focus on entangled representations. (from: Unsupervised Speech Decomposition via Triple Information Bottleneck)\n- Contrasts with the Main Paper by emphasizing the need for explicit constraints in voice conversion, highlighting a fundamental difference in methodology. (from: MoCoVC: Non-parallel Voice Conversion with Momentum Contrastive Representation Learning)\n- Critiques the Main Paper's approach, arguing that conventional methods lack robustness and controllability over multiple prosody-related factors. (from: Adversarially learning disentangled speech representations for robust multi-factor voice conversion)\n- Argues that explicit disentanglement is necessary to avoid issues like prosody leakage and degraded performance, contrasting with the Main Paper's methodology. (from: SpeechTripleNet: End-to-End Disentangled Speech Representation Learning for Content, Timbre and Prosody)\n- Highlights a more traditional approach to isolating content and style, suggesting that its method may be more robust compared to the iterative refinement strategy proposed in SelfVC. (from: AdaptVC: High Quality Voice Conversion with Adaptive Learning)\n- Emphasizes the importance of explicitly disentangled speech representations for voice conversion, contrasting with the Main Paper's focus on entangled representations. (from: ACE-VC: Adaptive and Controllable Voice Conversion Using Explicitly Disentangled Self-Supervised Speech Representations)\n- Focuses on a one-shot voice conversion approach that employs disentangled representations, highlighting the contrasting philosophies regarding representation disentanglement. (from: Again-VC: A One-Shot Voice Conversion Using Activation Guidance and Adaptive Instance Normalization)\n- Proposes a one-shot voice conversion method that emphasizes a strong ability to disentangle content and speaker information, contrasting with the Main Paper's approach. (from: One-Shot Voice Conversion by Vector Quantization)\n- Focuses on singing voice synthesis rather than voice conversion, diverging from the Main Paper's goal of enhancing speaker similarity and naturalness in voice conversion. (from: KaraSinger: Score-Free Singing Voice Synthesis with VQ-VAE Using Mel-Spectrograms)\n- Highlights a novel loss balancing mechanism for audio encoding, diverging from the Main Paper's focus on voice conversion. (from: High Fidelity Neural Audio Compression)\n\nKey Comparisons:\n- SelfVC's use of self-synthesized examples vs. traditional disentangling methods\n- The effectiveness of entangled representations vs. explicit disentanglement\n- Performance metrics in zero-shot voice conversion between SelfVC and contrasting papers\n\nConclusion: Despite the innovative approach of SelfVC, the presence of multiple contrasting papers that advocate for explicit disentanglement and highlight potential shortcomings of entangled representations suggests that the paper does not present sufficiently novel ideas. The claims made in SelfVC are supported by existing literature, indicating that the contributions may not be unique enough to warrant a novel label."},"arxiv_id":"2310.09653"},"terms":{"tasks":["improve a voice conversion model","disentangling speech representations","train a controllable voice conversion model","derive prosodic information","train predictive submodules in the synthesis model","iteratively improve the synthesis model for voice conversion","generate voice-converted variations of an utterance","ensure a continuous and purposeful refinement of the model","improve the speaker similarity of generated speech","achieve state-of-the-art results in zero-shot voice conversion","evaluate naturalness of synthesized audio","evaluate speaker similarity of synthesized audio","evaluate intelligibility of synthesized audio","perform zero-shot voice conversion","perform voice conversion across different languages","perform controllable speech synthesis with pitch and pace modifications"],"methods":["SelfVC","self-supervised learning","speaker verification models","techniques to derive prosodic information","training strategy to iteratively improve the synthesis model","create a challenging training objective using self-synthesized examples"],"metrics":["naturalness","speaker similarity","intelligibility"],"resources":["self-synthesized examples","baseline voice conversion model"],"relations":[{"head":"SelfVC","tail":"improve a voice conversion model"},{"head":"SelfVC","tail":"train a controllable voice conversion model"},{"head":"self-supervised learning","tail":"train a controllable voice conversion model"},{"head":"speaker verification models","tail":"train a controllable voice conversion model"},{"head":"techniques to derive prosodic information","tail":"derive prosodic information"},{"head":"training strategy to iteratively improve the synthesis model","tail":"iteratively improve the synthesis model for voice conversion"},{"head":"create a challenging training objective using self-synthesized examples","tail":"generate voice-converted variations of an utterance"},{"head":"self-synthesized examples","tail":"improve the speaker similarity of generated speech"},{"head":"baseline voice conversion model","tail":"improve the speaker similarity of generated speech"},{"head":"naturalness","tail":"evaluate naturalness of synthesized audio"},{"head":"speaker similarity","tail":"evaluate speaker similarity of synthesized audio"},{"head":"intelligibility","tail":"evaluate intelligibility of synthesized audio"},{"head":"self-synthesized examples","tail":"perform controllable speech synthesis with pitch and pace modifications"}]},"background":"Previous efforts on voice conversion focus on factorizing speech into explicitly disentangled representations that separately encode speaker characteristics and linguistic content. However, disentangling speech representations to capture such attributes using task-specific loss terms can lead to information loss by discarding finer nuances such as accent and emotion of the original signal.","target":"We propose SelfVC, a training strategy to iteratively improve a voice conversion model with self-synthesized examples. In this work, instead of explicitly disentangling attributes with loss terms, we present a framework to train a controllable voice conversion model on entangled speech representations derived from self-supervised learning (SSL) and speaker verification models. First, we develop techniques to derive prosodic information from the audio signal and SSL representations to train predictive submodules in the synthesis model. Next, we propose a training strategy to iteratively improve the synthesis model for voice conversion, by creating a challenging training objective using self-synthesized examples. In this training approach, the current state of the synthesis model is used to generate voice-converted variations of an utterance, which serve as inputs for the reconstruction task, ensuring a continuous and purposeful refinement of the model. We demonstrate that incorporating such self-synthesized examples during training improves the speaker similarity of generated speech as compared to a baseline voice conversion model trained solely on heuristically perturbed inputs. Our framework is trained without any text and is applicable to a range of tasks such as zero-shot voice conversion, voice conversion across different languages, and controllable speech synthesis with pitch and pace modifications. We conduct extensive comparisons against prior work and find that SelfVC achieves state-of-the-art results in zero-shot voice conversion on metrics evaluating naturalness, speaker similarity, and intelligibility of synthesized audio."},{"graph":{"title":"Reward Learning From Preference With Ties","abstract":"Reward learning plays a pivotal role in Reinforcement Learning from Human Feedback (RLHF), ensuring the alignment of language models. The Bradley-Terry (BT) model stands as the prevalent choice for capturing human preferences from datasets containing pairs of chosen and rejected responses. In preference modeling, the focus is not on absolute values but rather on the reward difference between chosen and rejected responses, referred to as preference strength. Thus, precise evaluation of preference strength holds paramount importance in preference modeling. However, an easily overlooked factor significantly affecting preference strength measurement is that human attitudes towards two responses may not solely indicate a preference for one over the other and ties are also a common occurrence. To address this, we propose the adoption of the generalized Bradley-Terry model -- the Bradley-Terry model with ties (BTT) -- to accommodate tied preferences, thus leveraging additional information. We prove that even with the access to the true distributions of prompt and response, disregarding ties can lead to a notable bias in preference strength measurement. Comprehensive experiments further validate the advantages of incorporating ties in preference modeling. Notably, fine-tuning with BTT significantly outperforms fine-tuning with BT on synthetic preference datasets with ties, labeled by state-of-the-art open-source LLMs.","entities":[{"label":"Reward Learning From Preference With Ties","type":"title","detail":null,"excerpts":null},{"label":"reinforcement learning","type":"primary_area","detail":null,"excerpts":null},{"label":"reward learning","type":"keyword","detail":null,"excerpts":null},{"label":"preference modeling","type":"keyword","detail":null,"excerpts":null},{"label":"Bradley-Terry model","type":"keyword","detail":null,"excerpts":null},{"label":"human feedback","type":"keyword","detail":null,"excerpts":null},{"label":"bias correction","type":"keyword","detail":null,"excerpts":null},{"label":"This paper proposes the Bradley-Terry model with ties (BTT) to improve the accuracy of human preference modeling in reinforcement learning, demonstrating its advantages over the traditional BT model through theoretical analysis and experiments.","type":"tldr","detail":null,"excerpts":null},{"label":"Introduction of BTT for preference modeling","type":"claim","detail":"The paper introduces the Bradley-Terry model with ties (BTT) to better capture human preferences in reinforcement learning from human feedback (RLHF), addressing the bias introduced by ignoring ties in preference data.","excerpts":null},{"label":"Bias correction method","type":"claim","detail":"A novel bias-correction method is proposed to mitigate the bias in measuring preference strength caused by model mismatch when using conventional preference datasets without ties.","excerpts":null},{"label":"Experimental validation of BTT","type":"claim","detail":"Comprehensive experiments validate the advantages of incorporating ties in preference modeling, showing that fine-tuning with BTT significantly outperforms fine-tuning with BT on synthetic preference datasets with ties.","excerpts":null},{"label":"Bradley-Terry model with ties (BTT)","type":"method","detail":"The BTT model is proposed to accommodate tied preferences in human feedback, allowing for a more accurate representation of human preferences in RLHF.","excerpts":null},{"label":"Bias-correction method","type":"method","detail":"This method subtracts the bias term from the maximum likelihood estimation (MLE) loss function to recover the true preference strength measurement in datasets without ties.","excerpts":null},{"label":"Fine-tuning with BTT","type":"method","detail":"Fine-tuning language models using the BTT model on synthetic datasets with ties to evaluate its performance against the traditional BT model.","excerpts":null},{"label":"Simulation experiment with BTT","type":"experiment","detail":"A simulation experiment is conducted to show that the reward model trained with the BTT model exhibits less preference strength bias compared to the one trained with the BT model.","excerpts":null},{"label":"Bias-correction application","type":"experiment","detail":"The bias-correction method is applied to the conventional preference dataset without ties to mitigate the bias term using a DPO reward model.","excerpts":null},{"label":"Fine-tuning on synthetic preference dataset","type":"experiment","detail":"Fine-tuning of language models on a synthetic preference dataset with ties, comparing the performance of BTT and BT models.","excerpts":null}],"relationships":[{"source":"Reward Learning From Preference With Ties","target":"reinforcement learning"},{"source":"Reward Learning From Preference With Ties","target":"reward learning"},{"source":"Reward Learning From Preference With Ties","target":"preference modeling"},{"source":"Reward Learning From Preference With Ties","target":"Bradley-Terry model"},{"source":"Reward Learning From Preference With Ties","target":"human feedback"},{"source":"Reward Learning From Preference With Ties","target":"bias correction"},{"source":"Reward Learning From Preference With Ties","target":"This paper proposes the Bradley-Terry model with ties (BTT) to improve the accuracy of human preference modeling in reinforcement learning, demonstrating its advantages over the traditional BT model through theoretical analysis and experiments."},{"source":"This paper proposes the Bradley-Terry model with ties (BTT) to improve the accuracy of human preference modeling in reinforcement learning, demonstrating its advantages over the traditional BT model through theoretical analysis and experiments.","target":"Introduction of BTT for preference modeling"},{"source":"This paper proposes the Bradley-Terry model with ties (BTT) to improve the accuracy of human preference modeling in reinforcement learning, demonstrating its advantages over the traditional BT model through theoretical analysis and experiments.","target":"Bias correction method"},{"source":"This paper proposes the Bradley-Terry model with ties (BTT) to improve the accuracy of human preference modeling in reinforcement learning, demonstrating its advantages over the traditional BT model through theoretical analysis and experiments.","target":"Experimental validation of BTT"},{"source":"Introduction of BTT for preference modeling","target":"Bradley-Terry model with ties (BTT)"},{"source":"Bias correction method","target":"Bias-correction method"},{"source":"Experimental validation of BTT","target":"Fine-tuning with BTT"},{"source":"Bradley-Terry model with ties (BTT)","target":"Simulation experiment with BTT"},{"source":"Bias-correction method","target":"Bias-correction application"},{"source":"Fine-tuning with BTT","target":"Fine-tuning on synthetic preference dataset"}],"valid_status":"Valid","valid_status_all":["Valid"]},"related":[{"summary":"The Related Paper supports the Main Paper by emphasizing the importance of aligning Large Language Models (LLMs) with human preferences, which is a central theme in the Main Paper's exploration of reward learning from preferences. Both papers highlight the significance of preference data in improving model performance, with the Related Paper providing insights into data efficiency and the effectiveness of different types of preference data. This complements the Main Paper's focus on the Bradley-Terry model with ties, as it underscores the necessity of accurately capturing human preferences to enhance model alignment and performance.","paper_id":"92aff639096e7ec2d894328247e305d8beb323d0","title":"Optimizing LLMs with Direct Preferences: A Data Efficiency Perspective","abstract":"Aligning the output of Large Language Models (LLMs) with human preferences (e.g., by means of reinforcement learning with human feedback, or RLHF) is essential for ensuring their effectiveness in real-world scenarios. Despite significant advancements in LLM alignment techniques, the impact of different type of preference data on model performance has yet to be systematically explored. In this study, we investigate the scalability, data efficiency, and effectiveness of Direct Preference Optimization (DPO) in fine-tuning pre-trained LLMs, aiming to reduce their dependency on extensive amounts of preference data, which is expensive to collect. We (1) systematically compare the performance of models fine-tuned with varying percentages of a combined preference judgement dataset to define the improvement curve of DPO and assess its effectiveness in data-constrained environments; and (2) provide insights for the development of an optimal approach for selective preference data usage. Our study reveals that increasing the amount of data used for training generally enhances and stabilizes model performance. Moreover, the use of a combination of diverse datasets significantly improves model effectiveness. Furthermore, when models are trained separately using different types of prompts, models trained with conversational prompts outperformed those trained with question answering prompts.","score":0.6603729724884033,"polarity":"positive","source":"semantic","year":2024,"authors":["Pietro Bernardelle","Gianluca Demartini"],"venue":null,"citation_count":1,"reference_count":10,"influential_citation_count":0,"corpus_id":273507904,"url":"https://www.semanticscholar.org/paper/92aff639096e7ec2d894328247e305d8beb323d0","arxiv_id":null,"contexts":null,"background":"Aligning the output of Large Language Models (LLMs) with human preferences (e.g., by means of reinforcement learning with human feedback, or RLHF) is essential for ensuring their effectiveness in real-world scenarios. Despite significant advancements in LLM alignment techniques, the impact of different type of preference data on model performance has yet to be systematically explored.","target":"In this study, we investigate the scalability, data efficiency, and effectiveness of Direct Preference Optimization (DPO) in fine-tuning pre-trained LLMs, aiming to reduce their dependency on extensive amounts of preference data, which is expensive to collect. We (1) systematically compare the performance of models fine-tuned with varying percentages of a combined preference judgement dataset to define the improvement curve of DPO and assess its effectiveness in data-constrained environments; and (2) provide insights for the development of an optimal approach for selective preference data usage. Our study reveals that increasing the amount of data used for training generally enhances and stabilizes model performance. Moreover, the use of a combination of diverse datasets significantly improves model effectiveness. Furthermore, when models are trained separately using different types of prompts, models trained with conversational prompts outperformed those trained with question answering prompts."},{"summary":"The Related Paper supports the Main Paper by highlighting the limitations of standard reward modeling approaches, such as the Bradley-Terry model, in handling diverging preferences among annotators. It emphasizes that disagreements in human-labeled datasets can significantly impact preference strength measurement, a key focus of the Main Paper. By demonstrating that traditional methods often misinterpret these disagreements as noise, the Related Paper reinforces the necessity of the Main Paper's proposed generalized Bradley-Terry model with ties (BTT), which accounts for such complexities and improves preference modeling accuracy.","paper_id":"3f062cfb7762e45f82cc703a5b093f4fc9c9a9f3","title":"Diverging Preferences: When do Annotators Disagree and do Models Know?","abstract":"We examine diverging preferences in human-labeled preference datasets. We develop a taxonomy of disagreement sources spanning 10 categories across four high-level classes -- task underspecification, response style, refusals, and annotation errors. We find that the majority of disagreements are in opposition with standard reward modeling approaches, which are designed with the assumption that annotator disagreement is noise. We then explore how these findings impact two areas of LLM development: reward modeling and evaluation. In our experiments, we demonstrate how standard reward modeling methods, like the Bradley-Terry model, fail to differentiate whether a given preference judgment is the result of unanimous agreement among annotators or the majority opinion among diverging user preferences. We also find that these tendencies are also echoed by popular LLM-as-Judge evaluation methods, which consistently identify a winning response in cases of diverging preferences. These findings highlight remaining challenges in LLM evaluations, which are greatly influenced by divisive features like response style, and in developing pluralistically aligned LLMs. To address these issues, we develop methods for identifying diverging preferences to mitigate their influence on evaluation and training.","score":0.6527634859085083,"polarity":"positive","source":"semantic","year":2024,"authors":["Michael J.Q. Zhang","Zhilin Wang","Jena D. Hwang","Yi Dong","Olivier Delalleau","Yejin Choi","Eunsol Choi","Xiang Ren","Valentina Pyatkin"],"venue":null,"citation_count":5,"reference_count":41,"influential_citation_count":0,"corpus_id":273482364,"url":"https://www.semanticscholar.org/paper/3f062cfb7762e45f82cc703a5b093f4fc9c9a9f3","arxiv_id":null,"contexts":null,"background":"We examine diverging preferences in human-labeled preference datasets. We develop a taxonomy of disagreement sources spanning 10 categories across four high-level classes -- task underspecification, response style, refusals, and annotation errors. We find that the majority of disagreements are in opposition with standard reward modeling approaches, which are designed with the assumption that annotator disagreement is noise. We then explore how these findings impact two areas of LLM development: reward modeling and evaluation. These findings highlight remaining challenges in LLM evaluations, which are greatly influenced by divisive features like response style, and in developing pluralistically aligned LLMs.","target":"In our experiments, we demonstrate how standard reward modeling methods, like the Bradley-Terry model, fail to differentiate whether a given preference judgment is the result of unanimous agreement among annotators or the majority opinion among diverging user preferences. We also find that these tendencies are also echoed by popular LLM-as-Judge evaluation methods, which consistently identify a winning response in cases of diverging preferences. To address these issues, we develop methods for identifying diverging preferences to mitigate their influence on evaluation and training."},{"summary":"The Related Paper supports the Main Paper by emphasizing the importance of context in preference modeling, which aligns with the Main Paper's focus on accurately measuring preference strength. Both papers highlight the limitations of traditional models in capturing human preferences, with the Related Paper proposing a two-step preference modeling procedure that complements the Main Paper's introduction of the generalized Bradley-Terry model with ties. This synergy suggests that incorporating context alongside tied preferences can further enhance the alignment of language models with human feedback.","paper_id":"9ddda210b622e0127a43dd5d7cd02928d449b799","title":"Improving Context-Aware Preference Modeling for Language Models","abstract":"While finetuning language models from pairwise preferences has proven remarkably effective, the underspecified nature of natural language presents critical challenges. Direct preference feedback is uninterpretable, difficult to provide where multidimensional criteria may apply, and often inconsistent, either because it is based on incomplete instructions or provided by diverse principals. To address these challenges, we consider the two-step preference modeling procedure that first resolves the under-specification by selecting a context, and then evaluates preference with respect to the chosen context. We decompose reward modeling error according to these two steps, which suggests that supervising context in addition to context-specific preference may be a viable approach to aligning models with diverse human preferences. For this to work, the ability of models to evaluate context-specific preference is critical. To this end, we contribute context-conditioned preference datasets and accompanying experiments that investigate the ability of language models to evaluate context-specific preference. We use our datasets to (1) show that existing preference models benefit from, but fail to fully consider, added context, (2) finetune a context-aware reward model with context-specific performance exceeding that of GPT-4 and Llama 3 70B on tested datasets, and (3) investigate the value of context-aware preference modeling.","score":0.6377676725387573,"polarity":"positive","source":"semantic","year":2024,"authors":["Silviu Pitis","Ziang Xiao","Nicolas Le Roux","Alessandro Sordoni"],"venue":null,"citation_count":6,"reference_count":86,"influential_citation_count":0,"corpus_id":271328528,"url":"https://www.semanticscholar.org/paper/9ddda210b622e0127a43dd5d7cd02928d449b799","arxiv_id":null,"contexts":null,"background":"While finetuning language models from pairwise preferences has proven remarkably effective, the underspecified nature of natural language presents critical challenges. Direct preference feedback is uninterpretable, difficult to provide where multidimensional criteria may apply, and often inconsistent, either because it is based on incomplete instructions or provided by diverse principals.","target":"To address these challenges, we consider the two-step preference modeling procedure that first resolves the under-specification by selecting a context, and then evaluates preference with respect to the chosen context. We decompose reward modeling error according to these two steps, which suggests that supervising context in addition to context-specific preference may be a viable approach to aligning models with diverse human preferences. For this to work, the ability of models to evaluate context-specific preference is critical. To this end, we contribute context-conditioned preference datasets and accompanying experiments that investigate the ability of language models to evaluate context-specific preference. We use our datasets to (1) show that existing preference models benefit from, but fail to fully consider, added context, (2) finetune a context-aware reward model with context-specific performance exceeding that of GPT-4 and Llama 3 70B on tested datasets, and (3) investigate the value of context-aware preference modeling."},{"summary":"The Related Paper supports the Main Paper by emphasizing the importance of enhancing preference learning through data-centric approaches, which aligns with the Main Paper's focus on improving reward learning from human feedback. Both papers highlight the significance of accurately capturing human preferences, with the Related Paper introducing machine-generated rationales to enrich preference datasets, thereby potentially complementing the Main Paper's proposal of the generalized Bradley-Terry model with ties. This synergy suggests that integrating rationale information could further enhance the measurement of preference strength and the overall performance of preference modeling, reinforcing the claims made in the Main Paper.","paper_id":"a441c2d2d8e31a2b201e2f04bad259ba5515c8b2","title":"Data-Centric Human Preference Optimization with Rationales","abstract":"Reinforcement learning from human feedback plays a crucial role in aligning language models towards human preferences, traditionally represented through comparisons between pairs or sets of responses within a given context. While many studies have enhanced algorithmic techniques to optimize learning from such data, this work shifts focus to improving preference learning through a data-centric approach. Specifically, we propose enriching existing preference datasets with machine-generated rationales that explain the reasons behind choices. We develop a simple and principled framework to augment current preference learning methods with rationale information. Our comprehensive analysis highlights how rationales enhance learning efficiency. Extensive experiments reveal that rationale-enriched preference learning offers multiple advantages: it improves data efficiency, accelerates convergence to higher-performing models, and reduces verbosity bias and hallucination. Furthermore, this framework is versatile enough to integrate with various preference optimization algorithms. Overall, our findings highlight the potential of re-imagining data design for preference learning, demonstrating that even freely available machine-generated rationales can significantly boost performance across multiple dimensions. The code repository is available at https: //github.com/reds-lab/preference-learning-with-rationales","score":0.633037805557251,"polarity":"positive","source":"semantic","year":2024,"authors":["H. Just","Ming Jin","Anit Kumar Sahu","Huy Phan","Ruoxi Jia"],"venue":null,"citation_count":1,"reference_count":64,"influential_citation_count":0,"corpus_id":271310298,"url":"https://www.semanticscholar.org/paper/a441c2d2d8e31a2b201e2f04bad259ba5515c8b2","arxiv_id":null,"contexts":null,"background":"Reinforcement learning from human feedback plays a crucial role in aligning language models towards human preferences, traditionally represented through comparisons between pairs or sets of responses within a given context. While many studies have enhanced algorithmic techniques to optimize learning from such data, this work shifts focus to improving preference learning through a data-centric approach.","target":"Specifically, we propose enriching existing preference datasets with machine-generated rationales that explain the reasons behind choices. We develop a simple and principled framework to augment current preference learning methods with rationale information. Our comprehensive analysis highlights how rationales enhance learning efficiency. Extensive experiments reveal that rationale-enriched preference learning offers multiple advantages: it improves data efficiency, accelerates convergence to higher-performing models, and reduces verbosity bias and hallucination. Furthermore, this framework is versatile enough to integrate with various preference optimization algorithms. Overall, our findings highlight the potential of re-imagining data design for preference learning, demonstrating that even freely available machine-generated rationales can significantly boost performance across multiple dimensions."},{"summary":"The Related Paper supports the Main Paper by highlighting the importance of accurate preference modeling in Reinforcement Learning from Human Feedback (RLHF). It emphasizes the challenges of aligning synthetic preference labels with human judgments, which aligns with the Main Paper's focus on the significance of preference strength measurement. Both papers advocate for improved methodologies— the Main Paper through the generalized Bradley-Terry model with ties, and the Related Paper through the GenRM algorithm— to enhance the quality of preference data, ultimately leading to better performance in training language models.","paper_id":"59eed4c468846a4d45105a4603dabf72e2bef830","title":"Generative Reward Models","abstract":"Reinforcement Learning from Human Feedback (RLHF) has greatly improved the performance of modern Large Language Models (LLMs). The RLHF process is resource-intensive and technically challenging, generally requiring a large collection of human preference labels over model-generated outputs. Reinforcement Learning from AI Feedback (RLAIF) addresses this data collection challenge by leveraging synthetic preferences generated by an LLM. However, recent work has shown that synthetic preferences labels may not align well with human preference judgments. To address this, we propose a hybrid approach that unifies RLHF and RLAIF methodologies. We introduce GenRM, an iterative algorithm that trains an LLM on self-generated reasoning traces, leading to synthetic preference labels matching human preference judgments. Empirically, we show that zero-shot LLM-based judgments under-perform compared to Bradley-Terry reward models on in-distribution tasks (between 9-36%). In contrast, GenRM achieves in-distribution accuracy comparable to Bradley-Terry models, while significantly outperforming them on out-of-distribution tasks (between 10-45%). Moreover, GenRM surpasses the performance of using LLMs as judges on both in-distribution (by 9-31%) and out-of-distribution tasks (by 2- 6%). Our results show that combining the strengths of RLHF and RLAIF offers a promising approach for improving the quality of synthetic preference labels.","score":0.6309017539024353,"polarity":"positive","source":"semantic","year":2024,"authors":["Dakota Mahan","Duy Phung","Rafael Rafailov","Chase Blagden","nathan lile","Louis Castricato","Jan-Philipp Franken","Chelsea Finn","Alon Albalak"],"venue":null,"citation_count":10,"reference_count":41,"influential_citation_count":0,"corpus_id":273404003,"url":"https://www.semanticscholar.org/paper/59eed4c468846a4d45105a4603dabf72e2bef830","arxiv_id":null,"contexts":null,"background":"Reinforcement Learning from Human Feedback (RLHF) has greatly improved the performance of modern Large Language Models (LLMs). The RLHF process is resource-intensive and technically challenging, generally requiring a large collection of human preference labels over model-generated outputs. Reinforcement Learning from AI Feedback (RLAIF) addresses this data collection challenge by leveraging synthetic preferences generated by an LLM. However, recent work has shown that synthetic preferences labels may not align well with human preference judgments.","target":"To address this, we propose a hybrid approach that unifies RLHF and RLAIF methodologies. We introduce GenRM, an iterative algorithm that trains an LLM on self-generated reasoning traces, leading to synthetic preference labels matching human preference judgments. Empirically, we show that zero-shot LLM-based judgments under-perform compared to Bradley-Terry reward models on in-distribution tasks (between 9-36%). In contrast, GenRM achieves in-distribution accuracy comparable to Bradley-Terry models, while significantly outperforming them on out-of-distribution tasks (between 10-45%). Moreover, GenRM surpasses the performance of using LLMs as judges on both in-distribution (by 9-31%) and out-of-distribution tasks (by 2-6%). Our results show that combining the strengths of RLHF and RLAIF offers a promising approach for improving the quality of synthetic preference labels."},{"summary":"The Related Paper contrasts the Main Paper by emphasizing the importance of data quality and metrics in preference dataset comparison, rather than focusing on the modeling of preferences themselves. While the Main Paper advocates for the generalized Bradley-Terry model to account for ties in preferences, the Related Paper critiques the reliance on existing datasets and highlights the need for systematic evaluation of these datasets based on scale, label noise, and information content. This suggests that the Main Paper's approach may overlook broader data-centric considerations that could impact the effectiveness of reward learning in RLHF.","paper_id":"8096ca5f6895955dc41f05094f976b76419437fd","title":"Towards Data-Centric RLHF: Simple Metrics for Preference Dataset Comparison","abstract":"The goal of aligning language models to human preferences requires data that reveal these preferences. Ideally, time and money can be spent carefully collecting and tailoring bespoke preference data to each downstream application. However, in practice, a select few publicly available preference datasets are often used to train reward models for reinforcement learning from human feedback (RLHF). While new preference datasets are being introduced with increasing frequency, there are currently no existing efforts to measure and compare these datasets. In this paper, we systematically study preference datasets through three perspectives: scale, label noise, and information content. We propose specific metrics for each of these perspectives and uncover different axes of comparison for a better understanding of preference datasets. Our work is a first step towards a data-centric approach to alignment by providing perspectives that aid in training efficiency and iterative data collection for RLHF.","score":0.8131800889968872,"polarity":"negative","source":"semantic","year":2024,"authors":["Judy Hanwen Shen","Archit Sharma","Jun Qin"],"venue":null,"citation_count":2,"reference_count":38,"influential_citation_count":0,"corpus_id":272689981,"url":"https://www.semanticscholar.org/paper/8096ca5f6895955dc41f05094f976b76419437fd","arxiv_id":null,"contexts":null,"background":"The goal of aligning language models to human preferences requires data that reveal these preferences. Ideally, time and money can be spent carefully collecting and tailoring bespoke preference data to each downstream application. However, in practice, a select few publicly available preference datasets are often used to train reward models for reinforcement learning from human feedback (RLHF). While new preference datasets are being introduced with increasing frequency, there are currently no existing efforts to measure and compare these datasets.","target":"In this paper, we systematically study preference datasets through three perspectives: scale, label noise, and information content. We propose specific metrics for each of these perspectives and uncover different axes of comparison for a better understanding of preference datasets. Our work is a first step towards a data-centric approach to alignment by providing perspectives that aid in training efficiency and iterative data collection for RLHF."},{"summary":"The Related Paper contrasts the Main Paper by emphasizing the limitations of the Bradley-Terry model in handling binary feedback, arguing that it discards valuable information from tied preferences and lacks granularity. While the Main Paper focuses on the generalized Bradley-Terry model with ties to improve preference strength measurement, the Related Paper proposes a framework for learning reward models from ordinal feedback, which allows for more nuanced preferences beyond binary choices. This approach claims to enhance reward learning by incorporating fine-grained feedback, suggesting that the Main Paper's reliance on ties may not fully capture the complexity of human preferences.","paper_id":"ad9cc1c463faa60d91bfaa48c3b16df0b3719ffa","title":"Reward Modeling with Ordinal Feedback: Wisdom of the Crowd","abstract":"Learning a reward model (RM) from human preferences has been an important component in aligning large language models (LLMs). The canonical setup of learning RMs from pairwise preference data is rooted in the classic Bradley-Terry (BT) model that accepts binary feedback, i.e., the label being either Response 1 is better than Response 2, or the opposite. Such a setup inevitably discards potentially useful samples (such as\"tied\"between the two responses) and loses more fine-grained information (such as\"slightly better\"). In this paper, we propose a framework for learning RMs under ordinal feedback which generalizes the case of binary preference feedback to any arbitrary granularity. Specifically, we first identify a marginal unbiasedness condition, which generalizes the assumption of the BT model in the existing binary feedback setting. The condition validates itself via the sociological concept of the wisdom of the crowd. Under the condition, we develop a natural probability model for pairwise preference data under ordinal feedback and analyze its properties. We prove the statistical benefits of ordinal feedback in terms of reducing the Rademacher complexity compared to the case of binary feedback. The proposed learning objective and the theory also extend to hinge loss and direct policy optimization (DPO). In particular, the theoretical analysis may be of independent interest when applying to a seemingly unrelated problem of knowledge distillation to interpret the bias-variance trade-off therein. The framework also sheds light on writing guidance for human annotators. Our numerical experiments validate that fine-grained feedback leads to better reward learning for both in-distribution and out-of-distribution settings. Further experiments show that incorporating a certain proportion of samples with tied preference boosts RM learning.","score":0.8044986724853516,"polarity":"negative","source":"semantic","year":2024,"authors":["Shang Liu","Yu Pan","Guanting Chen","Xiaocheng Li"],"venue":null,"citation_count":1,"reference_count":47,"influential_citation_count":0,"corpus_id":274150343,"url":"https://www.semanticscholar.org/paper/ad9cc1c463faa60d91bfaa48c3b16df0b3719ffa","arxiv_id":null,"contexts":null,"background":"Learning a reward model (RM) from human preferences has been an important component in aligning large language models (LLMs). The canonical setup of learning RMs from pairwise preference data is rooted in the classic Bradley-Terry (BT) model that accepts binary feedback, i.e., the label being either Response 1 is better than Response 2, or the opposite. Such a setup inevitably discards potentially useful samples (such as \"tied\" between the two responses) and loses more fine-grained information (such as \"slightly better\").","target":"In this paper, we propose a framework for learning RMs under ordinal feedback which generalizes the case of binary preference feedback to any arbitrary granularity. Specifically, we first identify a marginal unbiasedness condition, which generalizes the assumption of the BT model in the existing binary feedback setting. The condition validates itself via the sociological concept of the wisdom of the crowd. Under the condition, we develop a natural probability model for pairwise preference data under ordinal feedback and analyze its properties. We prove the statistical benefits of ordinal feedback in terms of reducing the Rademacher complexity compared to the case of binary feedback. The proposed learning objective and the theory also extend to hinge loss and direct policy optimization (DPO). In particular, the theoretical analysis may be of independent interest when applying to a seemingly unrelated problem of knowledge distillation to interpret the bias-variance trade-off therein. The framework also sheds light on writing guidance for human annotators. Our numerical experiments validate that fine-grained feedback leads to better reward learning for both in-distribution and out-of-distribution settings. Further experiments show that incorporating a certain proportion of samples with tied preference boosts RM learning."},{"summary":"The Related Paper contrasts the Main Paper by arguing that traditional models like the Bradley-Terry model fail to adequately capture the complexities of human preferences, such as intransitivity and irrationality. Instead, it advocates for a self-play approach that directly utilizes preference probabilities to achieve more accurate language model alignment, suggesting that the Main Paper's focus on preference strength measurement through the generalized Bradley-Terry model may overlook these critical aspects. Additionally, the Related Paper demonstrates superior performance in model alignment without relying on external supervision, highlighting a potential limitation in the Main Paper's methodology.","paper_id":"df8c3a325419d63366b9b347739fcbf3e2c4d22c","title":"Self-Play Preference Optimization for Language Model Alignment","abstract":"Standard reinforcement learning from human feedback (RLHF) approaches relying on parametric models like the Bradley-Terry model fall short in capturing the intransitivity and irrationality in human preferences. Recent advancements suggest that directly working with preference probabilities can yield a more accurate reflection of human preferences, enabling more flexible and accurate language model alignment. In this paper, we propose a self-play-based method for language model alignment, which treats the problem as a constant-sum two-player game aimed at identifying the Nash equilibrium policy. Our approach, dubbed Self-Play Preference Optimization (SPPO), utilizes iterative policy updates to provably approximate the Nash equilibrium. Additionally, we propose a new SPPO objective which is both strongly motivated by theory and is simple and effective in practice. In our experiments, using only 60k prompts (without responses) from the UltraFeedback dataset and without any prompt augmentation, by leveraging a pre-trained preference model PairRM with only 0.4B parameters, SPPO can obtain a model from fine-tuning Mistral-7B-Instruct-v0.2 that achieves the state-of-the-art length-controlled win-rate of 28.53% against GPT-4-Turbo on AlpacaEval 2.0. It also outperforms the (iterative) DPO and IPO on MT-Bench, Arena-Hard, and the Open LLM Leaderboard. Starting from a stronger base model Llama-3-8B-Instruct, we are able to achieve a length-controlled win rate of 38.77%. Notably, the strong performance of SPPO is achieved without additional external supervision (e.g., responses, preferences, etc.) from GPT-4 or other stronger language models. Codes are available at https://github.com/uclaml/SPPO.","score":0.7955182790756226,"polarity":"negative","source":"semantic","year":2024,"authors":["Yue Wu","Zhiqing Sun","Huizhuo Yuan","Kaixuan Ji","Yiming Yang","Quanquan Gu"],"venue":null,"citation_count":92,"reference_count":54,"influential_citation_count":14,"corpus_id":269484698,"url":"https://www.semanticscholar.org/paper/df8c3a325419d63366b9b347739fcbf3e2c4d22c","arxiv_id":null,"contexts":null,"background":"Standard reinforcement learning from human feedback (RLHF) approaches relying on parametric models like the Bradley-Terry model fall short in capturing the intransitivity and irrationality in human preferences. Recent advancements suggest that directly working with preference probabilities can yield a more accurate reflection of human preferences, enabling more flexible and accurate language model alignment.","target":"In this paper, we propose a self-play-based method for language model alignment, which treats the problem as a constant-sum two-player game aimed at identifying the Nash equilibrium policy. Our approach, dubbed Self-Play Preference Optimization (SPPO), utilizes iterative policy updates to provably approximate the Nash equilibrium. Additionally, we propose a new SPPO objective which is both strongly motivated by theory and is simple and effective in practice. In our experiments, using only 60k prompts (without responses) from the UltraFeedback dataset and without any prompt augmentation, by leveraging a pre-trained preference model PairRM with only 0.4B parameters, SPPO can obtain a model from fine-tuning Mistral-7B-Instruct-v0.2 that achieves the state-of-the-art length-controlled win-rate of 28.53% against GPT-4-Turbo on AlpacaEval 2.0. It also outperforms the (iterative) DPO and IPO on MT-Bench, Arena-Hard, and the Open LLM Leaderboard. Starting from a stronger base model Llama-3-8B-Instruct, we are able to achieve a length-controlled win rate of 38.77%. Notably, the strong performance of SPPO is achieved without additional external supervision (e.g., responses, preferences, etc.) from GPT-4 or other stronger language models. Codes are available at https://github.com/uclaml/SPPO."},{"summary":"The Related Paper contrasts the Main Paper by emphasizing the limitations of existing methods that do not model preference strength, which the Main Paper claims is crucial for accurate preference measurement. While the Main Paper focuses on the Bradley-Terry model with ties to enhance preference modeling, the Related Paper introduces Online Self-Preferring language models that utilize self-generated response pairs to capture preference strength. This approach claims to be more efficient and robust, particularly in scenarios with limited data, suggesting that the Main Paper's reliance on traditional models may overlook innovative alternatives that improve alignment performance.","paper_id":"4040099ed20718f418733cd201709cd950f11def","title":"Online Self-Preferring Language Models","abstract":"Aligning with human preference datasets has been critical to the success of large language models (LLMs). Reinforcement learning from human feedback (RLHF) employs a costly reward model to provide feedback for on-policy sampling responses. Recently, offline methods that directly fit responses with binary preferences in the dataset have emerged as alternatives. However, existing methods do not explicitly model preference strength information, which is crucial for distinguishing different response pairs. To overcome this limitation, we propose Online Self-Preferring (OSP) language models to learn from self-generated response pairs and self-judged preference strengths. For each prompt and corresponding self-generated responses, we introduce a ranked pairing method to construct multiple response pairs with preference strength information. We then propose the soft-preference cross-entropy loss to leverage such information. Empirically, we demonstrate that leveraging preference strength is crucial for avoiding overfitting and enhancing alignment performance. OSP achieves state-of-the-art alignment performance across various metrics in two widely used human preference datasets. OSP is parameter-efficient and more robust than the dominant online method, RLHF when limited offline data are available and generalizing to out-of-domain tasks. Moreover, OSP language models established by LLMs with proficiency in self-preferring can efficiently self-improve without external supervision.","score":0.7877927422523499,"polarity":"negative","source":"semantic","year":2024,"authors":["Yuanzhao Zhai","Zhuo Zhang","Kele Xu","Hanyang Peng","Yue Yu","Dawei Feng","Cheng Yang","Bo Ding","Huaimin Wang"],"venue":null,"citation_count":0,"reference_count":40,"influential_citation_count":0,"corpus_id":269982928,"url":"https://www.semanticscholar.org/paper/4040099ed20718f418733cd201709cd950f11def","arxiv_id":null,"contexts":null,"background":"Aligning with human preference datasets has been critical to the success of large language models (LLMs). Reinforcement learning from human feedback (RLHF) employs a costly reward model to provide feedback for on-policy sampling responses. Recently, offline methods that directly fit responses with binary preferences in the dataset have emerged as alternatives. However, existing methods do not explicitly model preference strength information, which is crucial for distinguishing different response pairs.","target":"To overcome this limitation, we propose Online Self-Preferring (OSP) language models to learn from self-generated response pairs and self-judged preference strengths. For each prompt and corresponding self-generated responses, we introduce a ranked pairing method to construct multiple response pairs with preference strength information. We then propose the soft-preference cross-entropy loss to leverage such information. Empirically, we demonstrate that leveraging preference strength is crucial for avoiding overfitting and enhancing alignment performance. OSP achieves state-of-the-art alignment performance across various metrics in two widely used human preference datasets. OSP is parameter-efficient and more robust than the dominant online method, RLHF when limited offline data are available and generalizing to out-of-domain tasks. Moreover, OSP language models established by LLMs with proficiency in self-preferring can efficiently self-improve without external supervision."},{"summary":"The Related Paper contrasts the Main Paper by challenging the approach of representing human preferences as a single numerical value through reward modeling. While the Main Paper emphasizes the importance of measuring preference strength using the Bradley-Terry model with ties, the Related Paper advocates for a more nuanced representation of preferences that extends beyond traditional reward modeling. This alternative framework, LRHP, aims to create richer representations of human preferences, thereby enhancing their applicability in various tasks, which the Main Paper does not address.","paper_id":"0f26f61818a85c18e8e95d0b377db8c28453f791","title":"LRHP: Learning Representations for Human Preferences via Preference Pairs","abstract":"To improve human-preference alignment training, current research has developed numerous preference datasets consisting of preference pairs labeled as\"preferred\"or\"dispreferred\". These preference pairs are typically used to encode human preferences into a single numerical value through reward modeling, which acts as a reward signal during reinforcement learning from human feedback (RLHF). However, representing these human preferences as a numerical value complicates the analysis of these preferences and restricts their broader applications other than RLHF. In contrast, in this work, we introduce a preference representation learning task that aims to construct a richer and more structured representation of human preferences. We further develop a more generalizable framework, Learning Representations for Human Preferences via preference pairs (namely LRHP), which extends beyond traditional reward modeling to tackle this task. We verify the utility of preference representations in two downstream tasks: preference data selection and preference margin prediction. Building upon the human preferences in representations, we achieve strong performance in both tasks, significantly outperforming baselines.","score":0.7765465974807739,"polarity":"negative","source":"semantic","year":2024,"authors":["Chenglong Wang","Yang Gan","Yifu Huo","Yongyu Mu","Qiaozhi He","Murun Yang","Tong Xiao","Chunliang Zhang","Tongran Liu","Jingbo Zhu"],"venue":null,"citation_count":0,"reference_count":46,"influential_citation_count":0,"corpus_id":273185588,"url":"https://www.semanticscholar.org/paper/0f26f61818a85c18e8e95d0b377db8c28453f791","arxiv_id":null,"contexts":null,"background":"To improve human-preference alignment training, current research has developed numerous preference datasets consisting of preference pairs labeled as 'preferred' or 'dispreferred'. These preference pairs are typically used to encode human preferences into a single numerical value through reward modeling, which acts as a reward signal during reinforcement learning from human feedback (RLHF). However, representing these human preferences as a numerical value complicates the analysis of these preferences and restricts their broader applications other than RLHF.","target":"In contrast, in this work, we introduce a preference representation learning task that aims to construct a richer and more structured representation of human preferences. We further develop a more generalizable framework, Learning Representations for Human Preferences via preference pairs (namely LRHP), which extends beyond traditional reward modeling to tackle this task. We verify the utility of preference representations in two downstream tasks: preference data selection and preference margin prediction. Building upon the human preferences in representations, we achieve strong performance in both tasks, significantly outperforming baselines."},{"summary":"The Related Paper supports the Main Paper by demonstrating the effectiveness of using human preferences in reinforcement learning tasks, which aligns with the Main Paper's focus on preference modeling. Both papers emphasize the importance of accurately capturing human preferences to improve learning outcomes, with the Related Paper providing empirical evidence that leveraging human feedback can lead to successful training of complex behaviors in RL systems. This reinforces the Main Paper's argument for the necessity of incorporating ties in preference strength measurement, as it highlights the practical implications of understanding and utilizing human preferences in RL.","paper_id":"5bbb6f9a8204eb13070b6f033e61c84ef8ee68dd","title":"Deep Reinforcement Learning from Human Preferences","abstract":"For sophisticated reinforcement learning (RL) systems to interact usefully with real-world environments, we need to communicate complex goals to these systems. In this work, we explore goals defined in terms of (non-expert) human preferences between pairs of trajectory segments. We show that this approach can effectively solve complex RL tasks without access to the reward function, including Atari games and simulated robot locomotion, while providing feedback on less than one percent of our agent's interactions with the environment. This reduces the cost of human oversight far enough that it can be practically applied to state-of-the-art RL systems. To demonstrate the flexibility of our approach, we show that we can successfully train complex novel behaviors with about an hour of human time. These behaviors and environments are considerably more complex than any that have been previously learned from human feedback.","score":0.5992391705513,"polarity":"positive","source":"citations","year":2017,"authors":["P. Christiano","J. Leike","Tom B. Brown","Miljan Martic","S. Legg","Dario Amodei"],"venue":null,"citation_count":2786,"reference_count":45,"influential_citation_count":377,"corpus_id":4787508,"url":"https://www.semanticscholar.org/paper/5bbb6f9a8204eb13070b6f033e61c84ef8ee68dd","arxiv_id":null,"contexts":[{"sentence":"Reinforcement learning from human feedback (RLHF) \\citep{christiano2017deep, ziegler2019fine, ouyang2022training} has played a pivotal role in aligning large language models (LLMs) \\citep{kenton2021alignment}, enhancing specific capabilities of LLMs in var","polarity":"positive"},{"sentence":"The reward model plays a crucial role in RLHF, guiding LLMs towards objectives aligned with human preferences \\citep{christiano2017deep, kaufmann2023survey}.","polarity":"positive"}],"background":null,"target":null},{"summary":"The Related Paper, 'Direct Preference Optimization: Your Language Model is Secretly a Reward Model', supports the Main Paper by addressing the challenges of aligning language models with human preferences, a key focus of the Main Paper. Both papers emphasize the importance of accurately capturing human preferences in reinforcement learning contexts. While the Main Paper introduces the generalized Bradley-Terry model with ties to improve preference strength measurement, the Related Paper presents a novel parameterization of the reward model that simplifies the fine-tuning process, enhancing stability and performance. Together, they highlight complementary approaches to refining reward learning from human feedback.","paper_id":"0d1c76d45afa012ded7ab741194baf142117c495","title":"Direct Preference Optimization: Your Language Model is Secretly a Reward Model","abstract":"While large-scale unsupervised language models (LMs) learn broad world knowledge and some reasoning skills, achieving precise control of their behavior is difficult due to the completely unsupervised nature of their training. Existing methods for gaining such steerability collect human labels of the relative quality of model generations and fine-tune the unsupervised LM to align with these preferences, often with reinforcement learning from human feedback (RLHF). However, RLHF is a complex and often unstable procedure, first fitting a reward model that reflects the human preferences, and then fine-tuning the large unsupervised LM using reinforcement learning to maximize this estimated reward without drifting too far from the original model. In this paper we introduce a new parameterization of the reward model in RLHF that enables extraction of the corresponding optimal policy in closed form, allowing us to solve the standard RLHF problem with only a simple classification loss. The resulting algorithm, which we call Direct Preference Optimization (DPO), is stable, performant, and computationally lightweight, eliminating the need for sampling from the LM during fine-tuning or performing significant hyperparameter tuning. Our experiments show that DPO can fine-tune LMs to align with human preferences as well as or better than existing methods. Notably, fine-tuning with DPO exceeds PPO-based RLHF in ability to control sentiment of generations, and matches or improves response quality in summarization and single-turn dialogue while being substantially simpler to implement and train.","score":0.5359464287757874,"polarity":"positive","source":"citations","year":2023,"authors":["Rafael Rafailov","Archit Sharma","E. Mitchell","Stefano Ermon","Christopher D. Manning","Chelsea Finn"],"venue":null,"citation_count":2660,"reference_count":59,"influential_citation_count":785,"corpus_id":258959321,"url":"https://www.semanticscholar.org/paper/0d1c76d45afa012ded7ab741194baf142117c495","arxiv_id":null,"contexts":[{"sentence":"Moreover, the direct preference optimization (DPO) method \\citep{rafailov2024direct} utilizes LLMs to implicitly represent the reward model through mathematical transformations, bypassing the complex RL optimization phase and focusing solely on the reward ","polarity":"positive"},{"sentence":"As a simplified alternative to RLHF, DPO \\citep{rafailov2024direct} has achieved significant success and impact.","polarity":"positive"},{"sentence":"We follow \\citet{rafailov2024direct}, fine-tuning on Anthropic's HH-RLHF dataset \\citep{bai2022training} and consistently setting MATH_PLACEHOLDER for DPO.","polarity":"positive"}],"background":null,"target":null},{"summary":"The Related Paper, 'Direct Preference Optimization with an Offset', supports the Main Paper by emphasizing the importance of nuanced preference modeling in aligning language models with human feedback. While the Main Paper introduces the generalized Bradley-Terry model to account for ties in preferences, the Related Paper extends the concept of preference strength by proposing an offset in direct preference optimization, highlighting that not all preferences are equal. Both papers advocate for improved methods of capturing human preferences, with the Related Paper's findings on the significance of preference strength complementing the Main Paper's focus on the impact of ties.","paper_id":"ed81c644692ec88f59e287366c8cba619948295a","title":"Direct Preference Optimization with an Offset","abstract":"Direct preference optimization (DPO) is a successful fine-tuning strategy for aligning large language models with human preferences without the need to train a reward model or employ reinforcement learning. DPO, as originally formulated, relies on binary preference data and fine-tunes a language model to increase the likelihood of a preferred response over a dispreferred response. However, not all preference pairs are equal. Sometimes, the preferred response is only slightly better than the dispreferred one. In other cases, the preference is much stronger. For instance, if a response contains harmful or toxic content, the annotator will have a strong preference for that response. In this paper, we propose a generalization of DPO, termed DPO with an offset (ODPO), that does not treat every preference pair equally during fine-tuning. Intuitively, ODPO requires the difference between the likelihood of the preferred and dispreferred response to be greater than an offset value. The offset is determined based on the extent to which one response is preferred over another. Our experiments on various tasks suggest that ODPO significantly outperforms DPO in aligning language models, especially when the number of preference pairs is limited.","score":0.5073037147521973,"polarity":"positive","source":"citations","year":2024,"authors":["Afra Amini","Tim Vieira","Ryan Cotterell"],"venue":null,"citation_count":45,"reference_count":26,"influential_citation_count":6,"corpus_id":267740352,"url":"https://www.semanticscholar.org/paper/ed81c644692ec88f59e287366c8cba619948295a","arxiv_id":null,"contexts":[{"sentence":"This method can be viewed as a variant of adaptive margin \\citep{touvron2023llama} when training the reward model and a variant of DPO with offset (ODPO) when training DPO \\citep{amini2024direct}.","polarity":"positive"},{"sentence":"\\citet{amini2024direct} propose DPO with an offset (ODPO), where the likelihood difference between the preferred and dispreferred response must exceed an offset value.","polarity":"positive"},{"sentence":"We note that this method can be viewed as a variant of DPO with an offset (ODPO) \\citep{amini2024direct} when fine tuning with DPO.","polarity":"positive"},{"sentence":"It is important to note that this approach can be viewed as a variant of the ODPO method \\citep{amini2024direct}, with the key difference being the bias-correction term.","polarity":"positive"}],"background":null,"target":null},{"summary":"The Related Paper on Advantage-Weighted Regression (AWR) supports the Main Paper by emphasizing the importance of leveraging off-policy data and maximizing the efficiency of learning algorithms, which aligns with the Main Paper's focus on improving preference modeling in reinforcement learning. Both papers highlight the significance of utilizing additional information—whether through ties in preferences or off-policy data—to enhance performance in reinforcement learning tasks. The theoretical foundations and empirical validations presented in the Related Paper bolster the Main Paper's claims regarding the necessity of refining reward learning methodologies.","paper_id":"ad14227e4f51276892ffc37aa43fd8750bb5eba8","title":"Advantage-Weighted Regression: Simple and Scalable Off-Policy Reinforcement Learning","abstract":"In this paper, we aim to develop a simple and scalable reinforcement learning algorithm that uses standard supervised learning methods as subroutines. Our goal is an algorithm that utilizes only simple and convergent maximum likelihood loss functions, while also being able to leverage off-policy data. Our proposed approach, which we refer to as advantage-weighted regression (AWR), consists of two standard supervised learning steps: one to regress onto target values for a value function, and another to regress onto weighted target actions for the policy. The method is simple and general, can accommodate continuous and discrete actions, and can be implemented in just a few lines of code on top of standard supervised learning methods. We provide a theoretical motivation for AWR and analyze its properties when incorporating off-policy data from experience replay. We evaluate AWR on a suite of standard OpenAI Gym benchmark tasks, and show that it achieves competitive performance compared to a number of well-established state-of-the-art RL algorithms. AWR is also able to acquire more effective policies than most off-policy algorithms when learning from purely static datasets with no additional environmental interactions. Furthermore, we demonstrate our algorithm on challenging continuous control tasks with highly complex simulated characters.","score":0.4879026710987091,"polarity":"positive","source":"citations","year":2019,"authors":["X. B. Peng","Aviral Kumar","Grace Zhang","S. Levine"],"venue":null,"citation_count":497,"reference_count":39,"influential_citation_count":57,"corpus_id":203610423,"url":"https://www.semanticscholar.org/paper/ad14227e4f51276892ffc37aa43fd8750bb5eba8","arxiv_id":null,"contexts":[{"sentence":"utilizes the fact that the optimization problem has the closed form solution \\citep{go2023aligning, korbak2022reinforcement, peng2019advantage, peters2007reinforcement}: _r(y x)= {Z(x)} ^{ }(y x) ( { } r(x, y) ) .","polarity":"positive"}],"background":null,"target":null},{"summary":"The Related Paper provides a comprehensive overview of Reinforcement Learning from Human Feedback (RLHF), emphasizing its significance in aligning intelligent systems with human values, which directly supports the Main Paper's focus on improving reward learning through the generalized Bradley-Terry model with ties. By highlighting the importance of preference-based reinforcement learning and the role of human feedback in training large language models, the Related Paper reinforces the Main Paper's claims about the necessity of accurately measuring preference strength, particularly in the presence of ties, thus validating the proposed enhancements in preference modeling.","paper_id":"6f9dbae279fa0c3a90d12f3b0f271dc8e6274817","title":"A Survey of Reinforcement Learning from Human Feedback","abstract":"Reinforcement learning from human feedback (RLHF) is a variant of reinforcement learning (RL) that learns from human feedback instead of relying on an engineered reward function. Building on prior work on the related setting of preference-based reinforcement learning (PbRL), it stands at the intersection of artificial intelligence and human-computer interaction. This positioning offers a promising avenue to enhance the performance and adaptability of intelligent systems while also improving the alignment of their objectives with human values. The training of large language models (LLMs) has impressively demonstrated this potential in recent years, where RLHF played a decisive role in directing the model's capabilities toward human objectives. This article provides a comprehensive overview of the fundamentals of RLHF, exploring the intricate dynamics between RL agents and human input. While recent focus has been on RLHF for LLMs, our survey adopts a broader perspective, examining the diverse applications and wide-ranging impact of the technique. We delve into the core principles that underpin RLHF, shedding light on the symbiotic relationship between algorithms and human feedback, and discuss the main research trends in the field. By synthesizing the current landscape of RLHF research, this article aims to provide researchers as well as practitioners with a comprehensive understanding of this rapidly growing field of research.","score":0.4249073266983032,"polarity":"positive","source":"citations","year":2023,"authors":["Timo Kaufmann","Paul Weng","Viktor Bengs","Eyke Hüllermeier"],"venue":null,"citation_count":98,"reference_count":0,"influential_citation_count":4,"corpus_id":266521540,"url":"https://www.semanticscholar.org/paper/6f9dbae279fa0c3a90d12f3b0f271dc8e6274817","arxiv_id":null,"contexts":[{"sentence":"The reward model guides the optimization procedure of RLHF towards objectives aligned with human preferences \\citep{kaufmann2023survey}.","polarity":"positive"},{"sentence":"The reward model plays a crucial role in RLHF, guiding LLMs towards objectives aligned with human preferences \\citep{christiano2017deep, kaufmann2023survey}.","polarity":"positive"}],"background":null,"target":null}],"paper":{"title":"Reward Learning From Preference With Ties","abstract":"Reward learning plays a pivotal role in Reinforcement Learning from Human Feedback (RLHF), ensuring the alignment of language models. The Bradley-Terry (BT) model stands as the prevalent choice for capturing human preferences from datasets containing pairs of chosen and rejected responses. In preference modeling, the focus is not on absolute values but rather on the reward difference between chosen and rejected responses, referred to as preference strength. Thus, precise evaluation of preference strength holds paramount importance in preference modeling. However, an easily overlooked factor significantly affecting preference strength measurement is that human attitudes towards two responses may not solely indicate a preference for one over the other and ties are also a common occurrence. To address this, we propose the adoption of the generalized Bradley-Terry model -- the Bradley-Terry model with ties (BTT) -- to accommodate tied preferences, thus leveraging additional information. We prove that even with the access to the true distributions of prompt and response, disregarding ties can lead to a notable bias in preference strength measurement. Comprehensive experiments further validate the advantages of incorporating ties in preference modeling. Notably, fine-tuning with BTT significantly outperforms fine-tuning with BT on synthetic preference datasets with ties, labeled by state-of-the-art open-source LLMs.","authors":["Jinsong Liu","Dongdong Ge","Ruihao Zhu"],"sections":[{"heading":"Introduction","text":"# Introduction\n\nReinforcement learning from human feedback (RLHF) [@christiano2017deep; @ziegler2019fine; @ouyang2022training] has played a pivotal role in aligning large language models (LLMs) [@kenton2021alignment], enhancing specific capabilities of LLMs in various fields, such as summarization [@stiennon2020learning], coding [@gao2023pal], and medical assistance [@moor2023foundation]. A crucial component of the RLHF process is the reward model, which serves as the primary mechanism for integrating human preferences and feedback into the learning process [@wang2024secrets]. The reward model guides the optimization procedure of RLHF towards objectives aligned with human preferences [@kaufmann2023survey]. Therefore, the accuracy of the reward model greatly affects or even determines the effectiveness of alignment with human preferences. Moreover, the direct preference optimization (DPO) method [@rafailov2024direct] utilizes LLMs to implicitly represent the reward model through mathematical transformations, bypassing the complex RL optimization phase and focusing solely on the reward modeling phase. As a simplified alternative to RLHF, DPO has demonstrated computational efficiency and competitive performance compared to RLHF.\n\nTo learn a reward model from human preferences, obtaining high-quality human preference data is crucial [@wang2024secrets], typically achieved by having human labelers annotate previously collected data consisting of a prompt and a pair of responses [@ouyang2022training; @bai2022training]. We note that conventional approaches [@rafailov2024direct; @ziegler2019fine; @stiennon2020learning] often assume that the latent human preference model follows the Bradley-Terry (BT) [@bradley1952rank] model, where the preference distribution can be expressed as: $$p_r\\left(y_1 \\succ y_2 \\mid x\\right)=\\frac{\\exp \\left(r\\left(x, y_1\\right)\\right)}{\\exp \\left(r\\left(x, y_1\\right)\\right)+\\exp \\left(r\\left(x, y_2\\right)\\right)} = \\sigma (-\\Delta r)$$ where $r$ is the latent reward model, $\\sigma$ is sigmoid function, and $\\Delta r = r(x,y_1) - r(x, y_2)$ is preference strength. Consequently, human labelers are presented with only two options for each prompt $x$ and pair of responses $y_1$ and $y_2$: either $y_1$ is preferred or $y_2$ is preferred. However, this approach does not fully align with human attitudes towards preferences, as humans often perceive two responses as ties when their rewards have very little difference. To illustrate this point, we refer to the findings of @wang2024secrets due to the scarcity of preference datasets including ties. In their study, $10$ different reward models are trained on Anthropic's HH-RLHF [@bai2022training] dataset assuming the BT preference model. The mean and standard deviation of these $10$ reward models can partially reflect the real human preferences of this dataset. From the results of @wang2024secrets, the absolute value of the preference strength for a large amount of data is close to $0$, indicating that the preference difference in these cases is minimal and the pair of responses can often be considered as ties. We provide examples of data with small absolute mean preference strength in [1](#data_examples){reference-type=\"ref+label\" reference=\"data_examples\"} to further emphasize the importance of introducing ties in preferences.\n\n::: {#data_examples}\n  Prompt                **Can you help me set up an outdoor running routine so that I can get in shape?**\n  --------------------- ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n  Chosen Response       What kind of running are you interested in?\n  Rejected Response     Sure, what kind of program are you looking for?\n  Preference Strength   Mean : 0.0027, Standard Deviation: 0.22\n  Prompt                **Can you tell me what the meaning and themes are of the book Oliver Twist**\n  Chosen Response       Sure, here are some common themes, and there are a few interpretations that scholars have proposed: - the evil and temptations of London, - the books portray Oliver as a victim of the harsh society, - Oliver is also a victim of his situation, - the story is an attack on\n  Rejected Response     The meaning of the book is about the theme of being homeless and being poor. This is an important thing in the society we live in today. When we have children, we want them to be able to live a good life with lots of opportunities. However, we don't have all the resources to provide that to them. So the book is about the struggles of poor people trying to live a life with a family, which is difficult and sad.\n  Preference Strength   Mean : 0.0, Standard Deviation: 0.36\n  Prompt                **I want to purchase a men's suit, but I don't know what I am looking for or how to get started.**\n  Chosen Response       Sure! In what price range are you looking to spend?\n  Rejected Response     Great, let me walk you through the process. First we'll start with the size, do you know your measurements?\n  Preference Strength   Mean : 0.00019, Standard Deviation: 0.23\n\n  : Examples of pairs of responses with low absolute mean preference strength in the annotated Anthropic's HH-RLHF dataset [@wang2024secrets].\n:::\n\nIn this paper, we propose to model human preferences with the Bradley-Terry model with ties (BTT) [@rao1967ties]. We demonstrate that if we blindly adopt BT as our preference model (with the true latent preference model being BTT) can lead to significant bias in measuring preference strength due to model mismatch. To illustrate the model mismatch problem occurring in conventional preference datasets lacking ties when simply assuming the latent preference model is BT, we first introduce a simulated preference generation procedure. Specifically, we generate two preference datasets---one with ties and one without ties. By analyzing the maximum likelihood estimates (MLE) based on BTT and BT preference models on these datasets respectively, we quantify the bias in measuring preference strength due to the model mismatch. Furthermore, we show that although the bias term is bounded, it can still have a substantial impact. Since most conventional preference datasets lack ties, we propose a novel method to address the preference model mismatch problem, which subtracts the bias term from the MLE loss function to recover the true preference strength measurement. This method can be viewed as a variant of adaptive margin [@touvron2023llama] when training the reward model and a variant of DPO with offset (ODPO) when training DPO [@amini2024direct]. To further demonstrate the benefit of incorporating ties in preference modeling, we use state-of-the-art open-source LLMs to simulate human judgment and label ties in a conventional preference dataset without ties, and then evaluate the fine-tuned models on this synthetic preference dataset with ties. It is important to note that the main limitation of this paper is the inability to conduct experiments on real human-labeled preference datasets with ties, due to the scarcity of such datasets and the high cost of manual annotation and evaluation. Addressing this limitation could be considered for future work.\n\n**Main Contributions.** Our contributions can be outlined as follows:\n\n-   We advocate for the inclusion of tie options when labeling preference data, aligning with human preference habits. To the best of our knowledge, we are the first to propose the use of BTT to model human preference.\n\n-   We derive the bias in measuring preference strength caused by model mismatch when assuming the latent preference model is BTT. To address this, we propose a novel bias-correction method to mitigate this bias in conventional preference datasets without ties, as validated by comprehensive experimental results.\n\n-   We generate a synthetic preference dataset with ties, labeled by state-of-the-art open-source LLMs, and evaluate fine-tuning with BTT and BT on this dataset. The results show that fine-tuning with BTT consistently outperforms fine-tuning with BT."},{"heading":"Related Work","text":"# Related Work\n\nThe reward model plays a crucial role in RLHF, guiding LLMs towards objectives aligned with human preferences [@christiano2017deep; @kaufmann2023survey]. Recent related work has addressed various aspects of reward modeling. @wang2024secrets conducted a comprehensive study on reward models, proposing a method to measure the strength of preferences within the data and introducing contrastive learning to enhance the ability of reward models to distinguish between chosen and rejected responses. @zhu2024iterative analyzed reward overfitting and overoptimization problems in RLHF, proposing to mitigate them using an iterative data smoothing method. @dai2023safe proposed training a cost model in addition to the reward model to decouple human preferences regarding helpfulness and harmlessness.\n\nAs a simplified alternative to RLHF, DPO [@rafailov2024direct] has achieved significant success and impact. The core concept of DPO involves implicitly representing the reward model using LLMs through a clever reparameterization. Recently, there has been extensive research focused on enhancing and broadening the scope of DPO. @amini2024direct propose DPO with an offset (ODPO), where the likelihood difference between the preferred and dispreferred response must exceed an offset value. @zhou2023beyond extend DPO for multiple alignment objectives by training LMs as implicit collective reward models, combining all objectives with specific weightings. @chowdhury2024provably propose robust DPO methods to mitigate the bias introduced by noise in preference data on average.\n\nThe preference model serves as the foundation for reflecting human feedback, with the Bradley-Terry (BT) model [@bradley1952rank] being the most commonly used preference model in RLHF. Indeed, various generalized models based on the BT model have been proposed to address different scenarios, such as handling home advantage [@agresti2012categorical], ties [@rao1967ties], multiple comparisons [@plackett1975analysis; @luce2005individual], and team comparisons [@huang2006generalized]. In particular, the Plackett-Luce (PL) model, a popular extension for handling multiple comparisons, has also found application in RLHF [@zhu2023principled; @song2024preference]."},{"heading":"Preliminaries","text":"# Preliminaries\n\n**RLHF** typically comprises three phases: supervised fine-tuning (SFT), reward learning, and reinforcement learning. In the first phase, a pre-trained language model undergoes fine-tuning via supervised learning on high-quality data tailored for specific tasks such as dialogue and summarization. This fine-tuning process yields the model $\\pi^{\\mathrm{SFT}}$. The second phase involves reward learning on a preference dataset. To construct this dataset, prompts $x \\sim \\mathcal{X}$ are fed to $\\pi^{\\mathrm{SFT}}$, generating pairs of responses $\\left(y_1, y_2\\right) \\sim \\pi^{\\mathrm{SFT}}(y \\mid x)$. These pairs are presented to human labelers, who express preferences. Conventional preference datasets do not allow ties and require one response to be preferred over the other, denoted as $y_w \\succ y_l \\mid x$, where $y_w$ and $y_l$ represent the preferred and dispreferred completions among $\\left(y_1, y_2\\right)$, respectively. The most popular approach to modeling preference is the Bradley-Terry (BT) model, which assumes the human preference distribution $p^*$ as: $$\\label{BT_model}\np^*\\left(y_1 \\succ y_2 \\mid x\\right)=\\frac{\\exp \\left(r^*\\left(x, y_1\\right)\\right)}{\\exp \\left(r^*\\left(x, y_1\\right)\\right)+\\exp \\left(r^*\\left(x, y_2\\right)\\right)} .$$ where $r^*(y, x)$ is the latent reward model which is inaccessible. Assuming access to a static dataset of comparisons $\\mathcal{D}=\\left\\{x^{(i)}, y_w^{(i)}, y_l^{(i)}\\right\\}_{i=1}^N$ sampled from $p^*$, we can parametrize a reward model $r_\\psi(x, y)$ and estimate the parameters via maximum likelihood. Framing the problem as a binary classification we have the negative log-likelihood loss: $$\\mathcal{L}_R\\left(r_\\psi, \\mathcal{D}\\right)=-\\mathbb{E}_{\\left(x, y_w, y_l\\right) \\sim \\mathcal{D}}\\left[\\log \\sigma\\left(r_\\psi\\left(x, y_w\\right)-r_\\psi\\left(x, y_l\\right)\\right)\\right] ,$$ where $\\sigma$ is the logistic function. And the third phase is to solve the following RL problem with the learned reward function: $$\\label{RL_prob}\n    \\max _{\\pi_\\theta} \\mathbb{E}_{x \\sim \\mathcal{D}, y \\sim \\pi_\\theta(y \\mid x)}\\left[r_\\psi(x, y)\\right]-\\beta \\mathbb{D}_{\\mathrm{KL}}\\left[\\pi_\\theta(y \\mid x) \\| \\pi^{\\mathrm{SFT}}(y \\mid x)\\right] ,$$ where $\\beta$ is a parameter controlling the deviation from the base reference policy $\\pi^{\\mathrm{SFT}}$.\n\n**DPO** utilizes the fact that the optimization problem [\\[RL_prob\\]](#RL_prob){reference-type=\"eqref\" reference=\"RL_prob\"} has the closed form solution [@go2023aligning; @korbak2022reinforcement; @peng2019advantage; @peters2007reinforcement]: $$\\pi_r(y \\mid x)=\\frac{1}{Z(x)} \\pi^{\\mathrm{SFT}}(y \\mid x) \\exp \\left(\\frac{1}{\\beta} r(x, y)\\right) .$$ Then a clever reparameterization is applied to express the reward function in terms of its corresponding optimal policy $\\pi_r$: $$r(x, y)=\\beta \\log \\frac{\\pi_r(y \\mid x)}{\\pi^{\\mathrm{SFT}}(y \\mid x)}+\\beta \\log Z(x) .$$ Applying this reparameterization to the ground-truth reward $r^*$ and corresponding optimal model $\\pi^*$, then substituting this reparameterization into the BT model [\\[BT_model\\]](#BT_model){reference-type=\"eqref\" reference=\"BT_model\"}, analogous to the reward modeling approach, the loss function of DPO becomes: $$\\mathcal{L}_{\\mathrm{DPO}}\\left(\\pi_\\theta ; \\pi_{\\mathrm{ref}}\\right)=-\\mathbb{E}_{\\left(x, y_w, y_l\\right) \\sim \\mathcal{D}}\\left[\\log \\sigma\\left(\\beta \\log \\frac{\\pi_\\theta\\left(y_w \\mid x\\right)}{\\pi^{\\mathrm{SFT}}\\left(y_w \\mid x\\right)}-\\beta \\log \\frac{\\pi_\\theta\\left(y_l \\mid x\\right)}{\\pi^{\\mathrm{SFT}}\\left(y_l \\mid x\\right)}\\right)\\right] .$$\n\n**Bradley-Terry model with ties (BTT)** [@rao1967ties] can be employed to model human preference with ties, i.e., the two response $\\left(y_1, y_2\\right) \\sim \\pi^{\\mathrm{SFT}}(y \\mid x)$ are considered equal with respect to the prompt $x$: $$\\begin{aligned}\np_{\\theta}^*\\left(y_1 = y_2 \\mid x \\right)=&\\frac{(\\theta^2 - 1)\\exp \\left(r^*\\left(x, y_1\\right)\\right)\\exp \\left(r^*\\left(x, y_2\\right)\\right)}{\\left(\\exp \\left(r^*\\left(x, y_1\\right)\\right)+ \\theta \\exp \\left(r^*\\left(x, y_2\\right)\\right)\\right)\\left(\\theta \\exp \\left(r^*\\left(x, y_1\\right)\\right)+ \\exp \\left(r^*\\left(x, y_2\\right)\\right)\\right)}\\\\\np_{\\theta}^*\\left(y_1 \\succ y_2 \\mid x\\right)=&\\frac{\\exp \\left(r^*\\left(x, y_1\\right)\\right)}{\\exp \\left(r^*\\left(x, y_1\\right)\\right)+ \\theta \\exp \\left(r^*\\left(x, y_2\\right)\\right)}\n\\end{aligned}$$ where $\\theta \\geq 1$ is the parameter controlling the tendency to ties, with a larger $\\theta$ indicating a higher probability of ties occurring. It's worth noting that when $\\theta = 1$, the BTT model is equivalent to the BT model."},{"heading":"Preference Modeling With Ties","text":"# Preference Modeling With Ties\n\nFor a given reward model $r$, RLHF focuses not on the absolute values $r(x,y_1), r(x,y_2)$ but on the preference strength between the pair of responses [@wang2024secrets]: $$\\Delta r = r(x,y_1) - r(x,y_2)$$ In this section, we will explain that if the real preference model is BTT, but we do not provide human labelers with the option of a tie to generate the preference dataset, the learned reward model will exhibit significant deviation from the real reward model in measuring preference strength.\n\n## Preference Dataset Under BTT {#pref_dataset_BTT}\n\nSince previous preference datasets do not include ties, we will first explain the simulation process for obtaining a preference dataset without ties when assuming the preference model is BTT. Suppose we have $n$ samples, each consisting of a prompt and a pair of responses, denoted as $D = \\{\\left(x_i, y_i^1, y_i^2\\right)\\}_{i=1}^n$. With $D$ available, if we assume the true preference model is BTT, we can obtain preference datasets with and without ties using the following methods:\n\n-   Offer three options to human labelers: $y_1 \\succ y_2$, $y_2 \\succ y_1$, or $y_1 = y_2$. Then, we can derive a preference dataset with ties $D^{BTT}$ from the original dataset $D$. We denote that $D^{BTT} = D^{BT} \\cup D^T$, where $D^{BT} = \\{\\left(x_i, y_i^{w}, y_i^{l}\\right)\\}, y_i^{w} \\succ y_i^{l}, i \\in \\mathcal{J}; D^{T} = \\{\\left(x_i, y_i^{1}, y_i^{2}\\right)\\}, y_i^{1} = y_i^{2}, i \\in \\mathcal{K}$, and $\\mathcal{J} \\cup \\mathcal{K} = \\{n\\}$.\n\n-   For the ties dataset $D^T$, ask human labelers to further specify which response is preferred, resulting in the dataset $D^{TN} = \\{\\left(x_i, y_i^{w}, y_i^{l}\\right)\\}, y_i^{w} \\succ y_i^{l}, i \\in \\mathcal{K}$. We denote $D^{BTTN} = D^{BT} \\cup D^{TN}$.\n\n::: {#ass_rand_label_in_ties .assumption}\n**Assumption 1**. Human labelers randomly label responses in ties, assigning each response an equal probability of being preferred.\n:::\n\nIn summary, if we assume the preference model is the BTT model and provide the option for ties to human labelers, we obtain the preference dataset with ties $D^{BTT}$. By subsequently asking human labelers to specify preferred responses within ties, we derive the preference dataset without ties $D^{BTTN}$. Therefore, we can consider conventional preference datasets without ties as $D^{BTTN}$.\n\n## Bias in Measuring Preference Strength\n\nAssuming we have both $D^{BTT}$ and $D^{BTTN}$ derived from $D$, we can illustrate how to estimate the latent reward model using maximum likelihood estimation (MLE). Since we assume that the latent preference model is BTT and thus obtain the dataset with ties, the most accurate log-likelihood is: $$\\label{LCE_BTT}\n    LCE^{BTT}(r, D) = \\sum_{(x, y_w,y_l) \\in D^{BT}} \\log p_r^{\\theta}(y_w \\succ y_l \\mid x) + \\sum_{(x, y_1,y_2) \\in D^{T}} \\log p_r^{\\theta}(y_1 = y_2 \\mid x)$$ Conventional approaches to estimate the latent reward model typically utilize $D^{BTTN}$ to fit the BT model, with the log-likelihood given by: $$\\label{LCE_BT}\n    LCE^{BT}(r, D) = \\sum_{(x, y_w, y_l) \\in D^{BTTN}} \\log p_r(y_w \\succ y_l \\mid x)$$ We can demonstrate that, even if we possess access to the true prompt and response distributions, there may exist a noteworthy discrepancy between the learned and the actual reward model in measuring preference strength, as illustrated by the following results.\n\nFirst, we can establish the relationship between the true reward model $r^*$ and the learned reward model $\\hat{r}$ by fully optimizing [\\[LCE_BT\\]](#LCE_BT){reference-type=\"eqref\" reference=\"LCE_BT\"} in [2](#relation){reference-type=\"ref+label\" reference=\"relation\"}.\n\n::: {#relation .theorem}\n**Theorem 2**. *$$\\mathbb{E}\\left[LCE^{BT}(r, D)\\right] \\le \\mathbb{E}\\left[LCE^{BT}(\\hat{r}, D)\\right], \\forall r \\neq \\hat{r}$$ where $\\hat{r}$ satisfies $$\\label{r_hat}\n    p_{\\hat{r}}(y_1 \\succ y_2 \\mid x) = q_{r^*}^{\\theta}(y_1 \\succ y_2 \\mid x), \\forall x \\sim \\mathcal{X}, (y_1, y_2) \\sim \\pi^{\\mathrm{SFT}}(y \\mid x)$$ and $$q_r^{\\theta}\\left(y_1 \\succ y_2 \\mid x\\right) = p_r^{\\theta}\\left(y_1 \\succ y_2 \\mid x\\right) + \\frac{1}{2}p_r^{\\theta}\\left(y_1 = y_2 \\mid x\\right)$$*\n:::\n\n::: proof\n*Proof.* By Assumption [1](#ass_rand_label_in_ties){reference-type=\"ref\" reference=\"ass_rand_label_in_ties\"} we know that the true preference distribution without ties is $q_r^{\\theta}$. Therefore, it is equivalent to verify that: $$\\begin{aligned}\n        \\mathbb{E}_{x\\sim \\mathcal{X}, (y_1, y_2) \\sim \\pi^{\\mathrm{SFT}}(y \\mid x), (y_w, y_l) \\sim q_{r^*}^{\\theta}}\\left[\\log \\frac{p_r(y_w \\succ y_l \\mid x)}{p_{\\hat{r}}(y_w \\succ y_l \\mid x)}\\right] \\leq 0\n    \n\\end{aligned}$$ by Jensen's inequality we have: $$\\begin{aligned}\n        &\\mathbb{E}\\left[\\log \\frac{p_r(y_w \\succ y_l \\mid x)}{p_{\\hat{r}}(y_w \\succ y_l \\mid x)}\\right] \\leq \\log \\left( \\mathbb{E}\\left[ \\frac{p_r(y_w \\succ y_l \\mid x)}{p_{\\hat{r}}(y_w \\succ y_l \\mid x)}\\right] \\right)\\\\\n        &=\\log \\left(\\mathbb{E}_{(x, y_1, y_2)}\\left[q_{r^*}^{\\theta}(y_1 \\succ y_2 \\mid x) \\frac{p_r(y_1 \\succ y_2 \\mid x)}{p_{\\hat{r}}(y_1 \\succ y_2 \\mid x)} + q_{r^*}^{\\theta}(y_2 \\succ y_1 \\mid x) \\frac{p_r(y_2 \\succ y_1 \\mid x)}{p_{\\hat{r}}(y_2 \\succ y_1 \\mid x)} \\right] \\right)\\\\\n        &=\\log \\left(\\mathbb{E}_{(x, y_1, y_2)}\\left[p_r(y_1 \\succ y_2 \\mid x) + p_r(y_2 \\succ y_1 \\mid x) \\right] \\right)\\\\\n        &=\\log \\left(\\mathbb{E}_{(x, y_1, y_2)}\\left[1 \\right] \\right)\\\\\n        &=0\n    \n\\end{aligned}$$ ◻\n:::\n\n::: {#main_theorem .theorem}\n**Theorem 3**. *Even if we have the access to the true prompt and response distributions, there can be a bias in measuring preference strength: $$\\label{bias}\n    \\Delta \\hat{r} = \\Delta r^* + \\log \\left( \\frac{2 \\theta + \\left(1 + \\theta^2\\right)\\exp (- \\Delta r^*)}{1 + \\theta^2 + 2\\theta \\exp(-\\Delta r^*)}\\right ), \\forall (x, y_1, y_2)$$ where $\\Delta r = r(x, y_1) - r(x, y_2)$.*\n:::\n\n**Proof Sketch:** From [\\[r_hat\\]](#r_hat){reference-type=\"eqref\" reference=\"r_hat\"}, we can know that: $$\\begin{aligned}\n        p_{\\hat{r}}(y_1 \\succ y_2 \\mid x) = &p_{r^*}^{\\theta}(y_1 \\succ y_2 \\mid x) + \\frac{1}{2}p_{r^*}^{\\theta}(y_1 = y_2 \\mid x)\\\\\n        p_{\\hat{r}}(y_2 \\succ y_1 \\mid x) = &p_{r^*}^{\\theta}(y_2 \\succ y_1 \\mid x) + \\frac{1}{2}p_{r^*}^{\\theta}(y_2 = y_1 \\mid x)\\\\\n    \\end{aligned}$$ By subtraction, we can get: $$p_{\\hat{r}}(y_1 \\succ y_2 \\mid x) - p_{\\hat{r}}(y_2 \\succ y_1 \\mid x) = p_{r^*}^{\\theta}(y_1 \\succ y_2 \\mid x) - p_{r^*}^{\\theta}(y_2 \\succ y_1 \\mid x)$$ Consequently, we can derive the relation between $\\Delta \\hat{r}$ and $\\Delta r^*$. Detailed proof can be found in the appendix [\\[proof_main_theorem\\]](#proof_main_theorem){reference-type=\"ref\" reference=\"proof_main_theorem\"}.\n\nTo analyze the bias term $\\log \\left( \\frac{2 \\theta + \\left(1 + \\theta^2\\right)\\exp (- \\Delta r^*)}{1 + \\theta^2 + 2\\theta \\exp(-\\Delta r^*)}\\right )$, we can observe that its sign is opposite to $\\Delta r^*$, indicating that the preference strength is attenuated due to latent preference model mismatch. Additionally, the bias term is a sigmoid-shaped function, bounded by $\\log(\\frac{1+\\theta^2}{2 \\theta})$ in absolute value. However, despite this bound, the bias term can still be substantial. As mentioned earlier, @wang2024secrets trained 10 different reward models on the Anthropic's HH-RLHF dataset [@bai2022training], and the mean preference strength of 83.6% of the data falls within the interval $[-0.6, 2.94]$. In this range, the ratio between the bias term and $\\Delta r^*$ can be considerable, as depicted in [1](#bias_fig){reference-type=\"ref+label\" reference=\"bias_fig\"}.\n\n<figure id=\"bias_fig\">\n<div class=\"minipage\">\n<img src=\"bias.png\" style=\"width:100.0%\" />\n</div>\n<div class=\"minipage\">\n<img src=\"bias_ratio.png\" style=\"width:100.0%\" />\n</div>\n<figcaption>Bias term has a significant impact</figcaption>\n</figure>\n\n## Preference Strength Bias Correction Algorithm\n\nSince conventional preference datasets typically lack ties, we propose a novel method to address the model mismatch issue on preference datasets without ties, assuming the latent preference model is the BTT model. We acknowledge that the right side of [\\[bias\\]](#bias){reference-type=\"eqref\" reference=\"bias\"} is a monotonic function with respect to $\\Delta r^*$, implying a one-to-one mapping relationship between $\\Delta \\hat{r}$ and $\\Delta r^*$. Thus, during the optimization procedure, when obtaining the value of $\\Delta \\hat{r}$, we can treat [\\[bias\\]](#bias){reference-type=\"eqref\" reference=\"bias\"} as a nonlinear equation and solve for the value of $\\Delta r^*$, subsequently subtracting the bias term from the current $\\Delta \\hat{r}$. The detailed description of this method can be found in Alg. [\\[alg1\\]](#alg1){reference-type=\"ref\" reference=\"alg1\"}. We note that this method can be viewed as a variant of DPO with an offset (ODPO) [@amini2024direct] when fine tuning with DPO.\n\n:::: algorithm\n**Input**: Preference dataset without ties $D^{BTTN}$,\\\n$\\theta$: Parameter of the BTT model,\\\n$r_{\\psi}$: Parameterized reward model with parameters $\\psi$,\\\n**Output**: $\\psi$\n\n::: algorithmic\nCalculate the current value of $r_{\\psi}$. Solve the nonlinear equation [\\[bias\\]](#bias){reference-type=\"eqref\" reference=\"bias\"} with $\\Delta \\hat{r} = \\Delta r_{\\psi}$, and get the value of $\\Delta r^*$. Plug $\\Delta r = \\Delta r_{\\psi} - \\log \\left( \\frac{2 \\theta + \\left(1 + \\theta^2\\right)\\exp (- \\Delta r^*)}{1 + \\theta^2 + 2\\theta \\exp(-\\Delta r^*)}\\right )$ into the loss function [\\[LCE_BT\\]](#LCE_BT){reference-type=\"eqref\" reference=\"LCE_BT\"}. Perform optimization step for the new loss function. **return** $\\psi$\n:::\n::::"},{"heading":"Experiments","text":"# Experiments {#exp_section}\n\nIn this section, we empirically demonstrate the benefits of incorporating ties in preference learning. First, we conduct a simulation experiment to show that, when the ground truth reward function is accessible and the preference dataset is labeled according to the BTT model, the reward model trained with the BT model exhibits a stronger preference strength bias compared to the one trained with the BTT model. Second, we apply Algorithm [\\[alg1\\]](#alg1){reference-type=\"ref\" reference=\"alg1\"} to address the model mismatch problem on conventional preference datasets without ties. Finally, we use two state-of-the-art open-source LLMs Llama3-70b (abbreviated as Llama) [@meta2024introducing] and Qwen2-72b-instruct (abbreviated as Qwen) [@yang2024qwen2] to label whether pairs in Anthropic's HH-RLHF dataset [@bai2022training] are tied, thereby generating a simulated preference dataset with ties. We then evaluate the fine-tuning using BT and BTT on this dataset. We choose DPO as our fine-tuning technique because it is an simplified and efficient alternative to RLHF and allows LLMs to be treated as implicit reward models. We follow @rafailov2024direct, fine-tuning on Anthropic's HH-RLHF dataset [@bai2022training] and consistently setting $\\beta = 0.1$ for DPO. Additional experimental details can be found in Appendix [8](#exp_detail){reference-type=\"ref\" reference=\"exp_detail\"}.\n\n## Preference Bias With The Ground Truth Reward\n\nIn this section, we randomly generate a ground truth reward function $r^*(x, y), x \\in \\mathbb{N}^+, y \\in [0,1,2,3]^n$, along with a preference dataset labeled by the BTT model using $r^*$ (with tied pairs randomly assigned preferences). We then train two reward models, both parameterized by the same neural network, on this dataset using the loss functions [\\[LCE_BTT\\]](#LCE_BTT){reference-type=\"ref\" reference=\"LCE_BTT\"} and [\\[LCE_BT\\]](#LCE_BT){reference-type=\"ref\" reference=\"LCE_BT\"}, respectively. These trained reward models are denoted as $r^{BTT}$ and $r^{BT}$. Next, we evaluate the average preference bias of these two reward models relative to the ground truth reward under varying preference parameters $\\theta$. The preference bias difference, $\\Delta = |\\Delta r^{BT} - \\Delta r^*| - |\\Delta r^{BTT} - \\Delta r^*|$, is shown in Table [2](#bias_ground_truth){reference-type=\"ref\" reference=\"bias_ground_truth\"}. From the results, we observe that the preference bias of $r^{BTT}$ is consistently smaller than that of $r^{BT}$, indicating that the BTT model effectively reduces the preference bias with respect to the ground truth reward function, resulting in a more accurate reward model. We also find that as $\\theta$ increases, the preference bias difference becomes larger, which aligns with the trends shown in [1](#bias_fig){reference-type=\"ref+label\" reference=\"bias_fig\"}, as a larger $\\theta$ in ground truth preference model indicates a higher probability of ties occurring.\n\n::: {#bias_ground_truth}\n  -------------- -------------- ---------------\n  $\\theta = 2$   $\\theta = 5$   $\\theta = 10$\n  0.0206         0.0237         0.0353\n  -------------- -------------- ---------------\n\n  : The preference bias difference between $r^{BT}$ and $r^{BTT}$\n:::\n\n## DPO With a Bias-Correction Offset\n\nWe apply Alg. [\\[alg1\\]](#alg1){reference-type=\"ref\" reference=\"alg1\"} to the conventional preference dataset without ties, Anthropic's HH-RLHF, in order to mitigate the bias term using a DPO reward model. It is important to note that this approach can be viewed as a variant of the ODPO method [@amini2024direct], with the key difference being the bias-correction term. We train the small Pythia-160M model [@biderman2023pythia] for one epoch and record the reward preference accuracy on the test set. It is also worth mentioning that we do not evaluate Pythia-160M's inference capability, as the model is too small to generate meaningful responses. The experimental results are presented in [3](#exp_odpo){reference-type=\"ref+label\" reference=\"exp_odpo\"}. As shown, when $\\theta = 1$, the bias-correction term is consistently zero, which essentially reduces the method to DPO, serving as our baseline. From [3](#exp_odpo){reference-type=\"ref+label\" reference=\"exp_odpo\"}, we observe that all three ODPO methods, with $\\theta \\in \\{2, 5, 10\\}$, significantly outperform DPO, with ODPO at $\\theta = 5$ showing more than a $10\\%$ improvement in accuracy.\n\n::: {#exp_odpo}\n  -------------- -------------- -------------- ---------------\n  $\\theta = 1$   $\\theta = 2$   $\\theta = 5$   $\\theta = 10$\n  0.5333         0.5583         0.6042         0.5958\n  -------------- -------------- -------------- ---------------\n\n  : Test Accuracy of DPO with a bias-Correction offset\n:::\n\nTo further validate the effectiveness of Alg [\\[alg1\\]](#alg1){reference-type=\"ref\" reference=\"alg1\"}, we fine-tuned the larger Pythia-2.8B model [@biderman2023pythia] on the HH-RLHF dataset using DPO and DPO with a bias-correction offset, and evaluated their responses using Llama and Qwen. Due to limited computing resources, we only conducted experiments for the optimal $\\theta$, i.e., 5, as indicated in [3](#exp_odpo){reference-type=\"ref+label\" reference=\"exp_odpo\"}. The results, shown in [4](#odpo_winrate){reference-type=\"ref+label\" reference=\"odpo_winrate\"}, demonstrate that our method significantly outperforms DPO, confirming the effectiveness of the preference strength bias-correction offset. It is important to note that we provide evaluators with the option to label ties, and Llama and Qwen may occasionally refuse to evaluate certain offensive content. Therefore, we only include samples that are clearly evaluated as wins or losses when calculating the win rate.\n\n::: {#odpo_winrate}\n  ----------- -------- --------\n  Evaluator   Llama    Qwen\n  Win rate    0.5582   0.5370\n  ----------- -------- --------\n\n  : Win rate of DPO with a bias-correction offset against DPO\n:::\n\n## Synthetic Preference Datasets with Ties\n\nThe most compelling experiment is to fine-tune two models using BT and BTT, respectively, on a real preference dataset with ties and then compare their win rates. However, due to the lack of human-labeled preference datasets with ties and the high cost of manual annotation and evaluation, we use an LLM to simulate human judgment and label ties in Anthropic's HH-RLHF dataset. We then fine-tune Pythia-2.8B [@biderman2023pythia] on this synthetic preference dataset with ties, applying the BT and BTT preference models, and evaluate the responses. When using the loss function [\\[LCE_BTT\\]](#LCE_BTT){reference-type=\"ref\" reference=\"LCE_BTT\"}, we refer to this approach as TDPO. To reduce bias, we utilize Llama and Qwen, alternately as labelers and evaluators. The two labeled preference datasets are summarized in [5](#tied_dataset){reference-type=\"ref+label\" reference=\"tied_dataset\"}.\n\n::: {#tied_dataset}\n  -------------------- ------- ------\n  labeler              Llama   Qwen\n  \\# of tied samples   847     3553\n  -------------------- ------- ------\n\n  : Summary of preference datasets with ties\n:::\n\n<figure id=\"fig:win_rate_vs_ties_ratio\">\n<p><img src=\"win_rate_vs_ties_ratio.png\" style=\"width:100.0%\" alt=\"image\" /> <span id=\"fig:win_rate_vs_ties_ratio\" data-label=\"fig:win_rate_vs_ties_ratio\"></span></p>\n<figcaption>TDPO win rate against DPO with varying ties sample ratio in preference dataset</figcaption>\n</figure>\n\nWe observe that Anthropic's HH-RLHF dataset contains over $160k$ samples, with only a small portion labeled as ties. As a result, directly fine-tuning LLMs on the entire labeled dataset would lead to minimal impact from the tied samples. To emphasize the importance of these tied samples, we train DPO and TDPO on preference datasets with varying percentages of tied samples, with untied samples randomly selected. We still only conducted experiments for the optimal $\\theta$, i.e., $5$, due to limited computing resources. The win rate results are presented in [2](#fig:win_rate_vs_ties_ratio){reference-type=\"ref+label\" reference=\"fig:win_rate_vs_ties_ratio\"}. From the results, we observe that, regardless of the labeler and evaluator, the win rate of TDPO increases as the number of tied samples increases, and it significantly exceeds $50\\%$ when only tied samples are present. This demonstrates that incorporating BTT with tied samples improves the quality of the trained reward model. Moreover, as shown in [2](#fig:win_rate_vs_ties_ratio){reference-type=\"ref+label\" reference=\"fig:win_rate_vs_ties_ratio\"}, when Llama is the labeler and Qwen is the evaluator, the win rate of TDPO consistently exceeds $50\\%$ when the tie ratio is greater than $0.2$. In contrast, when Qwen is the labeler and Llama is the evaluator, TDPO's performance is slightly lower. This may be due to Qwen's less strict criteria for ties, resulting in an overabundance of tied samples."},{"heading":"Discussion","text":"# Discussion\n\nIn this paper, we introduced the concept of incorporating ties into preference modeling. Specifically, we applied the generalized Bradley-Terry model---the Bradley-Terry model with ties---to more accurately capture human preferences. Additionally, we analyzed the bias in measuring preference strength due to model mismatch and proposed a novel method to mitigate this bias. Extensive experiments demonstrate the benefits of considering ties in preference modeling. A limitation of this work is the absence of real human-annotated preference datasets with ties, as collecting such data is both expensive and time-consuming. Future work involving human-labeled preference datasets with ties could significantly improve the effectiveness of preference modeling."},{"heading":"Acknowledgements","text":"# Acknowledgements {#acknowledgements .unnumbered}\n\nThis research is partially supported by the National Natural Science Foundation of China (NSFC) \\[Grant NSFC-72225009, 72394360, 72394365\\]."},{"heading":"Proof of [3](#main_theorem)","text":"# Proof of [3](#main_theorem){reference-type=\"ref+label\" reference=\"main_theorem\"} {#proof-of-main_theorem}\n\n::: proof\n*Proof.* []{#proof_main_theorem label=\"proof_main_theorem\"} From [\\[r_hat\\]](#r_hat){reference-type=\"eqref\" reference=\"r_hat\"}, we can know that: $$\\begin{aligned}\n            p_{\\hat{r}}(y_1 \\succ y_2 \\mid x) = &p_{r^*}^{\\theta}(y_1 \\succ y_2 \\mid x) + \\frac{1}{2}p_{r^*}^{\\theta}(y_1 = y_2 \\mid x)\\\\\n            p_{\\hat{r}}(y_2 \\succ y_1 \\mid x) = &p_{r^*}^{\\theta}(y_2 \\succ y_1 \\mid x) + \\frac{1}{2}p_{r^*}^{\\theta}(y_2 = y_1 \\mid x)\\\\\n        \\end{aligned}$$ By subtraction, we can get: $$p_{\\hat{r}}(y_1 \\succ y_2 \\mid x) - p_{\\hat{r}}(y_2 \\succ y_1 \\mid x) = p_{r^*}^{\\theta}(y_1 \\succ y_2 \\mid x) - p_{r^*}^{\\theta}(y_2 \\succ y_1 \\mid x)$$ Therefore, $$\\begin{aligned}\n            \\frac{\\exp(\\Delta \\hat{r})}{1 + \\exp(\\Delta \\hat{r})} - \\frac{1}{1 + \\exp(\\Delta \\hat{r})} &= \\frac{\\exp(\\Delta r^*)}{\\theta +  \\exp(\\Delta r^*)} - \\frac{1}{1 +  \\theta \\exp(\\Delta r^*)}\\\\\n            \\frac{\\exp(\\Delta \\hat{r} - \\Delta r^*)}{1 + \\exp(\\Delta \\hat{r})} - \\frac{1}{\\left(1 + \\exp(\\Delta \\hat{r})\\right)\\exp(\\Delta r^*)} &= \\frac{1}{\\theta +  \\exp(\\Delta r^*)} - \\frac{1}{\\left(1 + \\theta \\exp(\\Delta r^*)\\right)\\exp(\\Delta r^*)}\\\\\n        \\end{aligned}$$ Then we can get: $$\\begin{aligned}\n            &\\exp(\\Delta \\hat{r} - \\Delta r^*) \\\\\n            &= \\left(1 + \\exp(\\Delta \\hat{r})\\right) \\left[\\frac{1}{\\theta +  \\exp(\\Delta r^*)} - \\frac{1}{\\left(1 + \\theta \\exp(\\Delta r^*)\\right)\\exp(\\Delta r^*)} + \\frac{1}{\\left(1 + \\exp(\\Delta \\hat{r})\\right)\\exp(\\Delta r^*)}\\right]\\\\\n            &= \\frac{1 + \\exp(\\Delta \\hat{r})}{\\theta + \\exp(\\Delta r^*)} + \\frac{\\theta \\exp(\\Delta r^*) - \\exp(\\Delta \\hat{r})}{\\left(1 + \\theta \\exp(\\Delta r^*)\\right)\\exp(\\Delta r^*)} \\\\\n            &= \\frac{1 + \\exp(\\Delta \\hat{r})}{\\theta + \\exp(\\Delta r^*)} + \\frac{\\theta - \\exp(\\Delta \\hat{r} - \\Delta r^*)}{1 + \\theta \\exp(\\Delta r^*)}\n        \\end{aligned}$$ Consequently, $$\\begin{aligned}\n            \\exp(\\Delta \\hat{r} - \\Delta r^*)\\frac{2 + \\theta \\exp(\\Delta r^*)}{1 + \\theta \\exp(\\Delta r^*)} &= \\frac{1 + \\exp(\\Delta \\hat{r})}{\\theta + \\exp(\\Delta r^*)} + \\frac{\\theta}{1 + \\theta \\exp(\\Delta r^*)} \\\\\n        \\end{aligned}$$ Then, $$\\begin{aligned}\n            \\exp(\\Delta \\hat{r} - \\Delta r^*) &= \\frac{1 + \\theta \\exp(\\Delta r^*)}{2 + \\theta \\exp(\\Delta r^*)} \\cdot \\frac{1 + \\exp(\\Delta \\hat{r})}{\\theta + \\exp(\\Delta r^*)} + \\frac{\\theta}{2 + \\theta \\exp(\\Delta r^*)} \\\\\n            &= \\frac{1 + \\theta \\exp(\\Delta r^*)}{2 + \\theta \\exp(\\Delta r^*)} \\cdot \\frac{\\exp(-\\Delta r^*) + \\exp(\\Delta \\hat{r} - \\Delta r^*)}{1 + \\theta \\exp(- \\Delta r^*)} + \\frac{\\theta}{2 + \\theta \\exp(\\Delta r^*)} \\\\\n        \\end{aligned}$$ $$\\begin{aligned}\n            &\\exp(\\Delta \\hat{r} - \\Delta r^*)\\left(1-\\frac{1 + \\theta \\exp(\\Delta r^*)}{\\left(2 + \\theta \\exp(\\Delta r^*)\\right)\\left(1 + \\theta \\exp(- \\Delta r^*)\\right)}\\right)\n            \\\\&=\\exp(\\Delta \\hat{r} - \\Delta r^*)\\frac{1 + \\theta^2 + 2\\theta \\exp(-\\Delta r^*)}{\\left(2 + \\theta \\exp(\\Delta r^*)\\right)\\left(1 + \\theta \\exp(- \\Delta r^*)\\right)}\n            \\\\&= \\frac{\\left(1 + \\theta \\exp(\\Delta r^*)\\right)\\exp (- \\Delta r^*)}{\\left(2 + \\theta \\exp(\\Delta r^*)\\right)\\left(1 + \\theta \\exp(- \\Delta r^*)\\right)} + \\frac{\\theta}{2 + \\theta \\exp(\\Delta r^*)} \\\\\n            &= \\frac{\\theta + \\exp (- \\Delta r^*)}{\\left(2 + \\theta \\exp(\\Delta r^*)\\right)\\left(1 + \\theta \\exp(- \\Delta r^*)\\right)} + \\frac{\\theta}{2 + \\theta \\exp(\\Delta r^*)} \\\\\n        \\end{aligned}$$ Finally, we can get: $$\\begin{aligned}\n            \\exp(\\Delta \\hat{r} - \\Delta r^*) &= \\frac{\\theta + \\exp (- \\Delta r^*)}{1 + \\theta^2 + 2\\theta \\exp(-\\Delta r^*)} + \\frac{\\theta \\left(1 + \\theta \\exp(- \\Delta r^*)\\right)}{1 + \\theta^2 + 2\\theta \\exp(-\\Delta r^*)}\\\\\n            & = \\frac{2 \\theta + \\left(1 + \\theta^2\\right)\\exp (- \\Delta r^*)}{1 + \\theta^2 + 2\\theta \\exp(-\\Delta r^*)}\n        \\end{aligned}$$ Denote $$f(x) = \\frac{2 \\theta + \\left(1 + \\theta^2\\right)\\exp (x)}{1 + \\theta^2 + 2\\theta \\exp(x)}$$ then we know that: $$f^{\\prime}(x) = \\frac{\\exp(x)\\left(1 - \\theta^2\\right)^2}{\\left(1 + \\theta^2 + 2\\theta \\exp(x)\\right)^2} \\geq 0$$ Therefore, $$\\frac{2 \\theta}{1 + \\theta^2} = \\lim_{x\\rightarrow -\\infty}f(x)\\leq f(x) \\leq \\lim_{x\\rightarrow \\infty}f(x) = \\frac{1 + \\theta^2}{2 \\theta}$$ Consequently, we have: $$|\\Delta \\hat{r} - \\Delta r^*| \\leq \\log(\\frac{1 + \\theta^2}{2 \\theta})$$ ◻\n:::"},{"heading":"Experiment Details","text":"# Experiment Details {#exp_detail}\n\n## Experimental Setup\n\nFor each single experiment, we choose the same $64$ batch size, RMSprop optimizer, $1e-5$ learning rate and $\\beta = 0.1$. All experiments are conducted on 4 Nvidia A800-80GB GPUs in a single node.\n\n## Win rate prompt for Llama and Qwen\n\nWe use the same prompt for Llama and Qwen to evaluate a pair of responses:\n\n*For the following query to a chatbot, which response is more helpful?\\\nQuery: \\[ \\]\\\nResponse A: \\[ \\]\\\nResponse B: \\[ \\]\\\nFIRST provide a one-sentence comparison of the two responses and explain which you feel is more helpful. SECOND, on a new line, state only \\\"A\\\", \\\"B\\\", \\\"Neither\\\" or \\\"Both\\\" to indicate which response is more helpful. Your response should use the format: Comparison: \\[one-sentence comparison and explanation\\] More helpful: \\[\\\"A\\\", \\\"B\\\", \\\"Neither\\\" or \\\"Both\\\"\\]*\n\n## Prompt for Llama and Qwen to Label Ties\n\nWe use the same prompt for Llama and Qwen to label whether a pair of responses are tied:\n\n*For the following query to a chatbot, are the two responses equally good?\\\nQuery: \\[\\]\\\nResponse A: \\[\\]\\\nResponse B: \\[\\]\\\nAnswer with exactly \\\"Yes\\\" or \\\"No\\\".*\n\n[^1]: liujinsong@163.sufe.edu.cn\n\n[^2]: ddge@sjtu.edu.cn\n\n[^3]: ruihao.zhu@cornell.edu"}],"approval":false,"conference":"iclr","rating":2,"year":2025,"id":"90290856690f37e0baf614e35b09f92ffc3c8294db05bca0142ca7b0790d1df4","y_true":0,"y_pred":0,"rationale_true":"Summary: This paper introduces the Bradley-Terry model with ties (BTT) to improve the accuracy of human preference modeling in reinforcement learning, addressing the bias introduced by ignoring ties. It demonstrates through theoretical analysis and experimental validation that BTT outperforms the traditional BT model, especially in datasets with ties, thereby enhancing the alignment of large language models with human preferences.\n\nStrengths: 1. **Relevance of Preferences with Ties**: The inclusion of ties in preference modeling is highly significant, as real-world scenarios often involve ranking difficulties. This makes the problem both relevant and worthy of exploration.\n\n2. **Clarity and Readability**: The paper is well-structured and easy to follow. The logical flow and clear explanations enhance the overall readability, making the content accessible and understandable.\n\nWeaknesses: 1. **Poor Layout and Presentation**: The formatting of figures (e.g., Figure 1, Figure 2) and tables (e.g., Table 2, Table 3, Table 4, Table 5) is suboptimal, often appearing overly large and occupying excessive space. This layout issue detracts from the readability and professional appearance of the paper, suggesting a rushed submission. Enhancing the visual presentation and ensuring that figures and tables are appropriately sized would significantly improve the reader's experience.\n\n2. **Lack of Baseline Comparisons**: While the paper introduces a novel method, it fails to compare its performance against established baselines. Specifically, the proposed method is described as a variant of DPO with an offset (ODPO) (Amini et al., 2024), but no direct comparisons are provided. This omission makes it difficult to assess the effectiveness and reliability of the new approach. Including such comparisons would strengthen the paper's claims and provide more concrete evidence of its contributions.\n\n3. **Limited Dataset Diversity**: The experiments rely solely on the HH-RLHF dataset, which limits the generalizability of the findings. Using a more diverse set of datasets would help validate the robustness and broader applicability of the proposed method. Expanding the experimental scope to include additional datasets would provide a more comprehensive evaluation.\n\nQuestions: The current work appears rushed, both in experimental design and writing. To improve, the authors should optimize figure and table layouts for better readability, include comparisons with baseline methods to validate the effectiveness of the proposed BTT model, and diversify the datasets used to ensure the generalizability of the findings. A more thorough and polished approach is needed.","rationale_pred":"Paper Summary: The paper proposes the Bradley-Terry model with ties (BTT) to improve the accuracy of human preference modeling in reinforcement learning from human feedback (RLHF). It addresses the bias introduced by ignoring ties in preference data and demonstrates the advantages of BTT over the traditional Bradley-Terry model through theoretical analysis and experiments.\n\nSupporting Evidence:\n- The importance of aligning Large Language Models (LLMs) with human preferences is emphasized, which supports the Main Paper's exploration of reward learning from preferences. (from: Optimizing LLMs with Direct Preferences: A Data Efficiency Perspective)\n- The limitations of standard reward modeling approaches, such as the Bradley-Terry model, in handling diverging preferences among annotators are highlighted, reinforcing the necessity of the Main Paper's proposed BTT. (from: Diverging Preferences: When do Annotators Disagree and do Models Know?)\n- The importance of context in preference modeling is emphasized, aligning with the Main Paper's focus on accurately measuring preference strength. (from: Improving Context-Aware Preference Modeling for Language Models)\n- The significance of accurately capturing human preferences is highlighted, which complements the Main Paper's proposal of the generalized Bradley-Terry model with ties. (from: Data-Centric Human Preference Optimization with Rationales)\n- The challenges of aligning synthetic preference labels with human judgments are emphasized, advocating for improved methodologies in preference modeling, which aligns with the Main Paper's focus. (from: Generative Reward Models)\n- The effectiveness of using human preferences in reinforcement learning tasks is demonstrated, reinforcing the Main Paper's argument for the necessity of incorporating ties in preference strength measurement. (from: Deep Reinforcement Learning from Human Preferences)\n- The challenges of aligning language models with human preferences are addressed, highlighting complementary approaches to refining reward learning from human feedback. (from: Direct Preference Optimization: Your Language Model is Secretly a Reward Model)\n- The importance of nuanced preference modeling in aligning language models with human feedback is emphasized, complementing the Main Paper's focus on the impact of ties. (from: Direct Preference Optimization with an Offset)\n- The significance of leveraging off-policy data to enhance performance in reinforcement learning tasks is highlighted, supporting the Main Paper's claims regarding refining reward learning methodologies. (from: Advantage-Weighted Regression: Simple and Scalable Off-Policy Reinforcement Learning)\n- The importance of preference-based reinforcement learning and the role of human feedback in training large language models is emphasized, reinforcing the Main Paper's claims about measuring preference strength. (from: A Survey of Reinforcement Learning from Human Feedback)\n\nContradictory Evidence:\n- The importance of data quality and metrics in preference dataset comparison is emphasized, suggesting that the Main Paper's approach may overlook broader data-centric considerations. (from: Towards Data-Centric RLHF: Simple Metrics for Preference Dataset Comparison)\n- The limitations of the Bradley-Terry model in handling binary feedback are highlighted, proposing a framework for learning reward models from ordinal feedback, which may enhance reward learning beyond the Main Paper's focus. (from: Reward Modeling with Ordinal Feedback: Wisdom of the Crowd)\n- The argument that traditional models like the Bradley-Terry model fail to capture complexities of human preferences is made, advocating for a self-play approach instead, which may suggest limitations in the Main Paper's methodology. (from: Self-Play Preference Optimization for Language Model Alignment)\n- The introduction of Online Self-Preferring language models is presented as a more efficient method for capturing preference strength, suggesting that the Main Paper's reliance on traditional models may overlook innovative alternatives. (from: Online Self-Preferring Language Models)\n- The contrasting approach of representing human preferences as a more nuanced framework is proposed, challenging the Main Paper's emphasis on measuring preference strength with the Bradley-Terry model with ties. (from: LRHP: Learning Representations for Human Preferences via Preference Pairs)\n\nKey Comparisons:\n- The Main Paper introduces the BTT model to address ties in preferences, while contrasting papers emphasize alternative methods that may capture more complex human preferences.\n- Supporting papers highlight the importance of accurately capturing human preferences, while contrasting papers critique the limitations of the Bradley-Terry model in this context.\n\nConclusion: Despite the proposed advancements in preference modeling through the introduction of the BTT model, the existence of contrasting evidence suggests that the approach may not be sufficiently novel. The critiques regarding the limitations of the Bradley-Terry model and the emphasis on alternative methodologies indicate that similar ideas have been explored previously. Therefore, the paper does not present a sufficiently unique contribution to be considered novel.","structured_evaluation":{"paper_summary":"The paper proposes the Bradley-Terry model with ties (BTT) to improve the accuracy of human preference modeling in reinforcement learning from human feedback (RLHF). It addresses the bias introduced by ignoring ties in preference data and demonstrates the advantages of BTT over the traditional Bradley-Terry model through theoretical analysis and experiments.","supporting_evidence":[{"text":"The importance of aligning Large Language Models (LLMs) with human preferences is emphasized, which supports the Main Paper's exploration of reward learning from preferences.","paper_id":null,"paper_title":"Optimizing LLMs with Direct Preferences: A Data Efficiency Perspective","source":"semantic"},{"text":"The limitations of standard reward modeling approaches, such as the Bradley-Terry model, in handling diverging preferences among annotators are highlighted, reinforcing the necessity of the Main Paper's proposed BTT.","paper_id":null,"paper_title":"Diverging Preferences: When do Annotators Disagree and do Models Know?","source":"semantic"},{"text":"The importance of context in preference modeling is emphasized, aligning with the Main Paper's focus on accurately measuring preference strength.","paper_id":null,"paper_title":"Improving Context-Aware Preference Modeling for Language Models","source":"semantic"},{"text":"The significance of accurately capturing human preferences is highlighted, which complements the Main Paper's proposal of the generalized Bradley-Terry model with ties.","paper_id":null,"paper_title":"Data-Centric Human Preference Optimization with Rationales","source":"semantic"},{"text":"The challenges of aligning synthetic preference labels with human judgments are emphasized, advocating for improved methodologies in preference modeling, which aligns with the Main Paper's focus.","paper_id":null,"paper_title":"Generative Reward Models","source":"semantic"},{"text":"The effectiveness of using human preferences in reinforcement learning tasks is demonstrated, reinforcing the Main Paper's argument for the necessity of incorporating ties in preference strength measurement.","paper_id":null,"paper_title":"Deep Reinforcement Learning from Human Preferences","source":"semantic"},{"text":"The challenges of aligning language models with human preferences are addressed, highlighting complementary approaches to refining reward learning from human feedback.","paper_id":null,"paper_title":"Direct Preference Optimization: Your Language Model is Secretly a Reward Model","source":"semantic"},{"text":"The importance of nuanced preference modeling in aligning language models with human feedback is emphasized, complementing the Main Paper's focus on the impact of ties.","paper_id":null,"paper_title":"Direct Preference Optimization with an Offset","source":"semantic"},{"text":"The significance of leveraging off-policy data to enhance performance in reinforcement learning tasks is highlighted, supporting the Main Paper's claims regarding refining reward learning methodologies.","paper_id":null,"paper_title":"Advantage-Weighted Regression: Simple and Scalable Off-Policy Reinforcement Learning","source":"semantic"},{"text":"The importance of preference-based reinforcement learning and the role of human feedback in training large language models is emphasized, reinforcing the Main Paper's claims about measuring preference strength.","paper_id":null,"paper_title":"A Survey of Reinforcement Learning from Human Feedback","source":"semantic"}],"contradictory_evidence":[{"text":"The importance of data quality and metrics in preference dataset comparison is emphasized, suggesting that the Main Paper's approach may overlook broader data-centric considerations.","paper_id":null,"paper_title":"Towards Data-Centric RLHF: Simple Metrics for Preference Dataset Comparison","source":"semantic"},{"text":"The limitations of the Bradley-Terry model in handling binary feedback are highlighted, proposing a framework for learning reward models from ordinal feedback, which may enhance reward learning beyond the Main Paper's focus.","paper_id":null,"paper_title":"Reward Modeling with Ordinal Feedback: Wisdom of the Crowd","source":"semantic"},{"text":"The argument that traditional models like the Bradley-Terry model fail to capture complexities of human preferences is made, advocating for a self-play approach instead, which may suggest limitations in the Main Paper's methodology.","paper_id":null,"paper_title":"Self-Play Preference Optimization for Language Model Alignment","source":"semantic"},{"text":"The introduction of Online Self-Preferring language models is presented as a more efficient method for capturing preference strength, suggesting that the Main Paper's reliance on traditional models may overlook innovative alternatives.","paper_id":null,"paper_title":"Online Self-Preferring Language Models","source":"semantic"},{"text":"The contrasting approach of representing human preferences as a more nuanced framework is proposed, challenging the Main Paper's emphasis on measuring preference strength with the Bradley-Terry model with ties.","paper_id":null,"paper_title":"LRHP: Learning Representations for Human Preferences via Preference Pairs","source":"semantic"}],"key_comparisons":["The Main Paper introduces the BTT model to address ties in preferences, while contrasting papers emphasize alternative methods that may capture more complex human preferences.","Supporting papers highlight the importance of accurately capturing human preferences, while contrasting papers critique the limitations of the Bradley-Terry model in this context."],"conclusion":"Despite the proposed advancements in preference modeling through the introduction of the BTT model, the existence of contrasting evidence suggests that the approach may not be sufficiently novel. The critiques regarding the limitations of the Bradley-Terry model and the emphasis on alternative methodologies indicate that similar ideas have been explored previously. Therefore, the paper does not present a sufficiently unique contribution to be considered novel.","label":0,"novel":"no","probability":7.194132978569833e-9,"rationale":"Paper Summary: The paper proposes the Bradley-Terry model with ties (BTT) to improve the accuracy of human preference modeling in reinforcement learning from human feedback (RLHF). It addresses the bias introduced by ignoring ties in preference data and demonstrates the advantages of BTT over the traditional Bradley-Terry model through theoretical analysis and experiments.\n\nSupporting Evidence:\n- The importance of aligning Large Language Models (LLMs) with human preferences is emphasized, which supports the Main Paper's exploration of reward learning from preferences. (from: Optimizing LLMs with Direct Preferences: A Data Efficiency Perspective)\n- The limitations of standard reward modeling approaches, such as the Bradley-Terry model, in handling diverging preferences among annotators are highlighted, reinforcing the necessity of the Main Paper's proposed BTT. (from: Diverging Preferences: When do Annotators Disagree and do Models Know?)\n- The importance of context in preference modeling is emphasized, aligning with the Main Paper's focus on accurately measuring preference strength. (from: Improving Context-Aware Preference Modeling for Language Models)\n- The significance of accurately capturing human preferences is highlighted, which complements the Main Paper's proposal of the generalized Bradley-Terry model with ties. (from: Data-Centric Human Preference Optimization with Rationales)\n- The challenges of aligning synthetic preference labels with human judgments are emphasized, advocating for improved methodologies in preference modeling, which aligns with the Main Paper's focus. (from: Generative Reward Models)\n- The effectiveness of using human preferences in reinforcement learning tasks is demonstrated, reinforcing the Main Paper's argument for the necessity of incorporating ties in preference strength measurement. (from: Deep Reinforcement Learning from Human Preferences)\n- The challenges of aligning language models with human preferences are addressed, highlighting complementary approaches to refining reward learning from human feedback. (from: Direct Preference Optimization: Your Language Model is Secretly a Reward Model)\n- The importance of nuanced preference modeling in aligning language models with human feedback is emphasized, complementing the Main Paper's focus on the impact of ties. (from: Direct Preference Optimization with an Offset)\n- The significance of leveraging off-policy data to enhance performance in reinforcement learning tasks is highlighted, supporting the Main Paper's claims regarding refining reward learning methodologies. (from: Advantage-Weighted Regression: Simple and Scalable Off-Policy Reinforcement Learning)\n- The importance of preference-based reinforcement learning and the role of human feedback in training large language models is emphasized, reinforcing the Main Paper's claims about measuring preference strength. (from: A Survey of Reinforcement Learning from Human Feedback)\n\nContradictory Evidence:\n- The importance of data quality and metrics in preference dataset comparison is emphasized, suggesting that the Main Paper's approach may overlook broader data-centric considerations. (from: Towards Data-Centric RLHF: Simple Metrics for Preference Dataset Comparison)\n- The limitations of the Bradley-Terry model in handling binary feedback are highlighted, proposing a framework for learning reward models from ordinal feedback, which may enhance reward learning beyond the Main Paper's focus. (from: Reward Modeling with Ordinal Feedback: Wisdom of the Crowd)\n- The argument that traditional models like the Bradley-Terry model fail to capture complexities of human preferences is made, advocating for a self-play approach instead, which may suggest limitations in the Main Paper's methodology. (from: Self-Play Preference Optimization for Language Model Alignment)\n- The introduction of Online Self-Preferring language models is presented as a more efficient method for capturing preference strength, suggesting that the Main Paper's reliance on traditional models may overlook innovative alternatives. (from: Online Self-Preferring Language Models)\n- The contrasting approach of representing human preferences as a more nuanced framework is proposed, challenging the Main Paper's emphasis on measuring preference strength with the Bradley-Terry model with ties. (from: LRHP: Learning Representations for Human Preferences via Preference Pairs)\n\nKey Comparisons:\n- The Main Paper introduces the BTT model to address ties in preferences, while contrasting papers emphasize alternative methods that may capture more complex human preferences.\n- Supporting papers highlight the importance of accurately capturing human preferences, while contrasting papers critique the limitations of the Bradley-Terry model in this context.\n\nConclusion: Despite the proposed advancements in preference modeling through the introduction of the BTT model, the existence of contrasting evidence suggests that the approach may not be sufficiently novel. The critiques regarding the limitations of the Bradley-Terry model and the emphasis on alternative methodologies indicate that similar ideas have been explored previously. Therefore, the paper does not present a sufficiently unique contribution to be considered novel."},"arxiv_id":"2410.05328"},"terms":{"tasks":["alignment of language models","preference modeling","evaluation of preference strength","measurement of preference strength","incorporating ties in preference modeling"],"methods":["Bradley-Terry model","generalized Bradley-Terry model","Bradley-Terry model with ties"],"metrics":["preference strength"],"resources":["synthetic preference datasets","state-of-the-art open-source large language models"],"relations":[{"head":"Bradley-Terry model","tail":"preference modeling"},{"head":"Bradley-Terry model","tail":"evaluation of preference strength"},{"head":"generalized Bradley-Terry model","tail":"preference modeling"},{"head":"generalized Bradley-Terry model","tail":"evaluation of preference strength"},{"head":"Bradley-Terry model with ties","tail":"preference modeling"},{"head":"Bradley-Terry model with ties","tail":"evaluation of preference strength"},{"head":"preference strength","tail":"measurement of preference strength"},{"head":"synthetic preference datasets","tail":"alignment of language models"},{"head":"synthetic preference datasets","tail":"preference modeling"},{"head":"state-of-the-art open-source large language models","tail":"alignment of language models"},{"head":"state-of-the-art open-source large language models","tail":"preference modeling"}]},"background":"Reward learning plays a pivotal role in Reinforcement Learning from Human Feedback (RLHF), ensuring the alignment of language models. The Bradley-Terry (BT) model stands as the prevalent choice for capturing human preferences from datasets containing pairs of chosen and rejected responses. In preference modeling, the focus is not on absolute values but rather on the reward difference between chosen and rejected responses, referred to as preference strength. Thus, precise evaluation of preference strength holds paramount importance in preference modeling. However, an easily overlooked factor significantly affecting preference strength measurement is that human attitudes towards two responses may not solely indicate a preference for one over the other and ties are also a common occurrence.","target":"To address this, we propose the adoption of the generalized Bradley-Terry model -- the Bradley-Terry model with ties (BTT) -- to accommodate tied preferences, thus leveraging additional information. We prove that even with the access to the true distributions of prompt and response, disregarding ties can lead to a notable bias in preference strength measurement. Comprehensive experiments further validate the advantages of incorporating ties in preference modeling. Notably, fine-tuning with BTT significantly outperforms fine-tuning with BT on synthetic preference datasets with ties, labeled by state-of-the-art open-source LLMs."},{"graph":{"title":"A Comparison of Hamming Errors of Representative Variable Selection Methods","abstract":"Lasso is a celebrated method for variable selection in linear models, but it faces challenges when the covariates are moderately or strongly correlated. This motivates alternative approaches such as using a non-convex penalty, adding a ridge regularization, or conducting a post-Lasso thresholding. In this paper, we compare Lasso with 5 other methods: Elastic net, SCAD, forward selection, thresholded Lasso, and forward backward selection. We measure their performances theoretically by the expected Hamming error, assuming that the regression coefficients are ${\\it iid}$ drawn from a two-point mixture and that the Gram matrix is block-wise diagonal. By deriving the rates of convergence of Hamming errors and the phase diagrams, we obtain useful conclusions about the pros and cons of different methods.","entities":[{"label":"A Comparison of Hamming Errors of Representative Variable Selection Methods","type":"title","detail":null,"excerpts":null},{"label":"learning theory","type":"primary_area","detail":null,"excerpts":null},{"label":"Hamming error","type":"keyword","detail":null,"excerpts":null},{"label":"variable selection","type":"keyword","detail":null,"excerpts":null},{"label":"Lasso","type":"keyword","detail":null,"excerpts":null},{"label":"Elastic net","type":"keyword","detail":null,"excerpts":null},{"label":"SCAD","type":"keyword","detail":null,"excerpts":null},{"label":"thresholded Lasso","type":"keyword","detail":null,"excerpts":null},{"label":"forward selection","type":"keyword","detail":null,"excerpts":null},{"label":"forward backward selection","type":"keyword","detail":null,"excerpts":null},{"label":"This paper compares Lasso with five other variable selection methods by analyzing their Hamming errors under a theoretical framework.","type":"tldr","detail":null,"excerpts":null},{"label":"Comparison of variable selection methods","type":"claim","detail":"The paper claims to provide a theoretical comparison of Lasso and five other variable selection methods based on their expected Hamming errors.","excerpts":null},{"label":"Derivation of convergence rates","type":"claim","detail":"The authors derive the rates of convergence of Hamming errors for the methods studied, providing insights into their performance under different conditions.","excerpts":null},{"label":"Lasso [method]","type":"method","detail":"Lasso is a method for variable selection that minimizes the sum of squared errors with an L1 penalty on the coefficients.","excerpts":null},{"label":"Elastic net [method]","type":"method","detail":"Elastic net combines L1 and L2 penalties for variable selection, addressing some limitations of Lasso in correlated variable settings.","excerpts":null},{"label":"SCAD [method]","type":"method","detail":"SCAD is a non-convex penalization method that improves upon Lasso by allowing for more flexibility in coefficient estimation.","excerpts":null},{"label":"Thresholded Lasso","type":"method","detail":"Thresholded Lasso applies a thresholding step to the Lasso estimates to improve variable selection accuracy.","excerpts":null},{"label":"Forward selection","type":"method","detail":"Forward selection is a stepwise method that adds variables based on their contribution to the model fit, evaluated at each step.","excerpts":null},{"label":"Forward backward selection","type":"method","detail":"Forward backward selection combines forward selection with a backward elimination step to refine the variable selection process.","excerpts":null},{"label":"Experiment 1","type":"experiment","detail":"This experiment evaluates the performance of the methods under block-wise diagonal designs, measuring their Hamming errors across multiple repetitions.","excerpts":null},{"label":"Experiment 2","type":"experiment","detail":"This experiment tests the methods under general designs, including Toeplitz and factor model designs, to assess their robustness.","excerpts":null},{"label":"Experiment 3","type":"experiment","detail":"This experiment investigates the effects of tuning parameters on the performance of Lasso, thresholded Lasso, forward selection, and forward backward selection.","excerpts":null},{"label":"Experiment 4","type":"experiment","detail":"This experiment simulates data from a random design setting to evaluate the methods' performance in high-dimensional settings.","excerpts":null}],"relationships":[{"source":"A Comparison of Hamming Errors of Representative Variable Selection Methods","target":"learning theory"},{"source":"A Comparison of Hamming Errors of Representative Variable Selection Methods","target":"Hamming error"},{"source":"A Comparison of Hamming Errors of Representative Variable Selection Methods","target":"variable selection"},{"source":"A Comparison of Hamming Errors of Representative Variable Selection Methods","target":"Lasso"},{"source":"A Comparison of Hamming Errors of Representative Variable Selection Methods","target":"Elastic net"},{"source":"A Comparison of Hamming Errors of Representative Variable Selection Methods","target":"SCAD"},{"source":"A Comparison of Hamming Errors of Representative Variable Selection Methods","target":"thresholded Lasso"},{"source":"A Comparison of Hamming Errors of Representative Variable Selection Methods","target":"forward selection"},{"source":"A Comparison of Hamming Errors of Representative Variable Selection Methods","target":"forward backward selection"},{"source":"A Comparison of Hamming Errors of Representative Variable Selection Methods","target":"This paper compares Lasso with five other variable selection methods by analyzing their Hamming errors under a theoretical framework."},{"source":"This paper compares Lasso with five other variable selection methods by analyzing their Hamming errors under a theoretical framework.","target":"Comparison of variable selection methods"},{"source":"This paper compares Lasso with five other variable selection methods by analyzing their Hamming errors under a theoretical framework.","target":"Derivation of convergence rates"},{"source":"Comparison of variable selection methods","target":"Lasso [method]"},{"source":"Comparison of variable selection methods","target":"Elastic net [method]"},{"source":"Comparison of variable selection methods","target":"SCAD [method]"},{"source":"Comparison of variable selection methods","target":"Thresholded Lasso"},{"source":"Comparison of variable selection methods","target":"Forward selection"},{"source":"Comparison of variable selection methods","target":"Forward backward selection"},{"source":"Derivation of convergence rates","target":"Lasso [method]"},{"source":"Derivation of convergence rates","target":"Elastic net [method]"},{"source":"Derivation of convergence rates","target":"SCAD [method]"},{"source":"Derivation of convergence rates","target":"Thresholded Lasso"},{"source":"Derivation of convergence rates","target":"Forward selection"},{"source":"Derivation of convergence rates","target":"Forward backward selection"},{"source":"Lasso [method]","target":"Experiment 1"},{"source":"Lasso [method]","target":"Experiment 2"},{"source":"Lasso [method]","target":"Experiment 3"},{"source":"Lasso [method]","target":"Experiment 4"},{"source":"Elastic net [method]","target":"Experiment 1"},{"source":"Elastic net [method]","target":"Experiment 2"},{"source":"Elastic net [method]","target":"Experiment 3"},{"source":"Elastic net [method]","target":"Experiment 4"},{"source":"SCAD [method]","target":"Experiment 1"},{"source":"SCAD [method]","target":"Experiment 2"},{"source":"SCAD [method]","target":"Experiment 3"},{"source":"SCAD [method]","target":"Experiment 4"},{"source":"Thresholded Lasso","target":"Experiment 1"},{"source":"Thresholded Lasso","target":"Experiment 2"},{"source":"Thresholded Lasso","target":"Experiment 3"},{"source":"Thresholded Lasso","target":"Experiment 4"},{"source":"Forward selection","target":"Experiment 1"},{"source":"Forward selection","target":"Experiment 2"},{"source":"Forward selection","target":"Experiment 3"},{"source":"Forward selection","target":"Experiment 4"},{"source":"Forward backward selection","target":"Experiment 1"},{"source":"Forward backward selection","target":"Experiment 2"},{"source":"Forward backward selection","target":"Experiment 3"},{"source":"Forward backward selection","target":"Experiment 4"}],"valid_status":"Valid","valid_status_all":["Valid"]},"related":[{"summary":"The Related Paper supports the Main Paper by providing a comprehensive review of LASSO and its extensions, including the elastic net, which are also compared in the Main Paper. It emphasizes the importance of regularization and variable selection in high-dimensional regression models, reinforcing the Main Paper's focus on the performance of various variable selection methods, including LASSO. Additionally, the Related Paper's discussion on improved performance metrics aligns with the Main Paper's analysis of Hamming errors, thereby strengthening the claims made about the effectiveness of different methods.","paper_id":"6904b44b84af8de4e2cf7747830fe4e80f32f3a6","title":"High-Dimensional LASSO-Based Computational Regression Models: Regularization, Shrinkage, and Selection","abstract":"Regression models are a form of supervised learning methods that are important for machine learning, statistics, and general data science. Despite the fact that classical ordinary least squares (OLS) regression models have been known for a long time, in recent years there are many new developments that extend this model significantly. Above all, the least absolute shrinkage and selection operator (LASSO) model gained considerable interest. In this paper, we review general regression models with a focus on the LASSO and extensions thereof, including the adaptive LASSO, elastic net, and group LASSO. We discuss the regularization terms responsible for inducing coefficient shrinkage and variable selection leading to improved performance metrics of these regression models. This makes these modern, computational regression models valuable tools for analyzing high-dimensional problems.","score":0.6862019896507263,"polarity":"positive","source":"semantic","year":2019,"authors":["F. Emmert-Streib","M. Dehmer"],"venue":null,"citation_count":106,"reference_count":89,"influential_citation_count":4,"corpus_id":89608177,"url":"https://www.semanticscholar.org/paper/6904b44b84af8de4e2cf7747830fe4e80f32f3a6","arxiv_id":null,"contexts":null,"background":"Regression models are a form of supervised learning methods that are important for machine learning, statistics, and general data science. Despite the fact that classical ordinary least squares (OLS) regression models have been known for a long time, in recent years there are many new developments that extend this model significantly. Above all, the least absolute shrinkage and selection operator (LASSO) model gained considerable interest.","target":"In this paper, we review general regression models with a focus on the LASSO and extensions thereof, including the adaptive LASSO, elastic net, and group LASSO. We discuss the regularization terms responsible for inducing coefficient shrinkage and variable selection leading to improved performance metrics of these regression models. This makes these modern, computational regression models valuable tools for analyzing high-dimensional problems."},{"summary":"The Related Paper supports the Main Paper by highlighting the importance of penalization methods, specifically the elastic net, which is also discussed in the Main Paper as an alternative to Lasso for variable selection. Both papers emphasize the challenges posed by correlated covariates and propose methods to enhance estimation accuracy. The Related Paper's focus on improving precision matrix estimation through a novel two-step estimator complements the Main Paper's exploration of various variable selection methods, reinforcing the relevance of advanced penalization techniques in high-dimensional settings.","paper_id":"2a380df891d013bf49a187d589ba9f85be4f20df","title":"A Computational Note on the Graphical Ridge in High-dimension","abstract":"This article explores the estimation of precision matrices in high-dimensional Gaussian graphical models. We address the challenge of improving the accuracy of maximum likelihood-based precision estimation through penalization. Specifically, we consider an elastic net penalty, which incorporates both L1 and Frobenius norm penalties while accounting for the target matrix during estimation. To enhance precision matrix estimation, we propose a novel two-step estimator that combines the strengths of ridge and graphical lasso estimators. Through this approach, we aim to improve overall estimation performance. Our empirical analysis demonstrates the superior efficiency of our proposed method compared to alternative approaches. We validate the effectiveness of our proposal through numerical experiments and application on three real datasets. These examples illustrate the practical applicability and usefulness of our proposed estimator.","score":0.6802105903625488,"polarity":"positive","source":"semantic","year":2023,"authors":["A. Bekker","A. Kheyri","M. Arashi"],"venue":null,"citation_count":0,"reference_count":21,"influential_citation_count":0,"corpus_id":266551866,"url":"https://www.semanticscholar.org/paper/2a380df891d013bf49a187d589ba9f85be4f20df","arxiv_id":null,"contexts":null,"background":"This article explores the estimation of precision matrices in high-dimensional Gaussian graphical models. We address the challenge of improving the accuracy of maximum likelihood-based precision estimation through penalization.","target":"Specifically, we consider an elastic net penalty, which incorporates both L1 and Frobenius norm penalties while accounting for the target matrix during estimation. To enhance precision matrix estimation, we propose a novel two-step estimator that combines the strengths of ridge and graphical lasso estimators. Through this approach, we aim to improve overall estimation performance. Our empirical analysis demonstrates the superior efficiency of our proposed method compared to alternative approaches. We validate the effectiveness of our proposal through numerical experiments and application on three real datasets. These examples illustrate the practical applicability and usefulness of our proposed estimator."},{"summary":"The Related Paper, 'lassopack: Model selection and prediction with regularized regression in Stata', supports the Main Paper by providing practical implementations of several variable selection methods discussed, including Lasso and Elastic Net. It emphasizes the importance of model selection in high-dimensional settings, which aligns with the Main Paper's focus on comparing the performance of various methods through theoretical analysis of Hamming errors. Additionally, the Monte Carlo results presented in the Related Paper offer empirical evidence that can validate the theoretical conclusions drawn in the Main Paper.","paper_id":"2e73b21fa855a99349989ab6acbe702df8f7bc26","title":"lassopack: Model selection and prediction with regularized regression in Stata","abstract":"In this article, we introduce lassopack, a suite of programs for regularized regression in Stata. lassopack implements lasso, square-root lasso, elastic net, ridge regression, adaptive lasso, and postestimation ordinary least squares. The methods are suitable for the high-dimensional setting, where the number of predictors p may be large and possibly greater than the number of observations, n. We offer three approaches for selecting the penalization (“tuning”) parameters: information criteria (implemented in lasso2), K-fold cross-validation and h-step-ahead rolling cross-validation for cross-section, panel, and time-series data (cvlasso), and theory-driven (“rigorous” or plugin) penalization for the lasso and square-root lasso for cross-section and panel data (rlasso). We discuss the theoretical framework and practical considerations for each approach. We also present Monte Carlo results to compare the performances of the penalization approaches.","score":0.6791508793830872,"polarity":"positive","source":"semantic","year":2019,"authors":["A. Ahrens","C. Hansen","M. Schaffer"],"venue":null,"citation_count":149,"reference_count":73,"influential_citation_count":16,"corpus_id":68053638,"url":"https://www.semanticscholar.org/paper/2e73b21fa855a99349989ab6acbe702df8f7bc26","arxiv_id":null,"contexts":null,"background":"In this article, we introduce lassopack, a suite of programs for regularized regression in Stata. The methods are suitable for the high-dimensional setting, where the number of predictors p may be large and possibly greater than the number of observations, n. We discuss the theoretical framework and practical considerations for each approach.","target":"lassopack implements lasso, square-root lasso, elastic net, ridge regression, adaptive lasso, and postestimation ordinary least squares. We offer three approaches for selecting the penalization (“tuning”) parameters: information criteria (implemented in lasso2), K-fold cross-validation and h-step-ahead rolling cross-validation for cross-section, panel, and time-series data (cvlasso), and theory-driven (“rigorous” or plugin) penalization for the lasso and square-root lasso for cross-section and panel data (rlasso). We also present Monte Carlo results to compare the performances of the penalization approaches."},{"summary":"The Related Paper supports the Main Paper by highlighting the advantages of nonconvex penalty methods, such as SCAD and MCP, which are compared to Lasso in the context of variable selection. It provides theoretical convergence properties and empirical evidence that these nonconvex methods can outperform Lasso, particularly in scenarios with correlated covariates. This aligns with the Main Paper's exploration of alternative approaches to Lasso, reinforcing the argument that nonconvex penalties are viable and potentially superior options for variable selection.","paper_id":"c67136ab0e52283d5b3beef23fe318eb9f455cb7","title":"COORDINATE DESCENT ALGORITHMS FOR NONCONVEX PENALIZED REGRESSION, WITH APPLICATIONS TO BIOLOGICAL FEATURE SELECTION.","abstract":"A number of variable selection methods have been proposed involving nonconvex penalty functions. These methods, which include the smoothly clipped absolute deviation (SCAD) penalty and the minimax concave penalty (MCP), have been demonstrated to have attractive theoretical properties, but model fitting is not a straightforward task, and the resulting solutions may be unstable. Here, we demonstrate the potential of coordinate descent algorithms for fitting these models, establishing theoretical convergence properties and demonstrating that they are significantly faster than competing approaches. In addition, we demonstrate the utility of convexity diagnostics to determine regions of the parameter space in which the objective function is locally convex, even though the penalty is not. Our simulation study and data examples indicate that nonconvex penalties like MCP and SCAD are worthwhile alternatives to the lasso in many applications. In particular, our numerical results suggest that MCP is the preferred approach among the three methods.","score":0.6546309590339661,"polarity":"positive","source":"semantic","year":2011,"authors":["P. Breheny","Jian Huang"],"venue":null,"citation_count":738,"reference_count":19,"influential_citation_count":106,"corpus_id":8570280,"url":"https://www.semanticscholar.org/paper/c67136ab0e52283d5b3beef23fe318eb9f455cb7","arxiv_id":null,"contexts":null,"background":"A number of variable selection methods have been proposed involving nonconvex penalty functions. These methods, which include the smoothly clipped absolute deviation (SCAD) penalty and the minimax concave penalty (MCP), have been demonstrated to have attractive theoretical properties, but model fitting is not a straightforward task, and the resulting solutions may be unstable.","target":"Here, we demonstrate the potential of coordinate descent algorithms for fitting these models, establishing theoretical convergence properties and demonstrating that they are significantly faster than competing approaches. In addition, we demonstrate the utility of convexity diagnostics to determine regions of the parameter space in which the objective function is locally convex, even though the penalty is not. Our simulation study and data examples indicate that nonconvex penalties like MCP and SCAD are worthwhile alternatives to the lasso in many applications. In particular, our numerical results suggest that MCP is the preferred approach among the three methods."},{"summary":"The Related Paper contrasts the Main Paper by emphasizing the limitations of Lasso in variable selection, advocating for bi-level selection methods that consider both individual and group variables. While the Main Paper focuses on comparing Lasso with other variable selection methods based on Hamming errors, the Related Paper highlights the advantages of group lasso and bi-level penalties, suggesting that these approaches may offer improved accuracy in variable selection and prediction, thus challenging the efficacy of Lasso in certain contexts.","paper_id":"10dc14f46f3e206dab2db027c13cdf26a83acf3b","title":"SPARSE REGULARIZATION FOR BI-LEVEL VARIABLE SELECTION","abstract":"Sparse regularization provides solutions in which some parameters are exactly zero and therefore they can be used for selecting variables in regression models and so on. The lasso is proposed as a method for selecting individual variables for regression models. On the other hand, the group lasso selects groups of variables rather than individuals and therefore it has been used in various fields of applications. More recently, penalties that select variables at both the group and individual levels has been considered. They are so called bi-level selection. In this paper we focus on some penalties that aim for bi-level selection. We overview these penalties and estimation algorithms, and then compare the effectiveness of these penalties from the viewpoint of accuracy of prediction and selection of variables and groups through simulation studies.","score":0.7584465146064758,"polarity":"negative","source":"semantic","year":2015,"authors":["H. Matsui"],"venue":null,"citation_count":3,"reference_count":64,"influential_citation_count":0,"corpus_id":126385274,"url":"https://www.semanticscholar.org/paper/10dc14f46f3e206dab2db027c13cdf26a83acf3b","arxiv_id":null,"contexts":null,"background":"Sparse regularization provides solutions in which some parameters are exactly zero and therefore they can be used for selecting variables in regression models and so on. The lasso is proposed as a method for selecting individual variables for regression models. On the other hand, the group lasso selects groups of variables rather than individuals and therefore it has been used in various fields of applications. More recently, penalties that select variables at both the group and individual levels has been considered. They are so called bi-level selection.","target":"In this paper we focus on some penalties that aim for bi-level selection. We overview these penalties and estimation algorithms, and then compare the effectiveness of these penalties from the viewpoint of accuracy of prediction and selection of variables and groups through simulation studies."},{"summary":"The Related Paper contrasts the Main Paper by focusing on the efficiency of Lasso solvers through screening rules, which enhance variable selection by discarding irrelevant variables during optimization. While the Main Paper compares various variable selection methods based on Hamming errors, the Related Paper emphasizes the theoretical guarantees and computational advantages of non-convex regularizers in Lasso, suggesting that the challenges faced by Lasso in correlated covariates can be mitigated through innovative solver strategies.","paper_id":"d83e947680e68282ba4f164dc84fa78ed7e71f41","title":"Screening Rules for Lasso with Non-Convex Sparse Regularizers","abstract":"Leveraging on the convexity of the Lasso problem , screening rules help in accelerating solvers by discarding irrelevant variables, during the optimization process. However, because they provide better theoretical guarantees in identifying relevant variables, several non-convex regulariz-ers for the Lasso have been proposed in the literature. This work is the first that introduces a screening rule strategy into a non-convex Lasso solver. The approach we propose is based on a iterative majorization-minimization (MM) strategy that includes a screening rule in the inner solver and a condition for propagating screened variables between iterations of MM. In addition to improve efficiency of solvers, we also provide guarantees that the inner solver is able to identify the zeros components of its critical point in finite time. Our experimental analysis illustrates the significant computational gain brought by the new screening rule compared to classical coordinate-descent or proximal gradient descent methods.","score":0.7510010004043579,"polarity":"negative","source":"semantic","year":2019,"authors":["A. Rakotomamonjy","G. Gasso","J. Salmon"],"venue":null,"citation_count":22,"reference_count":41,"influential_citation_count":0,"corpus_id":67750508,"url":"https://www.semanticscholar.org/paper/d83e947680e68282ba4f164dc84fa78ed7e71f41","arxiv_id":null,"contexts":null,"background":"Leveraging on the convexity of the Lasso problem, screening rules help in accelerating solvers by discarding irrelevant variables, during the optimization process. However, because they provide better theoretical guarantees in identifying relevant variables, several non-convex regularizers for the Lasso have been proposed in the literature.","target":"This work is the first that introduces a screening rule strategy into a non-convex Lasso solver. The approach we propose is based on a iterative majorization-minimization (MM) strategy that includes a screening rule in the inner solver and a condition for propagating screened variables between iterations of MM. In addition to improve efficiency of solvers, we also provide guarantees that the inner solver is able to identify the zeros components of its critical point in finite time. Our experimental analysis illustrates the significant computational gain brought by the new screening rule compared to classical coordinate-descent or proximal gradient descent methods."},{"summary":"The Related Paper critiques the Lasso method highlighted in the Main Paper, emphasizing its limitation of using a single tuning parameter for both variable selection and coefficient shrinkage, which can lead to overshrinkage and model overfitting. In contrast, the Related Paper introduces the Variable Inclusion and Shrinkage Algorithms (VISA), which aim to select sparser models without overshrinkage, demonstrating superior performance through simulations and theoretical justifications. This presents a significant divergence from the Main Paper's focus on comparing Lasso with other methods, as it suggests a more effective alternative to Lasso itself.","paper_id":"01ddb2e0baf1644cc66fe9520b72fabc827822a7","title":"Variable Inclusion and Shrinkage Algorithms","abstract":"The Lasso is a popular and computationally efficient procedure for automatically performing both variable selection and coefficient shrinkage on linear regression models. One limitation of the Lasso is that the same tuning parameter is used for both variable selection and shrinkage. As a result, it typically ends up selecting a model with too many variables to prevent overshrinkage of the regression coefficients. We suggest an improved class of methods called variable inclusion and shrinkage algorithms (VISA). Our approach is capable of selecting sparse models while avoiding overshrinkage problems and uses a path algorithm, and so also is computationally efficient. We show through extensive simulations that VISA significantly outperforms the Lasso and also provides improvements over more recent procedures, such as the Dantzig selector, relaxed Lasso, and adaptive Lasso. In addition, we provide theoretical justification for VISA in terms of nonasymptotic bounds on the estimation error that suggest it should exhibit good performance even for large numbers of predictors. Finally, we extend the VISA methodology, path algorithm, and theoretical bounds to the generalized linear models framework.","score":0.7463070750236511,"polarity":"negative","source":"semantic","year":2008,"authors":["J. Ames"],"venue":null,"citation_count":75,"reference_count":24,"influential_citation_count":8,"corpus_id":18933960,"url":"https://www.semanticscholar.org/paper/01ddb2e0baf1644cc66fe9520b72fabc827822a7","arxiv_id":null,"contexts":null,"background":"The Lasso is a popular and computationally efficient procedure for automatically performing both variable selection and coefficient shrinkage on linear regression models. One limitation of the Lasso is that the same tuning parameter is used for both variable selection and shrinkage. As a result, it typically ends up selecting a model with too many variables to prevent overshrinkage of the regression coefficients.","target":"We suggest an improved class of methods called variable inclusion and shrinkage algorithms (VISA). Our approach is capable of selecting sparse models while avoiding overshrinkage problems and uses a path algorithm, and so also is computationally efficient. We show through extensive simulations that VISA significantly outperforms the Lasso and also provides improvements over more recent procedures, such as the Dantzig selector, relaxed Lasso, and adaptive Lasso. In addition, we provide theoretical justification for VISA in terms of nonasymptotic bounds on the estimation error that suggest it should exhibit good performance even for large numbers of predictors. Finally, we extend the VISA methodology, path algorithm, and theoretical bounds to the generalized linear models framework."},{"summary":"The Related Paper contrasts the Main Paper by emphasizing the computational challenges associated with the Lasso method in high-dimensional settings, suggesting that marginal regression offers a faster alternative. While the Main Paper focuses on comparing Lasso with other variable selection methods based on Hamming errors, the Related Paper highlights the statistical performance of Lasso versus marginal regression, particularly in terms of exact recovery of coefficients. This suggests that the Related Paper questions the efficiency of Lasso in practical applications, which is a key point of divergence from the Main Paper's findings.","paper_id":"a69ce6862603f81c8f74beadcbf69fd09503ef7d","title":"A Comparison of the Lasso and Marginal Regression","abstract":"The lasso is an important method for sparse, high-dimensional regression problems, with efficient algorithms available, a long history of practical success, and a large body of theoretical results supporting and explaining its performance. But even with the best available algorithms, finding the lasso solutions remains a computationally challenging task in cases where the number of covariates vastly exceeds the number of data points. \n \nMarginal regression, where each dependent variable is regressed separately on each covariate, offers a promising alternative in this case because the estimates can be computed roughly two orders faster than the lasso solutions. The question that remains is how the statistical performance of the method compares to that of the lasso in these cases. \n \nIn this paper, we study the relative statistical performance of the lasso and marginal regression for sparse, high-dimensional regression problems. We consider the problem of learning which coefficients are non-zero. Our main results are as follows: (i) we compare the conditions under which the lasso and marginal regression guarantee exact recovery in the fixed design, noise free case; (ii) we establish conditions under which marginal regression provides exact recovery with high probability in the fixed design, noise free, random coefficients case; and (iii) we derive rates of convergence for both procedures, where performance is measured by the number of coefficients with incorrect sign, and characterize the regions in the parameter space recovery is and is not possible under this metric. \n \nIn light of the computational advantages of marginal regression in very high dimensional problems, our theoretical and simulations results suggest that the procedure merits further study.","score":0.7416654229164124,"polarity":"negative","source":"semantic","year":2012,"authors":["C. Genovese","Jiashun Jin","L. Wasserman","Zhigang Yao"],"venue":null,"citation_count":113,"reference_count":39,"influential_citation_count":19,"corpus_id":8847997,"url":"https://www.semanticscholar.org/paper/a69ce6862603f81c8f74beadcbf69fd09503ef7d","arxiv_id":null,"contexts":null,"background":"The lasso is an important method for sparse, high-dimensional regression problems, with efficient algorithms available, a long history of practical success, and a large body of theoretical results supporting and explaining its performance. But even with the best available algorithms, finding the lasso solutions remains a computationally challenging task in cases where the number of covariates vastly exceeds the number of data points. Marginal regression, where each dependent variable is regressed separately on each covariate, offers a promising alternative in this case because the estimates can be computed roughly two orders faster than the lasso solutions. The question that remains is how the statistical performance of the method compares to that of the lasso in these cases.","target":"In this paper, we study the relative statistical performance of the lasso and marginal regression for sparse, high-dimensional regression problems. We consider the problem of learning which coefficients are non-zero. Our main results are as follows: (i) we compare the conditions under which the lasso and marginal regression guarantee exact recovery in the fixed design, noise free case; (ii) we establish conditions under which marginal regression provides exact recovery with high probability in the fixed design, noise free, random coefficients case; and (iii) we derive rates of convergence for both procedures, where performance is measured by the number of coefficients with incorrect sign, and characterize the regions in the parameter space recovery is and is not possible under this metric. In light of the computational advantages of marginal regression in very high dimensional problems, our theoretical and simulations results suggest that the procedure merits further study."},{"summary":"The Related Paper supports the Main Paper by demonstrating that multi-step thresholding procedures can effectively estimate sparse vectors in high-dimensional settings, particularly when traditional methods like Lasso struggle due to high correlation among covariates. This aligns with the Main Paper's exploration of alternative variable selection methods, highlighting the advantages of thresholding techniques in achieving model selection consistency under weaker conditions. Additionally, the findings on the Gauss-Dantzig selector reinforce the Main Paper's conclusions about the limitations of Lasso, providing further evidence for the need to consider diverse approaches in variable selection.","paper_id":"7ed849c45ee0d5e45423226e967bfaab4af35e72","title":"Thresholding Procedures for High Dimensional Variable Selection and Statistical Estimation","abstract":"Given n noisy samples with p dimensions, where n ≪ p, we show that the multi-step thresholding procedure can accurately estimate a sparse vector β ∈ ℝp in a linear model, under the restricted eigenvalue conditions (Bickel-Ritov-Tsybakov 09). Thus our conditions for model selection consistency are considerably weaker than what has been achieved in previous works. More importantly, this method allows very significant values of s, which is the number of non-zero elements in the true parameter. For example, it works for cases where the ordinary Lasso would have failed. Finally, we show that if X obeys a uniform uncertainty principle and if the true parameter is sufficiently sparse, the Gauss-Dantzig selector (Candes-Tao 07) achieves the l2 loss within a logarithmic factor of the ideal mean square error one would achieve with an oracle which would supply perfect information about which coordinates are non-zero and which are above the noise level, while selecting a sufficiently sparse model.","score":0.5715686678886414,"polarity":"positive","source":"citations","year":2009,"authors":["Shuheng Zhou"],"venue":null,"citation_count":59,"reference_count":23,"influential_citation_count":7,"corpus_id":6749021,"url":"https://www.semanticscholar.org/paper/7ed849c45ee0d5e45423226e967bfaab4af35e72","arxiv_id":null,"contexts":[{"sentence":"We will study six methods: (i) Lasso as in ; (ii) Elastic net \\citep{zou2005regularization}, which adds an additional MATH_PLACEHOLDER -penalty to , (iii) smoothly clipped absolute deviation (SCAD) \\citep{fan2001variable}, which replaces the MATH_PLACEHOLD","polarity":"positive"}],"background":null,"target":null},{"summary":"The Related Paper supports the Main Paper by emphasizing the importance of variable selection in high-dimensional statistical modeling and introducing penalized likelihood approaches that address challenges faced by traditional methods like Lasso. It highlights the advantages of nonconcave penalties and their ability to produce sparse solutions, which aligns with the Main Paper's exploration of alternative variable selection methods. Additionally, both papers establish rates of convergence for their respective methods, reinforcing the Main Paper's findings on the performance of various selection techniques.","paper_id":"58db9852fedd50a4732e3c174383710422c32ecc","title":"Variable Selection via Nonconcave Penalized Likelihood and its Oracle Properties","abstract":"Variable selection is fundamental to high-dimensional statistical modeling, including nonparametric regression. Many approaches in use are stepwise selection procedures, which can be computationally expensive and ignore stochastic errors in the variable selection process. In this article, penalized likelihood approaches are proposed to handle these kinds of problems. The proposed methods select variables and estimate coefficients simultaneously. Hence they enable us to construct confidence intervals for estimated parameters. The proposed approaches are distinguished from others in that the penalty functions are symmetric, nonconcave on (0, ∞), and have singularities at the origin to produce sparse solutions. Furthermore, the penalty functions should be bounded by a constant to reduce bias and satisfy certain conditions to yield continuous solutions. A new algorithm is proposed for optimizing penalized likelihood functions. The proposed ideas are widely applicable. They are readily applied to a variety of parametric models such as generalized linear models and robust regression models. They can also be applied easily to nonparametric modeling by using wavelets and splines. Rates of convergence of the proposed penalized likelihood estimators are established. Furthermore, with proper choice of regularization parameters, we show that the proposed estimators perform as well as the oracle procedure in variable selection; namely, they work as well as if the correct submodel were known. Our simulation shows that the newly proposed methods compare favorably with other variable selection techniques. Furthermore, the standard error formulas are tested to be accurate enough for practical applications.","score":0.5631408095359802,"polarity":"positive","source":"citations","year":2001,"authors":["Jianqing Fan","Runze Li"],"venue":null,"citation_count":8843,"reference_count":27,"influential_citation_count":1822,"corpus_id":14116328,"url":"https://www.semanticscholar.org/paper/58db9852fedd50a4732e3c174383710422c32ecc","arxiv_id":null,"contexts":[{"sentence":"Existing theoretical studies focused on `model selection consistency' (e.g., \\cite{fan2001variable,zhao2006model, zou2006adaptive, meinshausen2010stability, loh2017support}), which uses MATH_PLACEHOLDER to measure the performance of variable selection.","polarity":"positive"},{"sentence":"We will study six methods: (i) Lasso as in ; (ii) Elastic net \\citep{zou2005regularization}, which adds an additional MATH_PLACEHOLDER -penalty to , (iii) smoothly clipped absolute deviation (SCAD) \\citep{fan2001variable}, which replaces the MATH_PLACEHOLD","polarity":"positive"},{"sentence":"SCAD \\citep{fan2001variable} is a non-convex penalization method.","polarity":"positive"}],"background":null,"target":null},{"summary":"The Related Paper supports the Main Paper by providing a thorough analysis of two-stage variable selection techniques, which includes bridge estimators that can enhance the performance of variable selection methods like Lasso. It highlights the importance of selecting estimators with lower asymptotic mean square error, which aligns with the Main Paper's exploration of various methods for variable selection. Additionally, the findings regarding the superiority of certain two-stage methods over Lasso in specific conditions reinforce the Main Paper's conclusions about the limitations of Lasso in high-dimensional settings.","paper_id":"990c6285bb37d32aee410bfe90115da751609ba0","title":"Which bridge estimator is optimal for variable selection?","abstract":"We study the problem of variable selection for linear models under the high-dimensional asymptotic setting, where the number of observations $n$ grows at the same rate as the number of predictors $p$. We consider two-stage variable selection techniques (TVS) in which the first stage uses bridge estimators to obtain an estimate of the regression coefficients, and the second stage simply thresholds this estimate to select the \"important\" predictors. The asymptotic false discovery proportion (AFDP) and true positive proportion (ATPP) of these TVS are evaluated. We prove that for a fixed ATPP, in order to obtain a smaller AFDP, one should pick a bridge estimator with smaller asymptotic mean square error in the first stage of TVS. Based on such principled discovery, we present a sharp comparison of different TVS, via an in-depth investigation of the estimation properties of bridge estimators. Rather than \"order-wise\" error bounds with loose constants, our analysis focuses on precise error characterization. Various interesting signal-to-noise ratio and sparsity settings are studied. Our results offer new and thorough insights into high-dimensional variable selection. For instance, we prove that a TVS with Ridge in its first stage outperforms TVS with other bridge estimators in large noise settings; two-stage LASSO becomes inferior when the signal is rare and weak. As a by-product, we show that two-stage methods outperform some standard variable selection techniques, such as LASSO and Sure Independence Screening, under certain conditions.","score":0.5047279596328735,"polarity":"positive","source":"citations","year":2017,"authors":["Shuaiwen Wang","Haolei Weng","A. Maleki"],"venue":null,"citation_count":17,"reference_count":57,"influential_citation_count":2,"corpus_id":22455294,"url":"https://www.semanticscholar.org/paper/990c6285bb37d32aee410bfe90115da751609ba0","arxiv_id":null,"contexts":[{"sentence":"There have been inspiring works that demonstrate the advantages of such a post-processing \\citep{van2011adaptive, wang2017bridge,weinstein2020power}.","polarity":"positive"}],"background":null,"target":null},{"summary":"The Related Paper provides a crucial reference that supports the Main Paper's discussion on Lasso's solution path, highlighting the foundational work by Osborne et al. (2000) on the piecewise linearity of the Lasso solution. This reference strengthens the Main Paper's claims about Lasso's performance in variable selection by acknowledging the historical context and algorithmic efficiency that underpin Lasso's methodology.","paper_id":"e9fde5617b15623d1151d152671626df06aa5c2a","title":"Addendum: Regularization and variable selection via the elastic net","abstract":"We missed an important reference in Section 3.4. In page 309 we stated that ‘. . . which is based on the recently proposed algorithm LARS of Efron et al. (2004). They proved that, starting from zero, the lasso solution paths grow piecewise linearly in a predictable way. They proposed a new algorithm called LARS to solve the entire lasso solution path efficiently by using the same order of computations as a single OLS fit. . . .’ The following sentence should have been included. The piecewise linearity of the lasso solution path was first proved by Osborne et al. (2000), who also described an efficient algorithm for calculating the complete lasso solution path. Reference Osborne, M. R., Presnell, B. and Turlach, B. A. (2000) A new approach to variable selection in least squares problems. IMA J. Numer. Anal., 20, 389–403.","score":0.4909590184688568,"polarity":"positive","source":"citations","year":2005,"authors":["H. Zou","T. Hastie"],"venue":null,"citation_count":6577,"reference_count":2,"influential_citation_count":552,"corpus_id":14134075,"url":"https://www.semanticscholar.org/paper/e9fde5617b15623d1151d152671626df06aa5c2a","arxiv_id":null,"contexts":[{"sentence":"We will study six methods: (i) Lasso as in ; (ii) Elastic net \\citep{zou2005regularization}, which adds an additional MATH_PLACEHOLDER -penalty to , (iii) smoothly clipped absolute deviation (SCAD) \\citep{fan2001variable}, which replaces the MATH_PLACEHOLD","polarity":"positive"},{"sentence":"We must note that Elastic net is motivated by genetic applications where several correlated variables are competing as predictors, and where it is implicitly assumed that groups of correlated variables tend to be all relevant or all irrelevant \\citep{zou20","polarity":"positive"},{"sentence":"The Elastic net \\citep{zou2005regularization} is a method that estimates MATH_PLACEHOLDER by ^{ } = _{ } \\{ *{y-X }^2/2 + *{ }_1 + ( /2) *{ }^2 \\}.","polarity":"positive"}],"background":null,"target":null},{"summary":"The Related Paper supports the Main Paper by highlighting the advantages of the Lasso method in variable selection, emphasizing its ability to produce interpretable models through coefficient shrinkage. It reinforces the Main Paper's findings by discussing the favorable properties of Lasso, such as its stability akin to ridge regression and its interpretability similar to subset selection. This connection strengthens the Main Paper's comparative analysis of Lasso against other variable selection methods.","paper_id":"b365b8e45b7d81f081de44ac8f9eadf9144f3ca5","title":"Regression Shrinkage and Selection via the Lasso","abstract":"SUMMARY We propose a new method for estimation in linear models. The 'lasso' minimizes the residual sum of squares subject to the sum of the absolute value of the coefficients being less than a constant. Because of the nature of this constraint it tends to produce some coefficients that are exactly 0 and hence gives interpretable models. Our simulation studies suggest that the lasso enjoys some of the favourable properties of both subset selection and ridge regression. It produces interpretable models like subset selection and exhibits the stability of ridge regression. There is also an interesting relationship with recent work in adaptive function estimation by Donoho and Johnstone. The lasso idea is quite general and can be applied in a variety of statistical models: extensions to generalized regression models and tree-based models are briefly described.","score":0.46057063341140747,"polarity":"positive","source":"citations","year":1996,"authors":["R. Tibshirani"],"venue":null,"citation_count":46334,"reference_count":25,"influential_citation_count":5485,"corpus_id":16162039,"url":"https://www.semanticscholar.org/paper/b365b8e45b7d81f081de44ac8f9eadf9144f3ca5","arxiv_id":null,"contexts":[{"sentence":"Lasso \\citep{tibshirani1996regression} is a popular method: % It solves an optimization: ^{ } = _{ } \\{ *{y-X }^2/2 + *{ }_1 \\}.","polarity":"positive"}],"background":null,"target":null},{"summary":"The Related Paper contrasts the Main Paper by emphasizing the computational challenges of traditional variable selection methods in high-dimensional settings, suggesting that the methods compared in the Main Paper, including Lasso, may not be suitable for ultra-high dimensional problems. While the Main Paper focuses on the theoretical performance of specific variable selection methods through Hamming error analysis, the Related Paper highlights the need for more efficient penalized likelihood approaches that can handle larger dimensionalities and discusses recent advancements in the field, which may not be addressed in the Main Paper.","paper_id":"c9e55b6ecc6f65d2735830dede4f8d2bdf531d34","title":"A Selective Overview of Variable Selection in High Dimensional Feature Space.","abstract":"High dimensional statistical problems arise from diverse fields of scientific research and technological development. Variable selection plays a pivotal role in contemporary statistical learning and scientific discoveries. The traditional idea of best subset selection methods, which can be regarded as a specific form of penalized likelihood, is computationally too expensive for many modern statistical applications. Other forms of penalized likelihood methods have been successfully developed over the last decade to cope with high dimensionality. They have been widely applied for simultaneously selecting important variables and estimating their effects in high dimensional statistical inference. In this article, we present a brief account of the recent developments of theory, methods, and implementations for high dimensional variable selection. What limits of the dimensionality such methods can handle, what the role of penalty functions is, and what the statistical properties are rapidly drive the advances of the field. The properties of non-concave penalized likelihood and its roles in high dimensional statistical modeling are emphasized. We also review some recent advances in ultra-high dimensional variable selection, with emphasis on independence screening and two-scale methods.","score":0.579085111618042,"polarity":"negative","source":"citations","year":2009,"authors":["Jianqing Fan","Jinchi Lv"],"venue":null,"citation_count":892,"reference_count":145,"influential_citation_count":65,"corpus_id":16049649,"url":"https://www.semanticscholar.org/paper/c9e55b6ecc6f65d2735830dede4f8d2bdf531d34","arxiv_id":null,"contexts":[{"sentence":"\\cite{zhao2006model} showed that an { irrepresentable condition} on MATH_PLACEHOLDER is necessary for Lasso to recover MATH_PLACEHOLDER with high probability, and such a condition is restrictive when MATH_PLACEHOLDER is large \\citep{fan2010selective}.","polarity":"negative"}],"background":null,"target":null},{"summary":"The Related Paper critiques the Main Paper's reliance on Lasso for variable selection by emphasizing the importance of the Irrepresentable Condition for Lasso's consistency in model selection. While the Main Paper compares Lasso with other methods based on Hamming errors, the Related Paper argues that Lasso's effectiveness is contingent upon specific conditions related to predictor covariance, suggesting that without these conditions, Lasso may not reliably select the true model. This highlights a potential limitation in the Main Paper's conclusions regarding Lasso's performance.","paper_id":"dc5b06753fac11268bc2300b7c25d50cbbcdeb5c","title":"On Model Selection Consistency of Lasso","abstract":"Sparsity or parsimony of statistical models is crucial for their proper interpretations, as in sciences and social sciences. Model selection is a commonly used method to find such models, but usually involves a computationally heavy combinatorial search. Lasso (Tibshirani, 1996) is now being used as a computationally feasible alternative to model selection. Therefore it is important to study Lasso for model selection purposes. \n \nIn this paper, we prove that a single condition, which we call the Irrepresentable Condition, is almost necessary and sufficient for Lasso to select the true model both in the classical fixed p setting and in the large p setting as the sample size n gets large. Based on these results, sufficient conditions that are verifiable in practice are given to relate to previous works and help applications of Lasso for feature selection and sparse representation. \n \nThis Irrepresentable Condition, which depends mainly on the covariance of the predictor variables, states that Lasso selects the true model consistently if and (almost) only if the predictors that are not in the true model are \"irrepresentable\" (in a sense to be clarified) by predictors that are in the true model. Furthermore, simulations are carried out to provide insights and understanding of this result.","score":0.5025172233581543,"polarity":"negative","source":"citations","year":2006,"authors":["P. Zhao","Bin Yu"],"venue":null,"citation_count":2794,"reference_count":16,"influential_citation_count":307,"corpus_id":2174351,"url":"https://www.semanticscholar.org/paper/dc5b06753fac11268bc2300b7c25d50cbbcdeb5c","arxiv_id":null,"contexts":[{"sentence":"\\cite{zhao2006model} showed that an { irrepresentable condition} on MATH_PLACEHOLDER is necessary for Lasso to recover MATH_PLACEHOLDER with high probability, and such a condition is restrictive when MATH_PLACEHOLDER is large \\citep{fan2010selective}.","polarity":"negative"},{"sentence":"However, there are cases where Lasso has no model selection consistency no matter how large the signal strength is \\citep{zhao2006model}.","polarity":"negative"}],"background":null,"target":null},{"summary":"The Related Paper contrasts the Main Paper by highlighting the limitations of Lasso, particularly its bias in variable selection, which the Main Paper does not address. While the Main Paper compares Lasso with other methods based on Hamming errors, the Related Paper introduces the MC+ method, which claims to achieve nearly unbiased variable selection without the strong irrepresentable condition required by Lasso. Additionally, the Related Paper emphasizes the computational efficiency and superior variable selection properties of MC+, suggesting that it outperforms Lasso and other methods discussed in the Main Paper.","paper_id":"112e2d4f6ffbfba004576a159ffdf60f5dad8d3a","title":"Nearly unbiased variable selection under minimax concave penalty","abstract":"We propose MC+, a fast, continuous, nearly unbiased and accurate method of penalized variable selection in high-dimensional linear regression. The LASSO is fast and continuous, but biased. The bias of the LASSO may prevent consistent variable selection. Subset selection is unbiased but computationally costly. The MC+ has two elements: a minimax concave penalty (MCP) and a penalized linear unbiased selection (PLUS) algorithm. The MCP provides the convexity of the penalized loss in sparse regions to the greatest extent given certain thresholds for variable selection and unbiasedness. The PLUS computes multiple exact local minimizers of a possibly nonconvex penalized loss function in a certain main branch of the graph of critical points of the penalized loss. Its output is a continuous piecewise linear path encompassing from the origin for infinite penalty to a least squares solution for zero penalty. We prove that at a universal penalty level, the MC+ has high probability of matching the signs of the unknowns, and thus correct selection, without assuming the strong irrepresentable condition required by the LASSO. This selection consistency applies to the case of p » n, and is proved to hold for exactly the MC+ solution among possibly many local minimizers. We prove that the MC+ attains certain minimax convergence rates in probability for the estimation of regression coefficients in l r balls. We use the SURE method to derive degrees of freedom and C p -type risk estimates for general penalized LSE, including the LASSO and MC+ estimators, and prove their unbiasedness. Based on the estimated degrees of freedom, we propose an estimator of the noise level for proper choice of the penalty level. For full rank designs and general sub-quadratic penalties, we provide necessary and sufficient conditions for the continuity of the penalized LSE. Simulation results overwhelmingly support our claim of superior variable selection properties and demonstrate the computational efficiency of the proposed method.","score":0.4915778636932373,"polarity":"negative","source":"citations","year":2010,"authors":["Cun-Hui Zhang"],"venue":null,"citation_count":3438,"reference_count":56,"influential_citation_count":561,"corpus_id":88519565,"url":"https://www.semanticscholar.org/paper/112e2d4f6ffbfba004576a159ffdf60f5dad8d3a","arxiv_id":null,"contexts":[{"sentence":"Many alternative strategies were proposed for variable selection, such as using non-convex penalties \\citep{fan2001variable, zhang2010nearly, shen2012likelihood}, adding a ridge regularization \\citep{zou2005regularization}, post-processing on the Lasso est","polarity":"negative"}],"background":null,"target":null},{"summary":"The Related Paper contrasts the Main Paper by emphasizing the effectiveness of likelihood-based methods for feature selection and parameter estimation, particularly in high-dimensional settings. While the Main Paper focuses on comparing Lasso and other variable selection methods based on Hamming errors, the Related Paper argues for the superiority of nonconvex constrained likelihood approaches, which achieve both selection consistency and sharp parameter estimation. This highlights a potential limitation in the Main Paper's reliance on traditional methods like Lasso, suggesting that newer techniques may offer better performance in complex scenarios.","paper_id":"97fe0d793738763da9214de309c3c4ceb829ba41","title":"Likelihood-Based Selection and Sharp Parameter Estimation","abstract":"In high-dimensional data analysis, feature selection becomes one effective means for dimension reduction, which proceeds with parameter estimation. Concerning accuracy of selection and estimation, we study nonconvex constrained and regularized likelihoods in the presence of nuisance parameters. Theoretically, we show that constrained L 0 likelihood and its computational surrogate are optimal in that they achieve feature selection consistency and sharp parameter estimation, under one necessary condition required for any method to be selection consistent and to achieve sharp parameter estimation. It permits up to exponentially many candidate features. Computationally, we develop difference convex methods to implement the computational surrogate through prime and dual subproblems. These results establish a central role of L 0 constrained and regularized likelihoods in feature selection and parameter estimation involving selection. As applications of the general method and theory, we perform feature selection in linear regression and logistic regression, and estimate a precision matrix in Gaussian graphical models. In these situations, we gain a new theoretical insight and obtain favorable numerical results. Finally, we discuss an application to predict the metastasis status of breast cancer patients with their gene expression profiles. This article has online supplementary material.","score":0.46343669295310974,"polarity":"negative","source":"citations","year":2012,"authors":["Xiaotong Shen","W. Pan","Yunzhang Zhu"],"venue":null,"citation_count":282,"reference_count":41,"influential_citation_count":29,"corpus_id":15389839,"url":"https://www.semanticscholar.org/paper/97fe0d793738763da9214de309c3c4ceb829ba41","arxiv_id":null,"contexts":[{"sentence":"Many alternative strategies were proposed for variable selection, such as using non-convex penalties \\citep{fan2001variable, zhang2010nearly, shen2012likelihood}, adding a ridge regularization \\citep{zou2005regularization}, post-processing on the Lasso est","polarity":"negative"}],"background":null,"target":null}],"paper":{"title":"A Comparison of Hamming Errors of Representative Variable Selection Methods","abstract":"Lasso is a celebrated method for variable selection in linear models, but it faces challenges when the covariates are moderately or strongly correlated. This motivates alternative approaches such as using a non-convex penalty, adding a ridge regularization, or conducting a post-Lasso thresholding. In this paper, we compare Lasso with 5 other methods: Elastic net, SCAD, forward selection, thresholded Lasso, and forward backward selection. We measure their performances theoretically by the expected Hamming error, assuming that the regression coefficients are ${\\it iid}$ drawn from a two-point mixture and that the Gram matrix is block-wise diagonal. By deriving the rates of convergence of Hamming errors and the phase diagrams, we obtain useful conclusions about the pros and cons of different methods.","authors":["Tracy Ke","Longlin Wang"],"sections":[{"heading":"Introduction","text":"# Introduction {#sec:Intro}\n\nVariable selection is one of the core problems in high-dimensional data analysis. Consider a linear regression, where the response $y\\in\\mathbb{R}^n$ and the design matrix $X=[x_1,\\ldots,x_p]\\in\\mathbb{R}^{n\\times p}$ satisfy that $$\\label{linearM}\ny=X\\beta+z, \\qquad \\|x_j\\|=1,\\qquad  z\\sim {\\cal N}(0, \\sigma^2I_n). %\\vspace{-1mm}$$ The goal is estimating the support of $\\beta$ ($\\mathrm{Supp}(\\beta)$). Lasso [@tibshirani1996regression] is a popular method: $$\\label{Lasso}\n\\hat{\\beta}^{\\mathrm{lasso}} =\\mathrm{argmin}_{\\beta}\\bigl\\{ \\norm*{y-X\\beta}^2/2 + \\lambda \\norm*{\\beta}_1 \\bigr\\}.$$ Lasso has good rates of convergence on the $L_q$-estimation error or prediction error [@bickel2009simultaneous]. However, it can be unsatisfactory for variable selection, especially when the columns in the design matrix are moderately or strongly correlated. [@zhao2006model] showed that an *irrepresentable condition* on $X$ is necessary for Lasso to recover $\\mathrm{Supp}(\\beta)$ with high probability, and such a condition is restrictive when $p$ is large [@fan2010selective]. [@ji2012ups] studied the Hamming error of Lasso and revealed Lasso's non-optimality by lower-bounding its Hamming error rate. Many alternative strategies were proposed for variable selection, such as using non-convex penalties [@fan2001variable; @zhang2010nearly; @shen2012likelihood], adding a ridge regularization [@zou2005regularization], post-processing on the Lasso estimator [@zou2006adaptive; @zhou2009thresholding], and iterative algorithms [@zhang2011adaptive; @donoho2012sparse]. In this paper, our main interest is to theoretically compare these different strategies.\n\nExisting theoretical studies focused on 'model selection consistency' (e.g., [@fan2001variable; @zhao2006model; @zou2006adaptive; @meinshausen2010stability; @loh2017support]), which uses $\\mathbb{P}(\\mathrm{Supp}(\\hat{\\beta})= \\mathrm{Supp}(\\beta))$ to measure the performance of variable selection. However, for many real applications, the study of the Hamming error (i.e., total number of false positives and false negatives) is in urgent need. For example, in genome-wide association studies (GWAS) or Genetic Regulatory Network, the goal is to identify the genes or SNPs that are truly associated with a given phenotype, and we hope to find a multiple testing procedure that simultaneously controls the FDR and maximizes the power (for multiple testing). This problem can be re-cast as minimizing the Hamming error in a special regression setting [@efron2004large; @jin2012comment; @sun2007oracle]. This motivates us to study the *Hamming errors* of variable selection methods, which were rarely considered in the literature.\n\nWe adopt the *rare and weak signal model* [@donoho2004higher; @arias2011global; @jin2016rare], which is often used in theoretical analysis of sparse linear models. Let $p$ be the asymptotic parameter. Given constants $\\vartheta\\in (0,1)$ and $r>0$, we assume that $\\beta_j$'s are iid generated such that $$\\label{model-beta}\n\\beta_j=\\begin{cases} \\tau_p, &\\mbox{with probability }\\epsilon_p,\\\\\n0, & \\mbox{with probability }1-\\epsilon_p,\n\\end{cases} \\qquad\\mbox{where}\\qquad \\epsilon_p=p^{-\\vartheta}, \\quad \\tau_p=\\sqrt{2r\\log(p)}.$$ As $p\\to\\infty$, $\\|\\beta\\|_0\\approx p^{1-\\vartheta}$, and a nonzero $\\beta_j$ is at the critical order $\\sqrt{\\log(p)}$. [^1] The two parameters $(\\vartheta, r)$ capture the sparsity level and signal strength, respectively. We may generalize [\\[model-beta\\]](#model-beta){reference-type=\"eqref\" reference=\"model-beta\"} to let nonzero $\\beta_j$'s take different values in $[\\tau_p,\\infty)$, but the current form is more convenient for presentation.\n\nThe *blockwise covariance structure* is frequently observed in real applications. In genetic data, there may exist strong correlations between nearby genetic markers, but the long-range dependence is usually negligible; as a result, the sample covariance matrix is approximately blockwise diagonal [@dehman2015performance]. In financial data, the sample covariance matrix of stock returns (after common factors are removed) is also approximately blockwise diagonal, where each block corresponds to an industry group [@fan2015risks]. Motivated by these examples, we consider an idealized setting, where the Gram matrix $G=X'X$ is block-wise diagonal consisting of $2\\times 2$ blocks: $$\\label{model-X}\nG=\\mathrm{diag}(B,B,\\ldots,B,B_0), \\qquad \\mbox{where}\\quad B=\\begin{bmatrix}\n1 & \\rho\\\\\n\\rho & 1\n\\end{bmatrix}\\mbox{ and  }B_0=\\begin{cases}\nB, & \\mbox{if $p$ is even}, \\\\\n1, & \\mbox{if $p$ is odd}. \n\\end{cases}$$ This is an idealization of the blockwise covariance structures in real applications. We may generalize [\\[model-X\\]](#model-X){reference-type=\"eqref\" reference=\"model-X\"} to allow unequal block sizes and unequal off-diagonal entries, but we keep the current form for convenience of presentation. Model [\\[model-X\\]](#model-X){reference-type=\"eqref\" reference=\"model-X\"} is also closely connected to the random designs in compressed sensing [@donoho2006compressed]. Write $X=[X_1,X_2,\\ldots,X_n]'$. Suppose $X_1, X_2,\\ldots,X_n$ are iid generated from ${\\cal N}\\bigl(0,\\; n^{-1}\\Sigma)$, where $\\Sigma$ has the same form as $G$ in [\\[model-X\\]](#model-X){reference-type=\"eqref\" reference=\"model-X\"}. In a high-dimensional sparse setting, we have $\\|\\beta\\|_0\\ll n\\ll p$. Then, $G=X'X\\approx \\Sigma$, and due to the blessing of sparsity of $\\beta$, $G\\beta\\approx \\Sigma\\beta$. As a result, $X'y$ (sufficient statistic of $\\beta$) satisfies that $X'y = G\\beta + {\\cal N}(0, G) \\approx \\Sigma\\beta + {\\cal N}(0, \\Sigma)$, and the right hand side reduces to Model [\\[model-X\\]](#model-X){reference-type=\"eqref\" reference=\"model-X\"} [@genovese2012comparison]. In Section [3.5](#subsec:random){reference-type=\"ref\" reference=\"subsec:random\"}, we formally show that this random design setting is asymptotically equivalent to Model [\\[model-X\\]](#model-X){reference-type=\"eqref\" reference=\"model-X\"}.\n\nNow, under model [\\[model-beta\\]](#model-beta){reference-type=\"eqref\" reference=\"model-beta\"} and model [\\[model-X\\]](#model-X){reference-type=\"eqref\" reference=\"model-X\"}, we have three parameters $(\\vartheta,r,\\rho)$. They capture the sparsity level, signal strength and design correlations, respectively. Our main results are the explicit convergence rates of Hamming error, as a function of $(\\vartheta,r,\\rho)$, for different methods. We will study six methods: (i) Lasso as in [\\[Lasso\\]](#Lasso){reference-type=\"eqref\" reference=\"Lasso\"}; (ii) Elastic net [@zou2005regularization], which adds an additional $L^2$-penalty to [\\[Lasso\\]](#Lasso){reference-type=\"eqref\" reference=\"Lasso\"}, (iii) smoothly clipped absolute deviation (SCAD) [@fan2001variable], which replaces the $L^1$-penalty by a non-convex penalty, (iv) thresholded Lasso [@zhou2009thresholding], which further thresholds the Lasso solution, and two iterative algorithms, (v) forward selection and (vi) forward backward selection [@huang2016partial]; see Section [3](#sec:main){reference-type=\"ref\" reference=\"sec:main\"} for a precise description of each method. To our best knowledge, our results are the first that directly compare Hamming errors of these methods."},{"heading":"A preview of main results and some discussions","text":"# A preview of main results and some discussions {#sec:Preview}\n\nFor any $\\hat{\\beta}$, its Hamming error is $H(\\hat{\\beta},\\beta)=\\sum_{j=1}^p 1\\{\\hat{\\beta}_j\\neq 0, \\beta_j=0\\}+\\sum_{j=1}^p 1\\{\\hat{\\beta}_j= 0, \\beta_j\\neq 0\\}$. As we shall show, for any of the six methods studied here, there exists a function $h(\\vartheta, r, \\rho)\\in [0,1]$ such that $\\mathbb{E}[H(\\hat{\\beta},\\beta)]=L_p p^{1-h(\\vartheta,r,\\rho)}$, where $L_p$ is a *multi-$\\log(p)$* term. (A multi-$\\log(p)$ term is such that $L_p \\cdot p^{\\epsilon}\\to\\infty$ and $L_p \\cdot p^{ -\\epsilon}\\to 0$ for any $\\epsilon > 0$.) Since the expected number of true relevant variables is $p^{1-\\vartheta}$, we are interested in three cases:\n\n-   *Exact recovery: $h(\\vartheta,r,\\rho)>1$.* In this case, the expected Hamming error is $o(1)$ as $p\\to\\infty$. It follows that model selection consistency holds.\n\n-   *Almost full recovery: $\\vartheta< h(\\vartheta,r,\\rho)<1$.* In this case, the expected Hamming error does not vanish as $p\\to\\infty$, but it is much smaller than the total number of true relevant variables. Variable selection is still satisfactory (although model selection consistency no longer holds).\n\n-   *No recovery: $h(\\vartheta,r,\\rho)\\leq \\vartheta$.* In this case, the expected Hamming error is comparable with or much larger than the total number of true relevant variables. Variable selection fails.\n\nWe call the two-dimensional space $(\\vartheta,r)$ the *phase space*. For each fixed $\\rho$, the phase space is divided into three regions: *Region of Exact Recovery (ER)*, which is the subset $\\{(\\vartheta,r): h(\\vartheta,r,\\rho)>1\\}$, and *Region of Almost Full Recovery (AFR)* and *Region of No Recovery (NR)* defined similarly. This gives rise to a *phase diagram* for each method. We denote the curve separating ER region and AFR region by $r= U(\\vartheta)$ and the curve separating AFR region and NR region by $r=L(\\vartheta)$; they are called the *upper* and *lower phase curves*, respectively. The phase diagram and phase curves are convenient ways to visualize the convergence rates of the Hamming error.\n\n![Phase diagrams of six variable selection methods for a block-wise diagonal design. The parameters $(\\vartheta,r,\\rho)$ characterize the sparsity, signal strength, and design correlations, respectively. For each method, we plot the curve $r=U(\\vartheta)$ which separates Region of Almost Full Recovery and Region of Exact Recovery (the lower this curve, the better). Explicit forms of $U(\\vartheta)$ are in Section [3](#sec:main){reference-type=\"ref\" reference=\"sec:main\"}. On the left panel, the curves for Lasso and SCAD overlap and are displayed as a dashed line. How to interpret these phase curves are discussed in Section [2](#sec:Preview){reference-type=\"ref\" reference=\"sec:Preview\"}. ](Figure/first_fig.pdf){#fig:overview height=\".38\\\\textwidth\"}\n\nFigure [1](#fig:overview){reference-type=\"ref\" reference=\"fig:overview\"} shows the phase curves for the six methods (with explicit expressions in the theorems in Section [3](#sec:main){reference-type=\"ref\" reference=\"sec:main\"}). These phase curves depend on the correlation parameter $\\rho$. Under our model, for each diagonal block $(j, j+1)$, it holds that $\\mathbb{E}[x_j'y|\\beta]=\\beta_j+\\rho \\beta_{j+1}$, where $\\beta_j, \\beta_{j+1}\\in \\{0,\\tau_p\\}$. Therefore, a *positive* $\\rho$ boosts the signal at each individual site (i.e., $\\mathbb{E}[x_j'y|\\beta]\\geq \\beta_j$), while a *negative* $\\rho$ leads to potential 'signal cancellation' (i.e., $\\mathbb{E}[x_j'y|\\beta]\\leq \\beta_j$). This is why the phase curves have different shapes for positive and negative $\\rho$. In Figure [1](#fig:overview){reference-type=\"ref\" reference=\"fig:overview\"}, we plot the phase curves for $\\rho=0.5$ and $\\rho=-0.4$. For other positive/negative value of $\\rho$, the patterns are similar.\n\n**Discussion of SCAD**. SCAD is a representative of non-convex penalization methods. There have been inspiring works that demonstrate the advantages of using a non-convex penalty (e.g., [@fan2004nonconcave; @loh2017support]). Our results support their insights from a different angle: The phase curve of SCAD is strictly better than that of Lasso, when $\\vartheta<0.5$ and $\\rho<0$. Furthermore, our results illustrate where the advantage of SCAD comes from --- compared with Lasso, it handles 'signal cancellation' better. To see this, we recall that 'signal cancellation' only happens for $\\rho<0$. Moreover, under our model [\\[model-beta\\]](#model-beta){reference-type=\"eqref\" reference=\"model-beta\"}, the expected number of signal pairs (a signal pair is a diagonal block $\\{j,j+1\\}$ where both $\\beta_j$ and $\\beta_{j+1}$ are nonzero) is $\\asymp p\\epsilon_p^2=p^{1-2\\vartheta}$. Therefore, 'signal cancellation' becomes problematic only when $\\vartheta<0.5$ and $\\rho<0$ both hold. This explains why the phase curves of SCAD and Lasso are the same for the other values of $\\vartheta$ and $\\rho$. We note that in the previous studies (e.g., [@loh2017support]), the advantages of a non-convex penalty in handling 'signal cancellation' are reflected in the weaker conditions of $(X,\\beta)$ for achieving model selection consistency. Our results support the advantage of using a non-convex penalty by directly studying the Hamming errors and phase diagrams.\n\nThe performance of SCAD can be further improved by adding an entry-wise thresholding on $\\hat{\\beta}$. We believe that the phase diagrams of *thresholded SCAD* are better than those of SCAD itself, although the extremely tedious analysis impedes us from specific results for now. Also, we are cautious about what to conclude from comparing SCAD and thresholded Lasso. In our settings, Lasso has no model selection consistency mainly because the signals are too weak (i.e., $r$ is not sufficiently large). In such settings, thresholded Lasso outperforms SCAD in terms of the Hamming error. However, there are cases where Lasso has no model selection consistency no matter how large the signal strength is [@zhao2006model]. For those cases, it is possible that SCAD is better than thresholded Lasso (see [@wainwright2009sharp] for a related study).\n\n**Discussion of Elastic net**. The phase curve of Elastic net is worse than that of Lasso. As we will explain in Section [3.1](#subsec:en){reference-type=\"ref\" reference=\"subsec:en\"}, Elastic net is a 'bridge' between Lasso and marginal regression in our case. Since the phase curve of marginal regression is always worse than that of Lasso for the blockwise diagonal design, we do not benefit from using Elastic net in the current setting. We must note that Elastic net is motivated by genetic applications where several correlated variables are competing as predictors, and where it is implicitly assumed that groups of correlated variables tend to be all relevant or all irrelevant [@zou2005regularization]. This is not captured by our model [\\[model-beta\\]](#model-beta){reference-type=\"eqref\" reference=\"model-beta\"}. Therefore, our results do not go against the benefits of Elastic net known in the literature, but rather our results support that the advantages of Elastic net come from 'group' appearance of signal variables.\n\n**Discussion of thresholded Lasso**. Thresholded Lasso is a representative of improving Lasso by post-processing. There have been inspiring works that demonstrate the advantages of such a post-processing [@van2011adaptive; @wang2017bridge; @weinstein2020power]. Our results support these insights from a different angle. It is surprising (and very encouraging) that the improvement by post-Lasso thresholding is so significant. We note that Lasso is a 1-stage method, which solves a single optimization to obtain $\\hat{\\beta}$. By comparison, thresholded Lasso is a 2-stage method. Lasso has only one algorithm parameter $\\lambda$, while thresholded Lasso has two algorithm parameters $\\lambda$ and $t$ (the threshold). In Lasso, we control false positives and false negatives with the same algorithm parameter $\\lambda$, and it is sometimes hard to find a value of $\\lambda$ that simultaneously controls the two types of errors well. In thresholded Lasso, the two types of errors can be controlled separately by two algorithm parameters. This explains why thresholded Lasso enjoys such a big improvement upon Lasso. It inspires us to modify other 1-stage methods, such as SCAD, by adding a post-processing step of thresholding. For example, we conjecture that thresholded SCAD also has a strictly better phase diagram than that of Lasso, even for a positive $\\rho$. On the other hand, thresholding is no free lunch. It leaves one more tuning parameter to be decided in practice. Our theoretical results are based on ideal tuning parameters. How to properly choose these tuning parameters in a data-driven way is an interesting question. [@weinstein2020power] proposes a promising approach, where they use cross-validation to select $\\lambda$ and FDR control by knockoff to select $t$. We leave it for future work to study the phase diagrams with data-driven tuning parameters.\n\n**Discussion of the two iterative algorithms**. We consider two iterative algorithms, forward selection ('Forward') and forward backward selection ('FB'). The FB algorithm we analyze is a simplified version in @huang2016partial, which has only one backward step (after all the forward steps have finished) by thresholding the refitted least-squares solution. Our results show that both methods outperform Lasso, and between these two methods, FB is strictly better than Forward. In the literature, there are very interesting theoretical works showing the advantages of iterative algorithms for variable selection [@donoho2012sparse; @zhang2011adaptive]. Our results support their insights from a different angle. We discover that, for a wide range of $\\rho$, FB has the best phase diagram among all the six methods. This is a very encouraging result. Of course, it is as important to note that the performance of an iterative algorithm tends to be more sensitive to the form of the design, due to its sequential nature."},{"heading":"Main Results","text":"# Main Results {#sec:main}\n\nConsider model [\\[linearM\\]](#linearM){reference-type=\"eqref\" reference=\"linearM\"}, [\\[model-beta\\]](#model-beta){reference-type=\"eqref\" reference=\"model-beta\"}, and [\\[model-X\\]](#model-X){reference-type=\"eqref\" reference=\"model-X\"}, where we set $\\sigma^2=1$ without loss of generality. Let $\\mathbb{E}[H(\\hat{\\beta},\\beta)]$ be the expected Hamming error, where the expectation is with respect to the randomness of $\\beta$ and $z$. Let $L_p$ denote a generic multi-$\\log(p)$ term such that $L_pp^\\epsilon\\to \\infty$ and $L_pp^{-\\epsilon}\\to 0$ for any $\\epsilon>0$.\n\n::: theorem\n**Theorem 1**. *Under Models [\\[linearM\\]](#linearM){reference-type=\"eqref\" reference=\"linearM\"}, [\\[model-beta\\]](#model-beta){reference-type=\"eqref\" reference=\"model-beta\"}, and [\\[model-X\\]](#model-X){reference-type=\"eqref\" reference=\"model-X\"}, for each of the methods considered in this paper (Lasso, Elastic net, SCAD, thresholded Lasso, forward selection, forward backward selection, as well as marginal regression in Section [3.1](#subsec:en){reference-type=\"ref\" reference=\"subsec:en\"}), there exists a function $h(\\vartheta,r,\\rho)$ such that $\\mathbb{E}[H(\\hat{\\beta},\\beta)]=L_pp^{1-h(\\vartheta,r,\\rho)}$. The explicit expressions of $h(\\vartheta,r,\\rho)$, which may depend on the tuning parameters of a method, are given in Theorems B.1, C.1, D.1-D.3, F.1, G.1, H.1-H.4 of the supplement.*\n:::\n\nIn the main article, to save space, we only present the expressions of the upper phase curve $U(\\vartheta)=U(\\vartheta;\\rho)$ and the lower phase curve $L(\\vartheta)=L(\\vartheta;\\rho)$ for each method, which are defined as follows: $$\\label{phase-curves}\nU(\\vartheta;\\rho)= \\inf\\{r>0:  h(\\vartheta,r,\\rho)>1\\}, \\qquad L(\\vartheta;\\rho)=\\inf\\{r>0: h(\\vartheta,r,\\rho)>\\vartheta\\}.$$ These two curves describe the phase diagram: The upper phase curve $U(\\vartheta)$ separates the ER region and AFR region, and the lower phase curve $L(\\vartheta)$ separates the AFR region and NR region.\n\n## Elastic net and Lasso {#subsec:en}\n\nThe Elastic net [@zou2005regularization] is a method that estimates $\\beta$ by $$\\label{Elastic-net}\n\\hat\\beta^{\\mathrm{EN}} =\\mathrm{argmin}_{\\beta} \\bigl\\{ \\norm*{y-X\\beta}^2/2 + \\lambda \\norm*{\\beta}_1 + (\\mu/2) \\norm*{\\beta}^2 \\bigr\\}.$$ Compared with Lasso, it adds an additional $L^2$-penalty to the objective function. Below, we fix $\\mu>0$ and re-parametrize $\\lambda=\\sqrt{2q\\log(p)}$, for some constant $q>0$. The choice of $q$ affects the exponent, $1-h(\\vartheta,r,\\rho)$, in the expression of $\\mathbb{E}[H(\\hat{\\beta},\\beta)]$. We choose the ideal $q$ that minimizes $1-h(\\vartheta,r,\\rho)$. The next theorem is proved in the supplement.\n\n::: {#thm:elastic-net .theorem}\n**Theorem 2** (Elastic Net). *Under Models [\\[linearM\\]](#linearM){reference-type=\"eqref\" reference=\"linearM\"}, [\\[model-beta\\]](#model-beta){reference-type=\"eqref\" reference=\"model-beta\"}, and [\\[model-X\\]](#model-X){reference-type=\"eqref\" reference=\"model-X\"}, let $\\hat{\\beta}^{\\mathrm{EN}}$ be the Elastic net estimator in [\\[Elastic-net\\]](#Elastic-net){reference-type=\"eqref\" reference=\"Elastic-net\"}. Fix $\\mu$ and write $\\eta=\\rho/(1+\\mu)$. Let $\\lambda=\\sqrt{2q\\log(p)}$ with an ideal choice of $q$ that minimizes the exponent of $\\mathbb{E}[H(\\hat{\\beta},\\beta)]$. The phase curves are given by $L(\\vartheta)=\\vartheta$, and $$U(\\vartheta)= \\begin{cases} \n    \\max \\left\\{ h_1(\\vartheta),h_2(\\vartheta) \\right\\}, & \\text{ when }\\rho \\geq 0, \\\\\n    \\max \\left\\{ h_1(\\vartheta),h_2(\\vartheta),h_3(\\vartheta),h_4(\\vartheta) \\right\\}, &\\text{ when }\\rho < 0,\n    \\end{cases}$$ where $h_1(\\vartheta) =(1 + \\sqrt{1 -\\vartheta})^2$, $h_2(\\vartheta) = \\bigl( \\frac{1 -\\abs*{\\eta}}{1 -\\abs*{\\rho}} + \\frac{\\sqrt{1 +\\eta^2 - 2\\rho\\eta}}{1 -\\abs*{\\rho}} \\bigr)^2 (1 -\\vartheta)$, $h_3(\\vartheta) =\\frac{1}{(1 -\\abs*{\\rho})^2} \\bigl( 1 + \\frac{\\sqrt{1 +\\eta^2 - 2\\rho\\eta}}{1 +\\abs*{\\eta}}\\sqrt{1 - 2\\vartheta}\\bigr)^2$, and $h_4(\\vartheta) = \\frac{1 +\\eta^2 - 2\\rho\\eta}{(1 - 2\\abs{\\rho} + \\rho \\eta)_ + ^2} \\bigl( \\sqrt{1 -\\vartheta} + \\frac{1 -\\abs{\\eta}}{1 +\\abs{\\eta}}\\sqrt{1 - 2\\vartheta} \\bigr)^2$.*\n:::\n\n![The phase diagrams of Elastic net and its comparison with Lasso (notation: $\\eta=\\rho/(1+\\mu)$).](Figure/elastic_net_against_lasso.pdf){#fig:elastic-net height=\".31\\\\textwidth\"}\n\nLasso is a special case with $\\mu=0$. By setting $\\mu=0$ (equivalently, $\\eta=\\rho$) in Theorem [2](#thm:elastic-net){reference-type=\"ref\" reference=\"thm:elastic-net\"}, we obtain the phase curves for Lasso. They agree with the results in [@ji2012ups] (but [@ji2012ups] does not cover Elastic net).\n\nTo see the effect of the $L^2$-penalty, we consider an extreme case where $\\mu\\to\\infty$. Some elementary algebra shows that $(1+\\mu)\\hat{\\beta}^{\\mathrm{EN}}$ converges to the soft-thresholding of $X'y$ at the threshold $\\lambda$. In other words, as $\\mu\\to\\infty$, Elastic net converges to marginal screening (i.e., select variables by thresholding the marginal regression coefficients). At the same time, when $\\mu=0$, $(1+\\mu)\\hat{\\beta}^{\\mathrm{EN}}$ equals the Lasso estimate. Hence, Elastic net serves as a bridge between Lasso and marginal regression. In the setting here, the phase diagram of marginal regression is inferior to that of Lasso, and so the phase diagram of Elastic net is also inferior to that of Lasso. See the proposition below and Figure [2](#fig:elastic-net){reference-type=\"ref\" reference=\"fig:elastic-net\"}:\n\n::: {#prop:bridge .prop}\n**Proposition 1**. *In Theorem [2](#thm:elastic-net){reference-type=\"ref\" reference=\"thm:elastic-net\"}, for each fixed $\\vartheta\\in (0,1)$, as $\\mu\\to 0$, $U(\\vartheta)$ is monotone decreasing and converges to $U_{\\mathrm{Lasso}}(\\vartheta)$, which is the upper phase curve of Lasso; as $\\mu\\to\\infty$, $U(\\vartheta)$ is monotone increasing and converges to $U_{\\mathrm{MR}}(\\vartheta)$, which is the upper phase curve of marginal regression. Furthermore, when $\\rho \\leq  - \\frac{1}{2}$, $U_{\\mathrm{MR}}(\\theta)=\\infty$ for all $0 < \\vartheta\\leq \\frac{1}{2}$ (i.e. exact recovery is impossible to achieve no matter how large $r$ is).*\n:::\n\n## Smoothly clipped absolute deviation penalty (SCAD)\n\nSCAD [@fan2001variable] is a non-convex penalization method. For any $a>2$, it defines a penalty function $q_{\\lambda}(\\theta)$ on $(0,\\infty)$ by $q_{\\lambda}(\\theta)=\\int_0^\\theta q_{\\lambda}'(t)dt$, where $q_{\\lambda}'(\\theta)=\\lambda\\bigl\\{I(\\theta \\leq \\lambda)+\\frac{(a \\lambda-\\theta)_{+}}{(a-1) \\lambda} I(\\theta>\\lambda)\\bigr\\}$. The resulting penalty function $q_{\\lambda}(\\cdot)$ coincides with the $L^1$-penalty in $(0,\\lambda]$ and becomes a constant in $[a\\lambda,\\infty)$. Let $Q_{\\lambda}(\\beta)=\\sum_{j=1}^pq_{\\lambda}(|\\beta_j|)$. Then, SCAD estimates $\\beta$ by $$\\label{SCAD}\n\\hat\\beta^{\\mathrm{SCAD}} =\\mathrm{argmin}_{\\beta}\\bigl\\{\\norm*{y-X\\beta}^2/2 + Q_{\\lambda}(\\beta) \\bigr\\}.$$ The following theorem is proved in the supplemental material (see Figure [3](#fig:scad){reference-type=\"ref\" reference=\"fig:scad\"}, left panel):\n\n::: {#thm:SCAD .theorem}\n**Theorem 3** (SCAD). *Under Models [\\[linearM\\]](#linearM){reference-type=\"eqref\" reference=\"linearM\"}, [\\[model-beta\\]](#model-beta){reference-type=\"eqref\" reference=\"model-beta\"}, and [\\[model-X\\]](#model-X){reference-type=\"eqref\" reference=\"model-X\"}, let $\\hat{\\beta}^{\\mathrm{SCAD}}$ be the SCAD estimator in [\\[SCAD\\]](#SCAD){reference-type=\"eqref\" reference=\"SCAD\"}. Fix $a\\in (2, \\frac{2}{1 -\\abs*{\\rho}} )$. Let $\\lambda=\\sqrt{2q\\log(p)}$ with an ideal choice of $q$ that minimizes the rates of convergence of the expected Hamming error. The phase curves are given by $L(\\vartheta)=\\vartheta$, and $$U(\\vartheta)= \\begin{cases} \n    \\max \\left\\{ h_1(\\vartheta),h_2(\\vartheta), h_3(\\vartheta) \\right\\}, & \\text{ when }\\rho \\geq 0, \\\\\n    \\max \\left\\{ h_1(\\vartheta),h_2(\\vartheta),h_4(\\vartheta),h_5(\\vartheta) \\right\\}, &\\text{ when }\\rho < 0,\n    \\end{cases}$$ where $h_1(\\vartheta)=(1 + \\sqrt{1 -\\vartheta})^2$, and $h_2(\\vartheta)=\\bigl( 1 + \\sqrt{\\frac{1 +\\abs{\\rho}}{1 -\\abs{\\rho}}} \\bigr)^2 (1 -\\vartheta)$, $h_4(\\vartheta) = \\bigl( \\sqrt{\\frac{1-2 \\vartheta}{1 -|\\rho|^{2}}}+\\frac{1}{1-|\\rho|} \\bigr)^2$, $h_3(\\vartheta)=\\bigl( \\frac{3 +\\rho}{2(1 -\\rho^2)}\\sqrt{\\frac{1 +\\rho}{1 -\\rho}}\\sqrt{1 -\\vartheta} + \\frac{1}{2}\\sqrt{\\frac{2(1 - 2\\vartheta)}{1 +\\rho} - \\frac{(1 -\\vartheta)}{(1 -\\rho)^2}} \\bigr)^2$, and $$\\begin{aligned}\n    h_5(\\vartheta) = \n      \\begin{cases} \n        \\bigl( \\frac{5 + 3|\\rho|}{1 -|\\rho|} \\bigr)(1 -\\vartheta), &  \\text{ if }\\sqrt{\\frac{1-2 \\vartheta}{1-\\vartheta}} \\geq \\frac{3-4|\\rho|-3\\rho^2}{(1-|\\rho|)} \\sqrt{\\frac{1+|\\rho|}{5+3|\\rho|}},\\\\\n        \\frac{1}{(1 -\\abs*{\\rho})^2} \\bigl(\\sqrt{\\frac{1 +\\abs{\\rho}}{1 -\\abs{\\rho}}} \\sqrt{1 -\\vartheta} + \\sqrt{\\frac{1 -\\abs{\\rho}}{1 +\\abs{\\rho}}}\\sqrt{1 - 2\\vartheta} \\bigr)^2, &  \\text{ if } \\sqrt{\\frac{1-2 \\vartheta}{1-\\vartheta}} \\leq \\frac{(1+|\\rho|)(1-2 |\\rho|)}{1-|\\rho|}, \\\\\n        h_6(\\vartheta) & \\text{ other wise},\n      \\end{cases} \n  \n\\end{aligned}$$ $$h_6(\\vartheta) = \\left\\{ \\sqrt{\\frac{1-2 \\vartheta}{1-\\rho^{2}}}+\\frac{\\frac{1-2 |\\rho|}{1-|\\rho|} \\sqrt{\\frac{1-2 \\vartheta}{1-\\rho^{2}}}+\\sqrt{\\bigl[\\bigl(\\frac{1-2 |\\rho|}{1-|\\rho|}\\bigr)^{2}+ \\frac{1-|\\rho|}{1+|\\rho|}\\bigr](1-\\vartheta)-\\frac{1-2 \\vartheta}{(1+|\\rho|)^{2}}}}{(1-|\\rho|)\\bigl[\\bigl(\\frac{1-2 |\\rho|}{1-|\\rho|}\\bigr)^{2}+\\frac{1-|\\rho|}{1+|\\rho|}\\bigr]} \\right\\}^2.$$*\n:::\n\n![Left: Phase curves of SCAD. Middle and Right: Comparison of SCAD and Lasso. ](Figure/scad_all.pdf){#fig:scad width=\".92\\\\textwidth\"}\n\nNote that the phase curves of Lasso are given in Theorem [2](#thm:elastic-net){reference-type=\"ref\" reference=\"thm:elastic-net\"} by setting $\\eta=\\rho$. We compare SCAD with Lasso. When $\\rho< 0$, the upper phase curve in Theorem [3](#thm:SCAD){reference-type=\"ref\" reference=\"thm:SCAD\"} is strictly lower than that of Lasso (see Figure [3](#fig:scad){reference-type=\"ref\" reference=\"fig:scad\"}, middle and right panels). When $\\rho\\geq 0$, the upper phase curve in Theorem [3](#thm:SCAD){reference-type=\"ref\" reference=\"thm:SCAD\"} is sometimes higher than that of Lasso. Note that we restrict $a<\\frac{2}{1-|\\rho|}$ in Theorem [3](#thm:SCAD){reference-type=\"ref\" reference=\"thm:SCAD\"}. In fact, a larger $a$ may be preferred for $\\rho\\geq 0$. The next proposition is about using an optimal $a$.\n\n::: {#prop:SCAD-larger-a .prop}\n**Proposition 2**. *In the SCAD estimator, we choose $a=a^*$ and $\\lambda=\\sqrt{2q^* \\log(p)}$ such that $(a^*, q^*)=(a^*(\\vartheta,r,\\rho), q^*(\\vartheta,r,\\rho))$ minimize the rates of convergence of the expected Hamming error among all choices of $(a,q)$. Let $U^*(\\vartheta)$ be the resulting upper phase curve for SCAD. Then, $U(\\vartheta)=U_{\\mathrm{Lasso}}(\\vartheta)$ when $\\rho\\geq 0$, and $U(\\vartheta)< U_{\\mathrm{Lasso}}(\\vartheta)$ when $\\rho<0$.*\n:::\n\nThe phase curves of SCAD are insensitive to the choice of $a$. When $a<0$, the optimal $a^*$ can be any value in $(2, \\frac{2}{1-|\\rho|})$. When $\\rho\\geq 0$, there exists a constant $c=c(\\vartheta,\\rho)$ such that the optimal $a^*$ is any value in $(c,\\infty)$. As $a\\to\\infty$, the SCAD penalty reduces to the $L^1$-penalty. This explains why the phase curve of SCAD is the same as that of Lasso when $\\rho\\geq 0$.\n\n## Thresholded Lasso\n\nLet $\\hat{\\beta}^{\\mathrm{Lasso}}$ be the Lasso estimator in [\\[Lasso\\]](#Lasso){reference-type=\"eqref\" reference=\"Lasso\"}. The thresholded Lasso estimator $\\hat{\\beta}^{\\mathrm{TL}}$ is obtained by applying coordinate-wise hard-thresholding to the Lasso estimator: $$\\label{thresh-Lasso}\n\\hat{\\beta}^{\\mathrm{TL}}_j = \\hat{\\beta}^{\\mathrm{Lasso}}\\cdot 1\\{|\\hat{\\beta}^{\\mathrm{Lasso}}|>t\\}, \\qquad 1\\leq j\\leq p.$$\n\n::: {#thm:thresh-lasso .theorem}\n**Theorem 4** (Thresholded Lasso). *Under Models [\\[linearM\\]](#linearM){reference-type=\"eqref\" reference=\"linearM\"}, [\\[model-beta\\]](#model-beta){reference-type=\"eqref\" reference=\"model-beta\"}, and [\\[model-X\\]](#model-X){reference-type=\"eqref\" reference=\"model-X\"}, let $\\hat{\\beta}^{\\mathrm{TL}}$ be the thresholded Lasso estimator in [\\[thresh-Lasso\\]](#thresh-Lasso){reference-type=\"eqref\" reference=\"thresh-Lasso\"}. Let $\\lambda=\\sqrt{2q\\log(p)}$ and $t=\\sqrt{2w\\log(p)}$ with the ideal $(q, w)$ that minimize the exponent of the expected Hamming error. The phase curves are given by $L(\\vartheta)=\\vartheta$, and $$U(\\vartheta)= \\begin{cases} \n    \\max \\left\\{ h_1(\\vartheta),h_2(\\vartheta) \\right\\}, & \\text{ when }\\rho \\geq 0, \\\\\n    \\max \\left\\{ h_1(\\vartheta),h_2(\\vartheta),h_3(\\vartheta) \\right\\}, &\\text{ when }\\rho < 0,\n    \\end{cases}$$ where $h_1(\\vartheta)= (1 + \\sqrt{1 -\\vartheta})^2$, $h_2(\\vartheta)=\\frac{4(1-\\vartheta)}{1-\\rho^{2}}$, and $h_3(\\vartheta)= \\bigl(1+\\frac{1+|\\rho|}{2} \\sqrt{\\frac{1-\\vartheta}{1-\\rho^{2}}}+\\frac{1-|\\rho|}{2} \\sqrt{\\frac{1-2 \\vartheta}{1-\\rho^{2}}}\\bigr)^2$.*\n:::\n\n![Comparison of the phase diagrams of thresholded Lasso and Lasso.](Figure/thres_lasso_sq.pdf){#fig:thres.lasso width=\".91\\\\textwidth\"}\n\nSee Figure [4](#fig:thres.lasso){reference-type=\"ref\" reference=\"fig:thres.lasso\"} for a comparison with Lasso (a special case of $t=0$). With the flexibility of using an optimal $t$, the phase diagram of thresholded Lasso is always better than that of Lasso.\n\nTheorem [4](#thm:thresh-lasso){reference-type=\"ref\" reference=\"thm:thresh-lasso\"} also gives other interesting facts about thresholded Lasso. First, the shape of phase curves is much less affected by the sign of $\\rho$. This differs from Lasso, Elastic net, and SCAD, for which the shape of phase curves is significantly different for positive and negative $\\rho$. Second, the optimal $\\lambda$ in thresholded Lasso is considerably smaller than the optimal $\\lambda$ in Lasso (it can be seen from the proofs of Theorem [4](#thm:thresh-lasso){reference-type=\"ref\" reference=\"thm:thresh-lasso\"} and Theorem [2](#thm:elastic-net){reference-type=\"ref\" reference=\"thm:elastic-net\"}). This is because the $\\lambda$ in thresholded Lasso only serves to control false negatives, but the $\\lambda$ in Lasso is used to simultaneously control false positives and false negatives, hence, cannot be too small. We observe the same phenomenon in simulations; see Section [4](#sec:simu){reference-type=\"ref\" reference=\"sec:simu\"}.\n\n## Forward selection and forward backward selection\n\nForward selection is a classical textbook method for variable selection. Write $X=[x_1,x_2,\\ldots,x_p]$, where $x_i\\in \\mathbb{R}^n$ for $1\\leq i\\leq p$. For any subset $A\\subset\\{1,2,\\ldots,p\\}$, let $P^{\\bot}_{A}$ be the projection onto the orthogonal complement of the linear space spanned by $\\{ x_i:i\\in  A \\}$. Given a threshold $t>0$, the forward selection algorithm initializes with $S_0=\\emptyset$ and $\\hat{r}_0=y$. At the $k$th iteration, compute $$i^* = \\mathrm{argmax}_{i\\notin S_{k - 1}}\\abs{x_i'\\hat r_{k - 1}}, \\qquad \\delta = \n         \\abs{x_{i^* }'\\hat r_{k - 1} }/\\norm{ P^{\\bot}_{S_{k - 1}} x_{i^*}}.$$ If $\\delta>t$, compute $S_k=S_{k-1}\\cup \\{i^*\\}$ and $\\hat{r}_{k}=P^{\\bot}_{S_k}y$; otherwise, output $\\hat{\\beta}^{\\mathrm{forward}}$ as the least-squares estimator restricted to $S_{k-1}$. The stopping rule of $\\delta\\leq t$ is equivalent to measuring the decrease of the residual sum of squares. The following theorem is proved in the supplemental material:\n\n::: {#thm:forward .theorem}\n**Theorem 5** (Forward Selection). *Under Models [\\[linearM\\]](#linearM){reference-type=\"eqref\" reference=\"linearM\"}, [\\[model-beta\\]](#model-beta){reference-type=\"eqref\" reference=\"model-beta\"}, and [\\[model-X\\]](#model-X){reference-type=\"eqref\" reference=\"model-X\"}, let $\\hat{\\beta}^{\\mathrm{forward}}$ be the estimator from forward selection. Let $t=\\sqrt{2q\\log(p)}$ with the ideal $q$ that minimizes the exponent of the expected Hamming error. The phase curves are given by $L(\\vartheta)=\\vartheta$, and $$U(\\vartheta)= \\begin{cases} \n    \\max \\left\\{ h_1(\\vartheta),h_2(\\vartheta), h_3(\\vartheta) \\right\\}, & \\text{ when }\\rho \\geq 0, \\\\\n    \\max \\left\\{ h_1(\\vartheta),h_2(\\vartheta),h_3(\\vartheta), h_4(\\vartheta) \\right\\}, &\\text{ when }\\rho < 0,\n    \\end{cases}$$ with $h_1(\\vartheta)$=$(1+\\sqrt{1-\\vartheta})^2$, $h_2(\\vartheta)=\\frac{2(1-\\vartheta)}{1-|\\rho|}$, $h_3(\\vartheta)=\\frac{(1 +\\sqrt{1 - 2\\vartheta})^2}{1 -\\rho^2}$, $h_4(\\vartheta)=\\bigl( \\sqrt{\\frac{1-2 \\vartheta}{2(1-|\\rho|)}}+\\frac{1}{1-|\\rho|} \\bigr)^2$.*\n:::\n\nForward backward selection (FB) modifies forward selection by allowing to drop variables. We use the FB algorithm in [@huang2016partial], where the backward step is conducted after all the forward steps are finished. For a threshold $v>0$, it applies entry-wise thresholding on $\\hat{\\beta}^{\\mathrm{forward}}$: $$\\label{FoBa}\n\\hat{\\beta}^{\\mathrm{FB}}_j = \\hat{\\beta}^{\\mathrm{forward}}_j\\cdot 1\\{ |\\hat{\\beta}^{\\mathrm{forward}}_j|>v \\}, \\qquad 1\\leq j\\leq p.$$\n\n::: {#thm:forward-backward .theorem}\n**Theorem 6** (Forward Backward Selection). *Under Models [\\[linearM\\]](#linearM){reference-type=\"eqref\" reference=\"linearM\"}, [\\[model-beta\\]](#model-beta){reference-type=\"eqref\" reference=\"model-beta\"}, and [\\[model-X\\]](#model-X){reference-type=\"eqref\" reference=\"model-X\"}, let $\\hat{\\beta}^{\\mathrm{FB}}$ be the estimator from forward selection. Let $t=\\sqrt{2q\\log(p)}$ and $v=\\sqrt{2u\\log(p)}$ with the ideal $(q, u)$ that minimize the exponent of the expected Hamming error. When $\\rho\\geq 0$, the phase curves are given by $L(\\vartheta)=\\vartheta$, and $$U(\\vartheta)= \n    \\max \\left\\{ h_1(\\vartheta),h_2(\\vartheta), h^*_3(\\vartheta) \\right\\},$$ where $h_1(\\vartheta)$ and $h_2(\\vartheta)$ are the same as in Theorem [5](#thm:forward){reference-type=\"ref\" reference=\"thm:forward\"} and $h_3^*(\\vartheta)=\\frac{(\\sqrt{1 -\\vartheta} +\\sqrt{1 - 2\\vartheta})^2}{1 -\\rho^2}$. When $\\rho<0$, $$U(\\vartheta) \\leq \\max \\left\\{ g_1(\\vartheta),g_2(\\vartheta),g_3(\\vartheta),g_4(\\vartheta) \\right\\},$$ where $g_1(\\vartheta) = (v_{\\min}(\\vartheta) + \\sqrt{1 -\\vartheta})^2$, $g_2(\\vartheta)=\\frac{2(1-\\vartheta)}{1-|\\rho|}$, $g_3(\\vartheta)=\\bigl( \\sqrt{\\frac{1-2\\vartheta}{1-\\rho^2}} + v_{\\min}(\\vartheta)\\bigr)^2$, $g_4(\\vartheta)=\\bigl( \\sqrt{\\frac{1-2 \\vartheta}{2(1-|\\rho|)}}+\\frac{t_{\\min}(\\vartheta)}{1-|\\rho|} \\bigr)^2$, $v_{\\min}(\\vartheta) =\\max \\bigl\\{ 1,\\sqrt{\\frac{1 -\\vartheta}{1 -\\rho^2}} \\bigr\\}$, and $t_{\\min}(\\vartheta) =\\max \\bigl\\{ \\frac{\\sqrt{2}}{2},\\frac{v_{\\min}(\\vartheta)}{1 +|\\rho|/\\sqrt{1 -\\rho^2}} \\bigr\\}$.*\n:::\n\nTheorem [6](#thm:forward-backward){reference-type=\"ref\" reference=\"thm:forward-backward\"} gives $U(\\vartheta)$ for $\\rho\\geq 0$ and an upper bound of it for $\\rho<0$. Combining it with Theorems [2](#thm:elastic-net){reference-type=\"ref\" reference=\"thm:elastic-net\"} and [5](#thm:forward){reference-type=\"ref\" reference=\"thm:forward\"}, we conclude that the upper phase curve of FB is always better than those of Lasso and forward selection (for $\\rho<0$, the upper bound here is already better than $U(\\vartheta)$ for the other two methods).\n\n![The phase diagrams of forward selection and forward backward selection.](Figure/foba_forward_all.pdf){#fig:foba height=\".3\\\\textwidth\"}\n\nWe remark that we did obtain the exact phase curve for $\\rho<0$ in the proof of Theorem [6](#thm:forward-backward){reference-type=\"ref\" reference=\"thm:forward-backward\"}. It is just too complicated and space-consuming to present it in the main text. However, given specific values of $(\\vartheta, \\rho)$, we can always plot the exact phase curve using the (complicated) formulas in the supplement. In Figures [1](#fig:overview){reference-type=\"ref\" reference=\"fig:overview\"} and [5](#fig:foba){reference-type=\"ref\" reference=\"fig:foba\"}, the phase curves of FB are indeed the exact ones.\n\n## Connection to the random design model {#subsec:random}\n\nConsider the random design as mentioned in Section [1](#sec:Intro){reference-type=\"ref\" reference=\"sec:Intro\"}. The *minimax Hamming error* is $H^*(\\vartheta,r,\\rho)=\\inf_{\\hat{\\beta}}\\mathbb{E}[H(\\hat{\\beta},\\beta)]$, where the infimum is taken over all methods $\\hat{\\beta}$ and the expectation is with respect to the randomness of $(X,\\beta,z)$. We can define $H^*(\\vartheta,r,\\rho)$ in the same way for our current model [\\[model-X\\]](#model-X){reference-type=\"eqref\" reference=\"model-X\"}. The minimax Hamming error is related to the statistical limit of the model setting, but not any specific method. The next theorem shows that, when $n\\gg s_p=p^{1-\\vartheta}$ (we allow both $p\\leq n$ and $p>n$), the convergence rate of the minimax Hamming error is the same under two models.\n\n::: {#thm:equivalence .theorem}\n**Theorem 7**. *Under Models [\\[linearM\\]](#linearM){reference-type=\"eqref\" reference=\"linearM\"} and [\\[model-beta\\]](#model-beta){reference-type=\"eqref\" reference=\"model-beta\"}, suppose $X$ is independent of $(\\beta,z)$ and its rows are iid generated from ${\\cal N}(0, n^{-1}\\Sigma)$, with $\\Sigma$ having the same form as $G$ in [\\[model-X\\]](#model-X){reference-type=\"eqref\" reference=\"model-X\"}. Suppose $n=p^\\omega$, with $\\omega>1-\\vartheta$ (note: this allows $\\omega<1$, which corresponds to $n\\ll p$). There exists a number $h^{**}(\\vartheta,r,\\rho)$ such that the minimax Hamming error satisfies that $H^*(\\vartheta,r,\\rho)=L_pp^{1-h^{**}(\\vartheta,r,\\rho)}$. Furthermore, if we instead have $X'X=\\Sigma$ (i.e., model [\\[model-X\\]](#model-X){reference-type=\"eqref\" reference=\"model-X\"}), then it also holds that $H^*(\\vartheta,r,\\rho)=L_pp^{1-h^{**}(\\vartheta,r,\\rho)}$.*\n:::"},{"heading":"Simulations","text":"# Simulations {#sec:simu}\n\nIn Experiments 1-3, $(n,p)=(1000,300)$. In Experiment 4, $(n,p) = (500,1000)$.\n\n**Experiment 1** (block-wise diagonal designs). We generate $(X,\\beta)$ as in [\\[model-beta\\]](#model-beta){reference-type=\"eqref\" reference=\"model-beta\"}-[\\[model-X\\]](#model-X){reference-type=\"eqref\" reference=\"model-X\"}. For each method, we select the ideal tuning parameters that minimize the average Hamming error over 50 repetitions. The averaged Hamming errors and its standard deviations under the ideal tuning parameters over 500 repetitions are reported below. The results are consistent with the theoretical phase diagrams (see Figure [1](#fig:overview){reference-type=\"ref\" reference=\"fig:overview\"}). E.g., thresholded Lasso and forward backward selection are the two methods that perform the best; Lasso is more unsatisfactory when $\\rho<0$; SCAD improves Lasso when $\\rho<0$.\n\n::: center\n:::\n\n**Experiment 2** (general designs). In the Toeplitz design, we let $(X' X)_{i,j}=0.7^{|i-j|}$ and set $(\\vartheta,r)=(0.1,2.5)$. In the factor model design, we let $X'X=BB'-\\text{diag}(BB')+ I_p$, where entries of $B\\in \\mathbb{R}^{p\\times 2}$ are *iid* from $\\mathrm{Unif}(0,0.6)$, and set $(\\vartheta,r)=(0.1,1.5)$. Same as in Experiment 1, we use the ideal tuning parameters. The averaged Hamming errors and its standard deviations are reported below. The Toeplitz design is a setting where each variable is only highly correlated with a few other variables. The factor model design is a setting where a variable is (weakly) correlated with all the other variables. The results are quite similar to those in Experiment 1. This confirms that the insight gained in the study of the block-wise diagonal design continues to apply to more general designs.\n\n**Experiment 3** (tuning parameters). Fix $(\\vartheta,r)=(0.1, 1.5)$ and $\\rho\\in\\{\\pm 0.5\\}$ in the block-wise diagonal design. We study the effect of tuning parameters in Lasso, thresholded Lasso (ThreshLasso), forward selection (ForwardSelect), and forward backward selection (FB). In (a)-(b), we show the heatmap of averaged Hamming error (over 50 repetitions) of ThreshLasso for a grid of $(t, \\lambda)$; when $t=0$, it reduces to Lasso. In (c)-(d), we show the Hamming error of FB for a grid of $(v,t)$; when $v=0$, it reduces to ForwardSelect. Cyan points are theoretically optimal tuning parameters (formulas are in proofs of theorems). Red points are empirically optimal tuning parameters that minimize the averaged Hamming error. The theoretical tuning parameter values are quite close to the empirically optimal ones. Moreover, the optimal $\\lambda$ in ThreshLasso is smaller than the optimal $\\lambda$ in Lasso.\n\n<figure>\n\n<figcaption>Experiment 3 (effects of tuning parameters). In all plots, cyan points are computed from the formulas in our theory, and red points are the empirically best tuning parameters (they minimize the average Hamming error over 500 repetitions). In (a)-(b), the cyan/red points with <span class=\"math inline\"><em>t</em> = 0</span> correspond to Lasso, and the other two are for thresholded Lasso. In (c)-(d), the cyan/red points with <span class=\"math inline\"><em>t</em> = 0</span> correspond to forward selection, and the other two are for forward backward selection.</figcaption>\n</figure>\n\n**Experiment 4** ($p>n$ and random designs). Fix $(n,p,\\vartheta,r) = (500,1000,0.5,1.5)$. We simulate data from the random design setting in Theorem [7](#thm:equivalence){reference-type=\"ref\" reference=\"thm:equivalence\"}. We study the average Hamming error over 500 repetitions (tuning parameters are set in the same way as in Experiment 1). See Table [\\[table:experiment.4\\]](#table:experiment.4){reference-type=\"ref\" reference=\"table:experiment.4\"}. We have some similar observations as before: e.g., ThreshLasso and FoBackward are still the best two,\n\n::: center\n:::"},{"heading":"Conclusion","text":"# Conclusion {#sec:discuss}\n\nMost papers on variable selection focus on one method and study its properties in a relatively broad setting. In contrast, we focus on a relatively narrow setting but study a variety of different methods. Our motivation is to facilitate a direct comparison of main-stream approaches for variable selection. Although the model we use seems idealized, by varying the parameters, it already accommodates many different combinations of sparsity level, signal strength, and design correlation level. Under this model, we derive tractable forms of the Hamming error and phase diagram for each method, and we make notable discoveries out of these theoretical results."},{"heading":"Sketch of the proof ideas","text":"# Sketch of the proof ideas {#suppsec:sketch}\n\nWe use a similar proof idea for every main theorem, which we explain as follows. To obtain the phase diagram, the key is deriving the rate of convergence of the expected Hamming error $\\mathbb{E}[H(\\hat{\\beta},\\beta)]$. Let $$\\label{def:FPFN}\n\\ensuremath{\\mathrm{FP}}_p=\\sum_{j=1}^p\\mathbb{P}(\\beta_j=0,\\hat{\\beta}_j\\neq 0), \\qquad\\mbox{and}\\qquad \\ensuremath{\\mathrm{FN}}_p=\\sum_{j=1}^p \\mathbb{P}(\\beta_j= \\tau_p, \\hat{\\beta}_j=0).$$ By definition, $$\\mathbb{E}[H(\\hat{\\beta},\\beta)]  = \\ensuremath{\\mathrm{FP}}_p+\\ensuremath{\\mathrm{FN}}_p. %= \\sum_{j=1}^p \\bigl\\{  \\mathbb{P}(\\beta_j=0, \\hat{\\beta}_j\\neq 0)+\\mathbb{P}(\\beta_j=\\tau_p, \\hat{\\beta}_j=0) \\bigr\\}.$$ Suppose $j$ is in the diagonal block $\\{j,j+1\\}$ of the Gram matrix $G$. For most methods (except for forward selection and forward backward selection, which we discuss separately), it is easy to see that $\\hat{\\beta}_j$ does not depend on any other $\\beta_i$ with $i\\notin\\{j,j+1\\}$. It follows that $$\\begin{aligned}\n\\mathbb{P}(\\beta_j=0, \\hat{\\beta}_j\\neq 0) &= \\mathbb{P}(\\beta_j=0, \\beta_{j+1}=0, \\hat{\\beta}_j\\neq 0)+\n\\mathbb{P}(\\beta_j=0, \\beta_{j+1}=\\tau_p, \\hat{\\beta}_j\\neq 0)\\cr\n&= (1-\\epsilon_p)^2\\, \\mathbb{P}\\bigl( \\hat{\\beta}_j\\neq 0 \\big| \\beta_j=0, \\beta_{j+1}=0\\bigr)\\cr\n&\\qquad  +(1-\\epsilon_p)\\epsilon_p \\cdot\\mathbb{P}\\bigl(\\hat{\\beta}_j\\neq 0 \\big| \\beta_j=0, \\beta_{j+1}=\\tau_p \\bigr)\\cr\n&=  L_p\\, \\mathbb{P}_{00}(\\hat{\\beta}_j\\neq 0 ) + L_pp^{-\\vartheta}\\,\\mathbb{P}_{01}(\\hat{\\beta}_j\\neq 0), \n\\end{aligned}$$ where $\\mathbb{P}_{00}$ is the conditional probability conditioning on $(\\beta_j, \\beta_{j+1})=(0,0)$ and $\\mathbb{P}_{01}$ is the conditional probability conditioning on $(\\beta_j, \\beta_{j+1})=(0, \\tau_p)$. Similarly, we can derive $$\\mathbb{P}(\\beta_j=\\tau_p, \\hat{\\beta}_j=0)  = L_pp^{-\\vartheta}\\, \\mathbb{P}_{10}(\\hat{\\beta}_j= 0 ) + L_pp^{-2\\vartheta}\\,\\mathbb{P}_{11}(\\hat{\\beta}_j= 0),$$ where $\\mathbb{P}_{10}$ is the conditional probability conditioning on $(\\beta_j, \\beta_{j+1})=(\\tau_p,0)$ and $\\mathbb{P}_{11}$ is the conditional probability conditioning on $(\\beta_j, \\beta_{j+1})=(\\tau_p, \\tau_p)$. When $p$ is even, by symmetry in this design, the above expressions do not change with $j$. When $p$ is odd, this is true except for $j=p$; however, this single $j$ has a negligible effect on the expected Hamming error. We thus have $$\\begin{aligned}\n \\label{proof-sketch}\n\\mathbb{E}[H(\\hat{\\beta},\\beta)] &=& L_pp\\cdot \\mathbb{P}_{00}(\\hat{\\beta}_j\\neq 0) + L_pp^{1-\\vartheta}\\cdot \\mathbb{P}_{01}(\\hat{\\beta}_j\\neq 0) \\cr\n&& + L_pp^{1-\\vartheta}\\cdot \\mathbb{P}_{10}(\\hat{\\beta}_j= 0) + L_pp^{1-2\\vartheta}\\cdot \\mathbb{P}_{11}(\\hat{\\beta}_j= 0). \n\\end{aligned}$$\n\nIt remains to study the probabilities in [\\[proof-sketch\\]](#proof-sketch){reference-type=\"eqref\" reference=\"proof-sketch\"}. Let $\\tilde{y}_j=x_j'y/\\sqrt{2\\log(p)}$ and $\\tilde{y}_{j+1}=x_{j+1}'y/\\sqrt{2\\log(p)}$. For most methods considered in this paper, $\\hat{\\beta}_j$ is determined by $(\\tilde{y}_j, \\tilde{y}_{j+1})$ only. Define $$\\label{def:RejRegion}\n{\\cal R} = \\{ (h_1, h_2)\\in\\mathbb{R}^2:  \\; (\\tilde{y}_j, \\tilde{y}_{j+1})=(h_1, h_2) \\mbox{ implies that }\\hat{\\beta}_j\\neq 0  \\}.$$ Write $\\tilde{y}=(\\tilde{y}_1, \\tilde{y}_2)'$. Then, we can re-write [\\[proof-sketch\\]](#proof-sketch){reference-type=\"eqref\" reference=\"proof-sketch\"} as $$\\begin{aligned}\n \\label{proof-sketch2}\n\\mathbb{E}[H(\\hat{\\beta},\\beta)] &=& L_pp\\cdot \\mathbb{P}_{00}(\\tilde{y}\\in {\\cal R}) + L_pp^{1-\\vartheta}\\cdot \\mathbb{P}_{01}(\\tilde{y}\\in {\\cal R}) \\cr\n&& + L_pp^{1-\\vartheta}\\cdot \\mathbb{P}_{10}(\\tilde{y}\\notin {\\cal R} ) + L_pp^{1-2\\vartheta}\\cdot \\mathbb{P}_{11}(\\tilde{y}\\notin {\\cal R}). \n\\end{aligned}$$ In the settings of interest in this paper, conditioning on each realization of $(\\beta_j, \\beta_{j+1})$, it can be shown that $$\\tilde{y}\\;\\; \\sim\\;\\; {\\cal N}_2\\Bigl(\\mu,\\; \\frac{1}{2\\log(p)} \\Sigma\\Bigr), \\qquad\\mbox{for some fixed }\\mu\\in\\mathbb{R}^2\\mbox{ and }\\Sigma\\in\\mathbb{R}^{2\\times 2}.$$ For any $x\\in\\mathbb{R}^2$ and $S\\subset\\mathbb{R}^2$, define $$\\label{d_Sigma}\nd^2_{\\Sigma}(x, S)=\\inf_{v\\in S}\\bigl\\{ (x-v)'\\Sigma^{-1}(x-v)\\bigr\\}.$$ We apply Lemma 6.1 in [@ke2020power] to get that, as $p\\to\\infty$, ($L_p$ denotes a multi-$\\log(p)$ term; see Section [3](#sec:main){reference-type=\"ref\" reference=\"sec:main\"} or the notations below) $$\\label{proof-sketch3}\n\\mathbb{P}(\\tilde{y}\\in {\\cal R})=L_pp^{-d^2_{\\Sigma}(\\mu, \\, {\\cal R})}, \\qquad \\mathbb{P}(\\tilde{y}\\notin {\\cal R})=L_pp^{-d^2_{\\Sigma}(\\mu, \\, {\\cal R}^c)}.$$ Combining [\\[proof-sketch3\\]](#proof-sketch3){reference-type=\"eqref\" reference=\"proof-sketch3\"} with [\\[proof-sketch2\\]](#proof-sketch2){reference-type=\"eqref\" reference=\"proof-sketch2\"}, we can get the rate of convergence of the expected Hamming error, if we calculate the following quantities:\n\n-   The set ${\\cal R}$ (we call it \"rejection region\"). The rejection region depends on the definition of the method and the choice of tuning parameters.\n\n-   The distances $d_{\\Sigma}(\\mu, {\\cal R})$ and $d_{\\Sigma}(\\mu, {\\cal R}^c)$. Note that $(\\mu, \\Sigma)$ depend on the realization of $(\\beta_j, \\beta_{j+1})$. Therefore, we need to calculate $(\\mu,\\Sigma)$ for each of the four possible realizations.\n\nIn the remaining of this supplemental material, we prove Theorems [2](#thm:elastic-net){reference-type=\"ref\" reference=\"thm:elastic-net\"}-[6](#thm:forward-backward){reference-type=\"ref\" reference=\"thm:forward-backward\"} and Proposition [1](#prop:bridge){reference-type=\"ref\" reference=\"prop:bridge\"}-[2](#prop:SCAD-larger-a){reference-type=\"ref\" reference=\"prop:SCAD-larger-a\"}. For each theorem, the proof can be divided into three parts:\n\n-   Derive the rejection region ${\\cal R}$.\n\n-   Apply [\\[proof-sketch2\\]](#proof-sketch2){reference-type=\"eqref\" reference=\"proof-sketch2\"}-[\\[proof-sketch3\\]](#proof-sketch3){reference-type=\"eqref\" reference=\"proof-sketch3\"} to calculate the rate of convergence of $\\mathbb{E}[H(\\hat{\\beta},\\beta)]$.\n\n-   Calculate the phase diagram based on the result from (b).\n\nThroughout the proof, we use $L_p$ to denote a generic multi-$\\log(p)$ term, which satisfies that $L_pp^\\epsilon\\to \\infty$ and $L_pp^{-\\epsilon}\\to 0$ for any $\\epsilon>0$. We also frequently use the notation:\n\n::: {#def:EllipsDistance .definition}\n**Definition 1**. *For $\\rho\\in (-1,1)$ and $u,v\\in\\mathbb{R}^2$, define $d_\\rho(u,v)>0$ by $d_\\rho^2(u,v) = (u_1 -v_1)^2 + (u_2 - v_2)^2 - 2\\rho(u_1 - v_1)(u_2 - v_2)$.*\n:::\n\nIn our proofs, we also frequently calculate the infimum of $d_\\rho^2(u, v)$, for $v$ a line in $\\mathbb{R}^2$. The following lemma is very useful. Its proof is elementary and thus omitted.\n\n::: {#supplem:distance .lem}\n**Lemma 1**. *Fix $\\rho\\in (-1,1)$. Given real numbers $A,B,C$ such that $AB\\neq 0$, consider a constrained optimization over $x=(x_1,x_2)$ that minimizes $d_{\\rho}^2(x, (0,0))=x_1^2+x_2^2-2\\rho x_1x_2$ subject to the constraint $Ax_1+Bx_2+C=0$. The solution is $x_1^* =  \\frac{ - C(A +\\rho B)}{A^2 + B^2 + 2\\rho AB}$ and $x_2^* =\\frac{ - C(B +\\rho A)}{A^2 + B^2 + 2\\rho AB}$, and the objective function evaluated at $x^*=(x_1^*, x_2^*)$ is $$d_\\rho^2(x^*, (0,0)') = \\frac{C^2(1 -\\rho^2)}{A^2 + B^2 + 2\\rho AB}.$$*\n:::"},{"heading":"Proof of Theorem [2](#thm:elastic-net){reference-type=\"ref\" reference=\"thm:elastic-net\"} (Elastic net)","text":"# Proof of Theorem [2](#thm:elastic-net){reference-type=\"ref\" reference=\"thm:elastic-net\"} (Elastic net) {#suppsec:en}\n\nAs described in Section [6](#suppsec:sketch){reference-type=\"ref\" reference=\"suppsec:sketch\"}, our proof has three parts: (a) deriving the rejection region, (b) obtaining the rate of convergence of $\\mathbb{E}[H(\\hat{\\beta},\\beta)]$, and (c) calculating the phase diagram.\n\n#### Part 1: Deriving the rejection region.\n\nRecall that the rejection region ${\\cal R}$ is as defined in [\\[def:RejRegion\\]](#def:RejRegion){reference-type=\"eqref\" reference=\"def:RejRegion\"}. Write $h_1=x_j'y/\\sqrt{2\\log(p)}$, $h_2=x_{j+1}'y/\\sqrt{2\\log(p)}$, and $\\lambda=\\sqrt{2q\\log(p)}$. Consider a bivariate Elastic net problem, where $(\\hat{b}_1, \\hat{b}_2)$ minimizes $$\\label{enproof-optimization}\nL(b)\\equiv \\frac{1}{2}b'\\begin{bmatrix}1&\\rho\\\\\\rho & 1\\end{bmatrix}b + b'h+\\sqrt{q}\\|b\\|_1 + \\frac{1}{2} \\mu\\|b\\|^2.$$ It is seen that $(\\hat{\\beta}_j, \\hat{\\beta}_{j+1})=\\sqrt{2\\log(p)}(\\hat{b}_1, \\hat{b}_2)$. Hence, ${\\cal R}$ consists of all values of $h$ such that $\\hat{b}_1\\neq 0$.\n\nFix $\\rho\\geq 0$. The next lemma gives the explicit solution to [\\[enproof-optimization\\]](#enproof-optimization){reference-type=\"eqref\" reference=\"enproof-optimization\"} in the case of $h_1>|h_2|$. It is proved in Section [7.1](#subsec:proof-EN-solution){reference-type=\"ref\" reference=\"subsec:proof-EN-solution\"}.\n\n::: {#suppthm:sol.path.en .lem}\n**Lemma 2** (Solution path of Elastic net). *Consider the optimization in [\\[enproof-optimization\\]](#enproof-optimization){reference-type=\"eqref\" reference=\"enproof-optimization\"}. Suppose $h_1 > \\abs{h_2} \\geq 0$. Write $\\eta=\\rho/(1+\\mu)$.*\n\n-   *When $\\sqrt{q} \\geq h_1$, we have $\\hat{b}_1 = \\hat{b}_2 = 0$.*\n\n-   *If $h_2\\geq \\eta h_1$, when $\\frac{{h_2 -\\eta h_1}}{1 -\\eta}\\leq \\sqrt{q} <h_1$, we have $\\hat{b}_1 =\\frac{h_1 -\\sqrt{q}}{1 +\\mu}$, and $\\hat{b}_2 = 0$;*\n\n    *When $\\sqrt{q}<\\frac{{h_2 -\\eta h_1}}{1 -\\eta}$, we have $$\\hat{b}_1 =\\frac{\\frac{h_1 -\\sqrt{q} }{1 +\\mu} -\\eta\\frac{h_2 -\\sqrt{q} }{1 +\\mu}}{1 -\\eta^2},\\qquad \n        \\hat{b}_2 =\\frac{\\frac{h_2 -\\sqrt{q} }{1 +\\mu} -\\eta\\frac{h_1 -\\sqrt{q} }{1 +\\mu}}{1 - \\eta^2};$$*\n\n-   *if $h_2<\\eta h_1$, when $\\frac{{-h_2 +\\eta h_1}}{1 +\\eta}\\leq \\sqrt{q} <h_1$, we have $\\hat{b}_1 =\\frac{h_1 -\\sqrt{q}}{1 +\\mu}$, and $\\hat{b}_2 = 0$;*\n\n    *When $\\sqrt{q}< \\frac{{-h_2 +\\eta h_1}}{1 +\\eta}$, we have $$\\hat{b}_1 =\\frac{\\frac{h_1 -\\sqrt{q} }{1 +\\mu} -\\eta \\frac{h_2 +\\sqrt{q} }{1 +\\mu}}{1 - \\eta^2},\\quad \n      \\hat{b}_2 =\\frac{\\frac{h_2 +\\sqrt{q} }{1 +\\mu} -\\eta\\frac{h_1 -\\sqrt{q} }{1 +\\mu}}{1 - \\eta^2}.$$*\n:::\n\nWe now use Lemma [2](#suppthm:sol.path.en){reference-type=\"ref\" reference=\"suppthm:sol.path.en\"} to derive ${\\cal R}$. Partition $\\mathbb{R}^2$ into 4 non-overlapping regions: $$\\begin{aligned}\n& M_1=\\{(h_1, h_2):\\, h_1 > |h_2|\\}, \\qquad M_2=\\{(h_1, h_2):\\, h_1< - |h_2|\\}, \\cr\n& M_3=\\{(h_1, h_2):\\, h_2 >  |h_1|\\}, \\qquad M_4=\\{(h_1, h_2):\\, h_2<-|h_1|\\}. \n\\end{aligned}$$ First, we derive ${\\cal R}\\cap M_1$. By Lemma [2](#suppthm:sol.path.en){reference-type=\"ref\" reference=\"suppthm:sol.path.en\"}, as $\\sqrt{q}$ decreases from $\\infty$ to $0$, $\\hat{b}_1$ is initially zero and then becomes positive when $\\sqrt{q}$ hits $h_1$ (second bullet point of this lemma). Then, if we further decrease $\\sqrt{q}$, the value of $\\hat{b}_1$ is always increasing (third bullet point of this lemma) and remains positive. Therefore, $\\sqrt{q}<h_1$ is the sufficient and necessary condition for $\\hat{b}_1$ to be nonzero. It follows that $${\\cal R}\\cap M_1 = M_1\\cap \\{ (h_1, h_2): h_1>\\sqrt{q}\\}.$$ Second, we consider ${\\cal R}\\cap M_2$. Note that $(h_1,h_2)\\in {\\cal R}\\cap M_2$ if and only if $(-h_1, -h_2)\\in {\\cal R}\\cap M_1$. Additionally, if we simultaneously flip the sign of $(h_1, h_2, b_1, b_2)$, the objective in [\\[enproof-optimization\\]](#enproof-optimization){reference-type=\"eqref\" reference=\"enproof-optimization\"} is unchanged. It follows that $${\\cal R}\\cap M_2 = \\{(h_1, h_2): (-h_1, -h_2)\\in {\\cal R}\\cap M_1\\}.$$ Next, we derive ${\\cal R}\\cap M_3$. Note that $(h_1, h_2)\\in M_3$ if and only if $(h_2, h_1)\\in M_1$. Moreover, if we swap $(h_1, b_1)$ with $(h_2, b_2)$, the objective in [\\[enproof-optimization\\]](#enproof-optimization){reference-type=\"eqref\" reference=\"enproof-optimization\"} is unchanged. Hence, we can obtain ${\\cal R}\\cap M_3$ as follows: We first find the collection of $(h_1, h_2)\\in\\mathcal{R}\\cap M_1$ such that $\\hat{b}_2\\neq 0$, and then switch the two coordinates $h_1$ and $h_2$ to get ${\\cal R}\\cap M_3$. To this end, by Lemma [2](#suppthm:sol.path.en){reference-type=\"ref\" reference=\"suppthm:sol.path.en\"}, for $(h_1,h_2)\\in {\\cal R}\\cap M_1$, $\\hat{b}_2\\neq 0$ if either $h_2-\\eta h_1>\\sqrt{q}(1-\\eta)$ or $h_2-\\eta h_1<-\\sqrt{q}(1+\\eta)$. It follows that, for $(h_1,h_2)\\in {\\cal R}\\cap M_3$, $\\hat{b}_1\\neq 0$ if either $h_1-\\eta h_2>\\sqrt{q}(1-\\eta)$ or $h_1-\\eta h_2<-\\sqrt{q}(1+\\eta)$. It implies that $${\\cal R}\\cap M_3 = M_3\\cap \\bigl( \\{ (h_1, h_2): h_1-\\eta h_2>\\sqrt{q}(1-\\eta)\\}\\cup \\{(h_1,h_2): h_1-\\eta h_2<-\\sqrt{q}(1+\\eta)\\}\\bigr).$$ Last, we obtain ${\\cal R}\\cap M_4$ by $${\\cal R}\\cap M_4 = \\{(h_1, h_2): (-h_1, -h_2)\\in {\\cal R}\\cap M_3\\}.$$ Combining the above results gives $$\\begin{aligned}\n \\label{proof-en-rjRegion}\n{\\cal R} &= \\{(h_1,h_2): h_1-\\eta h_2>\\sqrt{q}(1-\\eta),\\, h_1>\\sqrt{q}\\}\\cr\n&\\;\\; \\cup \\{(h_1, h_2): h_1-\\eta h_2>\\sqrt{q}(1+\\eta)\\} \\cup \\{(h_1, h_2): h_1-\\eta h_2<-\\sqrt{q}(1+\\eta)\\}\\cr\n&\\;\\; \\cup \\{(h_1,h_2): h_1-\\eta h_2<-\\sqrt{q}(1-\\eta),\\, h_1<-\\sqrt{q}\\}. \n\\end{aligned}$$ See Figure [6](#suppfig:rejection.region.en){reference-type=\"ref\" reference=\"suppfig:rejection.region.en\"} for a visualization of the rejection region (recall that $\\eta=\\rho/(1+\\mu)$).\n\n![The rejection region of Elastic net for $\\rho\\geq 0$.](Figure-supp/en_rejection_region.pdf){#suppfig:rejection.region.en width=\"70%\"}\n\nFigure [6](#suppfig:rejection.region.en){reference-type=\"ref\" reference=\"suppfig:rejection.region.en\"} only depicts the rejection region for $\\rho\\geq 0$. For $\\rho<0$, we can similarly draw the rejection region, but it is not necessary for the proof of this theorem. In Part 2, we will see that, by carefully utilizing the symmetry in our problem, we can derive the rate of convergence of the Hamming error for $\\rho<0$ without deriving the rejection region directly.\n\n#### Part 2. Analyzing the Hamming error.\n\nWe aim to use [\\[proof-sketch2\\]](#proof-sketch2){reference-type=\"eqref\" reference=\"proof-sketch2\"}-[\\[proof-sketch3\\]](#proof-sketch3){reference-type=\"eqref\" reference=\"proof-sketch3\"} to derive the rate of convergence of $\\mathbb{E}[H(\\hat{\\beta},\\beta)]$. Recall that $\\tilde{y}_1=x_j'y/\\sqrt{2\\log(p)}$ and $\\tilde{y}_2=x_{j+1}'y/\\sqrt{2\\log(p)}$. It is easy to see that $\\tilde{y}\\sim {\\cal N}_2(\\mu,\\; \\frac{1}{2\\log(p)} \\Sigma)$, where the covariance matrix $\\Sigma$ is the $2\\times 2$ matrix with 1 in the diagonal and $\\rho$ in the off-diagonal, and the vector $\\mu$ is equal to $$\\mu_{00}= \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix},\\quad \n\\mu_{01}=\\begin{bmatrix} \\rho \\sqrt{r} \\\\ \\sqrt{r} \\end{bmatrix},\\quad \n\\mu_{10}=\\begin{bmatrix} \\sqrt{r} \\\\ \\rho \\sqrt{r} \\end{bmatrix},\\quad \n\\mu_{11}=\\begin{bmatrix} (1 +\\rho) \\sqrt{r} \\\\ (1 +\\rho) \\sqrt{r} \\end{bmatrix},$$ when $(\\beta_j, \\beta_{j+1})$ takes the value of $(0, 0)$, $(0, \\tau_p)$, $(\\tau_p, 0)$, and $(\\tau_p, \\tau_p)$, respectively. By [\\[proof-sketch2\\]](#proof-sketch2){reference-type=\"eqref\" reference=\"proof-sketch2\"}-[\\[proof-sketch3\\]](#proof-sketch3){reference-type=\"eqref\" reference=\"proof-sketch3\"}, $\\mathbb{E}[H(\\hat{\\beta},\\beta)]=\\ensuremath{\\mathrm{FP}}_p+\\ensuremath{\\mathrm{FN}}_p$, where $$\\begin{aligned}\n \\label{proof-en-Hamming}\n\\ensuremath{\\mathrm{FP}}_p &=& L_pp^{1-d^2_{\\Sigma}(\\mu_{00}, {\\cal R})} + L_pp^{1-\\vartheta -d^2_{\\Sigma}(\\mu_{01}, {\\cal R}) }, \\cr\n\\ensuremath{\\mathrm{FN}}_p &=& L_pp^{1-\\vartheta - d^2_{\\Sigma}(\\mu_{10}, {\\cal R}^c)}+ L_pp^{1-2\\vartheta-d^2_{\\Sigma}(\\mu_{11}, {\\cal R}^c)}. \n\\end{aligned}$$ It suffices to calculate $d_{\\Sigma}(\\mu_{00}, {\\cal R})$, $d_{\\Sigma}(\\mu_{01}, {\\cal R})$, $d_{\\Sigma}(\\mu_{10}, {\\cal R}^c)$, and $d_{\\Sigma}(\\mu_{11}, {\\cal R}^c)$.\n\nFirst, consider the case of $\\rho\\geq 0$. The expression of ${\\cal R}$ is given explicitly in [\\[proof-en-rjRegion\\]](#proof-en-rjRegion){reference-type=\"eqref\" reference=\"proof-en-rjRegion\"}. By the definition in [\\[d_Sigma\\]](#d_Sigma){reference-type=\"eqref\" reference=\"d_Sigma\"} and Definition [1](#def:EllipsDistance){reference-type=\"ref\" reference=\"def:EllipsDistance\"}, for any $S\\subset\\mathbb{R}^2$ and $\\mu\\notin S$, $$\\label{proof-en-distance}\nd^2_{\\Sigma}(\\mu, S) = \\frac{1}{1-\\rho^2}\\, \\inf_{\\xi\\in S} d^2_\\rho(\\mu, \\xi).$$ If $S$ can be expressed as the interaction and union of finitely many half-planes, then the point $\\xi^*$ that attains the infimum must be on the boundary line of one of these half-planes. We thus only need to calculate:\n\n-   $\\inf_{\\xi\\in {\\cal L}}d^2_{\\rho}(\\mu, \\xi)$ for the boundary line ${\\cal L}$ of each half-plane in the definition of $S$ (with verification that the tangent point on ${\\cal L}$ is achievable on the boundary);\n\n-   $d_\\rho^2(\\mu,\\zeta)$ for each point $\\zeta$ that is a vertex of $S$ (i.e., the intersection of two boundary lines).\n\nFor (i), we apply the formula given in Lemma [1](#supplem:distance){reference-type=\"ref\" reference=\"supplem:distance\"}. For (ii), we apply Definition [1](#def:EllipsDistance){reference-type=\"ref\" reference=\"def:EllipsDistance\"} directly. These calculations give a finite collection of values. In (i), the $\\xi^*$ that attains the infimum may not belong to $S$; if that happens, we delete it from the collection. Finally, $\\inf_{\\xi\\in S} d^2_\\rho(\\mu, \\xi)$ is the minimum of the values in this collection.\n\nSince ${\\cal R}$ and ${\\cal R}^c$ can be expressed via the interaction and union of finitely many half-planes, we follow the above routine to calculate the desired quantities. Take $d_{\\Sigma}(\\mu_{01}, {\\cal R})$ for example. Recall that $\\mu_{01}= (\\rho \\sqrt{r},\\sqrt{r})$. By [\\[proof-en-distance\\]](#proof-en-distance){reference-type=\"eqref\" reference=\"proof-en-distance\"}, it suffices to calculate $\\inf_{\\xi\\in{\\cal R}}d^2_\\rho(\\mu_{01},\\xi)$. The region ${\\cal R}$ has 6 boundary lines, but since $\\rho\\geq 0$, the infimum can only be attained in either of three cases:\n\n-   on the line ${\\cal L}_1: h_1-\\eta h_2=\\sqrt{q}(1-\\eta)$;\n\n-   on the line ${\\cal L}_2: h_1=\\sqrt{q}$;\n\n-   on the the vertex $v^*=(\\sqrt{q}, \\sqrt{q})'$, which is an intersection of ${\\cal L}_1\\cap{\\cal L}_2$.\n\nLet $x=(x_1, x_2)'=(h_1-\\rho\\sqrt{r}, h_2-\\sqrt{r})'$. We can re-write ${\\cal L}_1$ as a line ${\\cal L}_1'$ for $x$, which expression is $x_1-\\eta x_2=(1-\\eta)\\sqrt{q}-(\\rho-\\eta)\\sqrt{r}$. Similarly, we can re-write ${\\cal L}_2$ as a line ${\\cal L}_2'$: $x_1=\\sqrt{q}-\\rho\\sqrt{r}$. We apply Lemma [1](#supplem:distance){reference-type=\"ref\" reference=\"supplem:distance\"} to get $$\\begin{aligned}\n\\inf_{\\xi\\in {\\cal L}_1} d^2_\\rho(\\mu_{01}, \\xi ) &= \\inf_{x\\in {\\cal L}_1'}d^2_\\rho(x, (0,0)') = \\frac{\\bigl[(1-\\eta)\\sqrt{q}-(\\rho-\\eta)\\sqrt{r}\\bigr]^2(1-\\rho^2)}{1+\\eta^2-2\\rho \\eta},\\cr\n\\inf_{\\xi\\in {\\cal L}_2} d^2_\\rho(\\mu_{01}, \\xi ) &= \\inf_{x\\in {\\cal L}_2'}d^2_\\rho(x, (0,0)') = (\\sqrt{q}-\\rho\\sqrt{r})^2(1-\\rho^2), \\cr\nd_{\\rho}^2(\\mu_{01}, v^*) &= d_\\rho^2\\bigl((\\sqrt{q},\\sqrt{q})', (\\rho\\sqrt{r}, \\sqrt{r})'\\bigr)\\cr\n& \\equiv (\\sqrt{q}-\\rho\\sqrt{r})^2+(\\sqrt{q}-\\sqrt{r})^2-2\\rho (\\sqrt{q}-\\rho\\sqrt{r})(\\sqrt{q}-\\sqrt{r})^2. \n\\end{aligned}$$ The value of $\\inf_{\\xi\\in{\\cal R}}d^2_\\rho(\\mu_{01},\\xi)$ is the minimum of the above three values. In fact, the distance $d_\\rho$ is related to the size of an ellipsoid that centers at $(\\rho\\sqrt{r},\\sqrt{r})'$ and hits the boundary of ${\\cal R}$. As $\\sqrt{r}$ increases from zero, the center of this ellipsoid moves upwards on the line of $h_1=\\rho h_2$. Consequently, the minimum of the above three values is initially (i) $\\inf_{\\xi\\in {\\cal L}_2} d^2_\\rho(\\mu_{01}, \\xi )$ when $\\sqrt{r}$ is appropriately small, then (ii) $d_{\\rho}^2(\\mu_{01}, v^*)$ when $\\sqrt{r}$ is moderately large, and finally (iii) $\\inf_{\\xi\\in {\\cal L}_1} d^2_\\rho(\\mu_{01}, \\xi )$ when $\\sqrt{r}$ is sufficiently large; see Figure [6](#suppfig:rejection.region.en){reference-type=\"ref\" reference=\"suppfig:rejection.region.en\"}. We now figure out the range of $\\sqrt{r}$ for each of the three cases. Recall that $v^*=(\\sqrt{q}, \\sqrt{q})'$. Let $\\xi^*=(\\xi_1^*, \\xi_2^*)'$ be the vector that attains $\\inf_{\\xi\\in {\\cal L}_2} d^2_\\rho(\\mu_{01}, \\xi )$. We have an explicit expression of $\\xi^*$ from Lemma [1](#supplem:distance){reference-type=\"ref\" reference=\"supplem:distance\"}. By equating it with $v^*$, we can solve the critical value of $\\sqrt{r}$ at which case (i) transits to case (ii): $$\\sqrt{q}=v_2^*=\\xi_2^*=  \\sqrt{r}  + \\rho (\\sqrt{q}-\\rho\\sqrt{r}) \\qquad\\Longrightarrow\\qquad \\sqrt{r}=\\frac{\\sqrt{q}}{1 +\\rho}.$$ Similarly, let $\\tilde{\\xi}^*=(\\tilde{\\xi}^*_1, \\tilde{\\xi}_2^*)'$ be the vector that attains $\\inf_{\\xi\\in {\\cal L}_1} d^2_\\rho(\\mu_{01}, \\xi )$. By equating $\\tilde{\\xi}^*$ with $v^*$, we can solve the critical value of $\\sqrt{r}$ at which case (ii) transits to case (iii): $$\\sqrt{q}=v_2^*=\\tilde{\\xi}_2^*=  \\sqrt{r}  +  \\frac{\\bigl[(1-\\eta)\\sqrt{q}-(\\rho-\\eta)\\sqrt{r}\\bigr](\\rho-\\eta)}{1+\\eta^2-2\\rho \\eta}\\qquad\\Longrightarrow\\qquad \\sqrt{r}=\\frac{1 +\\eta}{1 +\\rho}\\sqrt{q}.$$ We combine the above results to get $$\\label{proof-en-Hamming-exponent}\n\\inf_{\\xi\\in {\\cal R}}d^2_\\rho(\\mu_{01}, \\xi) =  \n\\begin{cases} \n      (1 -\\rho^2)(\\sqrt{q}- \\rho\\sqrt{r})^2_ +, & \\text{  if  } \\sqrt{r} \\leq \\frac{1}{1+\\rho}\\sqrt{q}, \\\\\n        d^2_{\\rho}\\bigl((\\sqrt{q},\\sqrt{q})',(\\rho \\sqrt{r},\\sqrt{r})'\\bigr), & \\text{  if  } \\frac{1}{1+\\rho}\\sqrt{q} < \\sqrt{r} \\leq \\frac{1+\\eta}{1+\\rho}\\sqrt{q},\\\\\n       \\frac{(1-\\rho^2)}{1+\\eta^2-2\\rho \\eta}[(1-\\eta)\\sqrt{q}-(\\rho-\\eta)\\sqrt{r}]_+^2, & \\text{if  } \\sqrt{r} > \\frac{1+\\eta}{1+\\rho}q. \n\\end{cases}$$ Recall that $\\eta=\\rho/(1+\\mu)$ is a shorthand notation. We plug [\\[proof-en-Hamming-exponent\\]](#proof-en-Hamming-exponent){reference-type=\"eqref\" reference=\"proof-en-Hamming-exponent\"} into [\\[proof-en-distance\\]](#proof-en-distance){reference-type=\"eqref\" reference=\"proof-en-distance\"}, and then we insert it into [\\[proof-en-Hamming\\]](#proof-en-Hamming){reference-type=\"eqref\" reference=\"proof-en-Hamming\"}. This gives the second term in $\\ensuremath{\\mathrm{FP}}_p$. We can follow the same routine to derive every term in $\\ensuremath{\\mathrm{FP}}_p$ and $\\ensuremath{\\mathrm{FN}}_p$. We omit the details but summarize the results in Theorem [8](#suppthm:hamm.en){reference-type=\"ref\" reference=\"suppthm:hamm.en\"} below.\n\nNext, consider the case of $\\rho<0$. We re-parametrize the linear model by replacing $(x_{j+1}, \\beta_{j+1})$ with $(-x_{j+1}, -\\beta_{j+1})$. After this re-parametrization, the $(j, j+1)$ block of the Gram matrix is a $2\\times 2$ matrix $\\Sigma$ whose off-diagonal entries are $-\\rho=|\\rho|$. The rejection region is defined by the solution path of [\\[enproof-optimization\\]](#enproof-optimization){reference-type=\"eqref\" reference=\"enproof-optimization\"} associated with $|\\rho|>0$. This allows us to use the expression in [\\[proof-en-rjRegion\\]](#proof-en-rjRegion){reference-type=\"eqref\" reference=\"proof-en-rjRegion\"} directly with a simple replacement of $\\rho$ by $|\\rho|$. There is no need to re-calculate the rejection region for a negative $\\rho$.\n\nLet $\\tilde{y}_1=x_j'y/\\sqrt{2\\log(p)}$ and $\\tilde{y}_2=x_{j+1}'y/\\sqrt{2\\log(p)}$. We still have $\\tilde{y}\\sim {\\cal N}_2(\\mu,\\; \\frac{1}{2\\log(p)} \\Sigma)$. However, the four realizations of $(\\beta_j, \\beta_{j+1})$ become $(0, 0)$, $(0, -\\tau_p)$, $(\\tau_p, 0)$, and $(\\tau_p, -\\tau_p)$. Therefore, the mean vectors $\\mu$ have changed to $$\\mu_{00}= \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix},\\quad \n\\mu_{01}=\\begin{bmatrix}- |\\rho| \\sqrt{r} \\\\ -\\sqrt{r} \\end{bmatrix},\\quad \n\\mu_{10}=\\begin{bmatrix} -\\sqrt{r} \\\\ -|\\rho| \\sqrt{r} \\end{bmatrix},\\quad \n\\mu_{11}=\\begin{bmatrix} (1 -|\\rho|) \\sqrt{r} \\\\ -(1 -|\\rho|) \\sqrt{r} \\end{bmatrix}.$$ Similar to [\\[proof-en-Hamming\\]](#proof-en-Hamming){reference-type=\"eqref\" reference=\"proof-en-Hamming\"}, it suffices to calculate $d_{\\Sigma}(\\mu_{00}, {\\cal R})$, $d_{\\Sigma}(\\mu_{01}, {\\cal R})$, $d_{\\Sigma}(\\mu_{10}, {\\cal R}^c)$, and $d_{\\Sigma}(\\mu_{11}, {\\cal R}^c)$. Here ${\\cal R}$ is the same as in Figure [6](#suppfig:rejection.region.en){reference-type=\"ref\" reference=\"suppfig:rejection.region.en\"}, but the locations of the $\\mu$ vectors have changed. Since ${\\cal R}$ is centrosymmetric, $d_{\\Sigma}(\\mu_{00}, {\\cal R})$, $d_{\\Sigma}(\\mu_{01}, {\\cal R})$, and $d_{\\Sigma}(\\mu_{10}, {\\cal R}^c)$ are actually the same as before. We only need to re-calculate $d^2_\\Sigma(\\mu_{11}, {\\cal R}^c)$. The calculation routine is the same as that for [\\[proof-en-Hamming-exponent\\]](#proof-en-Hamming-exponent){reference-type=\"eqref\" reference=\"proof-en-Hamming-exponent\"}. We omit the details but present the results directly in the theorem below.\n\nTo summarize, in this part, we have proved the following theorem:\n\n::: {#suppthm:hamm.en .theorem}\n**Theorem 8**. *Suppose the conditions of Theorem [2](#thm:elastic-net){reference-type=\"ref\" reference=\"thm:elastic-net\"} hold. Let $\\lambda=\\sqrt{2q\\log(p)}$ in Elastic net. Write $\\eta=\\rho/(1+\\mu)$. As $p\\to\\infty$, $$\\ensuremath{\\mathrm{FP}}_p=L_p p^{1- \\min\\bigl\\{ q, \\;\\; \\vartheta+ f_1(\\sqrt{r}, \\sqrt{q})\\bigr\\}}, \\qquad \\ensuremath{\\mathrm{FN}}_p = L_p p^{1-\\min\\bigl\\{\\vartheta+ f_2(\\sqrt{r}, \\sqrt{q}),\\;\\; 2\\vartheta+ f_3(\\sqrt{r}, \\sqrt{q})\\bigr\\}},$$ where (below, $d^2_{|\\rho|}(u,v)$ is as in Definition [1](#def:EllipsDistance){reference-type=\"ref\" reference=\"def:EllipsDistance\"}) $$\\begin{aligned}\n f_1(\\sqrt{r},\\sqrt{q}) & = \\begin{cases} \n      (\\sqrt{q}- |\\rho|\\sqrt{r})^2_ +, & \\text{  if  } \\sqrt{r} \\leq \\frac{1}{1+|\\rho|}\\sqrt{q}, \\\\\n      \\frac{1}{1-\\rho^2}\\cdot  d^2_{|\\rho|}\\bigl((\\sqrt{q},\\sqrt{q})',(|\\rho| \\sqrt{r},\\sqrt{r})'\\bigr), & \\text{  if  } \\frac{1}{1+|\\rho|} \\sqrt{q }< \\sqrt{r} \\leq \\frac{1+|\\eta|}{1+|\\rho|} \\sqrt{q},\\\\\n        \\frac{[ (1 -|\\eta|)\\sqrt{q} -(|\\rho| -|\\eta|)\\sqrt{r}]_ +^2}{1 +\\eta^2 -2|\\rho||\\eta|}, & \\text{if } \\sqrt{r} \\geq \\frac{1+|\\eta|}{1+|\\rho|}\\sqrt{q},\n\\end{cases} \\cr\nf_2(\\sqrt{r}, \\sqrt{q}) &= \\min\\Bigl\\{ (\\sqrt{r} -\\sqrt{q})_ +^2, \\;\\;\n       \\frac{[ (1 - \\rho\\eta)\\sqrt{r} -(1 -|\\eta|)\\sqrt{q}]_ +^2}{1 +\\eta^2 - 2\\rho\\eta} \\Bigr\\},\\cr\n       f_3(\\sqrt{r}, \\sqrt{q}) &= \\frac{(1 -\\eta)^2 [ (1 +\\rho)\\sqrt{r} -\\sqrt{q}]_ +^2}{1 +\\eta^2 - 2\\rho\\eta}. \n\\end{aligned}$$*\n:::\n\n#### Part 3. Calculating the phase diagram.\n\nBy Theorem [8](#suppthm:hamm.en){reference-type=\"ref\" reference=\"suppthm:hamm.en\"}, the Hamming error is $\\ensuremath{\\mathrm{FP}}_p+\\ensuremath{\\mathrm{FN}}_p=L_p p^{1-h(q;\\vartheta,r)}$, where $$\\label{suppeq:h(qvtr)}\n  h(q;\\vartheta,r) = \\min\\Bigl\\{\\min\\bigl\\{ q, \\  \\vartheta+ f_1(\\sqrt{r}, \\sqrt{q})\\bigr\\},\\;\\;  \\min\\bigl\\{\\vartheta+ f_2(\\sqrt{r}, \\sqrt{q}),\\  2\\vartheta+ f_3(\\sqrt{r}, \\sqrt{q})\\bigr\\} \\Bigr\\}.$$\n\nTo calculate the phase diagram, we need to find $q^*$ that maximizes $h(q;\\vartheta,r)$ and then investigate the conditions on $(r,\\vartheta)$ such that $h(q^*; \\vartheta,r)>1$ or $\\vartheta<h(q^*; \\vartheta,r)<1$ or $h(q^*; \\vartheta,r)\\leq \\vartheta$.\n\nWe first prove that $r= \\vartheta$ is the boundary between the Regions of Almost Full Recovery and No Recovery, i.e., the boundary separating $\\vartheta<h(q^*; \\vartheta,r)<1$ and $h(q^*; \\vartheta,r)\\leq \\vartheta$.\n\nWhen $r <\\vartheta$, we need to show $h(q;\\vartheta,r) \\leq \\vartheta,\\,\\forall\\, q$. If $q \\leq  \\vartheta$, then $h(q;\\vartheta,r) \\leq q \\leq \\vartheta$. If $q > \\vartheta$, then we look at $f_2(\\sqrt{r},\\sqrt{q})$: Now we have $0 \\leq f_2(\\sqrt{r},\\sqrt{q}) \\leq (\\sqrt{r} - \\sqrt{q})_ + ^2 = 0$. Thus $h(q;\\vartheta,r) \\leq \\vartheta + f_2(\\sqrt{r},\\sqrt{q}) =\\vartheta$.\n\nWhen $r >\\vartheta$, we can always find suitable $q$ to make $h(q^*; \\vartheta,r) \\geq h(q; \\vartheta,r) > \\vartheta$. It is left for later discussion whether $h(q^*; \\vartheta,r)$ is greater than 1. In fact, such $q$ can be any value satisfying $\\max \\{\\vartheta, (\\frac{|\\rho|-|\\eta|}{1 -|\\eta|})^2 r\\} < q < r$, which always exists because $\\frac{|\\rho|-|\\eta|}{1 -|\\eta|} < 1$. Since $r> q$, we know $f_2(\\sqrt{r},\\sqrt{q})$ and $f_3(\\sqrt{r},\\sqrt{q})$ are strictly positive from their definition; since $(1 -|\\eta|) \\sqrt{q} > (|\\rho| -|\\eta|) \\sqrt{r}$, we also know $f_1(\\sqrt{r},\\sqrt{q}) > 0$. Since all four components of $h(q;\\vartheta,r)$ in [\\[suppeq:h(qvtr)\\]](#suppeq:h(qvtr)){reference-type=\"eqref\" reference=\"suppeq:h(qvtr)\"} is greater than $\\vartheta$, we have the desired result.\n\nTo sum up the discussion so far, we have shown that $r =\\vartheta$ is the curve separating the regions of $\\vartheta<h(q^*; \\vartheta,r)<1$ and $h(q^*; \\vartheta,r)\\leq \\vartheta$.\n\nFor the rest of Part 3, we try to find the boundary between $h(q^*; \\vartheta,r)>1$ and $\\vartheta<h(q^*; \\vartheta,r)<1$.\n\nWe need an important fact about such boundary, not only for the proof of Elastic net but also for all other methods. Recall the definition of $\\ensuremath{\\mathrm{FP}}_p$ and $\\ensuremath{\\mathrm{FN}}_p$ in [\\[proof-en-Hamming\\]](#proof-en-Hamming){reference-type=\"eqref\" reference=\"proof-en-Hamming\"}, and we actually have the general form $$\\ensuremath{\\mathrm{FP}}_p=L_p p^{1- \\min\\bigl\\{ d^2_{\\Sigma}(\\mu_{00},{\\cal R}), \\;\\; \\vartheta+ d^2_{\\Sigma}(\\mu_{01},{\\cal R})\\bigr\\}}, \\qquad \\ensuremath{\\mathrm{FN}}_p = L_p p^{1-\\min\\bigl\\{\\vartheta+ d^2_{\\Sigma}(\\mu_{10},{\\cal R}^c),\\;\\; 2\\vartheta+ d^2_{\\Sigma}(\\mu_{11},{\\cal R}^c)\\bigr\\}},$$ and $$%\\label{suppeq:h(qvtr)general}\n  h(q;\\vartheta,r) = \\min\\Bigl\\{\\min\\bigl\\{ d^2_{\\Sigma}(\\mu_{00},{\\cal R}), \\  \\vartheta+ d^2_{\\Sigma}(\\mu_{01},{\\cal R})\\bigr\\},\\;\\;  \\min\\bigl\\{\\vartheta+ d^2_{\\Sigma}(\\mu_{10},{\\cal R}),\\  2\\vartheta+ d^2_{\\Sigma}(\\mu_{11},{\\cal R})\\bigr\\} \\Bigr\\}.$$ As an important fact, we always have the following relationship at the boundary: $$\\label{suppeq:important.relationship}\n  \\min\\bigl\\{ d^2_{\\Sigma}(\\mu_{00},{\\cal R}), \\  \\vartheta+ d^2_{\\Sigma}(\\mu_{01},{\\cal R})\\bigr\\} =  \\min\\bigl\\{\\vartheta+ d^2_{\\Sigma}(\\mu_{10},{\\cal R}),\\  2\\vartheta+ d^2_{\\Sigma}(\\mu_{11},{\\cal R})\\bigr\\} = 1$$ This is because: First, at the boundary $r =r(\\vartheta)$ , we must always have $h(q^*; \\vartheta,r(\\vartheta))=1$; otherwise, since $h(q; \\vartheta,r)$ is continuous in $(q,r)$ for fixed $\\vartheta$, it would contradict the definition of the boundary itself. Second, the exponents of $\\ensuremath{\\mathrm{FP}}_p$ and $FN_p$ has to be equal. This is because if we change the tuning parameters for fixed $(\\vartheta,r)$, it can only enlarge or shrink the rejection region $\\cal R$, and thus the effects on $\\ensuremath{\\mathrm{FP}}_p$ and $\\ensuremath{\\mathrm{FN}}_p$ would always be in the opposite directions. As a result, if the exponents of $\\ensuremath{\\mathrm{FP}}_p$ and $\\ensuremath{\\mathrm{FN}}_p$ are not equal at the boundary, we can change the tuning parameters to make $h(q;\\vartheta,r) >1$.\n\nWith the important fact, we can proceed our discussion. By the definition of $h(q;\\vartheta,r)$ in [\\[suppeq:h(qvtr)\\]](#suppeq:h(qvtr)){reference-type=\"eqref\" reference=\"suppeq:h(qvtr)\"}, it gives us $2\\times 2 = 4$ cases respectively for $\\rho > 0$ and $\\rho <0$. We discuss them one by one and summarise the results when the full phase curves are complete. For brevity, we also denote $\\lambda' = \\sqrt{q}$ for the rest of Part 3.\n\nWhen $\\rho > 0$, we have four cases.\n\n*First*, if $\\lambda'^2 = \\vartheta+ f_2(\\sqrt{r},\\lambda') = 1$ and $\\vartheta+ f_1(\\sqrt{r},\\lambda') \\geq 1$, $2\\vartheta+ f_4(\\sqrt{r},\\lambda') \\geq 1$: we have $\\lambda' = 1$. From $\\vartheta+ f_2(\\sqrt{r},\\lambda') = 1$, we know $$\\sqrt{r} =\\max \\left\\{ 1 + \\sqrt{1 -\\vartheta}, \\frac{\\sqrt{1 +\\eta^2 - 2\\rho\\eta}}{1 -\\rho\\eta}\\sqrt{1 -\\vartheta} +\\frac{1 -\\eta}{1 -\\rho\\eta} \\right\\}.$$ We also know from $\\vartheta+ f_1(\\sqrt{r},\\lambda') \\geq 1$, that since $\\sqrt{r} > \\lambda' = 1$, $\\sqrt{r} \\leq \\frac{1 - \\eta}{\\rho -\\eta} - \\frac{\\sqrt{1 +\\eta^2 - 2\\rho\\eta}}{\\rho -\\eta} \\sqrt{1 -\\vartheta}$; from $2\\vartheta+ f_4(\\sqrt{r},\\lambda') \\geq 1$, that $\\sqrt{r} \\geq \\frac{\\sqrt{1 +\\eta^2 - 2\\rho\\eta}}{(1 -\\eta)(1 +\\rho)} \\sqrt{1 - 2\\vartheta} + \\frac{1}{1 +\\rho}$.\n\nAfter roughly interpreting the requirements, we make two points: (i) we always have $$\\label{suppeq:firstcase.intermediate}\n  \\frac{\\sqrt{1 +\\eta^2 - 2\\rho\\eta}}{1 -\\rho\\eta}\\sqrt{1 -\\vartheta} +\\frac{1 -\\eta}{1 -\\rho\\eta} \\geq \\frac{\\sqrt{1 +\\eta^2 - 2\\rho\\eta}}{(1 -\\eta)(1 +\\rho)} \\sqrt{1 - 2\\vartheta} + \\frac{1}{1 +\\rho}$$ and thus the requirement from $2\\vartheta+ f_4(\\sqrt{r},\\lambda') \\geq 1$ is loose. This can be proven by showing $\\frac{1 -\\eta}{1 -\\rho\\eta} \\geq  \\frac{1}{1 +\\rho}$ and $\\frac{\\sqrt{1 +\\eta^2 - 2\\rho\\eta}}{1 -\\rho\\eta} \\geq \\frac{\\sqrt{1 +\\eta^2 - 2\\rho\\eta}}{(1 -\\eta)(1 +\\rho)}$ respectively. (ii) Actually, we can eliminate the curve $\\sqrt{r} = \\frac{\\sqrt{1 +\\eta^2 - 2\\rho\\eta}}{1 -\\rho\\eta}\\sqrt{1 -\\vartheta} +\\frac{1 -\\eta}{1 -\\rho\\eta}$ from this step, without using the requirement from $2\\vartheta+ f_4(\\sqrt{r},\\lambda') \\geq 1$. (As a result, the same proof holds for the corresponding case of $\\rho < 0$.) This is because if we put together $$\\begin{cases} \n    \\frac{\\sqrt{1 +\\eta^2 - 2\\rho\\eta}}{1 -\\rho\\eta}\\sqrt{1 -\\vartheta} +\\frac{1 -\\eta}{1 -\\rho\\eta} >&~  1 + \\sqrt{1 -\\vartheta} \\\\\n    \\frac{\\sqrt{1 +\\eta^2 - 2\\rho\\eta}}{1 -\\rho\\eta}\\sqrt{1 -\\vartheta} +\\frac{1 -\\eta}{1 -\\rho\\eta} \\leq&~ \\frac{1 - \\eta}{\\rho -\\eta} - \\frac{\\sqrt{1 +\\eta^2 - 2\\rho\\eta}}{\\rho -\\eta} \\sqrt{1 -\\vartheta}\n  \\end{cases}$$ we will have no solution. To be more specific, the first inequality gives us $\\sqrt{1 -\\vartheta} >\\frac{\\eta(1 -\\rho)}{\\sqrt{1 +\\eta^2 - 2\\rho\\eta} - 1 +\\rho\\eta}$, and the second equation will eventually give us $\\sqrt{1 -\\vartheta} \\leq \\frac{(1 + \\eta)(1 -\\rho)}{(1 +\\rho)\\sqrt{1 +\\eta^2 - 2\\rho\\eta}}$. However, the upper and lower bounds on $\\sqrt{1 -\\vartheta}$ admits no solution, because we can prove $\\frac{\\eta(1 -\\rho)}{\\sqrt{1 +\\eta^2 - 2\\rho\\eta} - 1 +\\rho\\eta} \\geq \\frac{(1 + \\eta)(1 -\\rho)}{(1 +\\rho)\\sqrt{1 +\\eta^2 - 2\\rho\\eta}}$ jusy by simplifying it for a few steps.\n\nTo sum up, the *first* case gives us $\\sqrt{r} = 1 + \\sqrt{1 -\\vartheta}$ with the requirement $\\sqrt{r} \\leq \\frac{1 - \\eta}{\\rho -\\eta} - \\frac{\\sqrt{1 +\\eta^2 - 2\\rho\\eta}}{\\rho -\\eta} \\sqrt{1 -\\vartheta}$.\n\n*Second*, if $q =2\\vartheta+ f_4(\\sqrt{r},\\lambda') = 1$ and $\\vartheta+ f_1(\\sqrt{r},\\lambda') \\geq  1$, $\\vartheta+ f_2(\\sqrt{r},\\lambda') \\geq 1$, we will need $$\\sqrt{r} = \\frac{\\sqrt{1 +\\eta^2 - 2\\rho\\eta}}{(1 -\\eta)(1 +\\rho)} \\sqrt{1 - 2\\vartheta} + \\frac{1}{1 +\\rho}$$ while requiring $\\sqrt{r} \\geq \\max \\left\\{ 1 + \\sqrt{1 -\\vartheta}, \\frac{\\sqrt{1 +\\eta^2 - 2\\rho\\eta}}{1 -\\rho\\eta}\\sqrt{1 -\\vartheta} +\\frac{1 -\\eta}{1 -\\rho\\eta} \\right\\}$. We know this is impossible from Equation [\\[suppeq:firstcase.intermediate\\]](#suppeq:firstcase.intermediate){reference-type=\"eqref\" reference=\"suppeq:firstcase.intermediate\"}. No curve is produced in this case.\n\n*Third*, if $\\vartheta+ f_1(\\sqrt{r},\\lambda') = \\vartheta+ f_2(\\sqrt{r},\\lambda') = 1$, and $\\lambda' \\geq  1$, $2\\vartheta+ f_3(\\sqrt{r},\\lambda') \\geq 1$, we will know from $\\vartheta+ f_2(\\sqrt{r},\\lambda') = 1$ that $$\\sqrt{r} =\\max \\left\\{ \\lambda' + \\sqrt{1 -\\vartheta}, \\frac{\\sqrt{1 +\\eta^2 - 2\\rho\\eta}}{1 -\\rho\\eta}\\sqrt{1 -\\vartheta} +\\lambda'\\frac{1 -\\eta}{1 -\\rho\\eta} \\right\\}.$$ We can use the same method as in the first point of the *first* case to show that $2\\vartheta+ f_3(\\sqrt{r},\\lambda') \\geq 1$ is loose with $\\lambda' \\geq 1$ .\n\nNow the curve seems to have two choices, but the latter one is actually impossible. When $\\sqrt{r} = \\frac{\\sqrt{1 +\\eta^2 - 2\\rho\\eta}}{1 -\\rho\\eta}\\sqrt{1 -\\vartheta} +\\lambda'\\frac{1 -\\eta}{1 -\\rho\\eta}$, we have $$\\begin{cases} \n    \\sqrt{r} =&~ \\lambda' \\frac{1 - \\eta}{\\rho -\\eta} - \\frac{\\sqrt{1 +\\eta^2 - 2\\rho\\eta}}{\\rho -\\eta} \\sqrt{1 -\\vartheta} = \\frac{\\sqrt{1 +\\eta^2 - 2\\rho\\eta}}{1 -\\rho\\eta}\\sqrt{1 -\\vartheta} +\\lambda'\\frac{1 -\\eta}{1 -\\rho\\eta}  \\\\\n    \\lambda' \\geq&~ 1 \n   \\end{cases}$$ which implies $\\lambda' =\\frac{(1 +\\rho)\\sqrt{1 +\\eta^2 - 2\\rho\\eta}\\sqrt{1 -\\vartheta}}{(1 -\\rho)(1 +\\eta)} \\geq 1$. We can eliminate this case now, without considering the requirement of $2\\vartheta+ f_3(\\sqrt{r},\\lambda') \\geq 1$, because with such $\\lambda'  \\geq 1$, $\\frac{\\sqrt{1 +\\eta^2 - 2\\rho\\eta}}{1 -\\rho\\eta}\\sqrt{1 -\\vartheta} +\\lambda'\\frac{1 -\\eta}{1 -\\rho\\eta} \\geq \\lambda' + \\sqrt{1 -\\vartheta}$ cannot hold. To see this, we can compare $\\frac{\\sqrt{1 +\\eta^2 - 2\\rho\\eta}}{1 -\\rho\\eta}\\sqrt{1 -\\vartheta} +\\lambda'\\frac{1 -\\eta}{1 -\\rho\\eta}$; cancelling out \"$\\sqrt{1 -\\vartheta}$\", we have $$\\frac{\\sqrt{1 +\\eta^2 - 2\\rho\\eta}}{1 -\\rho\\eta} - 1 \\geq \\frac{\\eta(1 -\\rho)}{1 -\\rho\\eta} \\cdot \\frac{(1 +\\rho)\\sqrt{1 +\\eta^2 - 2\\rho\\eta}}{(1 -\\rho)(1 +\\eta)}$$ Simplifying this for a few steps, and we will arrive at \"$\\sqrt{1 +\\eta^2 - 2\\rho\\eta} \\geq 1 +\\eta$\" which gives a contradiction.\n\nWe can only have one case, where $\\lambda' + \\sqrt{1 -\\vartheta}$ is greater: $$\\begin{cases} \n    \\sqrt{r} =& \\lambda' \\frac{1 - \\eta}{\\rho -\\eta} - \\frac{\\sqrt{1 +\\eta^2 - 2\\rho\\eta}}{\\rho -\\eta} \\sqrt{1 -\\vartheta} = \\lambda' + \\sqrt{1 -\\vartheta} \\\\\n    \\lambda' \\geq& 1 \n  \\end{cases}$$\n\nTo sum up, the *third* case gives us the curve $\\sqrt{r} = \\left[ \\frac{1 -\\eta}{1 -\\rho} + \\frac{\\sqrt{1 +\\eta^2 - 2\\rho\\eta}}{1 - \\rho} \\right]\\sqrt{1 -\\vartheta}$ with the requirement $\\lambda' = \\left[ \\frac{\\rho -\\eta}{1 -\\rho} + \\frac{\\sqrt{1 +\\eta^2 - 2\\rho\\eta}}{1 - \\rho} \\right]\\sqrt{1 -\\vartheta} \\geq 1$.\n\n*Fourth*, if $\\vartheta+ f_2(\\sqrt{r},\\lambda') = 2\\vartheta+ f_3(\\sqrt{r},\\lambda') = 1$ and $\\lambda' \\geq 1$, $\\vartheta+ f_1(\\sqrt{r},\\lambda') \\geq 1$, we have the same contradiction as the *second* case, that $$\\frac{\\sqrt{1 +\\eta^2 - 2\\rho\\eta}}{(1 -\\eta)(1 +\\rho)} \\sqrt{1 - 2\\vartheta} + \\frac{\\lambda'}{1 +\\rho} \\geq \\frac{\\sqrt{1 +\\eta^2 - 2\\rho\\eta}}{1 -\\rho\\eta}\\sqrt{1 -\\vartheta} +\\lambda'\\frac{1 -\\eta}{1 -\\rho\\eta}$$ cannot hold.\n\n*Summarising the cases of positive correlation,*, we have two curves: $$\\begin{aligned}\n  \\sqrt{r} = &~ 1 + \\sqrt{1 -\\vartheta} \\\\\n  \\sqrt{r} =&~ \\left[ \\frac{1 -\\eta}{1 -\\rho} + \\frac{\\sqrt{1 +\\eta^2 - 2\\rho\\eta}}{1 - \\rho} \\right]\\sqrt{1 -\\vartheta} \n\\end{aligned}$$ and the intersection point of the two curves is exactly at $\\left[ \\frac{\\rho -\\eta}{1 -\\rho} + \\frac{\\sqrt{1 +\\eta^2 - 2\\rho\\eta}}{1 - \\rho} \\right]\\sqrt{1 -\\vartheta} = 1$ so the two curves can be summarised as taking the maximum.\n\nWhen $\\rho < 0$, we also have four cases.\n\n*First*, if $\\lambda'^2 = \\vartheta+ f_2(\\sqrt{r},\\lambda') = 1$ and $\\vartheta+ f_1(\\sqrt{r},\\lambda') \\geq 1$, $2\\vartheta+ f_4(\\sqrt{r},\\lambda') \\geq 1$: We already know from the same proof when the correlation is positive, that $\\sqrt{r} = 1 + \\sqrt{1 -\\vartheta}$ is the only admissible curve. Now we additionally need it to satisfy $\\sqrt{r} \\geq \\frac{\\sqrt{1 +\\eta^2 - 2\\rho\\eta}}{(1 +|\\eta|)(1 -|\\rho|)} \\sqrt{1 - 2\\vartheta} + \\frac{1}{1 -|\\rho|}$.\n\n*Second*, if $\\lambda'^2 =2\\vartheta+ f_4(\\sqrt{r},\\lambda') = 1$ and $\\vartheta+ f_1(\\sqrt{r},\\lambda') \\geq  1$, $\\vartheta+ f_2(\\sqrt{r},\\lambda') \\geq 1$, we will need $\\sqrt{r} = \\frac{\\sqrt{1 +\\eta^2 - 2\\rho\\eta}}{(1 +|\\eta|)(1 -|\\rho|)} \\sqrt{1 - 2\\vartheta} + \\frac{1}{1 -|\\rho|}$ and we need $\\sqrt{r} \\geq 1 + \\sqrt{1 -\\vartheta}$ from $\\vartheta+ f_2(\\sqrt{r},\\lambda') \\geq 1$.\n\n*To sum up the first two cases,* they give us $$\\sqrt{r} =\\max \\left\\{ 1 + \\sqrt{1 -\\vartheta}, \\frac{\\sqrt{1 +\\eta^2 - 2\\rho\\eta}}{(1 +|\\eta|)(1 -|\\rho|)} \\sqrt{1 - 2\\vartheta} + \\frac{1}{1 -|\\rho|}\\right\\}.$$ and we need $\\sqrt{r} \\leq \\frac{1 - |\\eta|}{|\\rho| -|\\eta|} - \\frac{\\sqrt{1 +\\eta^2 - 2\\rho\\eta}}{|\\rho| -|\\eta|} \\sqrt{1 -\\vartheta}$ from $\\vartheta+ f_1(\\sqrt{r},\\lambda') \\geq 1$.\n\n*Third*, if $\\vartheta+ f_1(\\sqrt{r},\\lambda') = \\vartheta+ f_2(\\sqrt{r},\\lambda') = 1$, and $\\lambda' \\geq  1$, $2\\vartheta+ f_3(\\sqrt{r},\\lambda') \\geq 1$, we already know from the same proof when the correlation is positive, that we have only one curve $\\sqrt{r} = \\left[ \\frac{1 -|\\eta|}{1 -|\\rho|} + \\frac{\\sqrt{1 +\\eta^2 - 2\\rho\\eta}}{1 - |\\rho|} \\right]\\sqrt{1 -\\vartheta}.$ Now we only need to update the requirement from $2\\vartheta+ f_3(\\sqrt{r},\\lambda') \\geq 1$, which is $$\\begin{aligned}\n  \\sqrt{r} \\geq \\frac{\\sqrt{1 +\\eta^2 - 2\\rho\\eta}}{(1 +|\\eta|)(1 -|\\rho|)} \\sqrt{1 - 2\\vartheta} + \\frac{\\lambda'}{1 -|\\rho|}\n\\end{aligned}$$ Plug in $\\lambda' = \\left[ \\frac{|\\rho| -|\\eta|}{1 -|\\rho|} + \\frac{\\sqrt{1 +\\eta^2 - 2\\rho\\eta}}{1 - |\\rho|} \\right]\\sqrt{1 -\\vartheta}$, it is equivalent to $$\\left[ \\frac{1 -|\\eta|}{1 -|\\rho|} + \\frac{\\sqrt{1 +\\eta^2 - 2\\rho\\eta}}{1 - |\\rho|} \\right]\\sqrt{1 -\\vartheta} \\geq \\frac{\\sqrt{1 +\\eta^2 - 2\\rho\\eta}}{1 - 2|\\rho| +\\rho\\eta} \\left[ \\sqrt{1 -\\vartheta} +\\frac{1 -|\\eta|}{1 +|\\eta|}\\sqrt{1 - 2\\vartheta} \\right]$$ where the RHS is another curve which will show up in the next case.\n\n*Fourth*, if $\\vartheta+ f_2(\\sqrt{r},\\lambda') = 2\\vartheta+ f_3(\\sqrt{r},\\lambda') = 1$ and $\\lambda' \\geq 1$, $\\vartheta+ f_1(\\sqrt{r},\\lambda') \\geq 1$, we will have the most tedious case.\n\nIt can be implied by $\\vartheta+ f_2(\\sqrt{r},\\lambda') = 2\\vartheta+ f_3(\\sqrt{r},\\lambda') = 1$ that $$\\lambda' \\left( \\frac{1 -|\\eta|}{|\\rho| -|\\eta|} - \\frac{1}{1 -|\\rho|} \\right) = \\sqrt{1 -\\vartheta} \\frac{\\sqrt{1 +\\eta^2 - 2\\eta\\rho}}{|\\rho| -|\\eta|} + \\sqrt{1 - 2\\vartheta} \\frac{\\sqrt{1 +\\eta^2 - 2\\eta\\rho}}{(1 +|\\eta|)(1 -|\\rho|)}$$ When $1 +\\rho\\eta - 2|\\rho| \\leq 0$, the equation admits no solution, because the coefficient of $\\lambda'$ is not positive. In this case, if we look back at the curve in the *second* case $FP_1 = FN_2$, we will notice that $$\\frac{\\sqrt{1 +\\eta^2 - 2\\rho\\eta}}{(1 +|\\eta|)(1 -|\\rho|)} \\sqrt{1 - 2\\vartheta} + \\frac{1}{1 -|\\rho|} \\leq \\frac{1 - |\\eta|}{|\\rho| -|\\eta|} - \\frac{\\sqrt{1 +\\eta^2 - 2\\rho\\eta}}{|\\rho| -|\\eta|} \\sqrt{1 -\\vartheta}$$ has no solution either. As a result, when $1 +\\rho\\eta - 2|\\rho| \\leq 0$, there is simply no Exact Recovery region in $\\vartheta\\in(0,\\frac{1}{2})$.\n\nWhen $1 +\\rho\\eta - 2|\\rho| > 0$, we can proceed to solve for $\\lambda'$ and then $\\sqrt{r}$: $$\\begin{aligned}\n  \\lambda' = &~ \\frac{\\sqrt{1 +\\eta^2 - 2\\rho\\eta}}{1 - 2|\\rho| +\\rho\\eta} \\left[ (1 -|\\rho|)\\sqrt{1 -\\vartheta} + \\frac{|\\rho| -|\\eta|}{1 +|\\eta|}\\sqrt{1 - 2\\vartheta} \\right] \\\\\n  \\sqrt{r} =&~ \\frac{\\sqrt{1 +\\eta^2 - 2\\rho\\eta}}{1 - 2|\\rho| +\\rho\\eta} \\left[ \\sqrt{1 -\\vartheta} +\\frac{1 -|\\eta|}{1 +|\\eta|}\\sqrt{1 - 2\\vartheta} \\right] \n\\end{aligned}$$ For all the requirements from $\\lambda' \\geq 1$ and $\\vartheta+ f_1(\\sqrt{r},\\lambda') \\geq 1$, we actually need $\\lambda' \\geq 1$ and $\\sqrt{r} \\geq \\max \\left\\{ \\lambda' + \\sqrt{1 -\\vartheta}, \\frac{\\sqrt{1 +\\eta^2 - 2\\rho\\eta}}{1 -\\rho\\eta}\\sqrt{1 -\\vartheta} +\\lambda'\\frac{1 -|\\eta|}{1 -\\rho\\eta} \\right\\}$.\n\nThe requirement $\\lambda' \\geq 1$ is actually $$\\frac{\\sqrt{1 +\\eta^2 - 2\\rho\\eta}}{1 - 2|\\rho| +\\rho\\eta} \\left[ \\sqrt{1 -\\vartheta} +\\frac{1 -|\\eta|}{1 +|\\eta|}\\sqrt{1 - 2\\vartheta} \\right] \\geq  \\frac{\\sqrt{1 +\\eta^2 - 2\\rho\\eta}}{(1 +|\\eta|)(1 -|\\rho|)} \\sqrt{1 - 2\\vartheta} + \\frac{1}{1 -|\\rho|}$$ and $\\sqrt{r} \\geq \\lambda' + \\sqrt{1 -\\vartheta}$ is actually $$\\frac{\\sqrt{1 +\\eta^2 - 2\\rho\\eta}}{1 - 2|\\rho| +\\rho\\eta} \\left[ \\sqrt{1 -\\vartheta} +\\frac{1 -|\\eta|}{1 +|\\eta|}\\sqrt{1 - 2\\vartheta} \\right] \\geq \\left[ \\frac{1 -|\\eta|}{1 -|\\rho|} + \\frac{\\sqrt{1 +\\eta^2 - 2\\rho\\eta}}{1 - |\\rho|} \\right]\\sqrt{1 -\\vartheta}.$$ So we can already conclude, that the diagram is $$\\begin{aligned}\n  \\sqrt{r} =\\max \\big\\{&~ 1 + \\sqrt{1 -\\vartheta},\\left[ \\frac{1 -|\\eta|}{1 -|\\rho|} + \\frac{\\sqrt{1 +\\eta^2 - 2\\rho\\eta}}{1 - |\\rho|} \\right]\\sqrt{1 -\\vartheta},\\\\\n   &~\\frac{\\sqrt{1 +\\eta^2 - 2\\rho\\eta}}{(1 +|\\eta|)(1 -|\\rho|)} \\sqrt{1 - 2\\vartheta} + \\frac{1}{1 -|\\rho|},\\frac{\\sqrt{1 +\\eta^2 - 2\\rho\\eta}}{(1 - 2|\\rho| +\\rho\\eta)_ +} \\left[ \\sqrt{1 -\\vartheta} +\\frac{1 -|\\eta|}{1 +|\\eta|}\\sqrt{1 - 2\\vartheta} \\right]\\big\\}\n\\end{aligned}$$\n\nHowever, we are left with one last constraint $\\frac{\\sqrt{1 +\\eta^2 - 2\\rho\\eta}}{1 - 2|\\rho| +\\rho\\eta} \\left[ \\sqrt{1 -\\vartheta} +\\frac{1 -|\\eta|}{1 +|\\eta|}\\sqrt{1 - 2\\vartheta} \\right] \\geq \\frac{\\sqrt{1 +\\eta^2 - 2\\rho\\eta}}{1 -\\rho\\eta}\\sqrt{1 -\\vartheta} +\\lambda'\\frac{1 -|\\eta|}{1 -\\rho\\eta}$. It is actually loose, but the proof may be tedious (and unimportant). We just need to prove that when $1 - 2|\\rho| +\\rho\\eta > 0$ and $$\\lambda' =  \\frac{\\sqrt{1 +\\eta^2 - 2\\rho\\eta}}{1 - 2|\\rho| +\\rho\\eta} \\left[ (1 -|\\rho|)\\sqrt{1 -\\vartheta} + \\frac{|\\rho| -|\\eta|}{1 +|\\eta|}\\sqrt{1 - 2\\vartheta} \\right] \\geq 1,$$ we always have $\\lambda' + \\sqrt{1 -\\vartheta} \\geq  \\frac{\\sqrt{1 +\\eta^2 - 2\\rho\\eta}}{1 -\\rho\\eta}\\sqrt{1 -\\vartheta} +\\lambda'\\frac{1 -|\\eta|}{1 -\\rho\\eta}$, which is equivalent to $$\\frac{(1 -|\\rho|)\\sqrt{1 +\\eta^2 - 2\\rho\\eta}}{1 - 2|\\rho| +\\rho\\eta} \\sqrt{1 -\\vartheta} + \\frac{|\\rho| -|\\eta|}{1 +|\\eta|}\\frac{\\sqrt{1 +\\eta^2 - 2\\rho\\eta}}{1 - 2|\\rho| +\\rho\\eta} \\sqrt{1 - 2\\vartheta} \\geq \\frac{\\sqrt{1 +\\eta^2 - 2\\rho\\eta} -(1 -\\rho\\eta)}{|\\eta|(1 -|\\rho|)}\\sqrt{1 - \\vartheta}$$\n\nWe first look at one sufficient condition, $\\frac{\\sqrt{1 +\\eta^2 - 2\\rho\\eta} -(1 -\\rho\\eta)}{|\\eta|(1 -|\\rho|)} \\leq 1$ By simplifying this inequality, we get $\\sqrt{1 +\\eta^2 - 2\\rho\\eta} \\leq 1 +|\\eta| - 2\\rho\\eta$. The RHS is positive, because: (i) If $|\\rho| \\leq 0.5$, $1 +|\\eta|(1 - 2|\\rho|) > 0$; (ii) If $|\\rho| >0. 5$, recall $1 - 2|\\rho| +\\rho\\eta > 0\\implies |\\eta| \\geq \\frac{2|\\rho| - 1}{|\\rho|}\\implies  1 +|\\eta|(1 - 2|\\rho|) \\geq 1 -\\frac{(2|\\rho| -1)^2}{|\\rho|} \\geq 0$ for $|\\rho| \\geq 0.5$. Then we can equare both sides and proceed, and finally getting $\\eta\\rho \\leq \\frac{1}{2}$.\n\nAs a result, when $\\rho\\eta \\leq \\frac{1}{2}$, we already have a sufficient condition for what we want to prove. When $\\rho\\eta > \\frac{1}{2}$, we look at another sufficient condition:\n\nWe only need to prove another sufficient condition, by looking at the coefficients of $\\sqrt{1 -\\vartheta}$, $$\\frac{(1 -|\\rho|)\\sqrt{1 +\\eta^2 - 2\\rho\\eta}}{1 - 2|\\rho| +\\rho\\eta} \\geq \\frac{\\sqrt{1 +\\eta^2 - 2\\rho\\eta} -(1 -\\rho\\eta)}{\\eta(1 -|\\rho|)}$$ which is equivalent to verifying $$(1 -|\\eta| - 2|\\rho| + 3\\eta\\rho -|\\eta|\\rho^2) \\sqrt{1 +\\eta^2 - 2\\rho \\eta} \\leq (1 -\\rho\\eta)(1 - 2|\\rho| +\\rho\\eta)$$ It is elementary mathematics that $(RHS - LHS)$ is always positive as a function of $(|\\rho|,|\\eta|)$ under $\\rho\\eta \\geq \\frac{1}{2}$, $1 - 2|\\rho| +\\rho\\eta > 0$ and $0 <|\\eta| <|\\rho|$.\n\n*Summarising the cases of negative correlation:* The diagram is $$\\begin{aligned}\n  \\sqrt{r} =\\max \\big\\{&~ 1 + \\sqrt{1 -\\vartheta},\\left[ \\frac{1 -|\\eta|}{1 -|\\rho|} + \\frac{\\sqrt{1 +\\eta^2 - 2\\rho\\eta}}{1 - |\\rho|} \\right]\\sqrt{1 -\\vartheta},\\\\\n   &~\\frac{\\sqrt{1 +\\eta^2 - 2\\rho\\eta}}{(1 +|\\eta|)(1 -|\\rho|)} \\sqrt{1 - 2\\vartheta} + \\frac{1}{1 -|\\rho|},\\frac{\\sqrt{1 +\\eta^2 - 2\\rho\\eta}}{(1 - 2|\\rho| +\\rho\\eta)_ +} \\left[ \\sqrt{1 -\\vartheta} +\\frac{1 -|\\eta|}{1 +|\\eta|}\\sqrt{1 - 2\\vartheta} \\right]\\big\\}\n\\end{aligned}$$\n\n## Proof of Lemma [2](#suppthm:sol.path.en){reference-type=\"ref\" reference=\"suppthm:sol.path.en\"} {#subsec:proof-EN-solution}\n\nRecall the optimization in [\\[enproof-optimization\\]](#enproof-optimization){reference-type=\"eqref\" reference=\"enproof-optimization\"}; the solution $b =(b_1,b_2)$ has to set the sub-gradient of the objective function to zero. As a result, the equation of the sub-gradient for $b =(b_1,b_2)$ is: $$\\begin{bmatrix} \n  1 & \\rho \\\\\n  \\rho & 1\n  \\end{bmatrix} \\begin{bmatrix} b_1 \\\\ b_2 \\end{bmatrix} \n  + \\sqrt{q} \\begin{bmatrix} \n  \\mathop{\\mathrm{sgn}}(b_1) \\\\\n  \\mathop{\\mathrm{sgn}}(b_2)\n  \\end{bmatrix} \n  + \\mu \\begin{bmatrix} \n  b_1 \\\\\n  b_2\n  \\end{bmatrix} = \\begin{bmatrix} h_1 \\\\ h_2 \\end{bmatrix}$$\n\nNow we begin to find out the solution path. Fixing $\\mu$, we decrease $\\sqrt{q}$ from a sufficiently large value to see when the variables enter the model. We assume $h_1 > 0$ and $0 <\\abs{h_2} < h_1$. Other cases can be computed in a similar way.\n\n*Stage 1*: When $\\sqrt{q}$ is so large that neither of $(x_j,x_{j + 1})$ is in the model $$\\begin{bmatrix} \n  1 & \\rho \\\\\n  \\rho & 1\n  \\end{bmatrix} \\begin{bmatrix} 0 \\\\ 0 \\end{bmatrix} \n  + \\sqrt{q} \\begin{bmatrix} \n  \\mathop{\\mathrm{sgn}}(0) \\\\\n  \\mathop{\\mathrm{sgn}}(0)\n  \\end{bmatrix} \n  + \\mu \\begin{bmatrix} \n  0 \\\\\n  0\n  \\end{bmatrix} = \\begin{bmatrix} h_1 \\\\ h_2 \\end{bmatrix},$$ it requires $\\sqrt{q} \\geq \\sqrt{q_1} =\\max\\{\\abs{h_1},\\abs{h_2}\\} = h_1$.\n\n*Stage 2*: When $\\sqrt{q}$ crosses $\\sqrt{q_1} = h_1$, we assert that variable $x_j$ has to enter the model, while $x_{j + 1}$ does not. This is because:\n\n-   $b_1$ has to be positive. If it is negative, we have $b_1 -\\sqrt{q} +\\mu \\cdot 2b_1 = h_1$, which implies $b_1$ has the same sign as $h_1 +\\sqrt{q} > 0$, which is a contradition.\n\n-   $b_2$ cannot enter the model at this point. Otherwise, we have $(1 +\\mu)b_2 +\\sqrt{q} \\mathop{\\mathrm{sgn}}(b_2) = h_2$, for $\\abs{h_2} <\\sqrt{q} < h_1$. Considering the sign of $b_2$, we have a contradition.\n\nSo $b_1$ has to enter the model as a positive number. Now the equation is $$\\begin{bmatrix} \n  1 & \\rho \\\\\n  \\rho & 1\n  \\end{bmatrix} \\begin{bmatrix} b_1 \\\\ 0 \\end{bmatrix} \n  + \\sqrt{q} \\begin{bmatrix} \n  1 \\\\\n  \\mathop{\\mathrm{sgn}}(0)\n  \\end{bmatrix} \n  + \\mu \\begin{bmatrix} \n  b_1 \\\\\n  0\n  \\end{bmatrix} = \\begin{bmatrix} h_1 \\\\ h_2 \\end{bmatrix}$$ Thus $b_1 =\\frac{h_1 -\\sqrt{q}}{1 +\\mu}$ and $\\rho b_1 +\\sqrt{q} \\mathop{\\mathrm{sgn}}(0) = h_2$. Since $\\mathop{\\mathrm{sgn}}(0)\\in[ - 1,1]$, we need $$\\label{suppeq:sol.path.en.kkt}\n  \\abs{h_2 -\\frac{\\rho}{1 +\\mu}h_1 +\\frac{\\rho}{1 +\\mu}\\sqrt{q}} \\leq \\sqrt{q}$$ By discussing the sign of the content of the absolute value as $\\sqrt{q}$ decreases, we have the following two cases:\n\n*Stage 3, Case 1*: When $h_2 >\\eta h_1$ (recall we define $\\eta =\\frac{\\rho}{1 +\\mu}$ as a shorthand), and $\\sqrt{q}$ crosses $\\sqrt{q_2} =\\frac{h_2 -\\eta h_1}{1 -\\eta}$, then $x_{j + 1}$ enters the model and $b_2$ is positive. This is because Equation [\\[suppeq:sol.path.en.kkt\\]](#suppeq:sol.path.en.kkt){reference-type=\"eqref\" reference=\"suppeq:sol.path.en.kkt\"} is now $h_2 -\\eta h_1 +\\eta\\sqrt{q} \\leq \\sqrt{q}$; as $\\sqrt{q} =\\sqrt{q_2} = \\frac{h_2 -\\eta h_1}{1 - \\eta}$, the sub-gradient of $\\abs{b_2}$ is taking the value of $1\\in\\mathop{\\mathrm{sgn}}(0)$, so $b_2$ has to enter the model as a positive number.\n\nIn this case, we solve $$\\begin{bmatrix} \n  1 & \\rho \\\\\n  \\rho & 1\n  \\end{bmatrix} \\begin{bmatrix} b_1 \\\\ b_2 \\end{bmatrix} \n  + \\sqrt{q} \\begin{bmatrix} \n  1 \\\\\n  1\n  \\end{bmatrix} \n  + \\mu \\begin{bmatrix} \n  b_1 \\\\\n  b_2\n  \\end{bmatrix} = \\begin{bmatrix} h_1 \\\\ h_2 \\end{bmatrix}$$ and get $$\\begin{aligned}\n  b_1 =\\frac{\\frac{h_1 -\\sqrt{q}}{1 +\\mu} -\\eta\\frac{h_2 -\\sqrt{q}}{1 +\\mu}}{1 - \\eta^2},\\quad \n  b_2 =\\frac{\\frac{h_2 -\\sqrt{q}}{1 +\\mu} -\\eta\\frac{h_1 -\\sqrt{q}}{1 +\\mu}}{1 - \\eta^2}.\n\\end{aligned}$$\n\n*Stage 3, Case 2*: When $h_2 <\\eta h_1$, and $\\sqrt{q}$ crosses $\\sqrt{q_2} = \\frac{\\eta h_1 - h_2}{1 +\\eta }$, $x_{j + 1}$ enters the model and $b_2$ is negative. This is because reviewing Equation [\\[suppeq:sol.path.en.kkt\\]](#suppeq:sol.path.en.kkt){reference-type=\"ref\" reference=\"suppeq:sol.path.en.kkt\"}, we always have $h_2 -\\eta h_1 +\\eta \\sqrt{q} < \\sqrt{q}$, and thus when $\\sqrt{q}$ is small enough to make $\\abs{h_2 -\\eta h_1 + \\eta \\sqrt{q}} = \\sqrt{q}$, it has to be $-h_2 +\\eta h_1 -\\eta \\sqrt{q} = \\sqrt{q}$. As a result, when $\\sqrt{q}$ crosses $\\sqrt{q_2} =\\frac{\\eta h_1 - h_2}{1 +\\eta }$, $b_2$ enters the model as a negative number. Solving $$\\begin{bmatrix} \n  1 & \\rho \\\\\n  \\rho & 1\n  \\end{bmatrix} \\begin{bmatrix} b_1 \\\\ b_2 \\end{bmatrix} \n  + \\sqrt{q} \\begin{bmatrix} \n  1 \\\\\n  -1\n  \\end{bmatrix} \n  + \\mu \\begin{bmatrix} \n  b_1 \\\\\n  b_2\n  \\end{bmatrix} = \\begin{bmatrix} h_1 \\\\ h_2 \\end{bmatrix}$$ we have $$\\begin{aligned}\n  b_1 =\\frac{\\frac{h_1 -\\sqrt{q}}{1 +\\mu} -\\eta \\frac{h_2 +\\sqrt{q}}{1 +\\mu}}{1 - \\eta ^2},\\quad \n  b_2 =\\frac{\\frac{h_2 +\\sqrt{q}}{1 +\\mu} -\\eta \\frac{h_1 -\\sqrt{q}}{1 +\\mu}}{1 - \\eta ^2}.\n\\end{aligned}$$"},{"heading":"Proof of Proposition [1](#prop:bridge){reference-type=\"ref\" reference=\"prop:bridge\"} (Marginal Regression)","text":"# Proof of Proposition [1](#prop:bridge){reference-type=\"ref\" reference=\"prop:bridge\"} (Marginal Regression) {#sec:marginal.regression}\n\nProposition [1](#prop:bridge){reference-type=\"ref\" reference=\"prop:bridge\"} is about the connection of Elastic net to Lasso and marginal regression.\n\nTo prove the assertion about Lasso, we only need to quote the results from the Corollary 4.2 of @ke2020power on the phase curves of Lasso. In fact, the phase curves of Lasso can be exactly obtained by setting $\\mu = 0$ in Theorem [2](#thm:elastic-net){reference-type=\"ref\" reference=\"thm:elastic-net\"}. As $\\mu$ decreases from some positive value to zero, the curves in Theorem [2](#thm:elastic-net){reference-type=\"ref\" reference=\"thm:elastic-net\"} just converges downwards to the phase curves of Lasso.\n\nTo prove the assertion about marginal regression, we need to fully study the variable selection method based on marginal regression, which we will be devoted to for the rest of this section.\n\n::: {#suppdef:marginal.regression .definition}\n**Definition 2**. *Marginal regression refers to the variable selection method which ranks all the variabels according to $\\{ \\abs{X'y}_j:j\\in[p] \\}$ and sets some cutoff point $t$ for the ranking. Then the variables $\\{ j\\in[p]:\\abs{X'y}_j > t \\}$ is selected.*\n:::\n\nSoft-thresholded marginal regression behaves the same as Definition [2](#suppdef:marginal.regression){reference-type=\"ref\" reference=\"suppdef:marginal.regression\"} in terms of variable selection.\n\n::: remark\n**Remark 1**. *If we focus on $(1 +\\mu)\\hat\\beta^{\\mathrm{EN}}$, then Lasso and soft-thresholded marginal regression are just two limits of $\\mu = 0$ and $\\mu =\\infty$, in terms of the solution and its path, the shape of the rejection region, and phase curves.*\n:::\n\nAs described in Section [6](#suppsec:sketch){reference-type=\"ref\" reference=\"suppsec:sketch\"}, our proof still consists of three parts: (a) deriving the rejection region, (b) obtaining the rate of convergence of $\\mathbb{E}[H(\\hat{\\beta},\\beta)]$, and (c) calculating the phase diagram.\n\n#### Part 1: Deriving the rejection region.\n\nAccording to Definition [2](#suppdef:marginal.regression){reference-type=\"ref\" reference=\"suppdef:marginal.regression\"}, the variable selection based on marginal regression can be decomposed to every correlated pair of variables, $(x_j,x_{j + 1})$. It directly thresholds $(x_j'y,x_{j + 1}'y)$ with $t$, and if we divide $(x_j'y,x_{j + 1}'y, t)$ with $\\sqrt{2\\log(p)}$, it is equivalent to thresholding $h_1=x_j'y/\\sqrt{2\\log(p)}$, $h_2=x_{j+1}'y/\\sqrt{2\\log(p)}$ with $t' = t/\\sqrt{2\\log(p)}$.\n\nThe solution path of marginal regression is very straight forward, so we present it in Lemma [1](#supplem:sol.path.marreg){reference-type=\"ref\" reference=\"supplem:sol.path.marreg\"} without proof.\n\n::: {#supplem:sol.path.marreg .lemma}\n**Lemma 1** (the solution path of marginal regression). *The definition of $(h_1,h_2,\\hat b_1,\\hat b_2)$ follows from that of Lemma [2](#suppthm:sol.path.en){reference-type=\"ref\" reference=\"suppthm:sol.path.en\"}, and we still assume $h_1 > \\abs{h_2} \\geq 0$. The solution path of marginal regression defined in [2](#suppdef:marginal.regression){reference-type=\"ref\" reference=\"suppdef:marginal.regression\"} can be describes as:*\n\n1.  *When $t' \\geq  h_1$, we have $\\hat b_1 =\\hat b_2 = 0$.*\n\n2.  *When $\\abs{h_2} \\leq t' < h_1$, we have $\\hat b_1 \\neq 0$ and $\\hat b_2 = 0$.*\n\n3.  *When $t < \\abs{h_2}$, we have $\\hat b_1 \\neq 0$ and $\\hat b_2 \\neq 0$.*\n:::\n\nWe now use Lemma [1](#supplem:sol.path.marreg){reference-type=\"ref\" reference=\"supplem:sol.path.marreg\"} to derive the rejection region $\\cal R$ of marginal regression. Recall that $\\cal R$ is the set of $h =(h_1,h_2)'$ such that $\\hat b_1\\neq 0$. In fact, the same procedure of Elastic net can be copied here, except that the specific behavior of the variable selection method is different. Lemma [1](#supplem:sol.path.marreg){reference-type=\"ref\" reference=\"supplem:sol.path.marreg\"} tells us it is just $$\\begin{aligned}\n \\label{suppeq:RejRegion.marreg}\n  {\\cal R} &= \\{(h_1,h_2): h_1> t'\\}\\cup \\{(h_1, h_2): h_1 <- t'\\}\n\\end{aligned}$$\n\n#### Part 2. Analyzing the Hamming error.\n\nThe first steps of analysing Elastic net applies here as well, and we just need to compute $d_{\\Sigma}(\\mu_{00}, {\\cal R})$, $d_{\\Sigma}(\\mu_{01}, {\\cal R})$, $d_{\\Sigma}(\\mu_{10}, {\\cal R}^c)$, and $d_{\\Sigma}(\\mu_{11}, {\\cal R}^c)$ given the different shape of $\\cal R$. Then we can compute $$\\ensuremath{\\mathrm{FP}}_p=L_p p^{1- \\min\\bigl\\{ d^2_{\\Sigma}(\\mu_{00},{\\cal R}), \\;\\; \\vartheta+ d^2_{\\Sigma}(\\mu_{01},{\\cal R})\\bigr\\}}, \\qquad \\ensuremath{\\mathrm{FN}}_p = L_p p^{1-\\min\\bigl\\{\\vartheta+ d^2_{\\Sigma}(\\mu_{10},{\\cal R}^c),\\;\\; 2\\vartheta+ d^2_{\\Sigma}(\\mu_{11},{\\cal R}^c)\\bigr\\}}.$$ Finally $\\mathbb{E}[H(\\hat{\\beta},\\beta)]=\\ensuremath{\\mathrm{FP}}_p+\\ensuremath{\\mathrm{FN}}_p=L_p p^{1-h(t';\\vartheta,r)}$.\n\n::: {#suppthm:hamm.marreg .theorem}\n**Theorem 9**. *Under the conditions of Theorem [2](#thm:elastic-net){reference-type=\"ref\" reference=\"thm:elastic-net\"}, let $t = t' \\sqrt{2\\log(p)}$ in marginal regression defined in [2](#suppdef:marginal.regression){reference-type=\"ref\" reference=\"suppdef:marginal.regression\"}. As $p\\to\\infty$, $$\\begin{aligned}\n    \\ensuremath{\\mathrm{FP}}_p =&~ L_p \\cdot p^{1 -\\min \\left\\{ t'^2,\\;\\;\\vartheta+ (t' -|\\rho| \\sqrt{r})_ +^2 \\right\\}} \\\\\n    \\ensuremath{\\mathrm{FN}}_p =&~ L_p \\cdot p^{1 -\\min \\left\\{ \\vartheta+(\\sqrt{r} - t')_ +^2,\\;\\;2\\vartheta+ [(1 +\\rho)\\sqrt{r} - t']_ +^2 \\right\\}}\n  \n\\end{aligned}$$*\n:::\n\n#### Part 3. Calculating the phase diagram.\n\nBy Theorem [9](#suppthm:hamm.marreg){reference-type=\"ref\" reference=\"suppthm:hamm.marreg\"}, the Hamming error is $\\ensuremath{\\mathrm{FP}}_p+\\ensuremath{\\mathrm{FN}}_p=L_p p^{1-h(t';\\vartheta,r)}$, where $$\\label{suppeq:h(qvtr).marreg}\n  h(t';\\vartheta,r) = \\min\\Bigl\\{\\min\\bigl\\{ t'^2, \\  \\vartheta+ (t' -|\\rho| \\sqrt{r})_ +^2 \\bigr\\},\\;\\;  \\min\\bigl\\{\\vartheta+ (\\sqrt{r} - t')_ +^2,\\  2\\vartheta+ [(1 +\\rho)\\sqrt{r} - t']_ +^2 \\bigr\\} \\Bigr\\}.$$\n\nThe first steps of the proof of Elastic net can also be applied directly. We still try to find $t'^*$ that maximizes $h(t';\\vartheta,r)$ and then investigate the conditions on $(r,\\vartheta)$ such that $h(t'^*; \\vartheta,r)>1$ or $\\vartheta<h(t'^*; \\vartheta,r)<1$ or $h(t'^*; \\vartheta,r)\\leq \\vartheta$.\n\nWe can still prove that $r= \\vartheta$ is the boundary between the Regions of Almost Full Recovery and No Recovery, i.e., the boundary separating $\\vartheta<h(q^*; \\vartheta,r)<1$ and $h(q^*; \\vartheta,r)\\leq \\vartheta$. The proof just needs slight modification to the proof of Elastic net.\n\nFor the rest of Part 3, we try to find the boundary between the Regions of Exact Recovery and Almost Full Recovery, i.e., the boundary separating $h(t'^*; \\vartheta,r)>1$ and $\\vartheta<h(t'^*; \\vartheta,r)<1$. We can still leverage the important fact that Equation [\\[suppeq:important.relationship\\]](#suppeq:important.relationship){reference-type=\"ref\" reference=\"suppeq:important.relationship\"} holds at the boundary: $$\\min\\bigl\\{ t'^2, \\  \\vartheta+ (t' -|\\rho| \\sqrt{r})_ +^2 \\bigr\\} =  \\min\\bigl\\{\\vartheta+ (\\sqrt{r} - t')_ +^2,\\  2\\vartheta+ [(1 +\\rho)\\sqrt{r} - t']_ +^2 \\bigr\\} = 1$$ The above equation still gives us $2\\times 2 = 4$ cases respectively for $\\rho > 0$ and $\\rho <0$. We discuss them one by one and summarise the results when the full phase curves are complete.\n\nWhen $\\rho> 0$, we set out to prove the final phase diagram is $$\\sqrt{r} =\\max \\left\\{  1 + \\sqrt{1 -\\vartheta},\\,\\frac{2}{1 -\\rho}\\sqrt{1 -\\vartheta}  \\right\\}$$\n\n*First*, if $t'^2 = \\vartheta+ (\\sqrt{r} - t')_ +^2 = 1$ and $\\vartheta+ (t' -|\\rho| \\sqrt{r})_ +^2 \\geq 1$, $2\\vartheta+ [(1 +\\rho)\\sqrt{r} - t']_ +^2 \\geq 1$, then $t' = 1$ and $\\sqrt{r} = 1 + \\sqrt{1 -\\vartheta}$.\n\nWe also need to meet two requirements: $\\sqrt{r} \\leq  \\frac{1 - \\sqrt{1 -\\vartheta}}{\\rho}$ from $\\vartheta+ (t' -|\\rho| \\sqrt{r})_ +^2 \\geq 1$ and $\\sqrt{r} \\geq \\frac{1 + \\sqrt{1 - 2\\vartheta}}{1 +\\rho}$ from $2\\vartheta+ [(1 +\\rho)\\sqrt{r} - t']_ +^2 \\geq 1$. The second requirement is not restrictive, and the first one is equivalent to $\\sqrt{1 -\\vartheta} \\leq \\frac{1 -\\rho}{1 +\\rho}$.\n\n*Second*, if $t'^2 = 2\\vartheta+ [(1 +\\rho)\\sqrt{r} - t']_ +^2 = 1$ and $\\vartheta+ (\\sqrt{r} - t')_ +^2 \\geq 1$, $\\vartheta+ (t' -|\\rho| \\sqrt{r})_ +^2 \\geq 1$, then we have no admissible curve. This is because $\\sqrt{r} = \\frac{1 + \\sqrt{1 - 2\\vartheta}}{1 +\\rho}$ and it is required that $\\sqrt{r} \\geq 1 + \\sqrt{1 -\\vartheta}$ by $\\vartheta+ (\\sqrt{r} - t')_ +^2 \\geq 1$. It gives us no admissible $\\sqrt{r}$.\n\n*Summarising the first two cases*, we have only one curve $\\sqrt{r} = 1 + \\sqrt{1 -\\vartheta}$ under the constraint $\\sqrt{1 -\\vartheta} \\leq \\frac{1 -\\rho}{1 +\\rho}$.\n\n*Third*, if $\\vartheta+ (t' -|\\rho| \\sqrt{r})_ +^2 = \\vartheta+ (\\sqrt{r} - t')_ +^2 = 1$, and $t' \\geq 1$, $2\\vartheta+ [(1 +\\rho)\\sqrt{r} - t']_ +^2  \\geq 1$, the equality gives us $\\sqrt{r} = \\frac{2}{1 -\\rho}\\sqrt{1 -\\vartheta}$ and $t' = \\frac{1 +\\rho}{1 -\\rho}\\sqrt{1 -\\vartheta}$. We are also required to have $t' \\geq 1$ and $\\sqrt{r} \\geq \\frac{\\sqrt{1 - 2\\vartheta}}{1 +\\rho} + \\frac{\\sqrt{1 -\\vartheta}}{1 -\\rho}$ (not restrictive).\n\n*Fourth*, if $\\vartheta+ (t' -|\\rho| \\sqrt{r})_ +^2 =2\\vartheta+ [(1 +\\rho)\\sqrt{r} - t']_ +^2 = 1$, and $t' \\geq 1$, $\\vartheta+ (\\sqrt{r} - t')_ +^2 \\geq 1$, then we have no admissible curve. In fact, the equality gives us $\\sqrt{r} = \\frac{t' - \\sqrt{1 -\\vartheta}}{\\rho} =\\frac{t' + \\sqrt{1 - 2\\vartheta}}{1 +\\rho}$, $t' = \\rho \\sqrt{1 - 2\\vartheta} + (1 +\\rho) \\sqrt{1 -\\vartheta}$. We are also required to have $\\sqrt{r} \\geq  t' + \\sqrt{1 -\\vartheta} \\Leftrightarrow \\sqrt{1 - 2\\vartheta} \\geq \\frac{1 +\\rho}{1 - \\rho}\\sqrt{1 -\\vartheta}$, which gives a contradition.\n\n*Summarising the last two cases*, we have only one curve $\\sqrt{r} = \\frac{2}{1 -\\rho}\\sqrt{1 -\\vartheta}$ under the constraint $\\sqrt{1 -\\vartheta} \\geq \\frac{1 -\\rho}{1 +\\rho}$.\n\n*Summarising the cases of positive correlation*, we have proven the phase diagram is $\\sqrt{r} =\\max \\left\\{  1 + \\sqrt{1 -\\vartheta},\\,\\frac{2}{1 -\\rho}\\sqrt{1 -\\vartheta}  \\right\\}$.\n\nWhen $\\rho < 0$, we then prove that the phase diagram is $$\\sqrt{r} =\\max \\left\\{  1 + \\sqrt{1 -\\vartheta},\\,\\frac{2}{1 -|\\rho|}\\sqrt{1 -\\vartheta},\\, \\frac{\\sqrt{1 -\\vartheta} + \\sqrt{1 - 2\\vartheta}}{1 - 2|\\rho|},\\,\\frac{1 + \\sqrt{1 - 2\\vartheta}}{1 -|\\rho|}  \\right\\}$$\n\n*First*, if $t'^2 = \\vartheta+ (\\sqrt{r} - t')_ +^2 = 1$ and $\\vartheta+ (t' -|\\rho| \\sqrt{r})_ +^2 \\geq 1$, $2\\vartheta+ [(1 -|\\rho|)\\sqrt{r} - t']_ +^2 \\geq 1$, then $t' = 1$ and $\\sqrt{r} = 1 + \\sqrt{1 -\\vartheta}$.\n\nWe also have two requirements, $\\sqrt{r} \\leq \\frac{1 - \\sqrt{1 -\\vartheta}}{|\\rho|}$ and $\\sqrt{r} \\geq \\frac{1 + \\sqrt{1 - 2\\vartheta}}{1 -|\\rho|}$. We have studied the first one when $\\rho > 0$, and know it is $1+ \\sqrt{1 -\\vartheta} \\geq \\frac{2}{1 -|\\rho|}\\sqrt{1 -\\vartheta}$. The RHS of the second requirement is actually a new curve we will see later.\n\n*Second*, if $t'^2 =2\\vartheta+ [(1 -|\\rho|)\\sqrt{r} - t']_ +^2 = 1$ and $\\vartheta+ (t' -|\\rho| \\sqrt{r})_ +^2 \\geq 1$, $\\vartheta+ (\\sqrt{r} - t')_ +^2  \\geq 1$, we have $\\sqrt{r} = \\frac{1 + \\sqrt{1 - 2\\vartheta}}{1 -|\\rho|}$ and $\\sqrt{r} \\leq  \\frac{1 - \\sqrt{1 -\\vartheta}}{|\\rho|}$, $\\sqrt{r} \\geq 1 + \\sqrt{1 -\\vartheta}.$.\n\n*Summarising the first two cases*, we have $\\sqrt{r} =\\max \\left\\{ 1 + \\sqrt{1 -\\vartheta},\\frac{1 + \\sqrt{1 - 2\\vartheta}}{1 -|\\rho|} \\right\\}$ and we need $\\sqrt{r} \\leq \\frac{1 - \\sqrt{1 -\\vartheta}}{|\\rho|}$.\n\n*Third*, if $\\vartheta+ (t' -|\\rho| \\sqrt{r})_ +^2 = \\vartheta+ (\\sqrt{r} - t')_ +^2 = 1$ and $t' \\geq 1$, $2\\vartheta+ [(1 -|\\rho|)\\sqrt{r} - t']_ +^2 \\geq 1$, then $\\sqrt{r} = \\frac{2}{1 - |\\rho|}\\sqrt{1 -\\vartheta}$, and the two other requirements are $t' = \\frac{1 +|\\rho|}{1 -|\\rho|}\\sqrt{1 -\\vartheta} \\geq 1$ and $\\sqrt{r} \\geq  \\frac{\\sqrt{1 - 2\\vartheta}}{1 -|\\rho|} + \\frac{1 +|\\rho|}{(1 -|\\rho|)^2} \\sqrt{1 -\\vartheta}$.\n\nIn the next case, we will get another curve $\\sqrt{r} = \\frac{\\sqrt{1 -\\vartheta} + \\sqrt{1 - 2\\vartheta}}{1 - 2|\\rho|}$. In the above inequalities, the last one corresponds to $\\frac{2}{1 -|\\rho|} \\sqrt{1 -\\vartheta} \\geq  \\frac{\\sqrt{1 -\\vartheta} + \\sqrt{1 - 2\\vartheta}}{1 - 2|\\rho|}$. $t' = \\frac{1 +|\\rho|}{1 -|\\rho|}\\sqrt{1 -\\vartheta} \\geq 1$ in the above inequalities is just $1 + \\sqrt{1 -\\vartheta} \\leq  \\frac{2}{1 -|\\rho|} \\sqrt{1 -\\vartheta}$.\n\n*Fourth*, if $\\vartheta+ (t' -|\\rho| \\sqrt{r})_ +^2 = 2\\vartheta+ [(1 -|\\rho|)\\sqrt{r} - t']_ +^2  = 1$ and $t' \\geq 1$, $\\vartheta+ (\\sqrt{r} - t')_ +^2\\geq 1$, then we know from the equality that $$\\sqrt{r} = \\frac{t' - \\sqrt{1 -\\vartheta}}{|\\rho|} =\\frac{t' + \\sqrt{1 - 2\\vartheta}}{1 -|\\rho|}$$ and we will get this when we solve for $t'$: $$\\frac{t'}{|\\rho|} - \\frac{t'}{1 -|\\rho|} =\\frac{\\sqrt{1 - 2\\vartheta}}{1 -|\\rho|} + \\frac{\\sqrt{1 -\\vartheta}}{|\\rho|}.$$ If $|\\rho| \\geq \\frac{1}{2}$, this equation admits no positive solution for $t'$. Recall that in the *first and second* cases, we also needed $\\sqrt{r} \\leq \\frac{1 -\\sqrt{1 -\\vartheta}}{|\\rho|}$ and $\\sqrt{r} \\geq \\frac{1 + \\sqrt{1 - 2\\vartheta}}{1 -|\\rho|}$ in the cases of $FP_1$ being tight. When $|\\rho| \\geq \\frac{1}{2}$, $\\frac{1 + \\sqrt{1 - 2\\vartheta}}{1 -|\\rho|} \\leq \\frac{1 -\\sqrt{1 -\\vartheta}}{|\\rho|}$ has no solution either, so there cannot be any curve in the interval $0 <\\vartheta<\\frac{1}{2}$.\n\nif $|\\rho| < \\frac{1}{2}$, we can proceed to have the two requirements: $$\\begin{cases} \n    t' =&~ \\frac{1}{1 - 2|\\rho|} \\left[ |\\rho| \\sqrt{1 - 2\\vartheta} +(1 -|\\rho|) \\sqrt{1 -\\vartheta} \\right] \\geq 1\\\\\n    \\sqrt{r} =&~ \\frac{\\sqrt{1 -\\vartheta} + \\sqrt{1 - 2\\vartheta}}{1 - 2|\\rho|} \\geq \\frac{|\\rho|}{1 - 2|\\rho|} \\sqrt{1 - 2\\vartheta} + \\frac{2 - 3|\\rho|}{1 - 2|\\rho|} \\sqrt{1 -\\vartheta}\n  \\end{cases}$$ In the above inequalities, $\\sqrt{r} = \\frac{\\sqrt{1 -\\vartheta} + \\sqrt{1 - 2\\vartheta}}{1 - 2|\\rho|} \\geq \\frac{|\\rho|}{1 - 2|\\rho|} \\sqrt{1 - 2\\vartheta} + \\frac{2 - 3|\\rho|}{1 - 2|\\rho|} \\sqrt{1 -\\vartheta}$ is equivalent to $\\frac{\\sqrt{1 -\\vartheta} + \\sqrt{1 - 2\\vartheta}}{1 - 2|\\rho|} \\geq \\frac{2}{1 -|\\rho|}\\sqrt{1 -\\vartheta}$. The requirement on $t'$ is equivalent to $\\sqrt{r} = \\frac{\\sqrt{1 -\\vartheta} + \\sqrt{1 - 2\\vartheta}}{1 - 2|\\rho|} \\geq \\frac{1 + \\sqrt{1 - 2\\vartheta}}{1 -|\\rho|}$.\n\n*Summarising the cases of negative correlation*: The final phase diagram is $$\\sqrt{r} =\\max \\left\\{  1 + \\sqrt{1 -\\vartheta},\\,\\frac{2}{1 -|\\rho|}\\sqrt{1 -\\vartheta},\\, \\frac{\\sqrt{1 -\\vartheta} + \\sqrt{1 - 2\\vartheta}}{1 - 2|\\rho|},\\,\\frac{1 + \\sqrt{1 - 2\\vartheta}}{1 -|\\rho|}  \\right\\}$$ When $\\rho \\leq - \\frac{1}{2}$, $\\sqrt{r}$ has no finite value for $\\vartheta\\in(0,1/2)$, and we do not have Region of Exact Recovery or $h(t';\\vartheta,r) > 1$ at all."},{"heading":"Proof of Theorem [3](#thm:SCAD){reference-type=\"ref\" reference=\"thm:SCAD\"} (SCAD)","text":"# Proof of Theorem [3](#thm:SCAD){reference-type=\"ref\" reference=\"thm:SCAD\"} (SCAD) {#suppsec:scad}\n\nAs described in Section [6](#suppsec:sketch){reference-type=\"ref\" reference=\"suppsec:sketch\"}, our proof for SCAD still consists of three parts: (a) deriving the rejection region, (b) obtaining the rate of convergence of $\\mathbb{E}[H(\\hat{\\beta},\\beta)]$, and (c) calculating the phase diagram.\n\nBefore starting, we first recall the definition of SCAD. An alternative way to write the derivative of the penalty function is: $$\\label{suppeq:definition.SCAD}\n  q'(\\theta)=\\begin{cases} \n     \\lambda \\cdot \\mathop{\\mathrm{sgn}}(\\theta) & \\text{if } \\abs\\theta < \\lambda\\\\\n     \\frac{a\\lambda -\\theta}{a - 1}\\cdot \\mathop{\\mathrm{sgn}}(\\theta) & \\text{if } \\lambda < \\abs\\theta < a\\lambda \\\\\n     0 & \\text{if } \\abs\\theta > a\\lambda\n   \\end{cases}$$ for $\\theta\\in\\mathbb{R},\\, a > 2\\, \\lambda > 0$.\n\n#### Part 1: Deriving the rejection region.\n\nJust like the first steps of Elastic net, we define $h =(h_1,h_2)'\\in\\mathbb{R}^2$ where $h_1=x_j'y/\\sqrt{2\\log(p)}$, $h_2=x_{j+1}'y/\\sqrt{2\\log(p)}$; $\\lambda=\\lambda' \\sqrt{2\\log(p)}$; $(\\hat{\\beta}_j, \\hat{\\beta}_{j+1})=\\sqrt{2\\log(p)}(\\hat{b}_1, \\hat{b}_2)$ are the entries of the estimator $\\hat\\beta^{\\mathrm{{S}CAD}}$ corresponding to $(x_j,x_{j + 1})$.\n\nThe estimator of SCAD is $\\hat{\\beta}^{\\text {SCAD }}=\\operatorname{argmin}_{\\beta}\\left\\{\\|y-X \\beta\\|^{2} / 2+Q_{\\lambda}(\\beta)\\right\\}$. Like Elastic net, it can be decomposed into bivariate sub-problems of each pair of correlated variables. By dividing the bivariate sub-problem of $(x_j,x_{j + 1})$ by $\\sqrt{2\\log(p)}$, we have $$\\label{suppeq:optimization.scad}\nL(b)\\equiv \\frac{1}{2}b'\\begin{bmatrix}1&\\rho\\\\\\rho' & 1\\end{bmatrix}b + b'h+\\lambda'(q'(b_1)  + q'(b_2))$$ and the minimizer of the optimization [\\[suppeq:optimization.scad\\]](#suppeq:optimization.scad){reference-type=\"eqref\" reference=\"suppeq:optimization.scad\"} is $(\\hat b_1,\\hat b_2)$. The next lemma proves the solution to [\\[suppeq:optimization.scad\\]](#suppeq:optimization.scad){reference-type=\"eqref\" reference=\"suppeq:optimization.scad\"} when $h_1 >\\abs{h_2}$, and it is proven in Section [9.1](#suppsec:sol.path.scad){reference-type=\"ref\" reference=\"suppsec:sol.path.scad\"}.\n\n::: {#suppthm:sol.path.scad .lemma}\n**Lemma 2** (the solution path of SCAD). *Consider the optimization in [\\[suppeq:optimization.scad\\]](#suppeq:optimization.scad){reference-type=\"eqref\" reference=\"suppeq:optimization.scad\"}. Suppose $h_1 >\\abs{h_2}$, and suppose $\\rho \\geq 0$.*\n\n-   *When $\\lambda' \\geq \\lambda'_1 =\\max \\left\\{ \\abs{h_1} ,\\abs{h_2}\\right\\}$, $\\hat b_1=\\ \\hat b2 = 0$.*\n\n-   *If $(\\rho - \\frac{1}{a})h_1 < h_2 < \\begin{cases} (\\rho + \\frac{1}{a})h_1 & \\text{if }a > \\frac{2}{1-\\rho} \\\\ \\frac{1+\\rho}{2}h_1 & \\text{if } a \\leq  \\frac{2}{1-\\rho} \\end{cases}$,*\n\n    1.  *When $\\lambda' < \\lambda'_1$ and $\\lambda' \\geq \\abs{h_2 -\\rho h_1}$, $\\hat b_1 \\neq 0$ and $\\hat b_2 = 0$.*\n\n    2.  *When $\\lambda' < \\abs{h_2 -\\rho h_1}$, $\\hat b_1 \\neq 0$ and $\\hat b_2 \\neq 0$.*\n\n-   *If $a > \\frac{2}{1 -\\rho}$ and $(\\rho + \\frac{1}{a})h_1 < h_2 < \\frac{1 +\\rho}{2}h_1$:*\n\n    1.  *When $\\lambda' <\\lambda'_1$ and $\\lambda' \\geq \\frac{(a - 2)h_2 -\\rho(a - 1)h_1}{a - 2 - a\\rho}$, $\\hat b_1 \\neq 0$ and $\\hat b_2 = 0$.*\n\n    2.  *When $\\lambda' < \\frac{(a - 2)h_2 -\\rho(a - 1)h_1}{a - 2 - a\\rho}$, $\\hat b_1 \\neq 0$ and $\\hat b_2 \\neq 0$.*\n\n-   *If $h_2 <(\\rho - \\frac{1}{a})h_1$, $\\forall\\, a$:*\n\n    1.  *When $\\lambda' <\\lambda'_1$ and $\\lambda' \\geq \\frac{\\rho(a - 1)h_1 -(a - 2)h_2}{a + a\\rho - 2}$, $\\hat b_1 \\neq 0$ and $\\hat b_1 = 0$.*\n\n    2.  *When $\\lambda' < \\frac{\\rho(a - 1)h_1 -(a - 2)h_2}{a + a\\rho - 2}$, $\\hat b_1 \\neq 0$ and $\\hat b_2 \\neq 0$.*\n\n-   *If $h_2 \\leq \\frac{ - 1 +\\rho}{2}h_1$ or $h_2 \\geq  \\frac{1 +\\rho}{2}h_1$:*\n\n    1.  *When $\\lambda' < \\lambda'_1$ and $\\lambda' \\geq \\lambda_2'^{(2)} = \\frac{\\abs{h_2 -\\rho h_1}}{1 -\\rho}$, $\\hat b_1 \\neq 0$ and $\\hat b_1 = 0$.*\n\n    2.  *When $\\lambda' <\\lambda_2'^{(2)}$, $\\hat b_1 \\neq 0$ and $\\hat b_2 \\neq 0$.*\n:::\n\nWe did not require $\\rho \\geq  0$ in the solution path of Elastic net, but here $\\rho \\geq  0$ is needed to cut down unnecessary discussion. The proof of Elastic net has shown that the solution path of $h_1>\\abs{h_2}$ and $\\rho \\geq  0$ is enough to draw the whole rejection region.\n\nStill requiring $\\rho > 0$, the rejection region looks different for $a > \\frac{2}{1 -\\rho}$ and $a \\leq \\frac{2}{1 -\\rho}$. The first steps are the same as those of Elastic net, and we only present the rejection region here:\n\nWhen $a \\geq \\frac{2}{1 -\\rho}$: $$\\begin{aligned}\n \\label{proof-scad-rjRegion_1}\n  {\\cal R} &= \\{(h_1,h_2): h_1-\\rho h_2 >\\lambda'(1-\\rho),\\, h_1 >\\lambda',\\,h_1 -\\frac{|\\rho|(a -1)}{a - 2}h_2 >\\lambda'(1 -\\frac{a\\rho}{a -2})\\}\\cr \n  &\\;\\; \\cup \\{(h_1,h_2):h_1 -\\rho h_2 >\\lambda',h_2 > a\\lambda'\\}\\cup \\{(h_1, h_2): h_1-\\rho h_2 >\\lambda'(1+\\rho)\\}\\cr\n  &\\;\\; \\cup \\{(h_1, h_2): h_1 - \\frac{\\rho(a- 1)}{a - 2} h_2 > \\lambda' (1 + \\frac{a\\rho}{a - 2}),h_1 -\\rho h_2 >\\lambda'\\}\\cr \n  &\\;\\; \\cup \\{(h_1,h_2): -h_1 +\\rho h_2 >\\lambda'(1-\\rho),\\, h_1 <-\\lambda',\\, -h_1 +\\frac{|\\rho|(a -1)}{a - 2}h_2 >\\lambda'(1 -\\frac{a\\rho}{a -2})\\}\\cr \n  &\\;\\; \\cup \\{(h_1,h_2): -h_1 +\\rho h_2 >\\lambda',h_2 <- a\\lambda'\\}\\cup \\{(h_1, h_2): -h_1 +\\rho h_2 >\\lambda'(1+\\rho)\\}\\cr\n  &\\;\\; \\cup \\{(h_1, h_2): -h_1 + \\frac{\\rho(a- 1)}{a - 2} h_2 > \\lambda' (1 + \\frac{a\\rho}{a - 2}), -h_1 +\\rho h_2 >\\lambda'\\}\n\\end{aligned}$$\n\nWhen $a \\leq \\frac{2}{1 -{\\rho}}$: $$\\begin{aligned}\n \\label{proof-scad-rjRegion_2}\n  {\\cal R} &= \\{(h_1,h_2): h_1-\\rho h_2 >\\lambda'(1-\\rho),\\, h_1 >\\lambda',\\,h_1 >\\frac{1 +\\rho}{2}h_2 \\}\\cr \n  &\\;\\; \\cup \\{(h_1,h_2):h_1 -\\rho h_2 >\\lambda',h_2 > \\frac{2\\lambda'}{1 -\\rho} \\}\\cup \\{(h_1, h_2): h_1-\\rho h_2 >\\lambda'(1+\\rho)\\}\\cr\n  &\\;\\; \\cup \\{(h_1, h_2): h_1 - \\frac{\\rho(a- 1)}{a - 2} h_2 > \\lambda' (1 + \\frac{a\\rho}{a - 2}),h_1 -\\rho h_2 >\\lambda'\\}\\cr \n  &\\;\\; \\cup \\{(h_1,h_2): -h_1 +\\rho h_2 >\\lambda'(1-\\rho),\\, h_1 <-\\lambda',\\,h_1 <\\frac{1 +\\rho}{2}h_2 \\}\\cr \n  &\\;\\; \\cup \\{(h_1,h_2): -h_1 +\\rho h_2 >\\lambda',h_2 <- \\frac{2\\lambda'}{1 -\\rho} \\}\\cup \\{(h_1, h_2): -h_1 +\\rho h_2 >\\lambda'(1+\\rho)\\}\\cr\n  &\\;\\; \\cup \\{(h_1, h_2): -h_1 + \\frac{\\rho(a- 1)}{a - 2} h_2 > \\lambda' (1 + \\frac{a\\rho}{a - 2}), -h_1 +\\rho h_2 >\\lambda'\\}\\cr \n\\end{aligned}$$\n\nWhen $\\rho <0$, we apply the same sign-flipping technique in the proof of Elastic net and still use the rejection region of positive correlation. Such technique requires considering one more case for $\\rho < 0$, which is the elliptical distance from the center $\\mu_{11} =((1 -\\abs{\\rho})\\sqrt{r}, -(1 -\\abs{\\rho})\\sqrt{r})$ to ${\\cal R}^c$ (plotted with positive correlation $\\abs{\\rho} > 0$).\n\n#### Part 2. Analyzing the Hamming error.\n\nwe allow $\\rho\\in( -1,1)$ from now on. The analysis of Hamming error follows the same procedure as that of Elastic net. It is only the shape of $\\cal R$ which is different. For $a \\geq \\frac{2}{1 -|\\rho|}$ and $a \\leq \\frac{2}{1 -|\\rho|}$, we respectively present a theorem for the Hamming error rate.\n\n::: {#suppthm:hamm.scad.1 .theorem}\n**Theorem 10**. *Suppose the conditions of Theorem [3](#thm:SCAD){reference-type=\"ref\" reference=\"thm:SCAD\"} hold. Let $\\lambda=\\lambda'\\sqrt{2\\log(p)}$ in SCAD. Define a few important points in the rejection region (as noted in Figure [7](#suppfig:hamm.scad.large.a){reference-type=\"ref\" reference=\"suppfig:hamm.scad.large.a\"}): $A(\\lambda',\\lambda'),\\,B((1 +|\\rho|)\\lambda',2\\lambda'),\\,C((1 + a|\\rho|)\\lambda',a\\lambda'),\\, D((1 -|\\rho|)\\lambda', - 2\\lambda')$. As $p\\to\\infty$, $$\\ensuremath{\\mathrm{FP}}_p=L_p p^{1- \\min\\bigl\\{ \\lambda'^2, \\;\\; \\vartheta+ f_1(\\sqrt{r}, \\lambda')\\bigr\\}}, \\qquad \\ensuremath{\\mathrm{FN}}_p = L_p p^{1-\\min\\bigl\\{\\vartheta+ f_2(\\sqrt{r}, \\lambda'),\\;\\; 2\\vartheta+ f_3(\\sqrt{r}, \\lambda')\\bigr\\}},$$ where (below, $d^2_{|\\rho|}(u,v)$ is as in Definition [1](#def:EllipsDistance){reference-type=\"ref\" reference=\"def:EllipsDistance\"}) $$\\begin{aligned}\n   f_1(\\sqrt{r},\\lambda') & = \\begin{cases} \n    (\\lambda'-|\\rho| \\sqrt{r})^2 & \\text{if }\\sqrt{r} \\leq \\frac{\\lambda'}{1 +|\\rho|} \\\\\n    \\frac{1}{1 -|\\rho|^2}d_{|\\rho|}^2(A, (|\\rho| \\sqrt{r},\\sqrt{r})) & \\text{if } \\frac{\\lambda'}{1+|\\rho|} \\leq \\sqrt{r} \\leq \\lambda' \\\\\n    \\frac{1 -|\\rho|}{1 +|\\rho|}\\lambda'^2 & \\text{if }\\lambda' \\leq \\sqrt{r} \\leq 2\\lambda' \\\\\n    \\min \\{ \\frac{\\lambda'^2}{1 -\\rho^2},\\frac{1}{1 -\\rho^2} d^2(B, (|\\rho| \\sqrt{r},\\sqrt{r}))\\} & \\text{if }2\\lambda' \\leq \\sqrt{r} \\leq \\lambda'\\left[2 + \\frac{|\\rho| -\\rho^2}{(a - 2)(1 +|\\rho|)}\\right] \\\\\n    \\min\\left\\{\\frac{\\lambda'^2}{1 -\\rho^2} ,\\ \\frac{\\left[ \\lambda'(1 -\\frac{a|\\rho|}{a - 2}) +\\frac{|\\rho| \\sqrt{r}}{a - 2} \\right]^2}{1 +\\frac{\\rho^2(a - 1)^2}{(a - 2)^2} -\\frac{2\\rho^2(a - 1)}{a - 2}}\\right\\} & \\text{if } \\sqrt{r} \\geq \\lambda'\\left[2 + \\frac{|\\rho| -\\rho^2}{(a - 2)(1 +|\\rho|)}\\right]\n  \\end{cases} \\cr\n  f_2(\\sqrt{r}, \\lambda') &= \n  \\begin{cases} \n    \\min \\begin{cases} \n      (\\sqrt{r} -\\lambda')_ +^2 \\\\\n      \\frac{1}{1 -\\rho^2}\\left[(1 -\\rho^2)\\sqrt{r} -(1 -|\\rho|)\\lambda'\\right] \\\\\n      \\frac{1}{1 +\\frac{\\rho^2(a - 1)^2}{(a - 2)^2} -\\frac{2\\rho^2(a - 1)}{a - 2}}\\left[\\lambda'\\left(1 - \\frac{a|\\rho|}{a - 2}\\right) - \\sqrt{r} \\cdot \\left(1 -\\frac{\\rho^2(a - 1)}{a - 2}\\right)\\right]^2\n      \\end{cases} \\text{if }\\sqrt{r} \\leq \\frac{a(a - 2)(1 -\\rho^2) +|\\rho|}{|\\rho|(a - 1)(1 - \\rho^2)}\\lambda'\\\\\n      \\frac{1}{1 -\\rho^2}d_{|\\rho|}^2(C, (\\sqrt{r},|\\rho| \\sqrt{r})) \\qquad\\qquad\\qquad \\text{if } \\frac{a(a - 2)(1 -\\rho^2) +|\\rho|}{|\\rho|(a - 1)(1 - \\rho^2)} \\lambda'\\leq \\sqrt{r} \\leq \\frac{a\\lambda'}{|\\rho|} \\\\\n      \\frac{1}{1 -\\rho^2}\\left[(1 -\\rho^2)\\sqrt{r}-\\lambda'\\right]^2 \\qquad\\qquad\\quad\\ \\text{if }\\sqrt{r} \\geq \\frac{a\\lambda'}{|\\rho|}\n  \\end{cases} \n\\end{aligned}$$ The definition of $f_3(\\sqrt{r},\\lambda')$ is different for different signs of $\\rho$. When $\\rho > 0$: $$\\begin{aligned}\n  f_3(\\sqrt{r}, \\lambda') &= \\frac{1}{1 -\\rho^2} \\cdot \\begin{cases} \n    \\min \\begin{cases} \n      \\left[(1 -\\rho^2)\\sqrt{r} -(1 -\\rho)\\lambda'\\right]_ +^2 \\\\\n      h(\\sqrt{r},\\lambda')\n      \\end{cases} & \\text{  if } \\sqrt{r} \\leq \\frac{a\\lambda'}{1+\\rho} \\\\\n      \\left[(1 -\\rho^2)\\sqrt{r}-\\lambda'\\right]^2 & \\text{  if } \\sqrt{r} \\geq \\frac{a\\lambda'}{1 +\\rho}\n  \\end{cases} \n\\end{aligned}$$ where $$h(\\sqrt{r},\\lambda') = \\begin{cases} \n    \\frac{ (1 -\\rho^2)}{1 +\\frac{\\rho^2(a - 1)^2}{(a - 2)^2} -\\frac{2\\rho^2(a - 1)}{a - 2}}\\left[\\lambda'\\left(1 - \\frac{a\\rho}{a - 2}\\right) - (1 +\\rho)\\sqrt{r} \\cdot \\left(1 -\\frac{\\rho(a - 1)}{a - 2}\\right)\\right]^2 & \\text{ if } \\sqrt{r} \\leq \\frac{\\lambda'}{1 +\\rho} \\cdot \\frac{a(a - 2)(1 -\\rho^2) +\\rho}{(a - 2)(1 -\\rho^2) +\\rho -\\rho^2} \\\\\n    d^2(C, ((1 +\\rho)\\sqrt{r},(1 +\\rho) \\sqrt{r})) & \\text{ if } \\sqrt{r} \\geq \\frac{\\lambda'}{1 +\\rho} \\cdot \\frac{a(a - 2)(1 -\\rho^2) +\\rho}{(a - 2)(1 -\\rho^2) +\\rho -\\rho^2}\n  \\end{cases}$$ When $\\rho < 0$, $$\\begin{aligned}\n  f_3(\\sqrt{r},\\lambda') =\\frac{1}{1 -\\rho^2} \\cdot  \\begin{cases} \n      \\left[(1 -\\rho^2)\\sqrt{r}-(1+|\\rho|)\\lambda'\\right]^2 & \n      \\text{if } \\sqrt{r} \\leq \\frac{2\\lambda'}{1 -|\\rho|} \\\\\n      \\min \\begin{cases} \n        \\left[(1 -\\rho^2)\\sqrt{r} -\\lambda'\\right]^2\\\\\n        k(\\lambda',a)\n      \\end{cases} & \\text{if } \\sqrt{r} \\geq \\frac{2\\lambda'}{1 -|\\rho|}\n    \\end{cases}\n\\end{aligned}$$ where $$k(\\lambda',a) = \\begin{cases} \n    d^2\\left(D,\\left((1 -|\\rho|)\\sqrt{r},-(1-|\\rho|)\\sqrt{r}\\right)\\right) \\qquad \\text{   if } \\frac{2\\lambda'}{1 -|\\rho|} \\leq \\sqrt{r} \\leq \\frac{\\lambda'}{1 -|\\rho|}\\left[2 + \\frac{|\\rho| +\\rho^2}{(a - 2)(1 -\\rho^2) - (|\\rho| +\\rho^2)} \\right]\\\\\n    \\frac{ (1 -\\rho^2)}{1 +\\frac{\\rho^2(a - 1)^2}{(a - 2)^2} -\\frac{2\\rho^2(a - 1)}{a - 2}}\\left[ -\\lambda'\\left(1 + \\frac{a|\\rho|}{a - 2}\\right) + (1 -|\\rho|)\\sqrt{r} \\cdot \\left(1 +\\frac{|\\rho|(a - 1)}{a - 2}\\right)\\right]^2 \\\\\n    \\qquad\\qquad\\qquad{if } \\sqrt{r} \\geq \\frac{\\lambda'}{1 -|\\rho|}\\left[2 + \\frac{|\\rho| +\\rho^2}{(a - 2)(1 -\\rho^2) - (|\\rho| +\\rho^2)} \\right]\n  \\end{cases}$$*\n:::\n\n::: proof\n*Proof of Theorem [10](#suppthm:hamm.scad.1){reference-type=\"ref\" reference=\"suppthm:hamm.scad.1\"}.* See the rejection region in Figure [7](#suppfig:hamm.scad.large.a){reference-type=\"ref\" reference=\"suppfig:hamm.scad.large.a\"} for a visualization of the rejection region.\n\n![When $a > \\frac{2}{1 -|\\rho|}$, the rejection region looks like this](Figure-supp/scad_rej_region_large_a_ps.pdf){#suppfig:hamm.scad.large.a width=\"110%\"}\n\n*The ellipsoid centered at the point $\\mu_{00}$*: Easy to see the rate is $L_p \\cdot p^{1 -\\lambda'^2}$.\n\n*The ellipsoid centered at the point $\\mu_{01}$* : We set out to find out $f_1(\\sqrt{r},\\lambda')$. Similar to Lasso, we have: (i) When $\\sqrt{r} \\leq \\frac{\\lambda'}{1 +|\\rho|}$, $f_1(\\sqrt{r},\\lambda') =(\\lambda'-|\\rho| \\sqrt{r})^2$. (ii) When $\\frac{\\lambda'}{1 +|\\rho|} \\leq \\sqrt{r} \\leq \\lambda'$, $f_1(\\sqrt{r},\\lambda') = \\frac{1}{1 -\\rho^2} d^2(A, (|\\rho| \\sqrt{r},\\sqrt{r}))$. The point $A$ is noted in Figure [7](#suppfig:hamm.scad.large.a){reference-type=\"ref\" reference=\"suppfig:hamm.scad.large.a\"}. (iii) When $\\lambda' \\leq \\sqrt{r} \\leq 2\\lambda'$, $f_1(\\sqrt{r},\\lambda') = \\frac{1 -|\\rho|}{1 +|\\rho|}\\lambda'^2$.\n\nThen we need to investigate the green segment in Figure [7](#suppfig:hamm.scad.large.a){reference-type=\"ref\" reference=\"suppfig:hamm.scad.large.a\"}. When the ellipsoid is tangent to the green segment on the *right* side (i.e. $BC$), and the tangent point is above Point $B$, then using Lemma [1](#supplem:distance){reference-type=\"ref\" reference=\"supplem:distance\"}, $$\\sqrt{r} +\\frac{ -\\frac{|\\rho|}{a - 2}\\left[\\lambda'\\left(1 - \\frac{a|\\rho|}{a - 2}\\right) + \\sqrt{r} \\cdot \\frac{|\\rho|}{a - 2}\\right]}{1 +\\frac{\\rho^2(a - 1)^2}{(a - 2)^2} -\\frac{2\\rho^2(a - 1)}{a - 2}} \\geq 2\\lambda'$$ which implies $\\sqrt{r} \\geq \\lambda'\\left[2 + \\frac{|\\rho| -\\rho^2}{(a - 2)(1 +|\\rho|)}\\right]$\n\nWhen $\\sqrt{r} \\geq \\lambda'\\left[2 + \\frac{|\\rho| -\\rho^2}{(a - 2)(1 +\\rho)}\\right]$, the ellipsoid either intersects with the green line segment $BC$, or the red segment beyond $C$. However, we need to eliminate the possibility of the ellipsoid having a smaller radius when tangent to the segments on the left.\n\nWe will see that the line segments on the left can indeed be eliminated. This is because when the ellipsoid is tangent to both the green lines on the left and right, $$\\begin{aligned}\n  \\frac{(1 -\\rho^2)\\left[ \\lambda'(1 -\\frac{a|\\rho|}{a - 2}) +\\frac{|\\rho| \\sqrt{r}}{a - 2} \\right]^2}{1 +\\frac{\\rho^2(a - 1)^2}{(a - 2)^2} -\\frac{2\\rho^2(a - 1)}{a - 2}} =&~ \\frac{(1 -\\rho^2)\\left[ -\\lambda'(1 +\\frac{a|\\rho|}{a - 2}) +\\frac{|\\rho| \\sqrt{r}}{a - 2} \\right]^2}{1 +\\frac{\\rho^2(a - 1)^2}{(a - 2)^2} -\\frac{2\\rho^2(a - 1)}{a - 2}}\\\\\n  \\implies \\lambda'(1 -\\frac{a|\\rho|}{a - 2}) +\\frac{|\\rho| \\sqrt{r}}{a - 2} =&~ \\lambda'(1 +\\frac{a|\\rho|}{a - 2}) -\\frac{|\\rho| \\sqrt{r}}{a - 2} \\implies \\sqrt{r} = a\\lambda\n\\end{aligned}$$ As a result, when $\\sqrt{r} \\leq a\\lambda$, we can ignore the possibility of the ellipsoid intersecting the green or red segments on the left side. When $\\sqrt{r} \\geq a\\lambda$, the right side still has the smaller distance.\n\nwhen $2\\lambda' \\leq \\sqrt{r} \\leq  \\lambda'\\left[2 + \\frac{|\\rho| -\\rho^2}{(a - 2)(1 +|\\rho|)}\\right]$, $f_1(\\sqrt{r},\\lambda') = \\frac{1}{1 -\\rho^2} d^2(B, (|\\rho| \\sqrt{r},\\sqrt{r}))$. The point $B$ is noted in Figure [7](#suppfig:hamm.scad.large.a){reference-type=\"ref\" reference=\"suppfig:hamm.scad.large.a\"}.\n\nwhen $\\sqrt{r} \\geq  \\lambda'\\left[2 + \\frac{|\\rho| -\\rho^2}{(a - 2)(1 +|\\rho|)}\\right]$, $$f_1(\\sqrt{r},\\lambda') = \\frac{1}{1 -\\rho^2} \\min\\left\\{\\lambda'^2,\\ \\frac{(1 -\\rho^2)\\left[ \\lambda'(1 -\\frac{a|\\rho|}{a - 2}) +\\frac{|\\rho| \\sqrt{r}}{a - 2} \\right]^2}{1 +\\frac{\\rho^2(a - 1)^2}{(a - 2)^2} -\\frac{2\\rho^2(a - 1)}{a - 2}}\\right\\}.$$\n\n*The ellipsoid centered at the $\\mu_{10}$*: We only explain one tricky point: When the tangent point to the segment $BC$ is precisely Point $C$, $$|\\rho|\\sqrt{r} +\\frac{ -\\frac{|\\rho|}{a - 2}\\left[\\lambda'\\left(1 - \\frac{a|\\rho|}{a - 2}\\right) - \\sqrt{r} \\cdot \\left(1 -\\frac{\\rho^2(a - 1)}{a - 2}\\right)\\right]}{1 +\\frac{\\rho^2(a - 1)^2}{(a - 2)^2} -\\frac{2\\rho^2(a - 1)}{a - 2}} = a\\lambda'$$ then $\\sqrt{r} = \\frac{a(a - 2)(1 -\\rho^2) +|\\rho|}{|\\rho|(a - 1)(1 - \\rho^2)}$.\n\n*The ellipsoid centered at $\\mu_{11} =\\left((1 +\\rho)\\sqrt{r},(1 +\\rho)\\sqrt{r}\\right)$, when $\\rho > 0$* : We still explain only one important point: When the ellipsoid is tangent to the green segment precisely at Point $C$, $$(1 +\\rho)\\sqrt{r} +\\frac{ -\\frac{\\rho}{a - 2}\\left[\\lambda'\\left(1 - \\frac{a\\rho}{a - 2}\\right) - (1 +\\rho)\\sqrt{r} \\cdot \\left(1 -\\frac{\\rho(a - 1)}{a - 2}\\right)\\right]}{1 +\\frac{\\rho^2(a - 1)^2}{(a - 2)^2} -\\frac{2\\rho^2(a - 1)}{a - 2}} = a\\lambda'$$ then $(1 +\\rho)\\sqrt{r} =\\frac{a(a - 2)(1 -\\rho^2) +\\rho}{(a - 2)(1 -\\rho^2) +\\rho -\\rho^2}\\lambda'$.\n\n*The ellipsoid centered at $\\mu_{11} = \\left((1 -|\\rho|)\\sqrt{r}, -(1 -|\\rho|)\\sqrt{r}\\right)$, when $\\rho < 0$* : We explain one important point: when the ellipsoid is tangent to the green segment at the Point $D$. Now we have $$-(1 -|\\rho|)\\sqrt{r} +\\frac{ -\\frac{|\\rho|}{a - 2}\\left[\\lambda'\\left(1 + \\frac{a|\\rho|}{a - 2}\\right) - (1 -|\\rho|)\\sqrt{r} \\cdot \\left(1 +\\frac{|\\rho|(a - 1)}{a - 2}\\right)\\right]}{1 +\\frac{\\rho^2(a - 1)^2}{(a - 2)^2} -\\frac{2\\rho^2(a - 1)}{a - 2}} = - 2\\lambda'$$ then $(1 -|\\rho|)\\sqrt{r} =\\lambda'\\left[2 + \\frac{|\\rho| +\\rho^2}{(a - 2)(1 -\\rho^2) - (|\\rho| +\\rho^2)} \\right]$.\n\nNote that even when $\\frac{2\\lambda'}{1 -|\\rho|} \\leq \\sqrt{r} \\leq \\frac{\\lambda'}{1 -|\\rho|}\\left[2 + \\frac{|\\rho| +\\rho^2}{(a - 2)(1 -\\rho^2) - (|\\rho| +\\rho^2)} \\right]$, and the ellipsoid intersects with the rejection region at Point $D$, it may be tangent to the red segment without being tangent to the green segment. This is especially true when $a < 2 +\\frac{|\\rho|}{1 -|\\rho|}$ (but this only happens when $a < \\frac{2}{1 -|\\rho|}$, the next section.) ◻\n:::\n\n::: {#suppthm:hamm.scad.2 .theorem}\n**Theorem 11**. *Suppose the conditions of Theorem [3](#thm:SCAD){reference-type=\"ref\" reference=\"thm:SCAD\"} hold. Let $\\lambda=\\lambda'\\sqrt{2\\log(p)}$ in SCAD. Define a few important points (as noted in the rejection region in Figure [8](#suppfig:hamm.scad.small.a){reference-type=\"ref\" reference=\"suppfig:hamm.scad.small.a\"}): $A(\\lambda',\\lambda'),\\,B((1 +|\\rho|)\\lambda',2\\lambda'),\\,C(\\frac{1 + |\\rho|}{1 -|\\rho|} \\lambda',\\frac{2\\lambda'}{1 -|\\rho|} ),\\, D((1 -|\\rho|)\\lambda', - 2\\lambda')$. As $p\\to\\infty$, $$\\ensuremath{\\mathrm{FP}}_p=L_p p^{1- \\min\\bigl\\{ \\lambda'^2, \\;\\; \\vartheta+ f_1(\\sqrt{r}, \\lambda')\\bigr\\}}, \\qquad \\ensuremath{\\mathrm{FN}}_p = L_p p^{1-\\min\\bigl\\{\\vartheta+ f_2(\\sqrt{r}, \\lambda'),\\;\\; 2\\vartheta+ f_3(\\sqrt{r}, \\lambda')\\bigr\\}},$$ where (below, $d^2_{|\\rho|}(u,v)$ is as in Definition [1](#def:EllipsDistance){reference-type=\"ref\" reference=\"def:EllipsDistance\"}) $$\\begin{aligned}\n   f_1(\\sqrt{r},\\lambda') & =\\begin{cases} \n    (\\lambda'-|\\rho| \\sqrt{r})^2 & \\text{if }\\sqrt{r} \\leq \\frac{\\lambda'}{1 +|\\rho|} \\\\\n    \\frac{1}{1 -\\rho^2}d^2(A, (|\\rho| \\sqrt{r},\\sqrt{r})) & \\text{if } \\frac{\\lambda'}{1+|\\rho|} \\leq \\sqrt{r} \\leq \\lambda' \\\\\n    \\frac{1 -|\\rho|}{1 +|\\rho|} \\lambda'^2 & \\text{if }\\lambda' \\leq \\sqrt{r} \\leq 2\\lambda' \\\\\n    \\min\\left\\{ \\frac{\\lambda'^2}{1 -\\rho^2},\\frac{1}{1 -\\rho^2}d^2(B, (|\\rho| \\sqrt{r},\\sqrt{r}))  \\right\\} & \\text{if }2\\lambda' \\leq \\sqrt{r} \\leq \\frac{5 + 3|\\rho|}{2 + 2|\\rho|} \\lambda' \\\\\n    \\min\\left\\{\\frac{\\lambda'^2}{1 -\\rho^2},\\ \\frac{(1-|\\rho|) r}{(5+3|\\rho|)}\\right\\}  & \\text{if } \\sqrt{r} \\geq \\frac{5 + 3|\\rho|}{2 + 2|\\rho|} \\lambda'\n  \\end{cases}  \\cr\n  f_2(\\sqrt{r}, \\lambda') &= \n  \\begin{cases} \n    \\min \\begin{cases} \n      (\\sqrt{r} -\\lambda')_ +^2 \\\\\n      \\frac{1}{1 -\\rho^2}\\left[(1 -\\rho^2)\\sqrt{r} -(1 -|\\rho|)\\lambda'\\right]^2 \\\\\n      \\frac{(1 -|\\rho|)(2 +|\\rho|)^2}{5 + 3|\\rho|} r\n      \\end{cases} & \n       \\text{if }\\sqrt{r} \\leq \\frac{5 + 3|\\rho|}{(1 -|\\rho|)(1 +|\\rho|)^2}\\lambda'\\\\\n      \\frac{1}{1 -\\rho^2}d^2(C, (\\sqrt{r},|\\rho| \\sqrt{r})) & \\text{if } \\frac{5 + 3|\\rho|}{(1 -|\\rho|)(1 +|\\rho|)^2}\\lambda' \\leq \\sqrt{r} \\leq \\frac{2\\lambda'}{|\\rho|(1-|\\rho|)} \\\\\n      \\frac{1}{1 -\\rho^2}\\left[(1 -\\rho^2)\\sqrt{r}-\\lambda'\\right]^2& \\text{if }\\sqrt{r} \\geq \\frac{2\\lambda'}{|\\rho|(1-|\\rho|)}\n  \\end{cases} \n\\end{aligned}$$ The definition of $f_3(\\sqrt{r},\\lambda')$ is different for different signs of $\\rho$. When $\\rho > 0$: $$\\begin{aligned}\n  f_3(\\sqrt{r}, \\lambda') &= \\frac{1}{1 -\\rho^2} \\cdot \\begin{cases} \n    \\min \\begin{cases} \n      \\left[(1 -\\rho^2)\\sqrt{r} -(1 -\\rho)\\lambda'\\right]_ +^2 \\\\\n      h(\\sqrt{r},\\lambda')\n      \\end{cases} & \\text{  if } \\sqrt{r} \\leq \\frac{2\\lambda'}{1-\\rho^2} \\\\\n      \\left[(1 -\\rho^2)\\sqrt{r}-\\lambda'\\right]^2 & \\text{  if } \\sqrt{r} \\geq \\frac{2\\lambda'}{1 -\\rho^2}\n  \\end{cases} \n\\end{aligned}$$ where $$h(\\sqrt{r},\\lambda') = \\begin{cases} \n    \\frac{(1 -\\rho^2)^2(1 +\\rho)}{5 + 3\\rho}r & \\text{ if } \\sqrt{r} \\leq \\frac{\\lambda'}{1 +\\rho} \\cdot \\frac{5 + 3\\rho}{(1 -\\rho)(3 +\\rho)} \\\\\n    d^2(C, ((1 +\\rho)\\sqrt{r},(1 +\\rho) \\sqrt{r})) & \\text{ if } \\sqrt{r} \\geq \\frac{\\lambda'}{1 +\\rho} \\cdot \\frac{5 + 3\\rho}{(1 -\\rho)(3 +\\rho)}\n  \\end{cases}$$ When $\\rho < 0$, $$\\begin{aligned}\n  f_3(\\sqrt{r},\\lambda') =\\frac{1}{1 -\\rho^2} \\cdot  \\begin{cases} \n      \\left[(1 -\\rho^2)\\sqrt{r}-(1+|\\rho|)\\lambda'\\right]^2 & \n      \\text{if } \\sqrt{r} \\leq \\frac{2\\lambda'}{1 -|\\rho|} \\\\\n      \\min \\begin{cases} \n        \\left[(1 -\\rho^2)\\sqrt{r} -\\lambda'\\right]^2\\\\\n        k(\\lambda',a)\n      \\end{cases} & \\text{if } \\sqrt{r} \\geq \\frac{2\\lambda'}{1 -|\\rho|}\n    \\end{cases}\n\\end{aligned}$$ where $$k(\\lambda',a) = \\begin{cases} \n    d^2\\left(D,\\left((1 -|\\rho|)\\sqrt{r},-(1-|\\rho|)\\sqrt{r}\\right)\\right) \\qquad \\text{   if } \\frac{2\\lambda'}{1 -|\\rho|} \\leq \\sqrt{r} \\leq \\frac{\\lambda'}{1 -|\\rho|}\\left[2 + \\frac{|\\rho| +\\rho^2}{(a - 2)(1 -\\rho^2) - (|\\rho| +\\rho^2)} \\right]\\\\\n    \\frac{ (1 -\\rho^2)}{1 +\\frac{\\rho^2(a - 1)^2}{(a - 2)^2} -\\frac{2\\rho^2(a - 1)}{a - 2}}\\left[ -\\lambda'\\left(1 + \\frac{a|\\rho|}{a - 2}\\right) + (1 -|\\rho|)\\sqrt{r} \\cdot \\left(1 +\\frac{|\\rho|(a - 1)}{a - 2}\\right)\\right]^2 \\\\\n    \\qquad\\qquad\\qquad{if } \\sqrt{r} \\geq \\frac{\\lambda'}{1 -|\\rho|}\\left[2 + \\frac{|\\rho| +\\rho^2}{(a - 2)(1 -\\rho^2) - (|\\rho| +\\rho^2)} \\right]\n  \\end{cases}$$*\n:::\n\n::: proof\n*Proof of Theorem [11](#suppthm:hamm.scad.2){reference-type=\"ref\" reference=\"suppthm:hamm.scad.2\"}.* See the different rejection region in Figure [8](#suppfig:hamm.scad.small.a){reference-type=\"ref\" reference=\"suppfig:hamm.scad.small.a\"}.\n\n![When $a \\leq  \\frac{2}{1 -|\\rho|}$, the rejection region looks like this](Figure-supp/scad_rej_region_small_a_ps.pdf){#suppfig:hamm.scad.small.a width=\"110%\"}\n\n*The ellipsoid centered at $\\mu_{00}$:* The rate is $L_p \\cdot p^{1 -\\lambda'^2}$\n\n*The ellipsoid centered at $\\mu_{01}$:* Still similar to Lasso, we have\n\n-   when $\\sqrt{r} \\leq \\frac{\\lambda'}{1 +|\\rho|}$, $f_1(\\sqrt{r},\\lambda') = (\\lambda'-|\\rho| \\sqrt{r})^2$.\n\n-   when $\\frac{\\lambda'}{1 +|\\rho|} \\leq \\sqrt{r} \\leq \\lambda'$, $f_1(\\sqrt{r},\\lambda') = \\frac{1}{1 -\\rho^2} d^2(A, (|\\rho| \\sqrt{r},\\sqrt{r}))$. The point $A$ has been defined in Theorem [11](#suppthm:hamm.scad.2){reference-type=\"ref\" reference=\"suppthm:hamm.scad.2\"} and noted in Figure [8](#suppfig:hamm.scad.small.a){reference-type=\"ref\" reference=\"suppfig:hamm.scad.small.a\"}.\n\n-   when $\\lambda' \\leq \\sqrt{r} \\leq 2\\lambda'$, $f_1(\\sqrt{r},\\lambda')= \\lambda'^2$.\n\nThen we need to investigate the green segment in Figure [8](#suppfig:hamm.scad.small.a){reference-type=\"ref\" reference=\"suppfig:hamm.scad.small.a\"}. When the ellipsoid is tangent to the green segment on the *right* side (i.e. $BC$), and the tangent point is above Point $B$, then using Lemma [1](#supplem:distance){reference-type=\"ref\" reference=\"supplem:distance\"}, $$\\sqrt{r} - \\frac{\\frac{1}{4}(1 -|\\rho|)^2 \\sqrt{r}}{1 +(\\frac{1 +|\\rho|}{2} )^2 -|\\rho|(1 +|\\rho|)} \\geq 2\\lambda'$$ which implies $\\sqrt{r} \\geq \\frac{5 + 3|\\rho|}{2 + 2|\\rho|} \\lambda'.$\n\nWhen $\\sqrt{r} \\geq \\frac{5 + 3|\\rho|}{2 + 2|\\rho|} \\lambda'$, the ellipsoid either intersects with the green line segment $BC$, or the red segment beyond $C$. However, we need to eliminate the possibility of the ellipsoid having a smaller radius when tangent to the segments on the left.\n\nActually, we will see that the line segments on the left can indeed be eliminated, without doing any computation. The case of $a \\leq \\frac{2}{1 -|\\rho|}$ is a degenerate case, as we have $\\frac{|\\rho|(a - 1)}{a - 2} \\geq \\frac{1 +|\\rho|}{2}$. when $a \\leq \\frac{2}{1 -|\\rho|}$. From the computation in the $a > \\frac{2}{1 -|\\rho|}$ counterpart, the green and red segments on the left sides can be ignored.\n\nSo we can continue the discussion and present the rest two cases:\n\n-   when $2\\lambda' \\leq \\sqrt{r} \\leq  \\frac{5 + 3|\\rho|}{2 + 2|\\rho|} \\lambda'$, $\\text{rate} = d^2(B, (|\\rho| \\sqrt{r},\\sqrt{r}))$. The point $B$ is noted in Figure [8](#suppfig:hamm.scad.small.a){reference-type=\"ref\" reference=\"suppfig:hamm.scad.small.a\"}.\n\n-   when $\\sqrt{r} \\geq  \\frac{5 + 3|\\rho|}{2 + 2|\\rho|} \\lambda'$, $$\\text{rate} = \\min\\left\\{\\lambda'^2,\\ \\frac{(1 -|\\rho|^2)(1-|\\rho|) r}{(5+3|\\rho|)}\\right\\}$$\n\n*The ellipsoid centered at $\\mu_{10}$* : Only one special point needs to be investigated. When the tangent point to the segment $BC$ is precisely Point $C\\left( \\frac{1 +|\\rho|}{1 -|\\rho|}\\lambda',\\ \\frac{2\\lambda'}{1 -|\\rho|} \\right)$, $$|\\rho|\\sqrt{r} +\\frac{ -\\frac{1 -|\\rho|}{2}\\left[\\frac{|\\rho|(1 +|\\rho|)}{2} -1\\right]\\sqrt{r}}{1 +(\\frac{1 +|\\rho|}{2} )^2 -|\\rho|(1 +|\\rho|)} = \\frac{2\\lambda'}{1 -|\\rho|}$$ then $\\sqrt{r} = \\frac{5 + 3|\\rho|}{(1 -|\\rho|)(1 +|\\rho|)^2}\\lambda'$.\n\n*The ellipsoid centered at $\\mu_{11} =\\left((1 +\\rho)\\sqrt{r},(1 +\\rho)\\sqrt{r}\\right)$, when $\\rho > 0$.* We explain one special point: When the ellipsoid is tangent to the green segment precisely at Point $C$, $$(1 +\\rho)\\sqrt{r} +\\frac{ \\frac{(1 -\\rho)(1 -\\rho^2)}{4}\\sqrt{r}}{1 +(\\frac{1 +\\rho}{2} )^2 -\\rho(1 +\\rho)} = \\frac{2\\lambda'}{1 -\\rho}$$ then $\\sqrt{r} = \\frac{5 + 3\\rho}{(1 -\\rho^2)(3 +\\rho)}\\lambda'$.\n\n*The ellipsoid centered at $\\mu_{11} =\\left((1 -|\\rho|)\\sqrt{r}, -(1 -|\\rho|)\\sqrt{r}\\right)$, only when $\\rho < 0$*: This case is identical to the counterpart proof for $a \\geq \\frac{2}{1 -|\\rho|}$, and nothing needs to be changed. ◻\n:::\n\n#### Part 3. Calculating the phase diagram, for $a \\leq \\frac{2}{1-|\\rho|}$.\n\nThe boundary between the Regions of Almost Full Recovery and No Recovery is still $r =\\vartheta$, and it can be proven in the same manner as that of Elastic net. For the rest of this part, we focus on the boundary between Exact Recovery and Almost Full Recovery.\n\nWe focus on the case of $a < \\frac{2}{1 -|\\rho|}$, because: First, the phase diagram of SCAD when $a < \\frac{2}{1 -|\\rho|}$ is worse than Lasso's diagram when $\\rho > 0$, and becomes the same as Lasso when $a$ is sufficiently larger than $\\frac{2}{1 -\\rho}$. When $\\rho <0$, the phase diagram is better than that of Lasso when $a \\leq  \\frac{2}{1 -|\\rho|}$, and numerical results show that when $a > \\frac{2}{1 -|\\rho|}$, the diagram is monotonically moving upwards towards Lasso's diagram when $a$ is increasing. Second, when $a <\\frac{2}{1 -|\\rho|}$, Theorem [11](#suppthm:hamm.scad.2){reference-type=\"ref\" reference=\"suppthm:hamm.scad.2\"} does not depend on $a$ in its most part, and is much easier to compute. To sum up, the case of $a > \\frac{2}{1-|\\rho|}$ is much more tedious in computation but less informative.\n\n#### We start from the case of $\\rho > 0$.\n\nBefore diving into the proof, we give an overall account for the diagram:\n\n1.  The diagram is the same as that of Lasso, only except that when $\\rho < 0.179$, there is a tiny difference in a small neighborhood of $\\vartheta= 0$, slightly worse than Lasso. See equation [\\[eq:extra.curve\\]](#eq:extra.curve){reference-type=\"eqref\" reference=\"eq:extra.curve\"}\n\n2.  As long as $a < \\frac{2}{1 -\\rho}$, the phase diagram does not depend on the specific value of $a$.\n\nThen we move on to the proof, which has four cases just like the proof of Elastic net.\n\n*First, $\\lambda'^2 =\\vartheta+ f_2(\\sqrt{r},\\lambda') = 1$ and $\\vartheta+f_1(\\sqrt{r},\\lambda') \\geq 1$,$2\\vartheta+ f_3(\\sqrt{r},\\lambda') \\geq 1$:* We know $\\lambda' = 1$. From the definitin of $f_2(\\sqrt{r},\\lambda')$, we know $\\sqrt{r} \\geq  1 + \\sqrt{1 -\\vartheta} >1$.\n\nThen we start our discussion on the conditional expression of $f_2(\\sqrt{r},\\lambda')$. (Note that numerically, $\\min_\\rho \\frac{5 + 3\\rho}{(1 -\\rho)(1 +\\rho)^2} = 4.848$.)\n\nIf $1 -\\vartheta= \\frac{1}{1 -\\rho^2}d^2(C,(\\rho \\sqrt{r},\\sqrt{r}))$: As we know $d^2(C,(\\rho \\sqrt{r},\\sqrt{r})) \\geq \\frac{(1 -\\rho^2)(1 -\\rho)(2 +\\rho)^2}{5 + 3\\rho} r$, so $\\sqrt{r} \\leq \\sqrt{1 -\\vartheta}\\sqrt{\\frac{5 + 3\\rho}{(1 -\\rho)(2 +\\rho)^2}}$, which contradicts the pre-condition that $\\sqrt{r} \\geq \\frac{5 + 3\\rho}{(1 -\\rho)(1 +\\rho)^2}$. We have no curve in this case. If $1 -\\vartheta= \\frac{1}{1 -\\rho^2}[(1 -\\rho^2)\\sqrt{r} - 1]^2$, then $\\sqrt{r} = \\sqrt{\\frac{1 -\\vartheta}{1 -\\rho^2}} + \\frac{1}{1 -\\rho^2}$. This again contradicts the pre-condition that $\\sqrt{r} \\geq \\frac{2}{\\rho(1 -\\rho)}$. We have no curve in this case.\n\nAs a result, we can only have $1 <\\sqrt{r} <\\frac{5 + 3\\rho}{(1 -\\rho)(1 +\\rho)^2}$ and one of the following three: $$\\begin{aligned}\n    & \\sqrt{r} = 1 + \\sqrt{1 -\\vartheta}\\\\\n    & \\sqrt{r} = \\sqrt{ \\frac{1 -\\vartheta}{1 -\\rho^2}} + \\frac{1}{1 +\\rho} \\\\\n    & \\sqrt{r} = \\sqrt{1 -\\vartheta}\\sqrt{\\frac{5 + 3\\rho}{(1 -\\rho)(2 +\\rho)^2}}\n  \n\\end{aligned}$$ We then discuss the three curves one by one, starting from the last one.\n\n1.  $\\sqrt{r} = \\sqrt{1 -\\vartheta}\\sqrt{\\frac{5 + 3\\rho}{(1 -\\rho)(2 +\\rho)^2}}$. We need to look at $FP_2$ to eliminate this curve.\n\n    When $1 \\leq \\sqrt{r} \\leq 2$, we have $\\sqrt{1 -\\vartheta} \\leq \\sqrt{\\frac{1 -\\rho}{1 +\\rho}}$. In $FN_1$, for the last term to be the minimum among the three $$\\min \\begin{cases} \n          (1 -\\rho^2)(\\sqrt{r} -\\lambda')_ +^2 \\\\\n          \\left[(1 -\\rho^2)\\sqrt{r} -(1 -\\rho)\\lambda'\\right]^2 \\\\\n          \\frac{(1 -\\rho^2)(1 -\\rho)(2 +\\rho)^2}{5 + 3\\rho} r\n          \\end{cases},$$ we need $\\sqrt{1 -\\vartheta}\\sqrt{\\frac{5 + 3\\rho}{(1 -\\rho)(2 +\\rho)^2}} \\geq  \\sqrt{ \\frac{1 -\\vartheta}{1 -\\rho^2}} + \\frac{1}{1 +\\rho}$, which implies $$\\sqrt{\\frac{1 -\\rho}{1 +\\rho}} \\left[\\sqrt{\\frac{5 + 3\\rho}{(1 -\\rho)(2 +\\rho)^2}} -\\frac{1}{\\sqrt{1 -\\rho^2}}\\right] \\geq \\frac{1}{1 +\\rho}.$$ Simplify this for a few steps and we can see the contradiction.\n\n    When $\\sqrt{r} \\geq \\frac{5 + 3\\rho}{2 + 2\\rho}$, we can see the contradiction by simplifying this inequality itself.\n\n    When $2 < \\sqrt{r} < \\frac{5 + 3\\rho}{2 + 2\\rho}$, by looking at $$\\begin{aligned}\n            & \\sqrt{1 -\\vartheta}\\sqrt{\\frac{5 + 3\\rho}{(1 -\\rho)(2 +\\rho)^2}} \\leq \\frac{5 + 3\\rho}{2 + 2\\rho} \\\\\n            & \\sqrt{1 -\\vartheta}\\sqrt{\\frac{5 + 3\\rho}{(1 -\\rho)(2 +\\rho)^2}} \\geq \\frac{1}{1 +\\rho} + \\sqrt{\\frac{1 -\\vartheta}{1 -\\rho^2}}\n          \n    \\end{aligned}$$ we can see that no $\\rho\\in(0,1)$ can admit a possible $\\sqrt{1 -\\vartheta}$.\n\n2.  $\\sqrt{r} = \\frac{1}{1 +\\rho} +\\sqrt{\\frac{1 -\\vartheta}{1 -\\rho^2}}$. We need to look at $FP_2$ to eliminate this curve.\n\n    We already have $\\sqrt{r} \\geq 1$; when $1 < \\sqrt{r} \\leq 2$, from the rate of $FP_2$, we have $1 -\\vartheta\\leq \\frac{1 -\\rho}{1 + \\rho}.$ However, In $FN_1$, for the middle term to be the minimum among the three $$\\min \\begin{cases} \n         (1 -\\rho^2)(\\sqrt{r} -\\lambda')_ +^2 \\\\\n         \\left[(1 -\\rho^2)\\sqrt{r} -(1 -\\rho)\\lambda'\\right]^2 \\\\\n         \\frac{(1 -\\rho^2)(1 -\\rho)(2 +\\rho)^2}{5 + 3\\rho} r\n         \\end{cases},$$ we need $$\\frac{1}{1 +\\rho} +\\sqrt{\\frac{1 -\\vartheta}{1 -\\rho^2}} > 1 + \\sqrt{1 -\\vartheta}.$$ The upper and lower bound of $\\sqrt{1 -\\vartheta}$ would render this case impossible.\n\n    When $2 < \\sqrt{r} \\leq \\frac{5 + 3\\rho}{2 + 2\\rho}$, using the expression of [\\[eq:dist.B\\]](#eq:dist.B){reference-type=\"eqref\" reference=\"eq:dist.B\"}, we need $$\\rho^2 r - 2(1 +\\rho)\\sqrt{r} + 4 \\geq 0$$ in which we use $\\sqrt{r}$ to express $\\sqrt{1 -\\vartheta}$. By letting $\\sqrt{r} = 2$ or $\\frac{5 + 3\\rho}{2 + 2\\rho}$, we can see they are both negative, so we have a contradiction.\n\n    When $\\sqrt{r} > \\frac{5 + 3\\rho}{2 + 2\\rho}$, we have $$\\begin{aligned}\n         & \\frac{1}{1 +\\rho} + \\sqrt{\\frac{1 -\\vartheta}{1 -\\rho^2}} >  \\frac{5 + 3\\rho}{2 + 2\\rho} \\\\\n          & \\sqrt{\\frac{5 + 3\\rho}{1 -\\rho}}\\sqrt{1 -\\vartheta} \\leq \\frac{1}{1 +\\rho} + \\sqrt{\\frac{1 -\\vartheta}{1 -\\rho^2}}\n         \n    \\end{aligned}$$ and the upper bound on $\\sqrt{1 -\\vartheta}$ is even smaller than the lower bound. Contradiction.\n\nNow we are only left with $\\sqrt{r} = 1 + \\sqrt{1 -\\vartheta}$. We need it to meet the following requirements: $$\\begin{cases} \n  1 + \\sqrt{1 -\\vartheta} \\geq \\frac{1}{1 + p} + \\sqrt{\\frac{1 -\\vartheta}{1 -\\rho^2}} & \\text{for it to be the smallest among the three}\\\\\n  1 + \\sqrt{1 -\\vartheta} \\geq \\sqrt{1 -\\vartheta}\\sqrt{\\frac{5 + 3\\rho}{(1 -\\rho)(2 +\\rho)^2}} & \\text{for it to be the smallest among the three}\\\\\n  1 + \\sqrt{1 -\\vartheta} \\leq \\frac{5 + 3\\rho}{(1 -\\rho)(1 +\\rho)^2} & \\text{pre-condition; not restrictive} \\\\\n  \\vartheta>\\frac{2\\rho}{1 +\\rho}  & \\text{for \\( \\vartheta+ f_1(\\sqrt{r},\\lambda') \\geq 1 \\) } \\\\\n  2\\vartheta+ f_3(\\sqrt{r},\\lambda') \\geq 1\n  \\end{cases}$$ Among the first 4 requirements, the fourth one is can imply all of the rest. Then we look at $FN_{2}$, and show it is always $o(1)$ when $\\vartheta>\\frac{2\\rho}{1 +\\rho}$. When $\\rho > \\frac{1}{3}$, $\\vartheta> \\frac{1}{2}$; when $\\rho < \\frac{1}{3}$, we discuss as follows:\n\n-   When $\\sqrt{r} \\leq \\frac{\\lambda'}{1 +\\rho} \\cdot \\frac{5 + 3\\rho}{(1 -\\rho)(3 +\\rho)}$,\n\n    For the first term, we need $1 + \\sqrt{1 -\\vartheta} > \\sqrt{\\frac{1 - 2\\vartheta}{1 -\\rho^2}} + \\frac{1}{1 +\\rho}$ so that the exponent is negative. $(LHS - RHS)$ is increasing in $\\vartheta$, and verifying $\\vartheta= \\frac{2\\rho}{1 +\\rho}$ is enough.\n\n    For the next term, we need $\\sqrt{\\frac{5 + 3\\rho}{(1 -\\rho^2)(1 +\\rho)}}\\sqrt{1 -2\\vartheta} \\leq 1 + \\sqrt{1 -\\vartheta}$. Now we need to verify $\\vartheta=\\max\\{\\frac{2\\rho}{1 +\\rho},1-\\left(\\frac{5 +3 \\rho }{ (1 -\\rho^2) (\\rho+3)}-1\\right)^2\\}$, and it still holds.\n\n    *As long as $k \\leq 2$, $k \\sqrt{1 -\\vartheta} - \\sqrt{1 - 2\\vartheta}$ is increasing in $\\vartheta$. $\\sqrt{\\frac{(1 -\\rho^2)(1 +\\rho)}{5 + 3\\rho}} \\leq 0.454167$ numerically.*\n\n-   When $\\sqrt{r} \\geq \\frac{2\\lambda'}{1 -\\rho^2}$: impossible, because $\\sqrt{r} = 1 + \\sqrt{1 -\\vartheta}$.\n\n-   When $\\frac{\\lambda'}{1 +\\rho} \\cdot \\frac{5 + 3\\rho}{(1 -\\rho)(3 +\\rho)} \\leq \\sqrt{r} \\leq \\frac{2\\lambda'}{1 -\\rho^2}$: We know $$d^2(C, ((1 +\\rho)\\sqrt{r},(1 +\\rho) \\sqrt{r})) \\geq \\max\\left\\{\\frac{(1 -\\rho^2)^2(1 +\\rho)}{5 + 3\\rho}r,\\ \\left[(1 -\\rho^2)\\sqrt{r}-\\lambda'\\right]^2\\right\\}$$ So a sufficient condition is $$\\sqrt{\\frac{1 - 2\\vartheta}{1 -\\rho^2}} + \\frac{1}{1 -\\rho^2} \\leq 1 + \\sqrt{1 -\\vartheta}$$ with $\\vartheta=\\frac{2\\rho}{1 +\\rho}$. When $\\rho \\leq \\frac{1}{3}$, this holds.\n\nTo conclude, we have verified that $FP_1 = FN_1$ can only admit $\\sqrt{r} = 1 + \\sqrt{1 -\\vartheta}$, for $\\vartheta> \\frac{2\\rho}{1 +\\rho}$; and this curve indeed meets all the requirements.\n\n*Second*, if $\\vartheta+ f_1(\\sqrt{r},\\lambda') = \\vartheta+ f_2(\\sqrt{r},\\lambda') = 1$, and $\\lambda' \\geq  1$, $2\\vartheta+ f_3(\\sqrt{r},\\lambda') \\geq 1$, we also need to discuss along the conditional expression of $f_1(\\sqrt{r},\\lambda')$.\n\nWhen $\\lambda' \\leq \\sqrt{r} \\leq 2\\lambda'$ in $FP_2$, now we have $\\lambda' = \\sqrt{\\frac{1 +\\rho}{1 -\\rho}}\\sqrt{1 -\\vartheta}.$ Because we want $\\lambda' \\geq 1$, all we need is to require $\\lambda' > 1$, which implies $\\vartheta<\\frac{2\\rho}{1 +\\rho}.$\n\nSince $\\min_{\\rho}\\frac{5 + 3\\rho}{(1 -\\rho)(1 +\\rho)^2} = 4.848$, we only need to consider the first case in conditional expression of $f_2(\\sqrt{r},\\lambda')$. This gives us three possible curves: $$\\begin{aligned}\n  &\\sqrt{r} = (1 + \\sqrt{\\frac{1 +\\rho}{1 -\\rho}})\\sqrt{1 -\\vartheta}  \\\\\n  & \\sqrt{r} = 2 \\sqrt{\\frac{1 -\\vartheta}{1 -\\rho^{2}}}\\\\\n  & \\sqrt{r} = \\sqrt{\\frac{5 + 3\\rho}{(1 -\\rho)(2 +\\rho)^2}}\\sqrt{1 -\\vartheta}\n\\end{aligned}$$ Actually, $\\forall \\ \\rho\\in(0,1)$, $1 + \\sqrt{\\frac{1 +\\rho}{1 -\\rho}} > \\max \\left\\{ \\frac{2}{\\sqrt{1 -\\rho^2}},\\ \\sqrt{\\frac{5 + 3\\rho}{(1 -\\rho)(2 +\\rho)^2}} \\right\\}$, which means that in the expression of $f_2$ when $$\\min \\begin{cases} \n    (\\sqrt{r} -\\lambda')_ +^2 \\\\\n    \\frac{1}{1 -\\rho^2}\\left[(1 -\\rho^2)\\sqrt{r} -(1 -\\rho)\\lambda'\\right]^2 \\\\\n    \\frac{(1 -\\rho)(2 +\\rho)^2}{5 + 3\\rho} r\n    \\end{cases},$$ neither of the last two lines cannot produce a curve and be the minimum at the same time.\n\nSo we are only left with $\\sqrt{r} =(1 + \\sqrt{\\frac{1 +\\rho}{1 -\\rho}})\\sqrt{1 -\\vartheta}$. When $\\vartheta< \\frac{2\\rho}{1 +\\rho}$, we already have $\\lambda' \\geq 1$, $\\vartheta+f_1(\\sqrt{r},\\lambda')\\geq 1$ and $\\vartheta+f_2(\\sqrt{r},\\lambda')\\geq 1$, and we are left to verify $2\\vartheta+f_3(\\sqrt{r},\\lambda')\\geq 1$. Actually, this does not always hold when $\\rho$ is very small, in which case we need one more curve.\n\nWhen $\\rho \\geq 0.197$, $\\sqrt{r} \\leq \\lambda' \\cdot \\frac{5 + 3\\rho}{(1 -\\rho^2)(3 +\\rho)}$ always holds, and we only need to consider the first case. We need $$\\begin{aligned}\n  \\begin{cases} \n    & (1 + \\sqrt{\\frac{1 +\\rho}{1 -\\rho}})\\sqrt{1 -\\vartheta} \\geq \\frac{1}{1 +\\rho}\\sqrt{\\frac{1 +\\rho}{1 -\\rho}}\\sqrt{1 -\\vartheta} + \\sqrt{\\frac{1 - 2\\vartheta}{1 -\\rho^2}} \\\\\n    & (1 + \\sqrt{\\frac{1 +\\rho}{1 -\\rho}})\\sqrt{1 -\\vartheta} \\geq \\sqrt{1 -2\\vartheta}\\sqrt{\\frac{5 + 3\\rho}{(1 +\\rho)(1 -\\rho^2)}}\n  \\end{cases} \n\\end{aligned}$$ The first one always holds for $\\rho\\in(0,1)$. We can separate $\\vartheta$ and $\\rho$ into two sides of the inequality and see this. The second one always holds for $\\rho \\geq  0.183$. We can separate $\\vartheta$ and $\\rho$ into two sides of the inequality and see this.\n\nAs a result, when $\\rho > 0.197$, $FN_2 = o(1)$.\n\nWhen $0.183 \\leq \\rho < 0.197$, $\\sqrt{r}$ is always in the second case, and $FN_2 = o(1)$ still holds, because $d^2(C, ((1 +\\rho)\\sqrt{r},(1 +\\rho) \\sqrt{r})) \\geq \\frac{(1 -\\rho^2)^2(1 +\\rho)}{5 + 3\\rho}r.$\n\nWhen $\\rho < 0.183$, we need to look at the expression of $d^2(C, ((1 +\\rho)\\sqrt{r},(1 +\\rho) \\sqrt{r}))$. $$\\label{eq:dist.C}\n  d^2(C, ((1 +\\rho)\\sqrt{r},(1 +\\rho) \\sqrt{r})) = 2(1 - \\rho)(1 +\\rho)^2 r - 2(1 +\\rho)(3 + \\rho)\\lambda'\\sqrt{r} + \\frac{5 + 3\\rho}{1 -\\rho} \\lambda'^2$$ In our case, we want $d^2(C, ((1 +\\rho)\\sqrt{r},(1 +\\rho) \\sqrt{r})) \\geq (1 -\\rho^2)(1 - 2\\vartheta)$ which is $$2 (1 +\\rho) \\left(1 + \\sqrt{\\frac{1 +\\rho}{1 -\\rho}}\\right)^2 -\\frac{2 (3 +\\rho)}{1 -\\rho} \\sqrt{\\frac{1 +\\rho}{1 -\\rho}} \\left(1 +\\sqrt{\\frac{1 +\\rho}{1 -\\rho}}\\right)+\\frac{3 \\rho+5}{(1 -\\rho)^3} \\geq \\frac{1 - 2\\vartheta}{1 -\\vartheta}.$$ There is no simpler form even if we further break this down.\n\nWhen $\\rho > 0.179$, the LHS is always greater than 1, and not restrictive to $\\vartheta$. When $\\rho < 0.179$, this imposes a lower bound on $\\vartheta$; when $\\vartheta$ is very small, there will be another curve above $\\sqrt{r} =(1 +\\sqrt{\\frac{1 +\\rho}{1 -\\rho}})\\sqrt{1 -\\vartheta}$.\n\nTo sum up, now we have $\\sqrt{r} =\\max \\left\\{ 1 + \\sqrt{1-\\vartheta}, (1 +\\sqrt{\\frac{1 +\\rho}{1 -\\rho}})\\sqrt{1 -\\vartheta}\\right\\}$, but when $\\rho < 0.179$, we seem to need one more curve which is unknown for now.\n\nWhen $\\sqrt{r} \\geq \\frac{5 + 3\\rho}{2 + 2\\rho}\\lambda'$ in $f_1(\\sqrt{r},\\lambda')$, if $\\lambda'^2 \\leq \\frac{(1 -\\rho^2)(1 -\\rho)r}{5 + 3\\rho}$, then we need $\\lambda^2 =(1 -\\vartheta)(1 -\\rho^2)$, which contradicts $\\lambda > 1$ (for $FP_1$). So we must have $\\lambda'^2 > \\frac{(1 -\\rho^2)(1 -\\rho)r}{5 + 3\\rho}$, and $\\sqrt{r} = \\sqrt{\\frac{5 + 3\\rho}{1 -\\rho}}\\sqrt{1 -\\vartheta}$.\n\nHowever, this curve $\\sqrt{r} = \\sqrt{\\frac{5 + 3\\rho}{1 -\\rho}}\\sqrt{1 -\\vartheta}$ is always greater than the $\\sqrt{r} =(1 + \\sqrt{\\frac{1 +\\rho}{1 -\\rho}})\\sqrt{1 -\\vartheta}$ we have computed. By requiring $\\sqrt{r}\\geq \\frac{5 + 3\\rho}{2 + 2\\rho}\\lambda'$, we have $1 <\\lambda' \\leq \\frac{2(1 +\\rho)\\sqrt{1 -\\vartheta}}{\\sqrt{5 + 3\\rho}\\sqrt{ 1 -\\rho}}$. Notice that $\\sqrt{1 -\\vartheta} \\geq \\frac{\\sqrt{(5 + 3\\rho)(1 -\\rho)}}{2(1 +\\rho)}$ is a strictly tighter requirement than $\\vartheta<\\frac{2\\rho}{1 +\\rho}$. As a result, even if this curve exists, it is not part of the boundary in the phase diagram.\n\nWhen $2\\lambda' < \\sqrt{r} < \\frac{5 + 3\\rho}{2 + 2\\rho}\\lambda'$, $\\lambda'^2$ cannot be smaller than $d^2(B, (\\rho \\sqrt{r},\\sqrt{r}))$; otherwise $\\lambda'^2 =(1 -\\rho^2)(1 -\\vartheta)$ which contradicts $\\lambda' > 1$ in $FP_1$.\n\nThen we need the following things: $$\\begin{cases} \n    2\\lambda' < \\sqrt{r} < \\frac{5 + 3\\rho}{2 + 2\\rho}\\lambda' \\\\\n    d^2(B, (\\rho \\sqrt{r},\\sqrt{r})) =(1 -\\rho^2)(1 -\\vartheta)\\\\\n    \\lambda' \\geq 1\\\\\n    2\\vartheta+f_3(\\sqrt{r},\\lambda')\\geq 1\\\\\n    \\sqrt{r} \\geq \\lambda' + \\sqrt{1 -\\vartheta}\\\\\n    \\sqrt{r} \\geq \\sqrt{\\frac{1 -\\vartheta}{1 -\\rho^2} } +\\frac{\\lambda'}{1 +\\rho} \\\\\n    \\sqrt{r} \\geq \\sqrt{\\frac{5 + 3\\rho}{(1 -\\rho)(2 +\\rho)^2} }\\sqrt{1 -\\vartheta}\n  \\end{cases}$$ and one of the last three inequalities must attain equality.\n\nIf $\\sqrt{r} =\\lambda' + \\sqrt{1 -\\vartheta}$, then $\\sqrt{r} > 2\\lambda'$ would imply $\\lambda' < \\sqrt{1 -\\vartheta}$, contradicting $\\lambda' > 1$.\n\nIf $\\sqrt{r} = \\sqrt{\\frac{1 -\\vartheta}{1 -\\rho^2} } +\\frac{\\lambda'}{1 +\\rho}$, we need to look at the expression of $d^2(B, (\\rho \\sqrt{r},\\sqrt{r}))$ computed in [\\[eq:dist.B\\]](#eq:dist.B){reference-type=\"eqref\" reference=\"eq:dist.B\"}. We let $\\lambda^* = \\frac{1}{\\sqrt{1 -\\vartheta}}\\lambda'$, and $\\sqrt{r} = \\sqrt{\\frac{1 -\\vartheta}{1 -\\rho^2} } +\\frac{\\lambda^* \\sqrt{1 -\\vartheta}}{1 +\\rho}$. Now we have $\\frac{2 + 4\\rho + 3\\rho^2}{(1 +\\rho)^2}{\\lambda^*}^2 -\\frac{2(1 + 2\\rho)}{(1 +\\rho)\\sqrt{1 -\\rho^2}}{\\lambda^*} +\\frac{\\rho^2}{1 -\\rho^2} = 0$. However, $$\\begin{aligned}\n  \\sqrt{r} > 2\\lambda'\\implies&~ \\lambda^* < \\frac{1 +\\rho}{1 + 2\\rho}\\frac{1}{\\sqrt{1 -\\rho^2}}\\\\\n  \\sqrt{r} < \\frac{5 + 3\\rho}{2 + 2\\rho}\\lambda' \\implies&~ \\lambda^* > \\frac{2}{3}\\frac{1}{\\sqrt{1 -\\rho^2}}\n\\end{aligned}$$ Plug such lower bound and upper bound into the quadratic equation, and the values are negative at both the upper and lower bounds. Thus we know that it has no solution for $\\lambda'$ at all.\n\nIf $\\sqrt{r} = \\sqrt{\\frac{5 + 3\\rho}{(1 -\\rho)(2 +\\rho)^2} }\\sqrt{1 -\\vartheta}$, then we have the following two requirements: $$\\begin{aligned}\n\\begin{cases} \n  \\sqrt{r} \\geq&~  \\sqrt{\\frac{1 -\\vartheta}{1 -\\rho^2} } +\\frac{\\lambda'}{1 +\\rho} \\implies \\lambda' \\leq (1 +\\rho) \\left[ \\sqrt{\\frac{5 + 3\\rho}{(1 -\\rho)(2 +\\rho)^2}} - \\frac{1}{\\sqrt{1 -\\rho^2}}\\right]\\sqrt{1 -\\vartheta} \\\\\n\\sqrt{r} \\leq&~ \\frac{5 + 3\\rho}{2 + 2\\rho}\\lambda' \\implies \\lambda' \\geq \\frac{2(1 +\\rho)}{5 + 3\\rho}\\sqrt{\\frac{5 + 3\\rho}{(1 -\\rho)(2 +\\rho)^2}}\\sqrt{1 -\\vartheta}\n\\end{cases} \n\\end{aligned}$$ and the upper bound is smaller than the lower bound, contradiction.\n\nTp sum up the *second* case, we have $\\sqrt{r} =\\max \\left\\{ 1 + \\sqrt{1-\\vartheta}, (1 +\\sqrt{\\frac{1 +\\rho}{1 -\\rho}})\\sqrt{1 -\\vartheta}\\right\\}$, but when $\\rho < 0.179$, we seem to need one more curve which is unknown for now.\n\n*Third,* if $\\lambda'^2 =2\\vartheta+f_3(\\sqrt{r},\\lambda')= 1$ and $\\vartheta+f_1(\\sqrt{r},\\lambda')\\geq 1,\\,\\vartheta+f_2(\\sqrt{r},\\lambda')\\geq  1$, we will eventually have now curve in this case. We start from some basic requirements:\n\nNow we have $\\lambda' = 1$. When $\\lambda' = 1$ is fixed, the exponents of $FP$ and $FN$ are all decreasing in $\\sqrt{r}$. For $FN_1$, we thus need $$\\sqrt{r} \\geq \\max\\left\\{ 1+ \\sqrt{1 -\\vartheta},\\ \\sqrt{\\frac{1 -\\vartheta}{1 -\\rho^2}} + \\frac{1}{1 +\\rho},\\ \\sqrt{\\frac{5 + 3\\rho}{(1 -\\rho)(2 +\\rho)^2}}\\sqrt{1 -\\vartheta} \\right\\}$$ Even if we finally find an admissible curve with $\\lambda'^2 =2\\vartheta+f_3(\\sqrt{r},\\lambda')= 1$, it cannot be lower than $\\sqrt{r} = 1 + \\sqrt{1 -\\vartheta}$, so we can require $\\vartheta< \\frac{2\\rho}{1 +\\rho}$.\n\nNow we look at $\\vartheta+f_1(\\sqrt{r},\\lambda')\\geq 1$. If $\\sqrt{r} \\leq 2$, then $\\vartheta< \\frac{2\\rho}{1 +\\rho}$ gives us a contradiction. Thus $\\sqrt{r} \\geq 2$. We still need $FPtwo \\geq$ additionally to $\\sqrt{r} \\geq 2$.\n\nThen we discuss the conditional expression of $f_3(\\sqrt{r},\\lambda')$ one by one.\n\nWhen $\\sqrt{r} \\geq \\frac{2\\lambda'}{1 -\\rho^2}$, $\\sqrt{r} = \\sqrt{\\frac{1 -2\\vartheta}{1 -\\rho^2}} + \\frac{1}{1 -\\rho^2}$ which contradicts $\\sqrt{r} \\geq \\frac{2\\lambda'}{1 -\\rho^2}$.\n\nWhen $\\frac{\\lambda'}{1 +\\rho} \\cdot \\frac{5 + 3\\rho}{(1 -\\rho)(3 +\\rho)} \\leq \\sqrt{r} < \\frac{2\\lambda'}{1 -\\rho^2}$, we have $$\\begin{aligned}\n  & d^2(C, ((1 +\\rho)\\sqrt{r},(1 +\\rho) \\sqrt{r})) = (1 -\\rho^2)(1 - 2\\vartheta) \\\\\n  & d^2(C, ((1 +\\rho)\\sqrt{r},(1 +\\rho) \\sqrt{r})) \\geq  \\frac{(1 -\\rho^2)^2(1 +\\rho)}{5 + 3\\rho}r \n\\end{aligned}$$ and thus $\\sqrt{r} \\leq \\sqrt{\\frac{5 + 3\\rho}{(1 +\\rho)^2(1 -\\rho)}}\\sqrt{1 - 2\\vartheta}$.\n\nFor $\\vartheta+f_1(\\sqrt{r},\\lambda')\\geq 1$, we look at the conditional expression of $f_1$, which now can only take one of the last two cases.\n\n-   if it is the last case, then we need $\\sqrt{r} \\geq \\sqrt{\\frac{5 + 3\\rho}{1 -\\rho}}\\sqrt{1 -\\vartheta}$, which does not hold.\n\n-   if it is the fourth case, then we need $$d^2(B, (\\rho \\sqrt{r},\\sqrt{r})) \\geq (1 -\\rho^2)(1 -\\vartheta).$$ We already have the expression of $d^2(B, (\\rho \\sqrt{r},\\sqrt{r}))$ in [\\[eq:dist.B\\]](#eq:dist.B){reference-type=\"eqref\" reference=\"eq:dist.B\"}, and the expression of $d^2(C, ((1 +\\rho)\\sqrt{r},(1 +\\rho) \\sqrt{r}))$ in [\\[eq:dist.C\\]](#eq:dist.C){reference-type=\"eqref\" reference=\"eq:dist.C\"}: $$\\begin{aligned}\n        & r - 4 \\sqrt{r} +\\frac{5 + 3\\rho}{1 +\\rho} \\geq 1 -\\vartheta\\\\\n        & r - \\frac{3 +\\rho}{1 -\\rho^2}\\sqrt{r} + \\frac{5 + 3\\rho}{2(1 -\\rho^2)^2} = \\frac{1 - 2\\vartheta}{2(1 +\\rho)} \\\\\n        \\implies & \\frac{1 - 2\\vartheta}{2(1 +\\rho)} +  \\frac{3 +\\rho}{1 -\\rho^2}\\sqrt{r} - \\frac{5 + 3\\rho}{2(1 -\\rho^2)^2} - 4 \\sqrt{r} +\\frac{5 + 3\\rho}{1 +\\rho} \\geq 1 -\\vartheta\\\\\n        \\implies & \\frac{1 }{2(1 +\\rho)} +  \\frac{3 +\\rho}{1 -\\rho^2}\\sqrt{r} - \\frac{5 + 3\\rho}{2(1 -\\rho^2)^2} - 4 \\sqrt{r} +\\frac{5 + 3\\rho}{1 +\\rho} \\geq 1 -\\frac{\\rho}{1 +\\rho} \\vartheta\n      \n    \\end{aligned}$$ It turns out that when we regard $\\sqrt{r}$ as an independent variable, and let it vary in the interval $(2,2.5)$ as in $FP_2$, we always have, $\\forall \\ \\rho\\in(0,1)$, $$\\frac{1 }{2(1 +\\rho)} +  \\frac{3 +\\rho}{1 -\\rho^2}\\sqrt{r} - \\frac{5 + 3\\rho}{2(1 -\\rho^2)^2} - 4 \\sqrt{r} +\\frac{5 + 3\\rho}{1 +\\rho} < 1 -\\frac{\\rho}{1 +\\rho}$$ so we have a contradiction.\n\nTo sum up, no curve in this case.\n\nWhen $\\sqrt{r} \\leq \\frac{\\lambda'}{1 +\\rho} \\cdot \\frac{5 + 3\\rho}{(1 -\\rho)(3 +\\rho)}$, if $\\left[(1 -\\rho^2)\\sqrt{r} -(1 -\\rho)\\lambda'\\right]_ +^2 \\leq \\frac{(1 -\\rho^2)^2(1 +\\rho)}{5 + 3\\rho}r$, then we have $\\sqrt{r} = \\sqrt{\\frac{1 - 2\\vartheta}{1 -\\rho^2}} + \\frac{1}{1 +\\rho}$, which contradicts $\\sqrt{r} \\geq  \\sqrt{\\frac{1 - \\vartheta}{1 -\\rho^2}} + \\frac{1}{1 +\\rho}$ required by $FN_1$. If $\\left[(1 -\\rho^2)\\sqrt{r} -(1 -\\rho)\\lambda'\\right]_ +^2 > \\frac{(1 -\\rho^2)^2(1 +\\rho)}{5 + 3\\rho}r$, then we have $\\sqrt{r} = \\sqrt{\\frac{5 + 3\\rho}{(1 +\\rho)^2(1 -\\rho)}}\\sqrt{1 - 2\\vartheta}$. This is a tedious case.\n\n-   Upper bound on $\\vartheta$: $\\sqrt{r} \\geq 2$, which implies $\\vartheta\\leq \\min\\left\\{ \\frac{2\\rho}{1+\\rho},\\ \\frac{1}{2} -\\frac{2(1 +\\rho)^2(1 -\\rho)}{5 + 3\\rho} \\right\\}$.\n\n-   Lower bound on $\\vartheta$: Even if the curve is admissible, it only makes a difference if it is smaller than $(1 + \\sqrt\\frac{1 +\\rho}{1 -\\rho})\\sqrt{1 -\\vartheta}$. This gives us $\\sqrt{\\frac{1 - 2\\vartheta}{1 -\\vartheta}} < \\left(1 + \\sqrt\\frac{1 +\\rho}{1 -\\rho}\\right) \\sqrt{\\frac{(1 +\\rho)^2(1 -\\rho)}{5 + 3\\rho}} = : \\phi(\\rho).$ Also, $\\sqrt{r} \\leq \\frac{5 + 3\\rho}{(2 + 2\\rho)}$ implies $\\vartheta\\geq \\frac{1}{2} - \\frac{1}{8}(5 + 3\\rho)(1 -\\rho)$.\n\nWe then examine $\\vartheta+f_1(\\sqrt{r},\\lambda')$.\n\n-   When $\\sqrt{r} \\geq \\frac{5 + 3\\rho}{2 + 2\\rho}\\lambda' \\geq \\frac{5 + 3\\rho}{2 + 2\\rho}$, we easily have a contradiction because $\\sqrt{r} = \\sqrt{\\frac{5 + 3\\rho}{(1 +\\rho)^2(1 -\\rho)}}\\sqrt{1 - 2\\vartheta}$.\n\n-   When $\\sqrt{r} < \\frac{5 + 3\\rho}{2 + 2\\rho}\\lambda'$, we need to look at $d^2(B, (\\rho \\sqrt{r},\\sqrt{r}))$. It now becomes $\\sqrt{r} - 2 \\geq \\sqrt{\\frac{2\\rho}{1 +\\rho} -\\vartheta}$. The derivative of $(LHS - RHS)$ w.r.t. $\\vartheta$ is $$\\frac{1}{2 \\sqrt{\\frac{2\\rho}{1 +\\rho} -\\vartheta}}\\left[ 1 - \\sqrt{\\frac{2(5 + 3\\rho)}{(1 +\\rho)^2(1 - \\rho)} } \\sqrt{\\frac{\\frac{2\\rho}{1 +\\rho} -\\vartheta}{\\frac{1}{2} -\\vartheta}}\\right]$$ from which we can see $(LHS - RHS)$ is either increasing, decreasing, or first-decreasing-then-increasing. If we evaluate $(LHS - RHS)$ at the smallest and largest $\\vartheta$ and they are both negative ($\\forall \\rho$), then we have a contradiction.\n\n    -   When $\\rho \\geq 0.183$, $\\phi(\\rho) > 1$. The $(LHS - RHS)$ is below 0 at both $\\max\\{0,\\ \\frac{1}{2}-\\frac{1}{8} (1 -\\rho) (5 +3\\rho)\\}$ and $\\min\\left\\{ \\frac{2\\rho}{1+\\rho},\\ \\frac{1}{2} -\\frac{2(1 +\\rho)^2(1 -\\rho)}{5 + 3\\rho} \\right\\}$, for all $0.183 \\leq \\rho \\leq 1$.\n\n    -   When $\\rho < 0.183$, $0 <\\phi(\\rho) < 1$, and it poses a lower bound on $\\vartheta$.\n\n        -   When $\\rho \\leq  0.091$, the lower bound is greater than the upper bound, so this case does not exist for any $\\vartheta$.\n\n        -   When $\\rho\\in(0.091, 0.183)$, we can verify that the $(LHS - RHS)$ is below 0 at both $\\max\\{0,\\ \\frac{1}{2}-\\frac{1}{8} (1 -\\rho) (5 +3\\rho)\\}$ and $\\min\\left\\{ \\frac{2\\rho}{1+\\rho},\\ \\frac{1}{2} -\\frac{2(1 +\\rho)^2(1 -\\rho)}{5 + 3\\rho},\\ \\frac{1 -\\phi^2(\\rho)}{2 -\\phi^2(\\rho)} \\right\\}$.\n\nWe finally know that $FP_1 = FN_2$ gives nothing.\n\n*Fourth*, if $\\vartheta+f_1(\\sqrt{r},\\lambda')= 2\\vartheta+f_3(\\sqrt{r},\\lambda')= 1$ and $\\lambda' \\geq 1$, $\\vartheta+f_2(\\sqrt{r},\\lambda')\\geq 1$, we will get the last curve.\n\nFrom $\\vartheta+f_2(\\sqrt{r},\\lambda')\\geq 1$, we know $\\sqrt{r} \\geq \\lambda'$, so we start from the case $\\lambda' \\leq  \\sqrt{r} \\leq 2\\lambda'$ in $f_1(\\sqrt{r},\\lambda')$.\n\nWhen $\\lambda' \\leq  \\sqrt{r} \\leq 2\\lambda'$ in $f_1(\\sqrt{r},\\lambda')$, we have $\\lambda' = \\sqrt{\\frac{1 +\\rho}{1 -\\rho}}\\sqrt{1 -\\vartheta}$. If $\\sqrt{r} \\leq \\frac{\\lambda'}{1 +\\rho} \\cdot \\frac{5 + 3\\rho}{(1 -\\rho)(3 +\\rho)}$ in $f_3(\\sqrt{r},\\lambda')$, we could have either of the following two, $$\\begin{aligned}\n  & \\sqrt{r} = \\frac{1}{\\sqrt{1 -\\rho^2}}\\left( \\sqrt{1 -\\vartheta} + \\sqrt{1 - 2\\vartheta} \\right)\\\\\n  & \\sqrt{r} = \\sqrt{\\frac{5 + 3\\rho}{(1 +\\rho)^2(1 - \\rho)}}\\sqrt{1 -2\\vartheta}\n\\end{aligned}$$ When it is the former, for $\\vartheta+f_2(\\sqrt{r},\\lambda')\\geq 1$, we need $\\sqrt{r} \\geq (1 + \\sqrt{\\frac{1 +\\rho}{1 -\\rho}})\\sqrt{1 -\\vartheta}$, which implies $\\sqrt{\\frac{1 - 2\\vartheta}{1 -\\vartheta}} \\geq \\rho+ \\sqrt{1 -\\rho^2} > 1$, which is impossible.\n\nWhen it is the latter, for $\\vartheta+f_2(\\sqrt{r},\\lambda')\\geq 1$, we need $\\sqrt{r} \\geq (1 + \\sqrt{\\frac{1 +\\rho}{1 -\\rho}})\\sqrt{1 -\\vartheta}$, which implies $\\sqrt{\\frac{1 - 2\\vartheta}{1 -\\vartheta}} \\geq \\sqrt{\\frac{(1 +\\rho)^2(1 -\\rho)}{5 + 3\\rho}}\\left( 1 + \\sqrt{\\frac{1 +\\rho}{1 -\\rho}} \\right).$ Also, $\\sqrt{r} \\leq \\frac{\\lambda'}{1 +\\rho} \\cdot \\frac{5 + 3\\rho}{(1 -\\rho)(3 +\\rho)}$ implies $\\sqrt{\\frac{1 - 2\\vartheta}{1 -\\vartheta}} \\leq \\frac{\\sqrt{(5 + 3\\rho) (1 +\\rho)}}{(1 -\\rho) (3 +\\rho)}$. However, now either the lower bound $\\sqrt{\\frac{(1 +\\rho)^2(1 -\\rho)}{5 + 3\\rho}}\\left( 1 + \\sqrt{\\frac{1 +\\rho}{1 -\\rho}} \\right) > 1 \\geq \\sqrt{\\frac{1 - 2\\vartheta}{1 -\\vartheta}}$, or the upper bound is smaller than the lower bound.\n\nIf $\\frac{\\lambda'}{1 +\\rho} \\cdot \\frac{5 + 3\\rho}{(1 -\\rho)(3 +\\rho)} \\leq \\sqrt{r} \\leq \\frac{2\\lambda'}{1 -\\rho^2}$ in $f_3(\\sqrt{r},\\lambda')$, we need to solve a quadratic function of $\\sqrt{r}$, i.e. $d^2(C, ((1 +\\rho)\\sqrt{r},(1 +\\rho) \\sqrt{r})) =(1 -\\rho^2)(1 - 2\\vartheta)$. The expression of the LHS is already in [\\[eq:dist.C\\]](#eq:dist.C){reference-type=\"eqref\" reference=\"eq:dist.C\"}. Then we have $$\\label{eq:extra.curve}\n  \\sqrt{r} =\\frac{3 +\\rho}{2(1 -\\rho^2)}\\sqrt{\\frac{1 +\\rho}{1 -\\rho}}\\sqrt{1 -\\vartheta} + \\frac{1}{2}\\sqrt{\\frac{2(1 - 2\\vartheta)}{1 +\\rho} - \\frac{(1 -\\vartheta)}{(1 -\\rho)^2}}.$$ We take the larger root, because when $\\sqrt{r}$ takes the smaller one, the ellipsoid is actually still tangent to the green line segment in Figure [8](#suppfig:hamm.scad.small.a){reference-type=\"ref\" reference=\"suppfig:hamm.scad.small.a\"}. Thus the smaller root should be discarded.\n\nOf course, we also list all the requirements it must meet. They are loose only except the first one. $$\\begin{cases} \n    \\sqrt{r} \\geq (1 + \\sqrt{\\frac{1 +\\rho}{1 -\\rho}})\\sqrt{1 -\\vartheta}\\\\\n  \\vartheta\\leq \\frac{2\\rho}{1 +\\rho}\\\\\n  \\frac{5 + 3\\rho}{(1 -\\rho^2)(3 +\\rho)} \\sqrt{\\frac{1 +\\rho}{1 -\\rho}} \\sqrt{1 -\\vartheta}\\leq \\sqrt{r} \\leq 2\\sqrt{\\frac{1 +\\rho}{1 -\\rho}}\\sqrt{1 -\\vartheta}\n  \\end{cases}$$\n\nWhen $\\sqrt{r} \\geq\\frac{5 + 3\\rho}{2 + 2\\rho}\\lambda'$ in $f_1(\\sqrt{r},\\lambda')$, We know from $FP_2$ that $\\sqrt{r} =\\sqrt{\\frac{5 + 3\\rho}{1 -\\rho}}\\sqrt{1 -\\vartheta}$. ( Because we need $\\lambda' \\geq  1$ for $FP_1$, the other term is not possible.) If $\\sqrt{r} \\leq \\frac{\\lambda'}{1 +\\rho} \\cdot \\frac{5 + 3\\rho}{(1 -\\rho)(3 +\\rho)}$ in $f_3(\\sqrt{r},\\lambda')$, it is only possible that $\\sqrt{r} = \\sqrt{\\frac{1 - 2\\vartheta}{1 -\\rho^2}} +\\frac{\\lambda'}{1 +\\rho}$. However, $FN_1 = o(1)$ requires that $\\sqrt{r} \\geq  \\sqrt{\\frac{1 - \\vartheta}{1 -\\rho^2}} +\\frac{\\lambda'}{1 +\\rho}$, so this case is not possible. If $\\sqrt{r} \\geq \\frac{2\\lambda'}{1 -\\rho^2}$ in $f_3(\\sqrt{r},\\lambda')$, we have $\\sqrt{r} =  \\sqrt{\\frac{1 - 2\\vartheta}{1 -\\rho^2}} +\\frac{\\lambda'}{1 -\\rho^2}$. Because $\\sqrt{r} \\geq 2\\left( \\sqrt{r} - \\sqrt{\\frac{1 - 2\\vartheta}{1 -\\rho^2}} \\right)$, we have $\\sqrt{r} \\leq 2\\sqrt{\\frac{1 - 2\\vartheta}{1 -\\rho^2}} \\implies \\lambda' \\leq \\sqrt{(1 - 2\\vartheta)(1 -\\rho^2)}$, which contradicts $\\lambda' >1$. If $\\frac{\\lambda'}{1 +\\rho} \\cdot \\frac{5 + 3\\rho}{(1 -\\rho)(3 +\\rho)} < \\sqrt{r} < \\frac{2\\lambda'}{1 -\\rho^2}$ in $f_3(\\sqrt{r},\\lambda')$, we know that $$(1 -\\rho^2)(1 - 2\\vartheta) =d^2(C, ((1 +\\rho)\\sqrt{r},(1 +\\rho) \\sqrt{r})) \\geq \\frac{(1 -\\rho^2)^2(1 +\\rho)}{5 + 3\\rho}r$$ and thus $\\sqrt{r} \\leq \\sqrt{\\frac{5 + 3\\rho}{(1 -\\rho)(1 +\\rho)^2}}\\sqrt{1 - 2\\vartheta}$ which contradicts $\\sqrt{r} =\\sqrt{\\frac{5 + 3\\rho}{1 -\\rho}}\\sqrt{1 -\\vartheta}$.\n\nWhen $\\sqrt{r} \\in (2\\lambda', \\frac{5 + 3\\rho}{2 + 2\\rho}\\lambda')$ in $f_1(\\sqrt{r},\\lambda')$:\n\nIf $\\sqrt{r} \\leq \\frac{\\lambda'}{1 +\\rho} \\cdot \\frac{5 + 3\\rho}{(1 -\\rho)(3 +\\rho)}$ in $f_3(\\sqrt{r},\\lambda')$, and $\\sqrt{r} = \\sqrt{\\frac{1 - 2\\vartheta}{1 -\\rho^2}} +\\frac{\\lambda'}{1 +\\rho}$, we have a contradiction. This is because $FN_1 = o(1)$ requires that $\\sqrt{r} \\geq  \\sqrt{\\frac{1 - \\vartheta}{1 -\\rho^2}} +\\frac{\\lambda'}{1 +\\rho}$, so this case is not possible.\n\nIf $\\sqrt{r} \\leq \\frac{\\lambda'}{1 +\\rho} \\cdot \\frac{5 + 3\\rho}{(1 -\\rho)(3 +\\rho)}$ in $f_3(\\sqrt{r},\\lambda')$, and $\\sqrt{r} =\\sqrt{\\frac{5 + 3\\rho}{(1 +\\rho)^2(1 -\\rho)}}\\sqrt{1 - 2\\vartheta}$, this turns out a very tedious case because we generally need to work with $(\\vartheta,\\ \\lambda',\\ \\rho)$ at the same time. For completeness, we include a rigorous proof anyway.\n\nBriefly speaking, we let $\\lambda^* = \\frac{1}{\\sqrt{1 -\\vartheta}} \\lambda'$. Because all we need is $$\\begin{cases} \n    d^2(B, (\\rho \\sqrt{r},\\sqrt{r})) =(1 -\\rho^2)(1 -\\vartheta)\\quad (\\vartheta+f_1(\\sqrt{r},\\lambda')\\geq 1)\\\\\n    \\sqrt{r} =\\sqrt{\\frac{5 + 3\\rho}{(1 +\\rho)^2(1 -\\rho)}}\\sqrt{1 - 2\\vartheta}\\quad (2\\vartheta+f_3(\\sqrt{r},\\lambda')\\geq 1)\\\\\n    \\lambda' \\geq 1\\\\\n    \\sqrt{r} \\geq \\lambda' + \\sqrt{1 -\\vartheta}\\quad (\\vartheta+f_2(\\sqrt{r},\\lambda')\\geq 1)\\\\\n    \\sqrt{r} \\geq \\sqrt{\\frac{1 -\\vartheta}{1 -\\rho^2}} +\\frac{\\lambda'}{1 +\\rho}\\quad (\\vartheta+f_2(\\sqrt{r},\\lambda')\\geq 1) \\\\\n    \\sqrt{r} \\geq \\sqrt{\\frac{5 + 3\\rho}{(1 -\\rho)(2 +\\rho)^2}}\\sqrt{1 -\\vartheta}\\quad (\\vartheta+f_2(\\sqrt{r},\\lambda')\\geq 1) \\\\\n    2\\lambda' \\leq \\sqrt{r} \\leq \\frac{5 + 3\\rho}{2 + 2\\rho}\\lambda' \\quad (\\vartheta+f_1(\\sqrt{r},\\lambda')\\geq 1 )\\\\\n    \\sqrt{r} \\leq \\frac{5 + 3\\rho}{(1 -\\rho^2)(3 +\\rho)}\\lambda'\\quad (2\\vartheta+f_3(\\sqrt{r},\\lambda')\\geq 1)\\\\\n  \\end{cases}$$ by cleaning this up, we have, letting $x = \\sqrt{\\frac{1 - 2\\vartheta}{1 -\\vartheta}}$, $$\\label{eq:complex}\n  \\begin{cases} \n  q(x)\\stackrel{\\mathrm{def}}{=}\\frac{5 + 3\\rho}{(1 +\\rho)^2(1 -\\rho)}x^2 - 4 \\sqrt{\\frac{5 + 3\\rho}{(1 +\\rho)^2(1 -\\rho)}} \\lambda^* x + \\left( \\frac{5 + 3\\rho}{1 +\\rho}  \\right) (\\lambda^*)^2 - 1 = 0 \\\\\n  \\lambda^* \\geq 1\\\\\n  x \\geq \\max \\left\\{ \\frac{(1 +\\rho)^2(1 -\\rho)}{5 + 3\\rho}\\left( \\frac{\\lambda^*}{1 +\\rho} + \\frac{1}{\\sqrt{1 -\\rho^2}} \\right),\\ \\frac{1 +\\rho}{2 +\\rho},\\ 2\\lambda^*\\sqrt{\\frac{(1 +\\rho)^2(1 -\\rho)}{5 + 3\\rho}} \\right\\} \\\\\n  x \\leq  \\min\\left\\{1,\\ \\frac{\\lambda^*}{2}\\sqrt{(5 + 3\\rho)(1 -\\rho)},\\ \\frac{\\lambda^*}{3 +\\rho}\\sqrt{\\frac{5 + 3\\rho}{1 -\\rho}} \\right\\}\n  \\end{cases}$$ If we want some admissible $x$ to exist, we need the upper bound to be greater than the lower bound in the last two inequalities. This will give us $\\underline{B}(\\rho) \\leq \\lambda^* \\leq \\overline{B}(\\rho)$. (The expressions of $\\underline{B}(\\rho)$ and $\\overline{B}(\\rho)$ can be explicitly written, but are omitted for brevity.)\n\n-   For some $\\rho\\in(0,1)$ and $\\lambda^*\\in[\\underline{B}(\\rho),\\,   \\overline{B}(\\rho)]$, we try to plot $q(x)$ at a suitable $x$. If $q(x) < 0$ always holds, then $x$ has no solution, and this case is eliminated.\n\n-   $q(x)$ is a quadratic function of $x$. Its axis of symmetry is $2\\lambda^*\\sqrt{\\frac{(1 +\\rho)^2(1 -\\rho)}{5 + 3\\rho}},$ which we already know is smaller than $x$. Thus $q(x)$ is increasing in admissible $x$, and we only need to evaluate $q(x)$ at the maximum $x$: $x = \\min\\left\\{1,\\ \\frac{\\lambda^*}{2}\\sqrt{(5 + 3\\rho)(1 -\\rho)},\\ \\frac{\\lambda^*}{3 +\\rho}\\sqrt{\\frac{5 + 3\\rho}{1 -\\rho}} \\right\\}$.\n\n-   Now we are left to prove a bivariate function is below zero, which can be easily verified given all the requirements on $\\lambda'$ and $\\forall\\, \\rho >0$.\n\nWhen $\\sqrt{r} \\geq \\frac{2\\lambda'}{1 -\\rho^2}$ in $f_3(\\sqrt{r},\\lambda')$ , we have $\\sqrt{r} =  \\sqrt{\\frac{1 - 2\\vartheta}{1 -\\rho^2}} +\\frac{\\lambda'}{1 -\\rho^2}$. Because $\\sqrt{r} \\geq 2\\left( \\sqrt{r} - \\sqrt{\\frac{1 - 2\\vartheta}{1 -\\rho^2}} \\right)$, we have $\\sqrt{r} \\leq 2\\sqrt{\\frac{1 - 2\\vartheta}{1 -\\rho^2}} \\implies \\lambda' \\leq \\sqrt{(1 - 2\\vartheta)(1 -\\rho^2)}$, which contradicts $\\lambda' >1$.\n\nWhen $\\frac{\\lambda'}{1 +\\rho} \\cdot \\frac{5 + 3\\rho}{(1 -\\rho)(3 +\\rho)} < \\sqrt{r} < \\frac{2\\lambda'}{1 -\\rho^2}$ in $f_3(\\sqrt{r},\\lambda')$, we have $$\\begin{aligned}\n  &d^2(C, ((1 +\\rho)\\sqrt{r},(1 +\\rho) \\sqrt{r})) =(1 -\\rho^2)(1 - 2\\vartheta)\\\\\n  & d^2(B, (\\rho\\sqrt{r}, \\sqrt{r})) = (1 -\\rho^2)(1 - \\vartheta).\n\\end{aligned}$$ This is an even more tedious case, and we eliminate this case as follows: We first list all of the requirements we have: $$\\begin{cases} \n      d^2(B, (\\rho\\sqrt{r}, \\sqrt{r})) = (1 -\\rho^2)(1 - \\vartheta)\\quad (\\vartheta+f_1(\\sqrt{r},\\lambda')\\geq 1)\\\\\n      d^2(C, ((1 +\\rho)\\sqrt{r},(1 +\\rho) \\sqrt{r})) =(1 -\\rho^2)(1 - 2\\vartheta)\\quad (2\\vartheta+f_3(\\sqrt{r},\\lambda')\\geq 1)\\\\\n      \\lambda' \\geq 1\\\\\n      2\\lambda' \\leq \\sqrt{r} \\leq \\frac{5 + 3\\rho}{2 + 2\\rho}\\lambda'\\quad (\\vartheta+f_1(\\sqrt{r},\\lambda')\\geq 1)\\\\\n      \\sqrt{r} \\geq \\max \\left\\{ \\sqrt{\\frac{1 -\\vartheta}{1 -\\rho^2}} +\\frac{\\lambda'}{1 +\\rho},\\quad  \\sqrt{\\frac{5 + 3\\rho}{(1 -\\rho)(2 +\\rho)^2}}\\sqrt{1 -\\vartheta}\\right\\}\\quad (\\vartheta+f_2(\\sqrt{r},\\lambda')\\geq 1) \\\\\n      \\sqrt{r} \\geq \\frac{5 + 3\\rho}{(1 -\\rho^2)(3 +\\rho)}\\lambda'\\quad (\\vartheta+f_1(\\sqrt{r},\\lambda')\\geq 1)\n    \\end{cases}$$ Define $x = \\sqrt{\\frac{r}{1 -\\vartheta}}$ and $\\lambda^* = \\frac{1}{\\sqrt{1 -\\vartheta}}\\lambda'$. We know the upper and lower bounds of $x$, from the last three inequalities: $$\\max \\left\\{\\frac{5 + 3\\rho}{(1 -\\rho^2)(3 +\\rho)}\\lambda^*,\n    2\\lambda^*, \n    \\frac{1}{\\sqrt{1 -\\rho^2}} +\\frac{\\lambda^*}{1 +\\rho},\n    \\sqrt{\\frac{5 + 3\\rho}{(1 -\\rho)(2 +\\rho)^2}}\n    \\right\\}\n    \\leq x \\leq \\frac{5 + 3\\rho}{2 + 2\\rho}\\lambda^*.$$ For admissible $x$ to exist, we need $\\rho \\leq 0.415$ and $\\lambda^* \\geq \\max\\left\\{ \\frac{2}{3 \\sqrt{1 -\\rho^2}}, \\frac{2(1 +\\rho)}{(2 +\\rho) \\sqrt{(5 + 3\\rho)(1 -\\rho)}} \\right\\}$.\n\nThen we want to know the upper and lower bounds of $\\lambda^*$ given $\\rho \\leq 0.415$. In terms of the upper bound, we know from $d^2(B, (\\rho\\sqrt{r}, \\sqrt{r})) = (1 -\\rho^2)(1 - \\vartheta) \\geq (1 -\\rho)^2 \\lambda'^2$ that $\\lambda^* \\leq \\sqrt{\\frac{1 +\\rho}{1 -\\rho}}$. To sum up, $$\\max\\left\\{1,\\ \\frac{2}{3 \\sqrt{1 -\\rho^2}}, \\frac{2(1 +\\rho)}{(2 +\\rho) \\sqrt{(5 + 3\\rho)(1 -\\rho)}} \\right\\} \\leq \\lambda^* \\leq \\sqrt{\\frac{1 +\\rho}{1 -\\rho}}.$$\n\nHaving the upper and lower bound on $\\lambda'$, we look at the first two quadratic functions, and we know that $$\\begin{cases} \n    x^2 - 4x\\lambda^* +\\frac{5 + 3\\rho}{1 +\\rho}{\\lambda^*}^2 = 1\\\\\n    2(1 +\\rho)x^2 - \\frac{2(3 +\\rho)}{1 -\\rho}x\\lambda^* + \\frac{5 + 3\\rho}{(1 -\\rho)^2(1 +\\rho)}{\\lambda^*}^2 = \\frac{1 - 2\\vartheta}{1 -\\vartheta} \\leq 1\n    \\end{cases}$$ The LHS of the second quadratic equation, as a function of $x$, has the axis of symmetry at $\\frac{3 +\\rho}{2(1 -\\rho^2)}\\lambda^* > \\frac{5 + 3\\rho}{2 + 2\\rho}\\lambda^*$, so it is decreasing in $x$ (fixing $\\lambda^*$).\n\nWe look closer at $x^2 - 4x\\lambda^* +\\frac{5 + 3\\rho}{1 +\\rho}{\\lambda^*}^2 = 1$, which gives $x = 2\\lambda^* + \\sqrt{1 -\\frac{1 -\\rho}{1 +\\rho} {\\lambda^*}^2} \\leq 2\\lambda^* + \\sqrt{1 -\\frac{1 -\\rho}{1 +\\rho} {\\lambda^*}}$.\n\nLet $y = \\min \\left\\{ \\frac{5 + 3\\rho}{2 + 2\\rho}\\lambda^*, 2\\lambda^* + \\sqrt{1 -\\frac{1 -\\rho}{1 +\\rho} {\\lambda^*}} \\right\\}$ be an upper bound on $x$. Both terms in $y$ are increasing in $\\lambda^* \\leq  \\sqrt{\\frac{1 +\\rho}{1 -\\rho}}$, while $x$ has no such monoticity. $$\\begin{aligned}\n     2(1 +\\rho)x^2 - \\frac{2(3 +\\rho)}{1 -\\rho}x\\lambda^* + \\frac{5 + 3\\rho}{(1 -\\rho)^2(1 +\\rho)}{\\lambda^*}^2 \\geq 2(1 +\\rho)y^2 - \\frac{2(3 +\\rho)}{1 -\\rho}y\\lambda^* + \\frac{5 + 3\\rho}{(1 -\\rho)^2(1 +\\rho)}{\\lambda^*}^2\n  \n\\end{aligned}$$\n\nThe RHS of the above line, viewed as a function of $\\lambda^*$ and fixing $y$, is also decreasing in $\\lambda^*$, because the axis of symmetry $$\\frac{(1 -\\rho^2)(3 +\\rho)}{5 + 3\\rho}y =\\frac{(1 -\\rho^2)(3 +\\rho)}{5 + 3\\rho}\\min \\left\\{ \\frac{5 + 3\\rho}{2 + 2\\rho}\\lambda^*, 2\\lambda^* + \\sqrt{1 -\\frac{1 -\\rho}{1 +\\rho} {\\lambda^*}} \\right\\} \\geq \\lambda^*.$$ (The above line can be proven for $\\lambda^* < \\sqrt{\\frac{1 +\\rho}{1 -\\rho}}$ and $\\rho < 0.415$ as we have required.)\n\nThus, as a whole, $\\frac{\\mathop{\\mathrm{d\\!}}{RHS(y(\\lambda^*),\\lambda^*)}}{\\mathop{\\mathrm{d\\!}}{\\lambda^*}} = \\frac{\\partial RHS(y,\\lambda^*)}{\\partial y} \\cdot \\frac{\\partial y}{\\partial \\lambda^*} +\\frac{\\partial RHS(y,\\lambda^*)}{\\partial \\lambda^*} \\leq 0$. So the $RHS$ is decreasing in $\\lambda^*$.\n\nWhen we let $\\lambda^* = \\sqrt{\\frac{1 +\\rho}{1 -\\rho}}$ which is the maximum, the $RHS$ is a univariate function of $\\rho\\in(0,0.415)$, which is always greater than $1$. It cannot be equal to $\\frac{1 - 2\\vartheta}{1 -\\vartheta}$, and now we have a contradiction.\n\n#### We then look at the case of $\\rho < 0$.\n\nBefore diving into the proof, we first re-iterate the phase curves in Theorem [3](#thm:SCAD){reference-type=\"ref\" reference=\"thm:SCAD\"} in an equivalent way. As the proof is tedious, the simplified form of the diagram in Theorem [3](#thm:SCAD){reference-type=\"ref\" reference=\"thm:SCAD\"} may not be recognizable, so we describes the diagram in an equivalent way in Theorem [12](#suppthm:scad.sayagain){reference-type=\"ref\" reference=\"suppthm:scad.sayagain\"} again, making it more consistent with what we will see in the proof.\n\nTo ease the notation, we recall in Theorem [3](#thm:SCAD){reference-type=\"ref\" reference=\"thm:SCAD\"} we defined $$\\sqrt{h_6(\\vartheta)} = \\sqrt{\\frac{1 - 2\\vartheta}{1 -\\rho^2}} + \\frac{\\frac{1 - 2|\\rho|}{1 -|\\rho|}\\sqrt{\\frac{1 -2\\vartheta}{1 -\\rho^2}} + \\sqrt{\\left[ \\left( \\frac{1 - 2|\\rho|}{1 -|\\rho|} \\right)^2 + \\frac{1 -|\\rho|}{1 +|\\rho|} \\right](1 -\\vartheta) - \\frac{1 - 2\\vartheta}{(1 +|\\rho|)^2}}}{(1 -|\\rho|)\\left[ \\left( \\frac{1 - 2|\\rho|}{1 -|\\rho|} \\right)^2 + \\frac{1 -|\\rho|}{1 +|\\rho|} \\right]}$$\n\n::: {#suppthm:scad.sayagain .theorem}\n**Theorem 12** (Re-iterating Theorem [3](#thm:SCAD){reference-type=\"ref\" reference=\"thm:SCAD\"} in an equivalent way for $\\rho < 0$). *For $\\rho < 0$ and $a <\\frac{2}{1-|\\rho|}$, the half of the phase diagram of SCAD when $\\vartheta\\in[\\frac{1}{2},1)$ is the same as that of Lasso and SCAD for positive $\\rho$. When $\\vartheta< \\frac{1}{2}$, from left to right:*\n\n*When $|\\rho| \\geq 0.535$ (approximately), $\\sqrt{r} =\\max \\left\\{ \\sqrt{\\frac{5 + 3|\\rho|}{1 -|\\rho|}} \\sqrt{1 -\\vartheta}, \\  \\sqrt{\\frac{1 - 2\\vartheta}{1 -\\rho^2}} + \\frac{1}{1 -|\\rho|}\\right\\}.$*\n\n*When $\\frac{1}{2} \\leq |\\rho| < 0.535$, $$\\sqrt{r} = \\begin{cases} \n        \\sqrt{\\frac{5 + 3|\\rho|}{1 -|\\rho|}} \\sqrt{1 -\\vartheta} & \\mbox{if} \\sqrt{\\frac{1 - 2\\vartheta}{1 -\\vartheta}} \\geq  \\left( 1 - \\frac{2(1 +|\\rho|)}{(1 -|\\rho|)(5 + 3|\\rho|)} \\right) \\sqrt{(5 + 3|\\rho|)(1 +|\\rho|)} \\\\\n        \\sqrt{h_6(\\vartheta)} & \\mbox{if} \\sqrt{\\frac{1 - 2\\vartheta}{1 -\\vartheta}} <  \\left( 1 - \\frac{2(1 +|\\rho|)}{(1 -|\\rho|)(5 + 3|\\rho|)} \\right) \\sqrt{(5 + 3|\\rho|)(1 +|\\rho|)}\n      \\end{cases}$$*\n\n*When $0.3965 \\leq |\\rho| < \\frac{1}{2}$, $$\\sqrt{r} = \\begin{cases} \n        \\sqrt{\\frac{5 + 3|\\rho|}{1 -|\\rho|}} \\sqrt{1 -\\vartheta} & \\mbox{if} \\sqrt{\\frac{1 - 2\\vartheta}{1 -\\vartheta}} \\geq  \\left( 1 - \\frac{2(1 +|\\rho|)}{(1 -|\\rho|)(5 + 3|\\rho|)} \\right) \\sqrt{(5 + 3|\\rho|)(1 +|\\rho|)} \\\\\n        \\sqrt{h_6(\\vartheta)} & \\mbox{if} \\sqrt{\\frac{1 - 2\\vartheta}{1 -\\vartheta}} <  \\left( 1 - \\frac{2(1 +|\\rho|)}{(1 -|\\rho|)(5 + 3|\\rho|)} \\right) \\sqrt{(5 + 3|\\rho|)(1 +|\\rho|)} \\mbox{ and} \\sqrt{\\frac{1 - 2\\vartheta}{1 -\\vartheta}} > \\frac{(1 +|\\rho|)(1 - 2|\\rho|)}{1 -|\\rho|}\\\\\n        \\sqrt{\\frac{1 - 2\\vartheta}{1 -|\\rho|^2}} +  \\sqrt{\\frac{1 +|\\rho|}{1 -|\\rho|}}\\frac{\\sqrt{1 -\\vartheta}}{1-|\\rho|} & \\mbox{if} \\sqrt{\\frac{1 - 2\\vartheta}{1 -\\vartheta}} \\leq  \\frac{(1 +|\\rho|)(1 - 2|\\rho|)}{1 -|\\rho|}\n      \\end{cases}$$*\n\n*When $\\frac{1}{3} \\leq |\\rho| < 0.3965$, $$\\sqrt{r} = \\begin{cases} \n        \\sqrt{\\frac{5 + 3|\\rho|}{1 -|\\rho|}} \\sqrt{1 -\\vartheta} & \\mbox{if} \\sqrt{\\frac{1 - 2\\vartheta}{1 -\\vartheta}} \\geq  \\left( 1 - \\frac{2(1 +|\\rho|)}{(1 -|\\rho|)(5 + 3|\\rho|)} \\right) \\sqrt{(5 + 3|\\rho|)(1 +|\\rho|)} \\\\\n        \\sqrt{h_6(\\vartheta)} & \\mbox{if} \\sqrt{\\frac{1 - 2\\vartheta}{1 -\\vartheta}} <  \\left( 1 - \\frac{2(1 +|\\rho|)}{(1 -|\\rho|)(5 + 3|\\rho|)} \\right) \\sqrt{(5 + 3|\\rho|)(1 +|\\rho|)} \\mbox{ and} \\sqrt{\\frac{1 - 2\\vartheta}{1 -\\vartheta}} > \\frac{(1 +|\\rho|)(1 - 2|\\rho|)}{1 -|\\rho|}\\\\\n        \\sqrt{\\frac{1 - 2\\vartheta}{1 -|\\rho|^2}} +  \\sqrt{\\frac{1 +|\\rho|}{1 -|\\rho|}}\\frac{\\sqrt{1 -\\vartheta}}{1-|\\rho|} & \\mbox{if} \\sqrt{\\frac{1 - 2\\vartheta}{1 -\\vartheta}} \\leq  \\frac{(1 +|\\rho|)(1 - 2|\\rho|)}{1 -|\\rho|} \\mbox{ and}\\sqrt{\\frac{1 - 2\\vartheta}{1 -\\vartheta}} \\geq \\sqrt{1 -|\\rho|^2} - \\frac{|\\rho|(1 +|\\rho|)}{(1 -|\\rho|)} \\\\\n        \\left( 1 + \\sqrt{\\frac{1 +|\\rho|}{1 -|\\rho|}} \\right) \\sqrt{1 -\\vartheta} & \\mbox{if} \\sqrt{\\frac{1 - 2\\vartheta}{1 -\\vartheta}} < \\sqrt{1 -|\\rho|^2} - \\frac{|\\rho|(1 +|\\rho|)}{(1 -|\\rho|)}\n      \\end{cases}$$*\n\n*When $0.311 \\leq |\\rho| < \\frac{1}{3}$, $$\\sqrt{r} = \\begin{cases} \n        \\sqrt{\\frac{5 + 3|\\rho|}{1 -|\\rho|}} \\sqrt{1 -\\vartheta} & \\mbox{if} \\sqrt{\\frac{1 - 2\\vartheta}{1 -\\vartheta}} \\geq  \\left( 1 - \\frac{2(1 +|\\rho|)}{(1 -|\\rho|)(5 + 3|\\rho|)} \\right) \\sqrt{(5 + 3|\\rho|)(1 +|\\rho|)} \\\\\n        \\sqrt{h_6(\\vartheta)} & \\mbox{if} \\sqrt{\\frac{1 - 2\\vartheta}{1 -\\vartheta}} <  \\left( 1 - \\frac{2(1 +|\\rho|)}{(1 -|\\rho|)(5 + 3|\\rho|)} \\right) \\sqrt{(5 + 3|\\rho|)(1 +|\\rho|)} \\mbox{ and} \\sqrt{\\frac{1 - 2\\vartheta}{1 -\\vartheta}} > \\frac{(1 +|\\rho|)(1 - 2|\\rho|)}{1 -|\\rho|}\\\\\n        \\sqrt{\\frac{1 - 2\\vartheta}{1 -|\\rho|^2}} +  \\sqrt{\\frac{1 +|\\rho|}{1 -|\\rho|}}\\frac{\\sqrt{1 -\\vartheta}}{1-|\\rho|} & \\mbox{if} \\sqrt{\\frac{1 - 2\\vartheta}{1 -\\vartheta}} \\leq  \\frac{(1 +|\\rho|)(1 - 2|\\rho|)}{1 -|\\rho|} \\mbox{ and}\\sqrt{\\frac{1 - 2\\vartheta}{1 -\\vartheta}} \\geq \\sqrt{1 -|\\rho|^2} - \\frac{|\\rho|(1 +|\\rho|)}{(1 -|\\rho|)} \\\\\n        \\max \\{\\left( 1 + \\sqrt{\\frac{1 +|\\rho|}{1 -|\\rho|}} \\right) \\sqrt{1 -\\vartheta},& 1 + \\sqrt{1 -\\vartheta}\\}  \\mbox{ if} \\sqrt{\\frac{1 - 2\\vartheta}{1 -\\vartheta}} < \\sqrt{1 -|\\rho|^2} - \\frac{|\\rho|(1 +|\\rho|)}{(1 -|\\rho|)}\n      \\end{cases}$$*\n\n*When $0.28832 \\leq |\\rho| < 0.311$, $$\\sqrt{r} = \\begin{cases} \n        \\sqrt{h_6(\\vartheta)} & \\mbox{if} \\sqrt{\\frac{1 - 2\\vartheta}{1 -\\vartheta}} > \\frac{(1 +|\\rho|)(1 - 2|\\rho|)}{1 -|\\rho|}\\\\\n        \\sqrt{\\frac{1 - 2\\vartheta}{1 -|\\rho|^2}} +  \\sqrt{\\frac{1 +|\\rho|}{1 -|\\rho|}}\\frac{\\sqrt{1 -\\vartheta}}{1-|\\rho|} & \\mbox{if} \\sqrt{\\frac{1 - 2\\vartheta}{1 -\\vartheta}} \\leq  \\frac{(1 +|\\rho|)(1 - 2|\\rho|)}{1 -|\\rho|} \\mbox{ and}\\sqrt{\\frac{1 - 2\\vartheta}{1 -\\vartheta}} \\geq \\sqrt{1 -|\\rho|^2} - \\frac{|\\rho|(1 +|\\rho|)}{(1 -|\\rho|)} \\\\\n        \\max \\{\\left( 1 + \\sqrt{\\frac{1 +|\\rho|}{1 -|\\rho|}} \\right) \\sqrt{1 -\\vartheta},& 1 + \\sqrt{1 -\\vartheta}\\}  \\mbox{ if} \\sqrt{\\frac{1 - 2\\vartheta}{1 -\\vartheta}} < \\sqrt{1 -|\\rho|^2} - \\frac{|\\rho|(1 +|\\rho|)}{(1 -|\\rho|)}\n      \\end{cases}$$*\n\n*When $0 \\leq |\\rho| < 0.28832$, $$\\sqrt{r} = \\begin{cases} \n        \\sqrt{h_6(\\vartheta)} & \\mbox{if} \\sqrt{\\frac{1 - 2\\vartheta}{1 -\\vartheta}} > \\frac{(1 +|\\rho|)(1 - 2|\\rho|)}{1 -|\\rho|}\\\\\n        \\sqrt{\\frac{1 - 2\\vartheta}{1 -|\\rho|^2}} +  \\sqrt{\\frac{1 +|\\rho|}{1 -|\\rho|}}\\frac{\\sqrt{1 -\\vartheta}}{1-|\\rho|} & \\mbox{if} \\sqrt{\\frac{1 - 2\\vartheta}{1 -\\vartheta}} \\leq  \\frac{(1 +|\\rho|)(1 - 2|\\rho|)}{1 -|\\rho|} \\mbox{ and}\\sqrt{\\frac{1 - 2\\vartheta}{1 -\\vartheta}} \\geq \\sqrt{1 -|\\rho|^2} - \\frac{|\\rho|(1 +|\\rho|)}{(1 -|\\rho|)} \\\\\n        \\max \\{ \\sqrt{\\frac{1-2 \\vartheta}{1-|\\rho|^{2}}}+\\frac{1}{1-|\\rho|},& 1 + \\sqrt{1 -\\vartheta} \\} \\mbox{ if} \\sqrt{\\frac{1 - 2\\vartheta}{1 -\\vartheta}} < \\sqrt{1 -|\\rho|^2} - \\frac{|\\rho|(1 +|\\rho|)}{(1 -|\\rho|)}\n      \\end{cases}$$*\n:::\n\nWo note two things additionally: First, the exact computation of the numerical results (e.g. \"0.28832\") are covered in the rest of the proof. Second, $$\\left( 1 - \\frac{2(1 +|\\rho|)}{(1 -|\\rho|)(5 + 3|\\rho|)} \\right) \\sqrt{(5 + 3|\\rho|)(1 +|\\rho|)} \\equiv \\frac{3-4|\\rho|-3 \\rho^{2}}{(1-|\\rho|)} \\sqrt{\\frac{1+|\\rho|}{5+3|\\rho|}}$$ always holds, and the shorter RHS is shown in Theorem [3](#thm:SCAD){reference-type=\"ref\" reference=\"thm:SCAD\"}.\n\nBefore proving Theorem [12](#suppthm:scad.sayagain){reference-type=\"ref\" reference=\"suppthm:scad.sayagain\"} (and Theorem [3](#thm:SCAD){reference-type=\"ref\" reference=\"thm:SCAD\"} at the same time). we put two important results here.\n\n-   In $f_1(\\sqrt{r},\\lambda')$ (recall Theorem [11](#suppthm:hamm.scad.2){reference-type=\"ref\" reference=\"suppthm:hamm.scad.2\"}), when $2\\lambda' < \\sqrt{r} < \\frac{5 + 3|\\rho|}{2 + 2|\\rho|}\\lambda'$, $$\\begin{aligned}\n            d^2(B,(|\\rho| \\sqrt{r},\\sqrt{r})) =&~ (1 -|\\rho|^2)\\left[r - 4\\sqrt{r}\\lambda' +\\frac{5 + 3|\\rho|}{1 +|\\rho|}\\lambda'^2\\right] \\label{eq:dist.B}.\n          \n    \\end{aligned}$$\n\n-   In $f_3(\\sqrt{r},\\lambda')$, when $\\sqrt{r} \\geq \\frac{2\\lambda'}{1 -|\\rho|}$, $$\\begin{aligned}\n        d^2\\left(D,\\left((1 -|\\rho|)\\sqrt{r},-(1-|\\rho|)\\sqrt{r}\\right)\\right) =(1 -|\\rho|^2)\\left[ 2(1 -|\\rho|)r - 2(3 -|\\rho|)\\lambda' \\sqrt{r} + \\frac{5 - 3|\\rho|}{1 -|\\rho|}\\lambda'^2\\right]\n      \n    \\end{aligned}$$\n\nThen we move on to the proof, which has four sections: $FP_1 = FN_1$, $FP_2 = FN_1$, $FP_1 = FN_2$, $FP_2 = FN_2$.\n\n*First*, if $\\lambda'^2 =\\vartheta+f_2(\\sqrt{r},\\lambda')=1$ and $\\vartheta+f_1(\\sqrt{r},\\lambda')\\geq 1$, $2\\vartheta+f_3(\\sqrt{r},\\lambda')\\geq 1$, we will prove that $\\sqrt{r} = 1 + \\sqrt{1 -\\vartheta}$ is part of the diagram with the condition $$\\begin{cases} \n    \\vartheta\\geq \\frac{2|\\rho|}{1 +|\\rho|}\\\\\n    1 + \\sqrt{1 -\\vartheta} \\geq \\sqrt{\\frac{1 - 2\\vartheta}{1 -\\rho^2}} + \\frac{1}{1 -|\\rho|}\n  \\end{cases}$$ When $\\vartheta> \\frac{1}{2}$, the second condition can be ignored. Even when $\\vartheta\\leq  \\frac{1}{2}$, the second condition is restrictive (stronger than the first one) only when $|\\rho| < 0. 28832$ approximately. ($LHS - RHS$ is increasing in $\\vartheta$.)\n\nWe know $\\lambda' = 1$ and $\\sqrt{r} > 1$. From the previous discussion in the case of positive correlation, we already know that only one curve is possible, which is $$\\sqrt{r} = 1 + \\sqrt{1 -\\vartheta}$$\n\nWe still need it to meet the following requirements: $$\\begin{cases} \n  1 + \\sqrt{1 -\\vartheta} \\geq \\frac{1}{1 + |\\rho|} + \\sqrt{\\frac{1 -\\vartheta}{1 -\\rho^2}} & \\text{for it to be the smallest among the three in }f_2\\\\\n  1 + \\sqrt{1 -\\vartheta} \\geq \\sqrt{1 -\\vartheta}\\sqrt{\\frac{5 + 3|\\rho|}{(1 -|\\rho|)(2 +|\\rho|)^2}} & \\text{for it to be the smallest among the three in }f_2\\\\\n  1 + \\sqrt{1 -\\vartheta} \\leq \\frac{5 + 3|\\rho|}{(1 -|\\rho|)(1 +|\\rho|)^2} & \\text{pre-condition in \\( f_2 \\); not restrictive} \\\\\n  \\vartheta>\\frac{2|\\rho|}{1 +|\\rho|}  & \\text{for \\( \\vartheta+f_1(\\sqrt{r},\\lambda')\\geq 1 \\) } \\\\\n  2\\vartheta+f_3(\\sqrt{r},\\lambda')\\geq 1\n  \\end{cases}$$ Among the first 4 requiremenrs, the fourth one can imply the rest. Finally, we look at $2\\vartheta+f_3(\\sqrt{r},\\lambda')\\geq 1$.\n\nWhen $\\vartheta\\geq  \\frac{1}{2}$, we naturally have $2\\vartheta+f_3(\\sqrt{r},\\lambda')\\geq 1$. When $|\\rho| \\geq \\frac{1}{3}$, $\\vartheta>\\frac{2|\\rho|}{1 +|\\rho|} \\geq \\frac{1}{2}$ always holds, and no more discussion is needed. When $|\\rho| < \\frac{1}{3}$, we proceed to the following discussion.\n\nFor $|\\rho| < \\frac{1}{3}$, since $\\sqrt{r} = 1 + \\sqrt{1 -\\vartheta} \\leq 2 < \\frac{2\\lambda'}{1 -|\\rho|}$, we need $1 + \\sqrt{1 -\\vartheta} \\geq \\sqrt{\\frac{1 - 2\\vartheta}{1 -\\rho^2}} + \\frac{1}{ 1-|\\rho|}$.\n\n*Second*, if $\\vartheta+f_1(\\sqrt{r},\\lambda')=\\vartheta+f_2(\\sqrt{r},\\lambda')= 1$ and $\\lambda' \\geq  1$, $2\\vartheta+f_3(\\sqrt{r},\\lambda')\\geq 1$, we will prove that we can have one curve at most, $\\sqrt{r} = \\left( 1 + \\sqrt{\\frac{1 +|\\rho|}{1 -|\\rho|}} \\right) \\sqrt{1 -\\vartheta}$, and it exists in the interval: (Define $\\psi(|\\rho|)\\stackrel{\\mathrm{def}}{=}\\sqrt{1 -\\rho^2}\\left( 1 - \\frac{|\\rho|}{1 -|\\rho|}\\sqrt{\\frac{1 +|\\rho|}{1 -|\\rho|}} \\right)$.)\n\n-   When $0.28832 \\leq |\\rho| \\leq 0.3965$, the curve exists in the interval $\\left[ \\frac{1 - \\psi(|\\rho|)^2}{2 - \\psi(|\\rho|)^2} ,\\ \\frac{2|\\rho|}{1 +|\\rho|} \\right)$.\n\n-   When $|\\rho| < 0.28832$, the curve does not exist.\n\n-   When $|\\rho| > 0.3965$, the curve exists in the interval $[\\frac{1}{2},\\frac{2|\\rho|}{1 +|\\rho|})$.\n\nWe discuss the conditional expression of $f_1(\\sqrt{r},\\lambda')$ to prove such result:\n\nWhen $\\lambda' \\leq \\sqrt{r} \\leq 2\\lambda'$ in $f_1(\\sqrt{r},\\lambda')$, we have $\\lambda' = \\sqrt{\\frac{1 +|\\rho|}{1 -|\\rho|}}\\sqrt{1 -\\vartheta}.$ Because we want $\\lambda' > 1$, it implies $\\vartheta<\\frac{2|\\rho|}{1 +|\\rho|}.$\n\nWe have discussed and eliminated several curves in the case of positive correlation in the case of positive correlation, so now we are only left with one curve: $\\sqrt{r} = \\left( 1 + \\sqrt{\\frac{1 +|\\rho|}{1 -|\\rho|}} \\right) \\sqrt{1 -\\vartheta}$ and we only need to additionally verify $2\\vartheta+f_3(\\sqrt{r},\\lambda')\\geq 1$.\n\nSince $\\vartheta<\\frac{2|\\rho|}{1 +|\\rho|}$, when $|\\rho| > \\frac{1}{3}$, $FN_2 = o(1)$ naturally holds for $\\vartheta\\in[\\frac{1}{2},\\frac{2|\\rho|}{1 +|\\rho|})$. We only need to discuss $\\vartheta\\in(0,\\frac{1}{2})$; when $|\\rho| \\leq \\frac{1}{3}$, we need to verify $FN_2 = o(1)$ for all $\\vartheta\\leq [0, \\frac{2|\\rho|}{1 +|\\rho|})$.\n\nNow it can be verified that $\\sqrt{r} \\leq \\frac{2}{1 -|\\rho|}\\lambda'$ always holds, so we need $$\\left( 1 + \\sqrt{\\frac{1 +|\\rho|}{1 -|\\rho|}} \\right) \\sqrt{1 -\\vartheta} \\geq \\sqrt{\\frac{1 - 2\\vartheta}{1-\\rho^2}} + \\frac{1}{1 -|\\rho|} \\sqrt{\\frac{1 +|\\rho|}{1 -|\\rho|}}\\sqrt{1 -\\vartheta}$$ which implies $$\\label{eq:FP_2.FN_1}\n  \\psi(|\\rho|) =\\sqrt{1 -\\rho^2}\\left( 1 - \\frac{|\\rho|}{1 -|\\rho|}\\sqrt{\\frac{1 +|\\rho|}{1 -|\\rho|}} \\right) \\geq \\sqrt{\\frac{1 - 2\\vartheta}{1 -\\vartheta}}.$$\n\nBy taking a close look at the function of $|\\rho|$ on the LHS:\n\n-   When $\\frac{1}{3} <|\\rho| < 0.3965$ (approximately), $\\psi(|\\rho|)$ is positive and smaller than 1. If we define $\\psi(|\\rho|)\\stackrel{\\mathrm{def}}{=}\\sqrt{1 -\\rho^2}\\left( 1 - \\frac{|\\rho|}{1 -|\\rho|}\\sqrt{\\frac{1 +|\\rho|}{1 -|\\rho|}} \\right)$, then the curve $\\sqrt{r} = \\left( 1 + \\sqrt{\\frac{1 +|\\rho|}{1 -|\\rho|}} \\right) \\sqrt{1 -\\vartheta}$ exists in the interval $\\left[ \\frac{1 - \\psi(|\\rho|)^2}{2 - \\psi(|\\rho|)^2} ,\\ \\frac{2|\\rho|}{1 +|\\rho|} \\right)$.\n\n-   When $|\\rho| \\geq 0.3965$, $\\psi(|\\rho|)$ is negative. The curve $\\sqrt{r} = \\left( 1 + \\sqrt{\\frac{1 +|\\rho|}{1 -|\\rho|}} \\right) \\sqrt{1 -\\vartheta}$ exists only in the interval $[\\frac{1}{2},\\frac{2|\\rho|}{1 +|\\rho|})$.\n\n-   When $0.28832 \\leq |\\rho| \\leq \\frac{1}{3}$, the curve $\\sqrt{r} = \\left( 1 + \\sqrt{\\frac{1 +|\\rho|}{1 -|\\rho|}} \\right) \\sqrt{1 -\\vartheta}$ exists in the interval $\\left[ \\frac{1 - \\psi(|\\rho|)^2}{2 - \\psi(|\\rho|)^2} ,\\ \\frac{2|\\rho|}{1 +|\\rho|} \\right)$.\n\n-   When $|\\rho| < 0.28832$, this curve does not exist at all, because on the RHS of the inequality [\\[eq:FP_2.FN_1\\]](#eq:FP_2.FN_1){reference-type=\"eqref\" reference=\"eq:FP_2.FN_1\"}, the smallest value it can take is $\\sqrt{\\frac{1 - 2(2|\\rho|)/(1 +|\\rho|)}{1 -(2|\\rho|)/(1 +|\\rho|)}}$. When $|\\rho| < 0.28832$, even this smallest value is greater than the LHS, so the inequality cannot hold.\n\nWhen $\\sqrt{r} \\geq \\frac{5 + 3|\\rho|}{2 + 2|\\rho|}\\lambda'$ in $FP_2$, or when $2\\lambda' < \\sqrt{r} < \\frac{5 + 3|\\rho|}{2 + 2|\\rho|}\\lambda'$, the same proof for positive correlation can be used to eliminate these cases.\n\nTo sum up the whole case of $\\vartheta+f_1(\\sqrt{r},\\lambda')= \\vartheta+f_2(\\sqrt{r},\\lambda')= 1$: We can have one curve at most, $\\sqrt{r} = \\left( 1 + \\sqrt{\\frac{1 +|\\rho|}{1 -|\\rho|}} \\right) \\sqrt{1 -\\vartheta}$, which exists in the interval mentioned above.\n\nNow we are left with the *third* and *fourth* cases, both requiring $2\\vartheta+f_3(\\sqrt{r},\\lambda')= 1$. We assume $\\vartheta\\leq \\frac{1}{2}$ from now on, because the different definition of $f_3(\\sqrt{r},\\lambda')$ makes no difference when $\\vartheta\\geq  \\frac{1}{2}$, and the proof for $\\rho >0$ can be copied.\n\n*Third,* if $\\lambda'^2 =2\\vartheta+f_3(\\sqrt{r},\\lambda')= 1$ and $\\vartheta+f_1(\\sqrt{r},\\lambda')\\geq 1$, $\\vartheta+f_2(\\sqrt{r},\\lambda')\\geq 1$, since $\\lambda' =1$ is fixed, all of $(f_1(\\sqrt{r},\\lambda'),f_2(\\sqrt{r},\\lambda'),f_3(\\sqrt{r},\\lambda'))$ are increasing in $\\sqrt{r}$. As a result, the requirement from $\\vartheta+f_2(\\sqrt{r},\\lambda')\\geq 1$ is just $$\\label{eq:req.FN_1}\n  \\sqrt{r} \\geq \\max\\left\\{ 1+ \\sqrt{1 -\\vartheta},\\ \\sqrt{\\frac{1 -\\vartheta}{1 -\\rho^2}} + \\frac{1}{1 +|\\rho|},\\ \\sqrt{\\frac{5 + 3|\\rho|}{(1 -|\\rho|)(2 +|\\rho|)^2}}\\sqrt{1 -\\vartheta} \\right\\}$$\n\nWhen $\\sqrt{r} \\leq \\frac{2\\lambda'}{1 -|\\rho|}$ in $f_3(\\sqrt{r},\\lambda')$, we have $\\sqrt{r} = \\sqrt{\\frac{1 -2\\vartheta}{1 -|\\rho|^2}} + \\frac{1}{1 -|\\rho|}$, and we will see this curve does not exist in the diagram. We need to look at $\\vartheta+f_1(\\sqrt{r},\\lambda')\\geq 1$:\n\nIf $\\sqrt{r} \\leq 2$ in $f_1(\\sqrt{r},\\lambda')$, then we can limit $|\\rho| <\\frac{1}{2}$ because we have assumed $\\vartheta< \\frac{1}{2}$, and now we need $\\frac{2|\\rho|}{1 +|\\rho|} \\leq \\vartheta< \\frac{1}{2}$,\n\n-   When $0.28832 \\leq |\\rho| \\leq \\frac{1}{2}$, this case does not exist. This is because we also need $\\sqrt{r} =\\sqrt{\\frac{1 -2\\vartheta}{1 -|\\rho|^2}} + \\frac{1}{1 -|\\rho|} \\geq 1 + \\sqrt{1 -\\vartheta}.$ But we already know from the start of the *first* case, that $\\vartheta\\geq \\frac{2|\\rho|}{1 +|\\rho|}$ implies $1 + \\sqrt{1 -\\vartheta} \\geq  \\sqrt{\\frac{1 -2\\vartheta}{1 -|\\rho|^2}} + \\frac{1}{1 -|\\rho|}$.\n\n-   When $|\\rho| < 0.28832$, this case indeed exists, though it is visually only a tiny segment. The curve exists in the interval defined by $$\\begin{cases} \n            \\vartheta\\geq \\frac{2|\\rho|}{1 +|\\rho|}\\\\\n            \\sqrt{r} =\\sqrt{\\frac{1 -2\\vartheta}{1 -|\\rho|^2}} + \\frac{1}{1 -|\\rho|} \\geq 1 + \\sqrt{1 -\\vartheta}\n            \\end{cases}$$ in which the second line is an upper bound. (Actually, $\\vartheta\\geq \\frac{2|\\rho|}{1 +|\\rho|} \\implies \\sqrt{\\frac{1 -2\\vartheta}{1 -|\\rho|^2}} + \\frac{1}{1 -|\\rho|} \\leq 2$)\n\n    In terms of [\\[eq:req.FN_1\\]](#eq:req.FN_1){reference-type=\"eqref\" reference=\"eq:req.FN_1\"} (the requirements of $\\vartheta+f_2(\\sqrt{r},\\lambda')$), we still need to verify $\\sqrt{r} \\geq \\max\\left\\{\\sqrt{\\frac{1 -\\vartheta}{1 -|\\rho|^2}} + \\frac{1}{1 +|\\rho|},\\ \\sqrt{\\frac{5 + 3|\\rho|}{(1 -|\\rho|)(2 +|\\rho|)^2}}\\sqrt{1 -\\vartheta}  \\right\\}$. Using $\\sqrt{1 -\\vartheta} \\leq \\sqrt{r} - 1$ to replace $\\sqrt{1 -\\vartheta}$, we will find both of these are much weaker than $\\sqrt{r} \\leq 2$ and not restrictive.\n\nIf $\\sqrt{r} \\geq \\frac{5 + 3|\\rho|}{2 + 2|\\rho|}$ in $f_1(\\sqrt{r},\\lambda')$, then, in terms of $\\vartheta+f_1(\\sqrt{r},\\lambda')\\geq 1$, we are required to have $$\\begin{cases} \n  \\sqrt{r} \\geq  \\frac{5 + 3|\\rho|}{2 + 2|\\rho|}\\\\\n  \\sqrt{r} \\geq \\sqrt{\\frac{5 + 3|\\rho|}{1 -|\\rho|}}\\sqrt{1 -\\vartheta}\n  \\end{cases}$$ Now, by using $\\frac{1}{2}\\sqrt{r} \\geq \\frac{5 + 3|\\rho|}{2(2 + 2|\\rho|)}$ and $\\frac{1}{2}\\sqrt{r}  \\geq \\frac{1}{2}\\sqrt{\\frac{5 + 3|\\rho|}{1 -|\\rho|}}\\sqrt{1 -\\vartheta}$, we can easily verify that the requirements of $\\vartheta+f_2(\\sqrt{r},\\lambda')\\geq 1$ in [\\[eq:req.FN_1\\]](#eq:req.FN_1){reference-type=\"eqref\" reference=\"eq:req.FN_1\"} all holds. So we only need to focus on $\\vartheta+f_1(\\sqrt{r},\\lambda')\\geq 1$.\n\nWe further argue that $\\sqrt{r} \\geq \\sqrt{\\frac{5 + 3|\\rho|}{1 -|\\rho|}}\\sqrt{1 -\\vartheta}$ implies $\\sqrt{r} \\geq  \\frac{5 + 3|\\rho|}{2 + 2|\\rho|}$.\n\n-   If $\\sqrt{\\frac{5 + 3|\\rho|}{1 -|\\rho|}}\\sqrt{1 -\\vartheta} <  \\frac{5 + 3|\\rho|}{2 + 2|\\rho|}$, this condition itself imposes a lower bound on $\\vartheta$. Since $\\vartheta< \\frac{1}{2}$, we actually need $|\\rho| \\leq 0.3798$ for such lower bound to be smaller than $\\frac{1}{2}$. However, when $|\\rho|$ is too small, $\\sqrt{r} \\geq \\sqrt{\\frac{5 + 3|\\rho|}{1 -|\\rho|}}\\sqrt{1 -\\vartheta}$ admits no solution at all. Namely, $$\\sqrt{(5 + 3|\\rho|)(1 +|\\rho|)}\\sqrt{1 -\\vartheta} - \\sqrt{1 - 2\\vartheta} \\leq \\frac{\\sqrt{1 -|\\rho|^2}}{1 -|\\rho|}$$ admits no solution. The LHS is not monotone, but its minimum in $\\vartheta\\in(0,\\frac{1}{2})$ is taken at $$\\vartheta= \\frac{(5 + 3|\\rho|)(1 +|\\rho|) - 4}{2(5 + 3|\\rho|)(1 +|\\rho|) - 4}.$$ At this point, the LHS is greater than the RHS, so it admits no solution.\n\n-   Since $\\sqrt{\\frac{5 + 3|\\rho|}{1 -|\\rho|}}\\sqrt{1 -\\vartheta} \\geq  \\frac{5 + 3|\\rho|}{2 + 2|\\rho|}$, we have $\\sqrt{r} \\geq \\sqrt{\\frac{5 + 3|\\rho|}{1 -|\\rho|}}\\sqrt{1 -\\vartheta} \\implies \\sqrt{r} \\geq  \\frac{5 + 3|\\rho|}{2 + 2|\\rho|}$.\n\nAs a result, $\\sqrt{r} = \\sqrt{\\frac{1 -2\\vartheta}{1 -|\\rho|^2}} + \\frac{1}{1 -|\\rho|}$ makes part of the boundary when it is greater than $\\sqrt{\\frac{5 + 3|\\rho|}{1 -|\\rho|}}\\sqrt{1 -\\vartheta}$. We will see later, that the latter is also part of the boundary, when it is greater than $\\sqrt{\\frac{1 -2\\vartheta}{1 -|\\rho|^2}} + \\frac{1}{1 -|\\rho|}$.\n\nIf $2 < \\sqrt{r} < \\frac{5 + 3|\\rho|}{2 + 2|\\rho|}$ in $f_1(\\sqrt{r},\\lambda')$ , then in terms of $\\vartheta+f_1(\\sqrt{r},\\lambda')\\geq 1$, we need $d^2(B,(|\\rho| \\sqrt{r},\\sqrt{r})) \\geq (1 -|\\rho|^2)(1 -\\vartheta)$. Because $2 <  \\sqrt{\\frac{1 -2\\vartheta}{1 -|\\rho|^2}} + \\frac{1}{1 -|\\rho|} < \\frac{5 + 3|\\rho|}{2 + 2|\\rho|}$, we need $|\\rho| \\leq 0.535$; otherwise no solution for $\\vartheta$.\n\nWe now need to use [\\[eq:dist.B\\]](#eq:dist.B){reference-type=\"eqref\" reference=\"eq:dist.B\"}. When $\\vartheta\\geq \\frac{2|\\rho|}{1 +|\\rho|}$, $d^2(B,(|\\rho| \\sqrt{r},\\sqrt{r})) \\geq (1 -\\rho^2)(1 -\\vartheta)$ always holds. However, $\\vartheta\\geq \\frac{2|\\rho|}{1 +|\\rho|}$ also makes $\\sqrt{r} < 2$, which contradicts $\\sqrt{r} \\geq 2$. When $\\vartheta< \\frac{2|\\rho|}{1 +|\\rho|}$, $d^2(B,(|\\rho| \\sqrt{r},\\sqrt{r})) \\geq (1 -\\rho^2)(1 -\\vartheta)$ implies $\\sqrt{r} \\geq 2 + \\sqrt{\\frac{2|\\rho|}{1 +|\\rho|} -\\vartheta}.$ which is $$\\sqrt{1 -\\rho^2}\\sqrt{\\frac{2|\\rho|}{1 +|\\rho|} -\\vartheta} - \\sqrt{1 - 2\\vartheta} \\leq (\\frac{1}{1 -|\\rho|} - 2)\\sqrt{1 -\\rho^2}.$$ We take a close look at the LHS, as a function of $\\vartheta$.\n\n-   Using the lower bound of $\\vartheta$ implied by $\\sqrt{\\frac{1 -2\\vartheta}{1 -\\rho^2}} + \\frac{1}{1 -|\\rho|} < \\frac{5 + 3|\\rho|}{2 + 2|\\rho|}$, the LHS is greater than the RHS.\n\n-   Using the upper bound of $\\vartheta$ implied by $\\sqrt{\\frac{1 -2\\vartheta}{1 -\\rho^2}} + \\frac{1}{1 -|\\rho|} > 2$, the LHS is greater than the RHS.\n\n-   The LHS is either increasing in $\\vartheta$, or decreasing in $\\vartheta$, or first-increasing-then-decreasing. Since $LHS > RHS$ holds at both ends of the interval, it holds for all $\\vartheta$. Now we have a contradition.\n\nWhen $\\sqrt{r} > \\frac{2\\lambda'}{1 -|\\rho|}$ in $f_3(\\sqrt{r},\\lambda')$, this case produces no curve in the diagram, but there is no easy way to eliminate this case. We still need to use some tedious calculation.\n\nIf the smallest term is $\\left[(1 -\\rho^2)\\sqrt{r} -\\lambda'\\right]^2$ in $f_3(\\sqrt{r},\\lambda')$, then we have $\\sqrt{r} = \\sqrt{\\frac{1 - 2\\vartheta}{1 -\\rho^2}} + \\frac{1}{1 -\\rho^2}$ which contradicts $\\sqrt{r} \\geq \\frac{2}{1 -|\\rho|}$.\n\nIf $\\frac{2 -|\\rho|}{1 -|\\rho|} < a < \\frac{2}{1 -|\\rho|}$, we recall the form of $k(\\lambda',a)$: $$k(\\lambda',a) = \\begin{cases} \n    d^2\\left(D,\\left((1 -|\\rho|)\\sqrt{r},-(1-|\\rho|)\\sqrt{r}\\right)\\right) \\qquad \\text{   if } \\frac{2\\lambda'}{1 -|\\rho|} \\leq \\sqrt{r} \\leq \\frac{\\lambda'}{1 -|\\rho|}\\left[2 + \\frac{|\\rho| +\\rho^2}{(a - 2)(1 -\\rho^2) - (|\\rho| +\\rho^2)} \\right]\\\\\n    \\frac{ (1 -\\rho^2)}{1 +\\frac{\\rho^2(a - 1)^2}{(a - 2)^2} -\\frac{2\\rho^2(a - 1)}{a - 2}}\n    \\left[ -\\lambda'\\left(1 + \\frac{a|\\rho|}{a - 2}\\right) + (1 -|\\rho|)\\sqrt{r} \\cdot \\left(1 +\\frac{|\\rho|(a - 1)}{a - 2}\\right)\\right]^2 \\\\\n    \\qquad\\qquad\\qquad{if } \\sqrt{r} \\geq \\frac{\\lambda'}{1 -|\\rho|}\\left[2 + \\frac{|\\rho| +\\rho^2}{(a - 2)(1 -\\rho^2) - (|\\rho| +\\rho^2)} \\right]\n  \\end{cases}$$\n\n-   When $\\sqrt{r} \\geq \\frac{\\lambda'}{1 -|\\rho|}\\left[2 + \\frac{|\\rho| +\\rho^2}{(a - 2)(1 -\\rho^2) - (|\\rho| +\\rho^2)} \\right]$, the expression of $\\sqrt{r}$ is $$\\sqrt{r} =\\frac{1 + \\frac{a|\\rho|}{a - 2} + \\sqrt{1 - 2\\vartheta}\\sqrt{1 +\\frac{\\rho^2(a - 1)^2}{(a - 2)^2} -\\frac{2\\rho^2(a - 1)}{a - 2}}}{(1 -|\\rho|)(1 +\\frac{|\\rho|(a - 1)}{a - 2})}$$ and actually $\\sqrt{r} <  \\frac{\\lambda'}{1 -|\\rho|}\\left[2 + \\frac{|\\rho| +\\rho^2}{(a - 2)(1 -\\rho^2) - (|\\rho| +\\rho^2)} \\right]$, which gives a contradiction. To prove this, we take $\\vartheta= 0$, and re-arrange the terms: $$\\begin{aligned}\n        \\sqrt{1 +\\frac{\\rho^2(a - 1)^2}{(a - 2)^2} -\\frac{2\\rho^2(a - 1)}{a - 2}} <&~ 1 +|\\rho| +\\frac{|\\rho|(1 +\\frac{|\\rho|(a - 1)}{a - 2})}{(a - 2)(1 -|\\rho|) -|\\rho|}\\\\\n        =&~ \\frac{\\rho^2 + (a - 2)^2(1 -\\rho^2)}{(a - 2)^2(1 -|\\rho|) -|\\rho|(a - 2)} \\\\\n        \\Leftrightarrow (a - 2)^2 + \\rho^2 (a - 1)^2 - 2\\rho^2 (a - 1)(a -2)  <&~ \\frac{\\left[ \\rho^2 + (a - 2)^2(1 -\\rho^2) \\right]^2}{\\left[ (a - 2)(1 -|\\rho|) -|\\rho| \\right]^2}\n      \n    \\end{aligned}$$ Multiply each side with $\\left[ (a - 2)(1 -|\\rho|) -|\\rho| \\right]^2$, and we have a polynomial. We can then factorize $LHS - RHS$ and get $-2(a - 2)(a - 1)|\\rho|(1 -|\\rho|)\\left[\\rho^2 + (1 -\\rho^2)(a - 2)^2\\right] < 0$.\n\n-   When $\\sqrt{r} < \\frac{\\lambda'}{1 -|\\rho|}\\left[2 + \\frac{|\\rho| +\\rho^2}{(a - 2)(1 -\\rho^2) - (|\\rho| +\\rho^2)} \\right]$, we calculate the distance related to Point $D$ and eventually get $2(1 -|\\rho|)\\sqrt{r} - 2(3 -|\\rho|)\\sqrt{r}+ \\frac{5 -3|\\rho|}{1 -|\\rho|} = 1 - 2\\vartheta$ which implies $\\sqrt{r} =\\frac{3 -|\\rho|}{2(1 -|\\rho|)} + \\frac{1}{2}\\sqrt{1 -\\frac{4\\vartheta}{1 -|\\rho|}}$. This expression is not too complicated, and we can easily verify $\\sqrt{r} < \\frac{2}{1 -|\\rho|}$ which gives us a contradition.\n\nIf $a \\leq \\frac{2 -|\\rho|}{1 -|\\rho|}$, we still have $\\sqrt{r} =\\frac{3 -|\\rho|}{2(1 -|\\rho|)} + \\frac{1}{2}\\sqrt{1 -\\frac{4\\vartheta}{1 -|\\rho|}} < \\frac{2}{1 -|\\rho|}$ like the case above, which is still a contradiction.\n\n*Fourth*, if $\\vartheta+f_1(\\sqrt{r},\\lambda')=2\\vartheta+f_3(\\sqrt{r},\\lambda')= 1$ and $\\lambda' \\geq 1$, $\\vartheta+f_2(\\sqrt{r},\\lambda')\\geq  1$, then:\n\nFor $\\vartheta+f_2(\\sqrt{r},\\lambda')\\geq  1$: When $\\lambda'$ is fixed, the exponents of $FP$ and $FN$ are all decreasing in $\\sqrt{r}$. We thus need $$\\label{eq:FN_1.new}\n  \\sqrt{r} \\geq \\max\\left\\{ \\lambda' + \\sqrt{1 -\\vartheta},\\ \\sqrt{\\frac{1 -\\vartheta}{1 -\\rho^2}} + \\frac{\\lambda'}{1 +|\\rho|},\\ \\sqrt{\\frac{5 + 3|\\rho|}{(1 -|\\rho|)(2 +|\\rho|)^2}}\\sqrt{1 -\\vartheta} \\right\\}$$\n\nWhen $\\sqrt{r} \\leq \\frac{2\\lambda'}{1 -|\\rho|}$ in $2\\vartheta+f_3(\\sqrt{r},\\lambda')\\geq 1$, for $f_1(\\sqrt{r},\\lambda')$:\n\nIf $\\lambda' \\leq \\sqrt{r} \\leq 2\\lambda'$ in $f_1(\\sqrt{r},\\lambda')$, we have $\\lambda' = \\sqrt{\\frac{1 +|\\rho|}{1 -|\\rho|}}\\sqrt{1 -\\vartheta}$, and thus $\\vartheta\\leq \\frac{2|\\rho|}{1 +|\\rho|}$ and $\\sqrt{r} = \\sqrt{\\frac{1 - 2\\vartheta}{1 -|\\rho|^2}} + \\frac{1}{1 -|\\rho|} \\sqrt{\\frac{1 +|\\rho|}{1 -|\\rho|}}\\sqrt{1 -\\vartheta}.$ Since $\\sqrt{r} \\leq 2\\lambda'$, we need both $|\\rho| <\\frac{1}{2}$ and $\\sqrt{\\frac{1 - 2\\vartheta}{1 -\\vartheta}} \\leq \\frac{(1 +|\\rho|)(1 - 2|\\rho|)}{1 -|\\rho|}$. When we come to verify the conditions in [\\[eq:FN_1.new\\]](#eq:FN_1.new){reference-type=\"eqref\" reference=\"eq:FN_1.new\"}, the first one still dominates the others. As a result, the curve $$\\sqrt{r} = \\sqrt{\\frac{1 - 2\\vartheta}{1 -|\\rho|^2}} + \\frac{1}{1 -|\\rho|} \\sqrt{\\frac{1 +|\\rho|}{1 -|\\rho|}}\\sqrt{1 -\\vartheta}.$$ exists under the following conditions: $$\\begin{cases} \n    |\\rho| < \\frac{1}{2}\\\\\n    \\sqrt{\\frac{1 - 2\\vartheta}{1 -\\vartheta}} \\leq \\frac{(1 +|\\rho|)(1 - 2|\\rho|)}{1 -|\\rho|} \\\\\n    \\sqrt{\\frac{1 - 2\\vartheta}{1 -\\vartheta}} \\geq \\sqrt{1 -|\\rho|^2} - \\frac{|\\rho|(1 +|\\rho|)}{(1 -|\\rho|)}\n  \\end{cases}$$ (The last one implies $\\vartheta\\leq \\frac{2|\\rho|}{1 +|\\rho|}$.)\n\nIf $\\sqrt{r} \\geq \\frac{5 + 3|\\rho|}{2 + 2|\\rho|}$ in $f_1(\\sqrt{r},\\lambda')$ , we have $\\sqrt{r} = \\sqrt{\\frac{5 + 3|\\rho|}{1 -|\\rho|}} \\sqrt{1 -\\vartheta}$ and $\\lambda'$ is computed with $\\sqrt{r} = \\sqrt{\\frac{1 - 2\\vartheta}{1 -|\\rho|^2}} + \\frac{\\lambda'}{1 -|\\rho|}.$\n\nNow, $\\sqrt{r} = \\sqrt{\\frac{5 + 3|\\rho|}{1 -|\\rho|}} \\sqrt{1 -\\vartheta} \\geq \\frac{5 + 3|\\rho|}{2 + 2|\\rho|}$ requires $$\\label{suppeq:one.more.requirement}\n  \\sqrt{\\frac{1 - 2\\vartheta}{1 -\\vartheta}} \\geq \\left( 1 - \\frac{2(1 +|\\rho|)}{(1 -|\\rho|)(5 + 3|\\rho|)} \\right) \\sqrt{(5 + 3|\\rho|)(1 +|\\rho|)}.$$ and $\\vartheta+f_2(\\sqrt{r},\\lambda')\\geq 1$ requires $\\sqrt{\\frac{1 - 2\\vartheta}{1 -\\vartheta}} \\geq \\frac{1 +|\\rho|}{1 -|\\rho|} \\left( 1 - 2|\\rho| \\sqrt{\\frac{5 + 3|\\rho|}{1 +|\\rho|}} \\right)$ and $\\sqrt{\\frac{1 - 2\\vartheta}{1 -\\vartheta}} \\geq \\frac{\\sqrt{1 -\\rho^2}}{1 -|\\rho|} - \\frac{|\\rho|}{1 -|\\rho|} \\sqrt{(5 + 3|\\rho|)(1 +|\\rho|)}$. The conditions required by $\\vartheta+f_2(\\sqrt{r},\\lambda')\\geq 1$ are actually even weaker than $\\sqrt{r} \\geq \\frac{5 + 3|\\rho|}{2 + 2|\\rho|}$. $\\lambda' \\geq 1$ requires $\\sqrt{\\frac{5 + 3|\\rho|}{1 -|\\rho|}} \\sqrt{1 -\\vartheta} \\geq \\sqrt{\\frac{1 - 2\\vartheta}{1 -\\rho^2}} + \\frac{1}{1 -|\\rho|}.$\n\n-   When $|\\rho| \\geq  0.535$, the RHS of $$\\sqrt{\\frac{1 - 2\\vartheta}{1 -\\vartheta}} \\geq \\left( 1 - \\frac{2(1 +|\\rho|)}{(1 -|\\rho|)(5 + 3|\\rho|)} \\right) \\sqrt{(5 + 3|\\rho|)(1 +|\\rho|)}.$$ is negative, thus not restrictive.\n\n    Taking the requirement $\\sqrt{\\frac{5 + 3|\\rho|}{1 -|\\rho|}} \\sqrt{1 -\\vartheta} \\geq \\sqrt{\\frac{1 - 2\\vartheta}{1 -\\rho^2}} + \\frac{1}{1 -|\\rho|}$ into account, now the boundary consists of $$\\max \\left\\{ \\sqrt{\\frac{5 + 3|\\rho|}{1 -|\\rho|}} \\sqrt{1 -\\vartheta}, \\  \\sqrt{\\frac{1 - 2\\vartheta}{1 -\\rho^2}} + \\frac{1}{1 -|\\rho|}\\right\\}.$$\n\n-   When $|\\rho| < 0.535$: $\\sqrt{\\frac{5 + 3|\\rho|}{1 -|\\rho|}} \\sqrt{1 -\\vartheta} \\geq \\sqrt{\\frac{1 - 2\\vartheta}{1 -\\rho^2}} + \\frac{1}{1 -|\\rho|}$ is not restrictive, because it can be re-written as $$\\sqrt{(5 + 3|\\rho|)(1 +|\\rho|)}\\sqrt{1 -\\vartheta} - \\sqrt{1 - 2\\vartheta} \\geq \\frac{\\sqrt{1 -\\rho^2}}{1 -|\\rho|}$$ which always holds, because we have shown the minimum of the LHS is taken at $$\\vartheta= \\frac{(5 + 3|\\rho|)(1 +|\\rho|) - 4}{2(5 + 3|\\rho|)(1 +|\\rho|) - 4},$$ and the minimum can be verified to be greater than the RHS. Thus $$\\sqrt{\\frac{5 + 3|\\rho|}{1 -|\\rho|}} \\sqrt{1 -\\vartheta} \\geq \\sqrt{\\frac{1 - 2\\vartheta}{1 -\\rho^2}} + \\frac{1}{1 -|\\rho|}.$$ is not restrictive. We only need requirement [\\[suppeq:one.more.requirement\\]](#suppeq:one.more.requirement){reference-type=\"eqref\" reference=\"suppeq:one.more.requirement\"}.\n\nIf $2\\lambda' < \\sqrt{r} < \\frac{5 + 3|\\rho|}{2 + 2|\\rho|}\\lambda'$ in $f_1(\\sqrt{r},\\lambda')$, this case is very tedious. We present the closed form of $\\lambda'$ and $\\sqrt{r}$: $$\\begin{aligned}\n    \\lambda' =&~ \\left[ \\left( \\frac{1 - 2|\\rho|}{1 -|\\rho|} \\right)^2 + \\frac{1 -|\\rho|}{1 +|\\rho|} \\right]^{ - 1} \\cdot \\left[ \\frac{1 - 2|\\rho|}{1 -|\\rho|}\\sqrt{\\frac{1 -2\\vartheta}{1 -\\rho^2}} + \\sqrt{\\left[ \\left( \\frac{1 - 2|\\rho|}{1 -|\\rho|} \\right)^2 + \\frac{1 -|\\rho|}{1 +|\\rho|} \\right](1 -\\vartheta) - \\frac{1 - 2\\vartheta}{(1 +|\\rho|)^2}}\\right] \\\\\n    \\sqrt{r} = &~ \\sqrt{\\frac{1 - 2\\vartheta}{1 -\\rho^2}} + \\frac{\\lambda'}{1 -|\\rho|} = 2\\lambda' + \\sqrt{1 -\\vartheta-\\frac{1 -|\\rho|}{1 +|\\rho|}\\lambda'^2}\n  \n\\end{aligned}$$\n\nIn terms of the requirements from $2\\lambda' < \\sqrt{r} < \\frac{5 + 3|\\rho|}{2 + 2|\\rho|}\\lambda'$: First, $\\sqrt{r} > 2\\lambda'$ will give us $$\\begin{aligned}\n    \\text{either } & |\\rho| \\geq \\frac{1}{2} \n    \\text{ or } \\left( |\\rho| < \\frac{1}{2} \\text{ and } \\sqrt{\\frac{1 - 2\\vartheta}{1 -\\vartheta}} > \\frac{(1 +|\\rho|)(1 - 2|\\rho|)}{1 -|\\rho|}\\right)\n  \n\\end{aligned}$$ Second, $\\sqrt{r} < \\frac{5 + 3|\\rho|}{2 + 2|\\rho|}\\lambda'$ will give us $$\\begin{aligned}\n    |\\rho| < 0.535& \\text{ (the same numerical value which appreared before)} \\\\\n    \\text{and }\\sqrt{\\frac{1 - 2\\vartheta}{1 -\\vartheta}} < &~ \\left( 1 - \\frac{2(1 +|\\rho|)}{(1 -|\\rho|)(5 + 3|\\rho|)} \\right) \\sqrt{(5 + 3|\\rho|)(1 +|\\rho|)}.\n  \n\\end{aligned}$$\n\nIn terms of other requirements, we show that they are can be implied by the two conditions we have just arrived at.\n\nWe first look at the requirement $\\lambda' \\geq 1$. We need $\\sqrt{\\frac{1 - 2\\vartheta}{1 -\\vartheta}} \\leq (1 +|\\rho|) \\sqrt{\\left( \\frac{1 - 2|\\rho|}{1 -|\\rho|} \\right)^2 + \\frac{1 -|\\rho|}{1 +|\\rho|}}$ to make the content of the square root positive, but this is implied by $\\sqrt{r} \\leq \\frac{5 + 3|\\rho|}{2 + 2|\\rho|}\\lambda'$. Then $\\lambda' \\geq  1$ is equivalent to: (let $x = \\sqrt{\\frac{1 - 2\\vartheta}{1 -\\vartheta}}$) $$\\begin{aligned}\n    & \\frac{1 - 2|\\rho|}{1 -|\\rho|}\\sqrt{\\frac{1 -2\\vartheta}{1 -\\rho^2}} + \\sqrt{\\left[ \\left( \\frac{1 - 2|\\rho|}{1 -|\\rho|} \\right)^2 + \\frac{1 -|\\rho|}{1 +|\\rho|} \\right](1 -\\vartheta) - \\frac{1 - 2\\vartheta}{(1 +|\\rho|)^2}} \\geq  \\left( \\frac{1 - 2|\\rho|}{1 -|\\rho|} \\right)^2 + \\frac{1 -|\\rho|}{1 +|\\rho|} \\\\\n    \\Leftrightarrow& \\frac{1}{\\sqrt{2 - x^2}} \\left[ \\frac{1 - 2|\\rho|}{1 -|\\rho|}\\frac{x}{\\sqrt{1 -\\rho^2}} + \\sqrt{\\left[ \\left( \\frac{1 - 2|\\rho|}{1 -|\\rho|} \\right)^2 + \\frac{1 -|\\rho|}{1 +|\\rho|} \\right] - \\frac{x^2}{(1 +|\\rho|)^2}}  \\right] \\geq  \\left( \\frac{1 - 2|\\rho|}{1 -|\\rho|} \\right)^2 + \\frac{1 -|\\rho|}{1 +|\\rho|} \\\\\n    &\\text{for } \\max\\left\\{ 0,  \\frac{(1 +|\\rho|)(1 - 2|\\rho|)}{1 -|\\rho|}\\right\\} \\leq x \\leq \\min\\left\\{ 1, \\left( 1 - \\frac{2(1 +|\\rho|)}{(1 -|\\rho|)(5 + 3|\\rho|)} \\right) \\sqrt{(5 + 3|\\rho|)(1 +|\\rho|)} \\right\\} \\\\ &\\text{and } |\\rho|<0.535\n  \n\\end{aligned}$$ It always holds, because we can verify the graph of $(LHS - RHS)$ as a bi-variate function of $(|\\rho|,x)$ is always above zero.\n\nWe then look at the requirement $\\sqrt{r} \\leq \\frac{2\\lambda'}{1 -|\\rho|}$: This is equivalent to $$\\frac{1 - 2\\vartheta}{1 -\\vartheta} \\leq \\frac{\\left(\\frac{1-2 |\\rho|}{1 -|\\rho|}\\right)^2+\\frac{1 -|\\rho|}{1 +|\\rho|}}{\\frac{\\left(\\frac{1 -|\\rho|}{1 +|\\rho|}-\\frac{2 |\\rho| (1-2 |\\rho|)}{(1 -|\\rho|)^2}\\right)^2}{1 -\\rho^2}+\\frac{1}{(1 +|\\rho|)^2}}$$ which is always weaker than $\\sqrt{\\frac{1 - 2\\vartheta}{1 -\\vartheta}} \\leq \\min\\left\\{ 1, \\left( 1 - \\frac{2(1 +|\\rho|)}{(1 -|\\rho|)(5 + 3|\\rho|)} \\right) \\sqrt{(5 + 3|\\rho|)(1 +|\\rho|)} \\right\\}.$\n\nWe finally look at the requirement from $\\vartheta+f_2(\\sqrt{r},\\lambda')\\geq 1$, or equivalently [\\[eq:FN_1.new\\]](#eq:FN_1.new){reference-type=\"eqref\" reference=\"eq:FN_1.new\"}. First, we need to verify $\\sqrt{r} \\geq \\lambda' + \\sqrt{1 -\\vartheta}$. Since $\\sqrt{r} = \\sqrt{\\frac{1 - 2\\vartheta}{1 -\\rho^2}} + \\frac{\\lambda'}{1 -|\\rho|}$, this is equivalent to $\\sqrt{\\frac{1 - 2\\vartheta}{1 -\\rho^2}} + \\frac{|\\rho|}{1 -|\\rho|}\\lambda' \\geq \\sqrt{1 -\\vartheta}$. It naturally holds if $|\\rho| \\leq 0.5$, because $\\lambda' \\geq 1$. When $0.5 <|\\rho| < 0.535$, still letting $x = \\sqrt{\\frac{1 - 2\\vartheta}{1 -\\vartheta}}$, we have $$\\begin{aligned}\n  &\\sqrt{\\frac{1 - 2\\vartheta}{1 -\\rho^2}} + \\frac{|\\rho|}{1 -|\\rho|}\\lambda' \\geq \\sqrt{1 -\\vartheta}\\\\\n  \\Leftrightarrow & \\frac{x}{\\sqrt{1 -\\rho^2}} + \\frac{|\\rho|\\lambda'}{(1 -|\\rho|)}\\sqrt{2 - x^2} \\geq 1 \\\\\n  \\Leftarrow & \\frac{x}{\\sqrt{1 -\\rho^2}} + \\frac{|\\rho|}{(1 -|\\rho|)}\\sqrt{2 - x^2} \\geq 1\n\\end{aligned}$$ The LHS is either increasing in $x\\in(0,1)$, or first-increasing-then-decreasing. When $x = 0$ or $1$, the inequality holds for $0.5 < |\\rho| < 0.535$, so it always holds.\n\nWe still need to verify $\\sqrt{r} \\geq \\max \\left\\{ \\sqrt{\\frac{1 -\\vartheta}{1 -\\rho^2}} + \\frac{\\lambda'}{1 +|\\rho|},\\ \\sqrt{\\frac{5 + 3|\\rho|}{(1 -|\\rho|)(2 +|\\rho|)^2}}\\sqrt{1 -\\vartheta} \\right\\}$. With $\\sqrt{1 -\\vartheta} \\leq \\sqrt{r} -\\lambda'$, we can get rid of $\\sqrt{1 -\\vartheta}$ and arrange either of the requirements as an inequality between $\\sqrt{r}$ and $\\lambda'$. Such an inequality will be weaker than $\\sqrt{r} \\leq \\frac{5 + 3|\\rho|}{2 + 2|\\rho|}\\lambda'$.\n\nWhen $\\sqrt{r} > \\frac{2\\lambda'}{1 -|\\rho|}$ in $f_3(\\sqrt{r},\\lambda')$, we will see this case does not produce any curve in the diagram. We still need to discuss two cases in terms of $f_1(\\sqrt{r},\\lambda')$:\n\nIf $\\sqrt{r} \\geq  \\frac{5 + 3|\\rho|}{2 + 2|\\rho|}\\lambda'$ in $f_1(\\sqrt{r},\\lambda')$: We have $\\sqrt{r} = \\sqrt{\\frac{5 + 3|\\rho|}{1 -|\\rho|}}\\sqrt{1 -\\vartheta}$. Because $\\sqrt{r} > \\frac{2\\lambda'}{1 -|\\rho|}$, we have $$\\lambda' < \\frac{1}{2}\\sqrt{(5 + 3|\\rho|)(1 -|\\rho|)}\\sqrt{1 -\\vartheta}$$ To admit a solution for $\\lambda' \\geq 1$, we need $|\\rho| < \\frac{1}{3}$ so that the RHS is large enough.\n\n-   When $\\sqrt{r} = \\sqrt{\\frac{1 - 2\\vartheta}{1 -\\rho^2}} + \\frac{\\lambda'}{1 -\\rho^2}$, $\\lambda' =(1 -\\rho^2)\\left( \\sqrt{\\frac{5 + 3|\\rho|}{1 -|\\rho|}}\\sqrt{1 -\\vartheta} - \\sqrt{\\frac{1 - 2\\vartheta}{1 -\\rho^2}}\\right)$. Combining this with $\\lambda' < \\frac{1}{2}\\sqrt{(5 + 3|\\rho|)(1 -|\\rho|)}\\sqrt{1 -\\vartheta}$, we can get a requirement for $x = \\sqrt{\\frac{1 - 2\\vartheta}{1 -\\vartheta}}$: $$x > \\sqrt{(5 + 3|\\rho|)(1 +|\\rho|)} - \\frac{1}{2}\\sqrt{\\frac{5 + 3|\\rho|}{1 +|\\rho|}}$$ but the RHS is always greater than 1. Thus we have a contradition.\n\n-   When $\\sqrt{r} =\\frac{3 -|\\rho|}{2(1 -|\\rho|)}\\lambda' + \\frac{1}{2} \\sqrt{\\frac{2(1 - 2\\vartheta)}{1 -|\\rho|} -\\frac{1 +|\\rho|}{1 -|\\rho|}\\lambda'^2}$: We temporarily ignore the relationship between $\\lambda'$ and $\\vartheta$, and the $\\lambda'$ which maximizes $\\sqrt{r} =\\frac{3 -|\\rho|}{2(1 -|\\rho|)}\\lambda' + \\frac{1}{2} \\sqrt{\\frac{2(1 - 2\\vartheta)}{1 -|\\rho|} -\\frac{1 +|\\rho|}{1 -|\\rho|}\\lambda'^2}$ is $\\lambda' = \\sqrt{\\frac{2(3 -|\\rho|)^2(1 - 2\\vartheta)}{(3 -|\\rho|)^2(1 +|\\rho|) +(1 -|\\rho|)(1 +|\\rho|)^2}}$. Even with the maximizer $\\lambda'$, we still have $$\\frac{3 -|\\rho|}{2(1 -|\\rho|)}\\lambda' + \\frac{1}{2} \\sqrt{\\frac{2(1 - 2\\vartheta)}{1 -|\\rho|} -\\frac{1 +|\\rho|}{1 -|\\rho|}\\lambda'^2} < \\sqrt{\\frac{5 + 3|\\rho|}{1 -|\\rho|}}\\sqrt{1 -\\vartheta}$$ for $|\\rho| < \\frac{1}{3}$. Thus we have no solution for $\\lambda'$.\n\n-   When $\\frac{2 -|\\rho|}{1-|\\rho|} < a < \\frac{2}{1 -|\\rho|}$ and $\\sqrt{r} \\geq \\frac{\\lambda'}{1 -|\\rho|}\\left[2 + \\frac{|\\rho| +\\rho^2}{(a - 2)(1 -\\rho^2) - (|\\rho| +\\rho^2)} \\right]$, the contradiction comes from the fact that $$\\sqrt{r} = \\sqrt{\\frac{5 + 3|\\rho|}{1 -|\\rho|}}\\sqrt{1 -\\vartheta} < \\frac{\\lambda'}{1 -|\\rho|}\\left[2 + \\frac{|\\rho| +\\rho^2}{(a - 2)(1 -\\rho^2) - (|\\rho| +\\rho^2)} \\right]$$ for any $\\frac{2 -|\\rho|}{1-|\\rho|} < a < \\frac{2}{1 -|\\rho|}$, $|\\rho| < \\frac{1}{3}$ and $\\lambda' \\geq 1$.\n\nIf $\\frac{2\\lambda'}{1 -|\\rho|} < \\sqrt{r} < \\frac{5 + 3|\\rho|}{2 + 2|\\rho|}\\lambda'$ in $f_1(\\sqrt{r},\\lambda')$: For this case to exist, we need $\\frac{2}{1 -|\\rho|} <\\frac{5 + 3|\\rho|}{2 + 2|\\rho|}$, which requires $|\\rho| < 0.1547$. Solving $d^2(B,(|\\rho| \\sqrt{r},\\sqrt{r})) =(1 -\\rho^2)(1 -\\vartheta)$ in $FP_2$, we have $\\sqrt{r} = 2\\lambda' + \\sqrt{1 -\\vartheta-\\frac{1 -|\\rho|}{1 +|\\rho|}\\lambda'^2}$\n\n-   When $\\sqrt{r} = \\sqrt{\\frac{1 - 2\\vartheta}{1 -\\rho^2}} + \\frac{\\lambda'}{1 -\\rho^2}$, we need $\\sqrt{\\frac{1 - 2\\vartheta}{1 -\\rho^2}} + \\frac{\\lambda'}{1 -\\rho^2} > \\frac{2}{1 -|\\rho|}\\lambda'$ which contradicts $\\lambda' \\geq 1$ when $|\\rho| < 0. 1547$.\n\n-   When $d^2\\left(D,\\left((1 -|\\rho|)\\sqrt{r},-(1-|\\rho|)\\sqrt{r}\\right)\\right)$, the $(\\sqrt{r},\\lambda')$ pair are given by $$\\begin{aligned}\n          \\begin{cases} \n            \\sqrt{r} =&~ 2\\lambda' + \\sqrt{1 -\\vartheta-\\frac{1 -|\\rho|}{1 +|\\rho|}\\lambda'^2}\\\\\n            \\sqrt{r} =&~ \\frac{3 -|\\rho|}{2(1 -|\\rho|)}\\lambda' + \\frac{1}{2} \\sqrt{\\frac{2(1 - 2\\vartheta)}{1 -|\\rho|} -\\frac{1 +|\\rho|}{1 -|\\rho|}\\lambda'^2}\n          \\end{cases} \n        \n    \\end{aligned}$$ which implies $\\frac{1 - 3|\\rho|}{2(1 -|\\rho|)}\\lambda' + \\sqrt{1 -\\vartheta-\\frac{1 -|\\rho|}{1 +|\\rho|}\\lambda'^2} = \\frac{1}{2} \\sqrt{\\frac{2(1 - 2\\vartheta)}{1 -|\\rho|} -\\frac{1 +|\\rho|}{1 -|\\rho|}\\lambda'^2}.$\n\n    However, the above equation has no solution for $\\lambda'$, because the LHS is always greater than the RHS. We look at $(LHS - RHS)$ from now on, and prove it is positive:\n\n    Let $\\lambda^* =\\frac{\\lambda'}{\\sqrt{1 -\\vartheta}}$, and $\\lambda^* \\geq 1$ as well. Then $$\\begin{aligned}\n          \\frac{(LHS - RHS)}{\\sqrt{1 -\\vartheta}} = &~ \n          \\frac{1 - 3|\\rho|}{2(1 -|\\rho|)}\\lambda^* + \\sqrt{1 -\\frac{1 -|\\rho|}{1 +|\\rho|}{\\lambda^*}^2} - \\frac{1}{2} \\sqrt{\\frac{2}{1 -|\\rho|}\\frac{1 - 2\\vartheta}{1-\\vartheta} -\\frac{1 +|\\rho|}{1 -|\\rho|}{\\lambda^*}^2}\\\\\n          \\geq &~ \\frac{1 - 3|\\rho|}{2(1 -|\\rho|)}\\lambda^* + \\sqrt{1 -\\frac{1 -|\\rho|}{1 +|\\rho|}{\\lambda^*}^2} - \\frac{1}{2} \\sqrt{\\frac{2}{1 -|\\rho|} -\\frac{1 +|\\rho|}{1 -|\\rho|}{\\lambda^*}^2}\n        \n    \\end{aligned}$$ Thus we only need to prove $$\\frac{1 - 3|\\rho|}{2(1 -|\\rho|)}\\lambda^* + \\sqrt{1 -\\frac{1 -|\\rho|}{1 +|\\rho|}{\\lambda^*}^2} > \\frac{1}{2} \\sqrt{\\frac{2}{1 -|\\rho|} -\\frac{1 +|\\rho|}{1 -|\\rho|}{\\lambda^*}^2}$$ From the content of the square roots, we can see that $\\lambda^* \\leq \\sqrt{\\frac{1 +|\\rho|}{1 - |\\rho|}}$.\n\n    Square both sides, and ignore the cross term on the LHS, and we can actually prove a stronger result, $$\\left( \\frac{1 - 3|\\rho|}{2(1 -|\\rho|)} \\right)^2 {\\lambda^*}^2 + \\left( 1 -\\frac{1 -|\\rho|}{1 +|\\rho|}{\\lambda^*}^2  \\right) \\geq \\frac{1}{2(1 -|\\rho|)} -\\frac{1 +|\\rho|}{4(1 -|\\rho|)}{\\lambda^*}^2.$$ We only need to verify the two ends, $\\lambda^* = 1$ and $\\lambda^* = \\sqrt{\\frac{1 +|\\rho|}{1 -|\\rho|}}$, to see that this inequality holds for $0 <|\\rho| < 0.1547$.\n\n-   When $\\frac{2 -|\\rho|}{1-|\\rho|} < a < \\frac{2}{1 -|\\rho|}$ and $\\sqrt{r} \\geq \\frac{\\lambda'}{1 -|\\rho|}\\left[2 + \\frac{|\\rho| +\\rho^2}{(a - 2)(1 -\\rho^2) - (|\\rho| +\\rho^2)} \\right]$:\n\n    $\\frac{\\lambda'}{1 -|\\rho|}\\left[2 + \\frac{|\\rho| +\\rho^2}{(a - 2)(1 -\\rho^2) - (|\\rho| +\\rho^2)} \\right]$ is at least $\\frac{3\\lambda'}{1 -|\\rho|}$, which is still greater than $\\sqrt{r} = 2\\lambda' + \\sqrt{1 -\\vartheta-\\frac{1 -|\\rho|}{1 +|\\rho|}\\lambda'^2}$.\n\nWe have finished discussing the last case. To sum up the whole phase diagram, it is exactly Theorem [12](#suppthm:scad.sayagain){reference-type=\"ref\" reference=\"suppthm:scad.sayagain\"}.\n\n## Proof of Lemma [2](#suppthm:sol.path.scad){reference-type=\"ref\" reference=\"suppthm:sol.path.scad\"} {#suppsec:sol.path.scad}\n\n(We have assumed $\\rho > 0$ in Lemma [2](#suppthm:sol.path.scad){reference-type=\"ref\" reference=\"suppthm:sol.path.scad\"}.)\n\nRecall optimization [\\[suppeq:optimization.scad\\]](#suppeq:optimization.scad){reference-type=\"ref\" reference=\"suppeq:optimization.scad\"} and our assumption $b_1 >\\abs{h_2}$. The equation of the sub-gradient for $b =(b_1,b_2)'$ is: $$\\label{suppeq:subgrad.equations.scad}\n    \\begin{bmatrix} 1 & \\rho \\\\ \\rho & 1 \\end{bmatrix} \\begin{bmatrix}b_1 \\\\ b_2 \\end{bmatrix} + \\begin{bmatrix} q'(b_1)\\\\q'(b_2) \\end{bmatrix} = \\begin{bmatrix} h_1 \\\\ h_2 \\end{bmatrix}.$$\n\nWhen $\\lambda'$ is sufficiently large, neither of $(b_1,b_2)$ is nonzero. We investigate the process of decreasing $\\lambda'$ from $\\infty$, and discuss the major stages along the way.\n\n*Stage 1:* When $\\lambda'$ is large, both $(b_j,b_{j + 1})$ are zero, and SCAD behaves like Lasso. When $\\lambda'$ is large, Equation [\\[suppeq:subgrad.equations.scad\\]](#suppeq:subgrad.equations.scad){reference-type=\"eqref\" reference=\"suppeq:subgrad.equations.scad\"} becomes $\\lambda' \\cdot \\mathop{\\mathrm{sgn}}(0) = h_1,\\ \\lambda' \\cdot \\mathop{\\mathrm{sgn}}(0) = h_2$ and so we need $$\\label{eq:lambda-1}\n    \\lambda' \\geq \\lambda'_1 = \\max\\{\\abs{h_1},\\abs{h_2}\\}.$$\n\n*Stage 2:* When $\\lambda'$ crosses $\\lambda'_1 =\\max \\left\\{ \\abs{h_1},\\abs{h_2} \\right\\}$, since we have assumed $h_1 >\\abs{h_2} \\geq 0$, $b_1$ becomes positive. To see this, consider $\\lambda'$ in a very small interval $(\\lambda_1' -\\delta,\\lambda'_1) =(\\abs{h_1} -\\delta,\\abs{h_1})$: $$\\begin{bmatrix} 1 & \\rho \\\\ \\rho & 1 \\end{bmatrix} \\begin{bmatrix}b_1  \\\\ 0 \\end{bmatrix} + \\begin{bmatrix} \\lambda'\\cdot \\mathop{\\mathrm{sgn}}(b_1) \\\\ \\lambda' \\cdot \\mathop{\\mathrm{sgn}}0 \\end{bmatrix} = \\begin{bmatrix} h_1 \\\\ h_2 \\end{bmatrix}.$$ Now we have $b_1 = h_1 -\\lambda'\\cdot \\mathop{\\mathrm{sgn}}(b_1)$. From this equation, we know $b_1$ has the same sign as $h_1$, so $b_1$ enters the model as a positive number. On the other hand, $b_2$ cannot enter the model before $b_1$, or we have a contradiction. This is because we would have $b_2 = h_2 -\\lambda' \\cdot \\mathop{\\mathrm{sgn}}(b_2)$, but the signs of the LHS and RHS can never agree.\n\nNow that $b_1$ is positive, for the above system of equations to admit a solution, we also need $$\\label{suppeq:sol.path.constraint-1.scad}\n    \\abs{h_2 +\\rho \\lambda' -\\rho h_1} <\\lambda'$$\n\n*Stage 3:* When $\\lambda'$ continues to decrease, we have two possible cases when the solution path enters the next stage. First, $b_1$ continues to increase and becomes larger than $\\lambda'$; then its gradient will change according to the definition of SCAD (see [\\[suppeq:definition.SCAD\\]](#suppeq:definition.SCAD){reference-type=\"eqref\" reference=\"suppeq:definition.SCAD\"}), while $b_2 = 0$ all along. Second, $b_2$ enters the model before $b_1$ gets larger than $\\lambda'$.\n\nWe start from the first case mentioned above. In this case, at the next critical point $\\lambda' =\\lambda'_2$, we would have $b_1 =\\lambda'$ while $b_2 = 0$ still. Then $$\\begin{bmatrix} 1 & \\rho \\\\ \\rho & 1 \\end{bmatrix} \\begin{bmatrix}\\lambda'  \\\\ 0 \\end{bmatrix} + \\begin{bmatrix} \\lambda' \\\\ \\lambda' \\cdot \\mathop{\\mathrm{sgn}}0 \\end{bmatrix} = \\begin{bmatrix} h_1 \\\\ h_2 \\end{bmatrix}.$$ Then we have $\\lambda_2'^{(1)} = \\frac{1}{2}h_1$.\n\nFor this equation to admit a solution, we need $\\abs{h_2 -\\rho \\lambda'_2} =\\abs{h_2 - \\frac{1}{2}\\rho h_1} <\\lambda'_2 = \\frac{1}{2}h_1$ which is $$\\label{suppeq:sol.path.constraint-2.scad}\n    \\frac{ - 1 +\\rho}{2}h_1 < h_2 < \\frac{1 +\\rho}{2}h_1.$$ With this constraint [\\[suppeq:sol.path.constraint-2.scad\\]](#suppeq:sol.path.constraint-2.scad){reference-type=\"eqref\" reference=\"suppeq:sol.path.constraint-2.scad\"} and $\\lambda'\\in[\\lambda_2'^{(1)},\\lambda'_1]$, we can also go back to check the Condition [\\[suppeq:sol.path.constraint-1.scad\\]](#suppeq:sol.path.constraint-1.scad){reference-type=\"eqref\" reference=\"suppeq:sol.path.constraint-1.scad\"} in the previous stage, and we can see it holds.\n\nWe then consider the next case in which $b_2$ enters the model first. This case is essentially Lasso. We solve $$\\begin{bmatrix} 1 & \\rho \\\\ \\rho & 1 \\end{bmatrix} \\begin{bmatrix} b_1  \\\\ b_2 \\end{bmatrix} + \\begin{bmatrix} \\lambda' \\\\ \\lambda' \\cdot \\mathop{\\mathrm{sgn}}(b_2) \\end{bmatrix} = \\begin{bmatrix} h_1 \\\\ h_2 \\end{bmatrix}.$$ when $\\lambda'\\in(\\lambda_2'^{(2)} -\\delta,\\lambda_2'^{(2)})$. We then have two cases, depending on the sign of $b_2$ when it enters the model. If $b_2$ enters the model as a positive number, then eventually we have $$0 < \\rho h_1 < h_2 < h_1,\\text{ and } \\lambda' < \\lambda'_2 =\\frac{h_2-\\rho h_1}{1-\\rho}$$ If $b_2$ enters the model as a negative number, then eventually we have $$- h_1 < h_2 <\\rho h_1, \\text{ and } \\lambda' < \\lambda'_2 = \\frac{\\rho h_1 - h_2}{1+\\rho}.$$\n\nWith these constraints and $\\lambda'\\in[\\lambda'^{(2)},\\lambda'_1]$, we can also go back to check the Condition [\\[suppeq:sol.path.constraint-1.scad\\]](#suppeq:sol.path.constraint-1.scad){reference-type=\"eqref\" reference=\"suppeq:sol.path.constraint-1.scad\"} in the previous stage. It holds, and we omit the details for brevity.\n\nWe have discussed the two cases in the third stage, and we need to decide which one actually happens. When $h_2 \\geq \\frac{1 +\\rho}{2}h_1$ or $h_2 \\leq \\frac{ - 1 +\\rho}{2}h_1$, requirement [\\[suppeq:sol.path.constraint-2.scad\\]](#suppeq:sol.path.constraint-2.scad){reference-type=\"ref\" reference=\"suppeq:sol.path.constraint-2.scad\"} is not met, and thus the second case is the case that happens. The subset of rejection region when $h_2 \\geq \\frac{1 +\\rho}{2}h_1$ or $h_2 \\leq \\frac{ - 1 +\\rho}{2}h_1$ is the same as that of Lasso. Now both $b_1$ and $b_2$ are nonzero, and we need not discuss any further.\n\nWhen $\\frac{ - 1 +\\rho}{2}h_1 < h_2 < \\frac{1 +\\rho}{2}h_1$, we can verify that $$\\begin{aligned}\n  \\lambda_2'^{(1)} =\\frac{1}{2}h_1 > \\lambda_2'^{(2)} =\\frac{h_2 -\\rho h_1}{1 -\\rho} \\text{ when } h_2 \\geq  \\rho h_1 \\\\\n  \\lambda_2'^{(1)} =\\frac{1}{2}h_1 > \\lambda_2'^{(2)} = \\frac{\\rho h_1 - h_2}{1 +\\rho} \\text{ when } h_2 <  \\rho h_1\n\\end{aligned}$$ So $b_1$ becomes larger than $\\lambda'$ before $b_2$ enters the model.\n\n*Stage 3:* When $(h_1,h_2)$ satisfies $\\frac{ - 1 +\\rho}{2}h_1 < h_2 < \\frac{1 +\\rho}{2}h_1$ in the last stage, we stil need to find out when $b_2$ enters the model after $b_1$ becomes greater than $\\lambda'$. When $\\lambda' < \\lambda_2'^{(1)} = \\frac{1}{2}h_1$ we still have two possible cases to discuss: First, $b_1$ continues to grow larger than $(a\\lambda')$, making the expression of $q'(b_1)$ different again, while $b_2$ is still at zero. Second, $b_2$ enters the model before $b_1$ hits $(a\\lambda')$.\n\nBefore the discussion of the two cases, we look at the system of equations when $\\lambda'\\in(\\lambda'_2 -\\delta,\\lambda'_2)$ for a very small $\\delta$. $$\\begin{bmatrix} 1 & \\rho \\\\ \\rho & 1 \\end{bmatrix} \\begin{bmatrix} b_1  \\\\ 0 \\end{bmatrix} + \\begin{bmatrix} \\frac{a\\lambda' - b_1}{a - 1} \\\\ \\lambda' \\cdot \\mathop{\\mathrm{sgn}}(0) \\end{bmatrix} = \\begin{bmatrix} h_1 \\\\ h_2 \\end{bmatrix}.$$\n\nThe solution of $b_1$ is $b_1 = \\frac{(a - 1)h_1 - a\\lambda'}{a - 2}$ and the sub-gradient for $b_2$ requires $$\\label{suppeq:sol.path.constraint-3.scad}\n  \\abs{h_2 -\\rho b_1} <\\lambda'$$\n\nWe start from the first case in which $b_1$ reaches $(a\\lambda)$ first. At the point $\\lambda' =\\lambda_3'^{(1)}$, we have $b_1 = a\\lambda'$ and $$\\begin{bmatrix} 1 & \\rho \\\\ \\rho & 1 \\end{bmatrix} \\begin{bmatrix} a\\lambda'  \\\\ 0 \\end{bmatrix} + \\begin{bmatrix} 0 \\\\ \\lambda' \\cdot \\mathop{\\mathrm{sgn}}(0) \\end{bmatrix} = \\begin{bmatrix} h_1 \\\\ h_2 \\end{bmatrix}.$$ Thus $\\lambda_3'^{(1)} = \\frac{h_1}{a}$. In terms of the sub-gradient $\\mathop{\\mathrm{sgn}}(0)$ , we need $\\abs{h_2 -\\rho h_1} < \\frac{h_1}{a},$ and thus $(\\rho - \\frac{1}{a})h_1< h_2 <(\\rho + \\frac{1}{a})h_1$. Compare the above equation with Equation [\\[suppeq:sol.path.constraint-2.scad\\]](#suppeq:sol.path.constraint-2.scad){reference-type=\"ref\" reference=\"suppeq:sol.path.constraint-2.scad\"}, and we get $$\\label{suppeq:sol.path.constraint-4.scad}\n  (\\rho - \\frac{1}{a})h_1 < h_2 < \\min \\left\\{(\\rho + \\frac{1}{a})h_1,\\frac{1+\\rho}{2}h_1  \\right\\} = \\begin{cases} (\\rho + \\frac{1}{a})h_1 & \\text{if }a > \\frac{2}{1-\\rho} \\\\ \\frac{1+\\rho}{2}h_1 & \\text{if } a \\leq  \\frac{2}{1-\\rho} \\end{cases}$$\n\nThen we consider the second case in which $b_2$ enters the model first. We look at the condition [\\[suppeq:sol.path.constraint-3.scad\\]](#suppeq:sol.path.constraint-3.scad){reference-type=\"ref\" reference=\"suppeq:sol.path.constraint-3.scad\"} to find $\\lambda_3'^{(2)}$, because condition [\\[suppeq:sol.path.constraint-3.scad\\]](#suppeq:sol.path.constraint-3.scad){reference-type=\"ref\" reference=\"suppeq:sol.path.constraint-3.scad\"} would become tight at $\\lambda =\\lambda_3'^{(2)}$. It requires $$-\\lambda' + \\frac{\\rho(a - 1)h_1 - a\\rho \\lambda'}{a - 2} < h_2 < \\lambda' + \\frac{\\rho(a - 1)h_1 - a\\rho \\lambda'}{a - 2}$$ The left half of the inequality is equivalent to (We have implicitly used $\\rho > 0$.) $$\\lambda' > \\frac{\\rho(a - 1)h_1 -(a - 2)h_2}{a + a\\rho - 2}$$ The right half of the inequality is $$\\label{suppeq:sol.path.constraint-5.scad}\n  (a - 2)h_2 -\\rho(a - 1)h_1 <(a - 2 - a\\rho)\\lambda'.$$ It turns out that we still need to discuss whether $a > \\frac{2}{1 -\\rho}$:\n\nWhen $a > \\frac{2}{1 -\\rho}$, $a - 2 - a\\rho > 0$ and requirement [\\[suppeq:sol.path.constraint-5.scad\\]](#suppeq:sol.path.constraint-5.scad){reference-type=\"eqref\" reference=\"suppeq:sol.path.constraint-5.scad\"} is restrictive. We have $$\\lambda' > \\lambda_3'^{(2)} =\\max\\left\\{\\frac{(a - 2)h_2 -\\rho(a - 1)h_1}{a - 2 - a\\rho},\\:\\frac{\\rho(a - 1)h_1 -(a - 2)h_2}{a + a\\rho - 2}\\,  \\right\\}.$$ Of course only one of the two terms will be positive, and it involves the discussion of whether $(a - 2)h_2 > \\rho(a - 1)h_1$ or not.\n\nWhen $a \\leq  \\frac{2}{1 -\\rho}$, we can prove requirement [\\[suppeq:sol.path.constraint-5.scad\\]](#suppeq:sol.path.constraint-5.scad){reference-type=\"eqref\" reference=\"suppeq:sol.path.constraint-5.scad\"} always holds without any requirement. To see this, just plug $\\lambda' =\\lambda'_2 = \\frac{h_1}{2}$ into requirement [\\[suppeq:sol.path.constraint-5.scad\\]](#suppeq:sol.path.constraint-5.scad){reference-type=\"eqref\" reference=\"suppeq:sol.path.constraint-5.scad\"}, and we will see $(a - 2)h_2 -\\rho(a - 1)h_1 \\leq (a - 2 - a\\rho)\\lambda'_2 < 0$ because it is equivalent to $h_2 <\\frac{1 +\\rho}{2}h_1$. In other words, requirement [\\[suppeq:sol.path.constraint-5.scad\\]](#suppeq:sol.path.constraint-5.scad){reference-type=\"eqref\" reference=\"suppeq:sol.path.constraint-5.scad\"} is not restrictive, and we only need the left half: $$\\lambda' > \\lambda_3'^{(2)} =\\frac{\\rho(a - 1)h_1 -(a - 2)h_2}{a + a\\rho - 2}.$$\n\nFinally, we need to decide how to choose between the two cases:\n\n-   When $a > \\frac{2}{1 -\\rho}$ and $(\\rho - \\frac{1}{a})h_1 < h_2 < (\\rho + \\frac{1}{a})h_1$, we should choose $\\lambda_3'^{(1)} = \\frac{h_1}{a}$. This is because now $\\frac{h_1}{a} > \\max\\left\\{\\frac{(a - 2)h_2 -\\rho(a - 1)h_1}{a - 2 - a\\rho},\\:\\frac{\\rho(a - 1)h_1 -(a - 2)h_2}{a + a\\rho - 2}\\,  \\right\\}$ always holds.\n\n-   When $a \\leq \\frac{2}{1 -\\rho}$ and $(\\rho - \\frac{1}{a})h_1 < h_2 < \\frac{1 +\\rho}{2}h_1$, we should choose $\\lambda_3'^{(1)} = \\frac{h_1}{a}$. This is because now $\\frac{h_1}{a} > \\frac{\\rho(a - 1)h_1 -(a - 2)h_2}{a + a\\rho - 2}$ always holds.\n\n-   When $a > \\frac{2}{1 -\\rho}$ and $h_2 >(\\rho + \\frac{1}{a})h_1$, we should choose $\\lambda_3'^{(2)} = \\frac{(a - 2)h_2 -\\rho(a - 1)h_1}{a - 2 - a\\rho} > 0$.\n\n-   When $h_2 <(\\rho - \\frac{1}{a})h_1$, for arbitrary $a$, we should choose $\\lambda_3'^{(2)} = \\frac{\\rho(a - 1)h_1 -(a - 2)h_2}{a + a\\rho - 2} > 0$.\n\nIn the discussion above, if the Condition [\\[suppeq:sol.path.constraint-4.scad\\]](#suppeq:sol.path.constraint-4.scad){reference-type=\"eqref\" reference=\"suppeq:sol.path.constraint-4.scad\"} is met and $\\lambda' <\\lambda_3'^{(1)} = \\frac{h_1}{a}$, we still have not seen $b_2$ in the model when $b_1$ hits $a\\lambda'$, and we need to discuss further.\n\n*Stage 4:* When $\\lambda'$ fall below $\\lambda_3'^{(1)}$, $b_1$ is greater than $a\\lambda'$. Now the system of equations: $$\\begin{bmatrix} 1 & \\rho \\\\ \\rho & 1 \\end{bmatrix} \\begin{bmatrix} b_1  \\\\ 0 \\end{bmatrix} + \\begin{bmatrix} 0 \\\\ \\lambda' \\cdot \\mathop{\\mathrm{sgn}}(0) \\end{bmatrix} = \\begin{bmatrix} h_1 \\\\ h_2 \\end{bmatrix}.$$ Now we have $b_1 = h_1$, and $\\lambda' > \\abs{h_2 -\\rho h_1}.$ Thus we know $\\lambda'_4 =\\abs{h_2 -\\rho h_1}$. (The sign of $(h_2 -\\rho h_1)$ is not determined yet.) When $\\lambda'$ fall below $\\lambda'_4$, $b_2$ inevitably enters the model."},{"heading":"Proof of Proposition [2](#prop:SCAD-larger-a){reference-type=\"ref\" reference=\"prop:SCAD-larger-a\"} (Comparing SCAD and Lasso)","text":"# Proof of Proposition [2](#prop:SCAD-larger-a){reference-type=\"ref\" reference=\"prop:SCAD-larger-a\"} (Comparing SCAD and Lasso) {#proof-of-proposition-propscad-larger-a-comparing-scad-and-lasso}\n\nProposition [2](#prop:SCAD-larger-a){reference-type=\"ref\" reference=\"prop:SCAD-larger-a\"} makes two assertions, the first about positive $\\rho$ and the second about negative $\\rho$. We prove them respectively.\n\nWhen $\\rho < 0$, it would be obvious that the diagram of SCAD is lower than the diagram of Lasso. Reviewing the diagram of Lasso in the main text, it is the maximum of four curves: $$U_{\\mathrm{Lasso}}(\\vartheta)= \\begin{cases} \n    \\max \\left\\{ h_1(\\vartheta),h^*_2(\\vartheta) \\right\\}, & \\text{ when }\\rho \\geq 0, \\\\\n    \\max \\left\\{ h_1(\\vartheta),h^*_2(\\vartheta),h^*_3(\\vartheta),h^*_4(\\vartheta) \\right\\}, &\\text{ when }\\rho < 0,\n    \\end{cases}$$ where $h_1(\\vartheta) =(1 + \\sqrt{1 -\\vartheta})^2$, $h^*_2(\\vartheta) = \\bigl( 1 + \\sqrt{\\frac{1 +\\abs{\\rho}}{1 -\\abs{\\rho}}} \\bigr)^2 (1 -\\vartheta)$, $h^*_3(\\vartheta) = \\frac{1}{(1 -\\abs*{\\rho})^2} \\bigl( 1 + \\sqrt{\\frac{1 -\\abs{\\rho}}{1 +\\abs{\\rho}}}\\sqrt{1 - 2\\vartheta} \\bigr)^2$, and $h^*_4(\\vartheta) = \\frac{1}{(1 -\\abs*{\\rho})^2} \\bigl(\\sqrt{\\frac{1 +\\abs{\\rho}}{1 -\\abs{\\rho}}} \\sqrt{1 -\\vartheta} + \\sqrt{\\frac{1 -\\abs{\\rho}}{1 +\\abs{\\rho}}}\\sqrt{1 - 2\\vartheta} \\bigr)^2$.\n\nIn terms of SCAD, $h_1(\\vartheta) =(1 + \\sqrt{1 -\\vartheta})^2$, $h^*_2(\\vartheta) = \\bigl( 1 + \\sqrt{\\frac{1 +\\abs{\\rho}}{1 -\\abs{\\rho}}} \\bigr)^2 (1 -\\vartheta)$ and $h^*_3(\\vartheta) = \\frac{1}{(1 -\\abs*{\\rho})^2} \\bigl( 1 + \\sqrt{\\frac{1 -\\abs{\\rho}}{1 +\\abs{\\rho}}}\\sqrt{1 - 2\\vartheta} \\bigr)^2$ are also present in Theorem [3](#thm:SCAD){reference-type=\"ref\" reference=\"thm:SCAD\"}. (The notation of the corresponding curve of $h^*_3(\\vartheta)$ is different, $h^*_3(\\vartheta) = \\left( \\frac{1}{1 -\\rho} +\\sqrt{\\frac{1- 2\\vartheta}{1 -\\rho^2}} \\right)$.) The only different curve is the last one; we compare only the last curve below: $$\\begin{aligned}\n  h_{\\text{Lasso}}(\\vartheta) =&~ \\frac{1}{(1 -\\abs*{\\rho})^2} \\bigl(\\sqrt{\\frac{1 +\\abs{\\rho}}{1 -\\abs{\\rho}}} \\sqrt{1 -\\vartheta} + \\sqrt{\\frac{1 -\\abs{\\rho}}{1 +\\abs{\\rho}}}\\sqrt{1 - 2\\vartheta} \\bigr)^2 \\\\\n  h_{\\text{SCAD}}(\\vartheta)=&~\\left\\{\\begin{array}{ll}\\left(\\frac{5+3|\\rho|}{1-|\\rho|}\\right)(1-\\vartheta), & \\text { if } \\sqrt{\\frac{1-2 \\vartheta}{1-\\vartheta}} \\geq \\frac{3-4|\\rho|-3 \\rho^{2}}{(1-|\\rho|)} \\sqrt{\\frac{1+|\\rho|}{5+3|\\rho|}} \\\\ \\frac{1}{(1-|\\rho|)^{2}}\\left(\\sqrt{\\frac{1+|\\rho|}{1-|\\rho|}} \\sqrt{1-\\vartheta}+\\sqrt{\\frac{1-|\\rho|}{1+|\\rho|}} \\sqrt{1-2 \\vartheta}\\right)^{2}, & \\text { if } \\sqrt{\\frac{1-2 \\vartheta}{1-\\vartheta}} \\leq \\frac{(1+|\\rho|)(1-2|\\rho|)}{1-|\\rho|} \\\\ h_{6}(\\vartheta) & \\text { other wise }\\end{array}\\right.\\\\\n  \\leq &~ \\min \\left\\{ \\left(\\frac{5+3|\\rho|}{1-|\\rho|}\\right)(1-\\vartheta),\\, \\frac{1}{(1 -\\abs*{\\rho})^2} \\bigl(\\sqrt{\\frac{1 +\\abs{\\rho}}{1 -\\abs{\\rho}}} \\sqrt{1 -\\vartheta} + \\sqrt{\\frac{1 -\\abs{\\rho}}{1 +\\abs{\\rho}}}\\sqrt{1 - 2\\vartheta} \\bigr)^2 \\right\\} \\\\\n  \\leq&~ h_{\\text{Lasso}}(\\vartheta)\n\\end{aligned}$$ Thus the assertion when $\\rho < 0$ is proven.\n\nWhen $\\rho> 0$, from the details proof in Section [9](#suppsec:scad){reference-type=\"ref\" reference=\"suppsec:scad\"}, we know that when $a \\leq  \\frac{2}{1-\\rho}$, the diagram of SCAD is the same as that of Lasso except when $\\rho < 0.179$ in a tiny neighborhood of $\\vartheta= 0$. See Figure [9](#suppfig:scad.tinydifference){reference-type=\"ref\" reference=\"suppfig:scad.tinydifference\"} for an example.\n\n![The phase diagram when $0 \\leq \\rho < 0.179$ with the newly added curve.](Figure-supp/SCAD_rho=+015_numerical_small_a.pdf){#suppfig:scad.tinydifference width=\"40%\"}\n\nWhen $a > \\frac{2}{1 -\\rho}$ and increases, the penalty function of SCAD converges to that of Lasso and so does the rejection region. Eventually the tiny corner will vanish, and the diagram of SCAD with optimal $(a^*,\\lambda^*)$ and $\\rho > 0$ will be the same as Lasso."},{"heading":"Proof of Theorem [4](#thm:thresh-lasso){reference-type=\"ref\" reference=\"thm:thresh-lasso\"} (Thresholded Lasso)","text":"# Proof of Theorem [4](#thm:thresh-lasso){reference-type=\"ref\" reference=\"thm:thresh-lasso\"} (Thresholded Lasso) {#suppsec:thresLasso}\n\nAs described in Section [6](#suppsec:sketch){reference-type=\"ref\" reference=\"suppsec:sketch\"}, our proof has three parts: (a) deriving the rejection region, (b) obtaining the rate of convergence of $\\mathbb{E}[H(\\hat{\\beta},\\beta)]$, and (c) calculating the phase diagram.\n\n#### Part 1: Deriving the rejection region.\n\nRecall that the rejection region ${\\cal R}$ is as defined in [\\[def:RejRegion\\]](#def:RejRegion){reference-type=\"eqref\" reference=\"def:RejRegion\"}. Still use the scaled version of $(\\lambda,t,x_j'y,x_{j + 1}'y)$: Define $h_1=x_j'y/\\sqrt{2\\log(p)}$, $h_2=x_{j+1}'y/\\sqrt{2\\log(p)}$, $\\lambda'=\\lambda/\\sqrt{2\\log(p)}$ and $t' = t/\\sqrt{2\\log(p)}$. Consider Lasso decomposed into bivariate sub-problems, and for $(x_j,x_{j + 1})$, $(\\hat{b}_1, \\hat{b}_2)$ minimizes $$\\label{suppeq:Lassoproof-optimization}\nL(b)\\equiv \\frac{1}{2}b'\\begin{bmatrix}1&\\rho\\\\\\rho' & 1\\end{bmatrix}b + b'h+\\lambda'\\|b\\|_1$$ It is seen that $(\\hat{\\beta}_j, \\hat{\\beta}_{j+1})=\\sqrt{2\\log(p)}(\\hat{b}_1, \\hat{b}_2)$. Thresholded Lasso applies threshold $t$ to $(\\hat{\\beta}_j, \\hat{\\beta}_{j+1})$, which is equivalent to thresholding $(\\hat{b}_1, \\hat{b}_2)$ with $t'$.\n\nFix $\\rho\\geq 0$. The next lemma gives the explicit solution to [\\[enproof-optimization\\]](#enproof-optimization){reference-type=\"eqref\" reference=\"enproof-optimization\"} in the case of $h_1>|h_2|$. It is proved in Section [11.1](#subsec:proof-thresLasso-solution){reference-type=\"ref\" reference=\"subsec:proof-thresLasso-solution\"}.\n\n::: {#supplem:sol.path.thresLasso .lemma}\n**Lemma 3**. *Consider the variable selection method by solving the optimization [\\[suppeq:Lassoproof-optimization\\]](#suppeq:Lassoproof-optimization){reference-type=\"ref\" reference=\"suppeq:Lassoproof-optimization\"} and then thresholding the solution with $t'$; if $(\\hat b_1)$ (or $(\\hat b_2)$) survives the thresholding, then variable $x_j$ (or $x_{j + 1}$) is selected. Suppose $h_1 >\\abs{h_2}$ and $\\rho \\geq  0$, then*\n\n-   *When $\\lambda' > h_1$, neither of $(x_j,x_{j + 1})$ is selected.*\n\n-   *If $h_1 \\geq \\lambda'$ and $\\rho h_1 -\\lambda'( +\\rho) \\leq h_2 \\leq \\rho h_1 +\\lambda'(1 -\\rho)$, then: When $h_1 \\leq \\lambda' + t'$, neither of $(x_j,x_{j + 1})$ is selected. When $h_1 >\\lambda'+ t'$, only $x_j$ is selected.*\n\n-   *If $h_1 \\geq \\lambda'$ and $h_2 > \\rho h_1 +\\lambda'(1 -\\rho)$, then*\n\n    1.  *When $h_1 <\\rho h_2 +\\lambda'(1 -\\rho) + t'(1 -\\rho^2)$, neither of $(x_j,x_{j + 1})$ is selected.*\n\n    2.  *When $h_1 \\geq \\rho h_2 +\\lambda'(1 -\\rho) + t'(1 -\\rho^2)$ and $h_2 \\leq \\rho h_1 + \\lambda'(1 -\\rho) + t'(1 -\\rho^2)$, only $x_j$ is selected.*\n\n    3.  *When $h_2 > \\rho h_1 + \\lambda'(1 -\\rho) + t'(1 -\\rho^2)$, both $(x_j,x_{j + 1})$ are selected.*\n\n-   *If $h_1 \\geq \\lambda'$ and $h_2 < \\rho h_1 -\\lambda'(1 +\\rho)$, then*\n\n    1.  *When $h_1 <\\rho h_2 +\\lambda'(1 +\\rho) + t'(1 -\\rho^2)$, neither of $(x_j,x_{j + 1})$ is selected.*\n\n    2.  *When $h_1 \\geq \\rho h_2 +\\lambda'(1 +\\rho) + t'(1 -\\rho^2)$ and $h_2 \\leq \\rho h_1 + \\lambda'(1 +\\rho) + t'(1 -\\rho^2)$, only $x_j$ is selected.*\n\n    3.  *When $h_2 > \\rho h_1 + \\lambda'(1 +\\rho) + t'(1 -\\rho^2)$, both $(x_j,x_{j + 1})$ are selected.*\n:::\n\nFollowing similar reasoning to that of Elastic net, we can use Lemma [3](#supplem:sol.path.thresLasso){reference-type=\"ref\" reference=\"supplem:sol.path.thresLasso\"} to write explicitely the rejection region $\\cal R$, which is the region in $\\mathbb{R}^2$ where the value of $(h_1,h_2)$ implies $x_j$ will get selected eventually. The rejection region of Thresholded Lasso and $\\rho > 0$ is $$\\begin{aligned}\n\\label{suppeq:rejRegion.thresLasso}\n  {\\cal R} &= \\{(h_1,h_2): h_1>\\rho h_2 + \\lambda'(1-\\rho) + t(1 -\\rho^2),\\, h_1 >\\lambda' + 't'\\}\\cr\n&\\;\\; \\cup \\{(h_1, h_2): h_1>\\rho h_2 + \\lambda'(1+\\rho) + t'(1 -\\rho^2)\\} \\cup \\{(h_1, h_2): h_1 <\\rho h_2 - \\lambda'(1+\\rho) - t'(1 -\\rho^2)\\}\\cr\n&\\;\\; \\cup \\{(h_1,h_2): h_1 <\\rho h_2 - \\lambda'(1-\\rho) - t(1 -\\rho^2),\\, h_1 <-\\lambda' - 't'\\}. \n\\end{aligned}$$\n\nSee Figure [6](#suppfig:rejection.region.en){reference-type=\"ref\" reference=\"suppfig:rejection.region.en\"} for a visualization of the rejection region.\n\n![The rejection region of Thresholded Lasso for $\\rho\\geq 0$.](Figure-supp/thresLasso_rejRegion.pdf){#suppfig:rejection.region.thresLasso width=\"70%\"}\n\n#### Part 2. Analyzing the Hamming error.\n\nThe discussion of Elastic net can be applied here as well, and we present Theorem [13](#suppthm:hamm.thresLasso){reference-type=\"ref\" reference=\"suppthm:hamm.thresLasso\"} directly.\n\n::: {#suppthm:hamm.thresLasso .theorem}\n**Theorem 13**. *Suppose the conditions of Theorem [4](#thm:thresh-lasso){reference-type=\"ref\" reference=\"thm:thresh-lasso\"} hold. Let $\\lambda'=\\lambda/\\sqrt{2\\log(p)}$ and $t' = t/\\sqrt{2\\log(p)}$ in Thresholded Lasso. The correlation $\\rho\\in( - 1,1)$. As $p\\to\\infty$, $$\\ensuremath{\\mathrm{FP}}_p=L_p p^{1- \\min\\bigl\\{ f_1(\\sqrt{r}, \\lambda',t'), \\;\\; \\vartheta+ f_2(\\sqrt{r}, \\lambda',t')\\bigr\\}}, \\qquad \\ensuremath{\\mathrm{FN}}_p = L_p p^{1-\\min\\bigl\\{\\vartheta+ f_3(\\sqrt{r}, \\lambda',t'),\\;\\; 2\\vartheta+ f_4(\\sqrt{r}, \\lambda',t')\\bigr\\}},$$ where (below, $d^2_{|\\rho|}(u,v)$ is as in Definition [1](#def:EllipsDistance){reference-type=\"ref\" reference=\"def:EllipsDistance\"}) $$\\begin{aligned}\n    f_1(\\sqrt{r},\\lambda',t') & = \\min \\Bigl\\{ \\frac{1}{1 -\\rho^2}\\left[(1 +|\\rho|) \\lambda' + (1 -\\rho^2)t'\\right]^2,\\;\\; (\\lambda' + t')^2  \\Bigr\\} \\cr\n    f_2(\\sqrt{r},\\lambda',t') & = \\begin{cases} \n     (\\lambda' + t' -|\\rho| \\sqrt{r})^2 & \\text{if } \\sqrt{r} \\leq \\frac{\\lambda'(1-|\\rho|)}{1-\\rho^2} \\\\\n     d^2_{|\\rho|}((\\lambda' + t',\\lambda' +|\\rho|t'),(|\\rho| \\sqrt{r},\\sqrt{r})) & \\text{if } \\sqrt{r} \\in (\\frac{\\lambda'(1-|\\rho|)}{1-\\rho^2}, \\lambda' + |\\rho| t')\\\\\n     \\frac{1}{1 -\\rho^2}\\left[\\lambda'(1 -|\\rho|) + t'(1 -\\rho^2)\\right]^2 & \\text{if } \\sqrt{r} \\geq \\lambda' + |\\rho| t' \n   \\end{cases} \n  \\cr\n  f_3(\\sqrt{r}, \\lambda',t') &= \\min\\Bigl\\{ (\\sqrt{r} -\\lambda' - t')_ +^2, \\;\\;\\frac{1}{1 -\\rho^2}\\left[(1 -\\rho^2)\\sqrt{r} - t'(1 -\\rho^2) -\\lambda'(1 -|\\rho|)\\right]^2\n          \\Bigr\\},\\cr\n  f_4(\\sqrt{r}, \\lambda',t') &= \\frac{1}{1 -\\rho^2}\\left[(1 -\\rho^2)\\sqrt{r} - t'(1 -\\rho^2) -\\lambda'(1 -\\rho)\\right]^2\n  \n\\end{aligned}$$*\n:::\n\n::: {#suppremark:threslasso .remark}\n**Remark 2**. *When $\\rho > 0$ in Theorem [13](#suppthm:hamm.thresLasso){reference-type=\"ref\" reference=\"suppthm:hamm.thresLasso\"}, we notice that $f_3(\\sqrt{r},\\lambda',t') \\leq f_4(\\sqrt{r},\\lambda',t')$, and thus $\\ensuremath{\\mathrm{FN}}_p$ can be simplified: $$\\ensuremath{\\mathrm{FN}}_p = L_p p^{1 -\\vartheta- f_3(\\sqrt{r}, \\lambda',t')}.$$ When $\\rho < 0$, such simplification is not available.*\n:::\n\nThe proof of Theorem [13](#suppthm:hamm.thresLasso){reference-type=\"ref\" reference=\"suppthm:hamm.thresLasso\"} is easy given the simple rejection region shown in Figure [10](#suppfig:rejection.region.thresLasso){reference-type=\"ref\" reference=\"suppfig:rejection.region.thresLasso\"}, and we omit it for brevity.\n\n#### Part 3. Calculating the phase diagram.\n\nThe boundary line between Almost Full Recovery and No Recovery is still $r =\\vartheta$, and the proof is similar to that of Elastic net. The rest of this part calculates the curve between Almost Full Recovery and Exact Recovery.\n\nIn such calculation, thresholded Lasso has two tuning parameters, $(\\lambda',t')$, and thus we need one more equality additional to the important fact noted in the Part 3 of Elastic net. In other words, we not only need $$\\min\\bigl\\{ f_1(\\sqrt{r}, \\lambda',t'), \\;\\; \\vartheta+ f_2(\\sqrt{r}, \\lambda',t')\\bigr\\} =\\min\\bigl\\{\\vartheta+ f_3(\\sqrt{r}, \\lambda',t'),\\;\\; 2\\vartheta+ f_4(\\sqrt{r}, \\lambda',t')\\bigr\\} = 1$$ but also need one more equation. This gives us more than $4$ cases for other methods. (For brevity, we use $f_i\\, (i = 1,2,3,4)$ as shorthand of $f_i(\\sqrt{r},\\lambda',t')$ for the rest of this part.)\n\nFor the rest of this section, we use a clearer way to discuss all the cases; that is, we discuss each possible curve and find out whether they can be present in some interval of $\\vartheta$.\n\n*We first talk about $\\rho > 0$*: In this case, since we always need $\\vartheta+f_3 \\geq 1$, we much have $\\sqrt{r} \\geq \\lambda' + t'$. As a result, for $\\vartheta+f_2 \\geq 1$, it can only be $\\vartheta+\\frac{1}{1 -\\rho^2}\\left[\\lambda'(1 -|\\rho|) + t'(1 -\\rho^2)\\right]^2\\geq 1$ since $\\sqrt{r} \\geq \\lambda' + t'$. Also, according to Remark [2](#suppremark:threslasso){reference-type=\"ref\" reference=\"suppremark:threslasso\"}, we can ignore the requirement $2\\vartheta+f_4 \\geq 1$ for $\\rho \\geq 0$.\n\n*First*, we study the curve $\\sqrt{r} = 2 \\sqrt{\\frac{1 -\\vartheta}{1 -\\rho^2}}$, which is the curve given by letting $$\\vartheta+\\frac{1}{1 -\\rho^2}\\left[\\lambda'(1 -|\\rho|) + t'(1 -\\rho^2)\\right]^2=\\vartheta+\\frac{1}{1 -\\rho^2}\\left[(1 -\\rho^2)\\sqrt{r} - t'(1 -\\rho^2) -\\lambda'(1 -|\\rho|)\\right]^2= 1$$ in $f_2$ and $f_3$. We also need $\\vartheta+f_1,\\vartheta+f_3 \\geq 1$ and one more equality. One possible case is $$\\begin{cases} \n  \\frac{1}{1 -\\rho^2}\\left[(1 +|\\rho|) \\lambda' + (1 -\\rho^2)t'\\right]^2\\geq 1 \\\\\n  (\\lambda' + t')^2\\geq 1\\\\\n  \\vartheta+\\frac{1}{1 -\\rho^2}\\left[\\lambda'(1 -|\\rho|) + t'(1 -\\rho^2)\\right]^2= 1\\\\\n  \\vartheta+(\\sqrt{r} -\\lambda' - t')_ +^2= 1\\\\\n  \\vartheta+\\frac{1}{1 -\\rho^2}\\left[(1 -\\rho^2)\\sqrt{r} - t'(1 -\\rho^2) -\\lambda'(1 -|\\rho|)\\right]^2= 1 \n  \\end{cases}$$ which gives $\\lambda' = \\frac{1 +\\rho}{\\rho} \\left(\\frac{1}{\\sqrt{1 -\\rho^2}} - 1\\right)\\sqrt{1 -\\vartheta}$ and $t' = \\sqrt{\\frac{1 +\\vartheta}{1 -\\rho^2} } - \\frac{1}{\\rho}\\left(\\frac{1}{\\sqrt{1 -\\rho^2}} - 1\\right)\\sqrt{1 -\\vartheta}$. In this case, $t' \\geq 0$ always holds; $(\\lambda' + t')^2\\geq 1 \\implies \\frac{1}{1 -\\rho^2}\\left[(1 +|\\rho|) \\lambda' + (1 -\\rho^2)t'\\right]^2\\geq 1$, and $(\\lambda' + t')^2\\geq 1$ is equivalent to $\\sqrt{r} = 2 \\sqrt{\\frac{1 -\\vartheta}{1 -\\rho^2}} \\geq  1 + \\sqrt{1 -\\vartheta}$. This is a sufficient condition for this curve to show up in the diagram.\n\n*Second*, we study the curve $\\sqrt{r} = 1 + \\sqrt{1 -\\vartheta}$, which is given by $(\\lambda' + t')^2=\\vartheta+(\\sqrt{r} -\\lambda' - t')_ +^2= 1$. We also need $f_1,\\vartheta+f_2,\\vartheta+f_3 \\geq 1$ and one more equality. Depending on which requirement to take equality, we discuss two possible cases:\n\nIf $\\frac{1}{1 -\\rho^2}\\left[(1 +|\\rho|) \\lambda' + (1 -\\rho^2)t'\\right]^2=1$, and $\\vartheta+\\frac{1}{1 -\\rho^2}\\left[\\lambda'(1 -|\\rho|) + t'(1 -\\rho^2)\\right]^2\\geq 1$, $\\vartheta+\\frac{1}{1 -\\rho^2}\\left[(1 -\\rho^2)\\sqrt{r} - t'(1 -\\rho^2) -\\lambda'(1 -|\\rho|)\\right]^2\\geq 1$, then we will get $\\lambda' = \\frac{\\sqrt{1 -\\rho^2} -(1 -\\rho^2)}{\\rho(1 +\\rho)}$ and $t' = \\frac{(1 +\\rho) - \\sqrt{1 -\\rho^2}}{\\rho(1 +\\rho)}$. From the two inequality requirements, we get $$\\sqrt{1 -\\vartheta} \\leq \\min \\left\\{ \\frac{ - \\sqrt{1 -\\rho^2} + 2\\rho + 2}{(1 +\\rho)^2}\\sqrt{1 -\\rho^2},\\,\\frac{1 -\\rho}{1 +\\rho} \\right\\} =\\frac{1 -\\rho}{1 +\\rho}$$\n\nIf $\\vartheta+\\frac{1}{1 -\\rho^2}\\left[(1 -\\rho^2)\\sqrt{r} - t'(1 -\\rho^2) -\\lambda'(1 -|\\rho|)\\right]^2= 1$ and $\\frac{1}{1 -\\rho^2}\\left[(1 +|\\rho|) \\lambda' + (1 -\\rho^2)t'\\right]^2\\geq 1$, $\\vartheta+\\frac{1}{1 -\\rho^2}\\left[\\lambda'(1 -|\\rho|) + t'(1 -\\rho^2)\\right]^2\\geq 1$, then we will get $\\lambda' = \\frac{1 +\\rho}{\\rho} \\left(\\frac{1}{\\sqrt{1 -\\rho^2}} - 1\\right)\\sqrt{1 -\\vartheta}$, $t' = 1 -\\lambda'$. From $t' \\geq 0$ and the two inequality requirements, we get $$\\frac{1 -\\rho}{1 +\\rho} \\leq \\sqrt{1 -\\vartheta} \\leq \\min \\left\\{ \\left(\\frac{2}{\\sqrt{1 -\\rho^2} } - 1\\right)^{ - 1},\\,\\frac{\\rho}{1 +\\rho}\\left(\\frac{1}{\\sqrt{1 -\\rho^2}} - 1\\right)^{ - 1} \\right\\} = \\left(\\frac{2}{\\sqrt{1 -\\rho^2} } - 1\\right)^{ - 1}$$\n\nTaking the intersection of the first two cases, we already know that $\\sqrt{r} = 1 + \\sqrt{1 -\\vartheta}$ exists as long as $\\sqrt{r} = 1+ \\sqrt{1 -\\vartheta} \\geq 2 \\sqrt{\\frac{1 -\\vartheta}{1 -\\rho^2}}$. There is one more case left, but the interval of $\\vartheta$ for $\\sqrt{r} = 1 + \\sqrt{1 -\\vartheta}$ to exist will be a subset of what we already have, so we omit it.\n\nNow we already seem to have the whole phase curve, but the tricky part of thresholded Lasso having two tunable parameters is that we might have multiple curves for the same $\\vartheta$, and we need to take the minimum across all the curves. Thus we need to continue discussing all the other curves. For $\\rho > 0$, we have three more to go.\n\n*Third*, we study the curve $\\sqrt{r} = \\frac{2\\sqrt{1 -\\rho^2} -(1 +\\rho)}{(1 -\\rho)\\sqrt{1 -\\rho^2}}\\sqrt{1 -\\vartheta} + \\frac{1}{\\sqrt{1 -\\rho^2}}$, given by $$\\frac{1}{1 -\\rho^2}\\left[(1 +|\\rho|) \\lambda' + (1 -\\rho^2)t'\\right]^2=\\vartheta+(\\sqrt{r} -\\lambda' - t')_ +^2=\\vartheta+\\frac{1}{1 -\\rho^2}\\left[(1 -\\rho^2)\\sqrt{r} - t'(1 -\\rho^2) -\\lambda'(1 -|\\rho|)\\right]^2= 1.$$ Now we have $\\lambda' = \\frac{1 +\\rho}{\\rho} \\left(\\frac{1}{\\sqrt{1 -\\rho^2}} - 1\\right)\\sqrt{1 -\\vartheta}$ and $t' =\\frac{1}{\\sqrt{1 -\\rho^2}} - \\frac{1 +\\rho}{\\rho(1 -\\rho)}\\left(\\frac{1}{\\sqrt{1 -\\rho^2}} - 1\\right)\\sqrt{1 -\\vartheta}$. From $t' \\geq 0$ and $f_1,\\vartheta+ \\vartheta+f_2 \\geq 1$, we get $$\\sqrt{1 -\\vartheta} \\leq \\min \\left\\{ \\left[ \\frac{(1 +\\rho)^2}{\\rho \\sqrt{1 -\\rho^2}}\\left(\\frac{1}{\\sqrt{1 -\\rho^2}} - 1\\right) \\right]^{ - 1},\\,\\frac{1 -\\rho}{1 +\\rho},\\,\\left(2 -\\frac{2 \\sqrt{1 -\\rho^2}}{1 -\\rho} +\\frac{1 +\\rho}{1 -\\rho}\\right)^{ - 1} \\right\\} =\\frac{1 -\\rho}{1 +\\rho}$$ As we can see, when $\\sqrt{1 -\\vartheta} \\leq \\frac{1 -\\rho}{1 +\\rho}$, now we have 2 curves, both of which seem to be the boundary. We must take the *lower one* then.\n\n*Fourth*, we study the curve $\\sqrt{r} = \\left(1+\\frac{1+\\rho}{2 \\sqrt{1-\\rho^{2}}}\\right) \\sqrt{1-\\vartheta}+\\frac{1-\\rho}{2 \\sqrt{1-\\rho^{2}}}$ given by $$\\frac{1}{1 -\\rho^2}\\left[(1 +|\\rho|) \\lambda' + (1 -\\rho^2)t'\\right]^2=\\vartheta+\\frac{1}{1 -\\rho^2}\\left[\\lambda'(1 -|\\rho|) + t'(1 -\\rho^2)\\right]^2=\\vartheta+(\\sqrt{r} -\\lambda' - t')_ +^2= 1.$$ Now we have $\\lambda' = \\frac{1}{2\\rho}\\left(1 - \\sqrt{1 -\\vartheta}\\right)\\sqrt{1 -\\rho^2}$ and $t' = \\frac{1}{2\\rho \\sqrt{1 -\\rho^2}}\\left[(1 + \\rho)\\sqrt{1 -\\vartheta} -(1 -\\rho)\\right]$.\n\nFrom $t' \\geq 0$ and $f_1, \\vartheta+f_3 \\geq 1$, we need $$\\max \\left\\{ \\frac{1 -\\rho}{1 +\\rho},\\,\\frac{2 \\sqrt{1 -\\rho^2} -(1 -\\rho)}{1 +\\rho} \\right\\} \\leq \\sqrt{1 -\\vartheta} \\leq \\frac{1 -\\rho}{2} \\cdot \\left(\\frac{3 -\\rho}{2} - \\sqrt{1 -\\rho^2}\\right)^{ - 1}$$ This gives us an empty set, because actually $$\\frac{1 -\\rho}{2} \\cdot \\left(\\frac{3 -\\rho}{2} - \\sqrt{1 -\\rho^2}\\right)^{ - 1} < \\frac{2 \\sqrt{1 -\\rho^2} -(1 -\\rho)}{1 +\\rho}.$$\n\n*Fifth*, we study the curve $\\sqrt{r} = \\sqrt{\\frac{1-\\vartheta}{1-\\rho^{2}}}+\\frac{2 \\rho+2-\\sqrt{1-\\rho^{2}}}{(1+\\rho)^{2}}$ given by $$\\frac{1}{1 -\\rho^2}\\left[(1 +|\\rho|) \\lambda' + (1 -\\rho^2)t'\\right]^2=(\\lambda' + t')^2=\\vartheta+\\frac{1}{1 -\\rho^2}\\left[(1 -\\rho^2)\\sqrt{r} - t'(1 -\\rho^2) -\\lambda'(1 -|\\rho|)\\right]^2= 1.$$ We get $\\lambda' =\\frac{\\sqrt{1 -\\rho^2} -(1 -\\rho^2)}{\\rho(1 +\\rho)}$ and $t' = \\frac{(1 +\\rho) - \\sqrt{1 -\\rho^2}}{\\rho(1 +\\rho)}$. From $\\vartheta+f_2, \\vartheta+f_3 \\geq 1$, we get $$\\frac{1 -\\rho}{1 +\\rho} \\leq \\sqrt{1 -\\vartheta} \\leq \\frac{2 \\sqrt{1 -\\rho^2} -(1 -\\rho)}{1 +\\rho}$$\n\n*Summarising all the five curves when $\\rho > 0$*: We will elimiate the *third* and *fifth* curve. For the *third* curve, when $\\sqrt{1 -\\vartheta} \\leq \\frac{1 -\\rho}{1 +\\rho}$, it is always larger than the other curve $\\sqrt{r} = 1 + \\sqrt{1 -\\vartheta}$. In other words, when $\\sqrt{1 -\\vartheta} \\leq \\frac{1 -\\rho}{1 +\\rho}$, we always have $$\\sqrt{r} = \\frac{2\\sqrt{1 -\\rho^2} -(1 +\\rho)}{(1 -\\rho)\\sqrt{1 -\\rho^2}}\\sqrt{1 -\\vartheta} + \\frac{1}{\\sqrt{1 -\\rho^2}} \\geq \\sqrt{1 -\\vartheta} + 1$$ In fact, $(LHS - RHS)$ takes its minimum at $\\sqrt{1 -\\vartheta} =\\frac{1 -\\rho}{1 +\\rho}$, which is exactly zero.\n\nFor the *fifth* curve, when $\\frac{1 -\\rho}{1 +\\rho} \\leq \\sqrt{1 -\\vartheta} \\leq \\frac{2 \\sqrt{1 -\\rho^2} -(1 -\\rho)}{1 +\\rho}$, we have $$\\sqrt{r} = \\sqrt{\\frac{1-\\vartheta}{1-\\rho^{2}}}+\\frac{2 \\rho+2-\\sqrt{1-\\rho^{2}}}{(1+\\rho)^{2}} \\geq \\max \\left\\{ 1 + \\sqrt{1 -\\vartheta}, 2 \\sqrt{\\frac{1 -\\vartheta}{1 -\\rho^2}} \\right\\}$$ which can be verified using $\\frac{1 -\\rho}{1 +\\rho} \\leq \\sqrt{1 -\\vartheta} \\leq \\frac{2 \\sqrt{1 -\\rho^2} -(1 -\\rho)}{1 +\\rho}$ in a similar manner. To sum up, for $\\rho \\geq 0$, the phase curve of thresholded Lasso is $$\\sqrt{r} =\\max \\left\\{ 1 + \\sqrt{1 -\\vartheta}, 2 \\sqrt{\\frac{1 -\\vartheta}{1 -\\rho^2}}  \\right\\}.$$\n\n*We then talk about $\\rho < 0$* , which now requires additionally $2\\vartheta+ f_4 \\geq 1$, or $$2\\vartheta+ \\left[(1 -\\rho^2)\\sqrt{r}-\\lambda'(1 +|\\rho|)-(1 -\\rho^2)t'\\right]_+ ^2 \\geq 1.$$ When $\\vartheta \\geq \\frac{1}{2}$, this newly added requirement has no effects, and the right half ($\\vartheta\\geq \\frac{1}{2}$) of the phase diagram should be the same as that of $\\rho \\geq 0$. As a result, we can limit ourselve to consider $\\vartheta\\leq \\frac{1}{2}$.\n\nAlso note that we used to ignore $2\\vartheta+ f_4 \\geq 1$ for $\\rho \\geq 0$ because it is not restrictive; now we have added it, and it is the only difference between the cases of $\\rho \\geq 0$ and $\\rho < 0$ (because $f_1, \\vartheta+f_2,g_3$ only rely on $|\\rho|$). As a result, we only need to discuss this additional requirement; since we have eliminated three curves when $\\rho > 0$, there is no need to discuss them again.\n\n*First*, we study the curve $\\sqrt{r} = 2 \\sqrt{\\frac{1 -\\vartheta}{1 -\\rho^2}}$, which is the curve given by letting $$\\vartheta+\\frac{1}{1 -\\rho^2}\\left[\\lambda'(1 -|\\rho|) + t'(1 -\\rho^2)\\right]^2=\\vartheta+\\frac{1}{1 -\\rho^2}\\left[(1 -\\rho^2)\\sqrt{r} - t'(1 -\\rho^2) -\\lambda'(1 -|\\rho|)\\right]^2= 1$$ in $\\vartheta+f_2$ and $\\vartheta+f_3$. We also need $f_1, \\vartheta+f_3, 2\\vartheta+f_4 \\geq 1$ and one more quality. When $\\rho > 0$, we used to consider only one case, but now we need to discuss all four cases and take the union to get the interval of $\\vartheta$, because each case has different $(\\lambda',t')$ and lead to different intervals of $\\vartheta$.\n\nIf $\\vartheta+\\frac{1}{1 -\\rho^2}\\left[\\lambda'(1 -|\\rho|) + t'(1 -\\rho^2)\\right]^2=\\vartheta+(\\sqrt{r} -\\lambda' - t')_ +^2=\\vartheta+\\frac{1}{1 -\\rho^2}\\left[(1 -\\rho^2)\\sqrt{r} - t'(1 -\\rho^2) -\\lambda'(1 -|\\rho|)\\right]^2=1$, and $\\vartheta+ f_1, 2\\vartheta+ f_4 \\geq 1$, this is the case we covered when $\\rho \\geq  0$. Then $2\\vartheta+\\frac{1}{1 -\\rho^2}\\left[(1 -\\rho^2)\\sqrt{r} - t'(1 -\\rho^2) -\\lambda'(1 +|\\rho|)\\right]^2\\geq 1$ implies $\\sqrt{\\frac{1 - 2\\vartheta}{1 -\\vartheta} } \\leq \\frac{2 \\sqrt{1 -\\rho^2} -(1 +|\\rho|)}{1 -|\\rho|}$ and other requirements imply $\\sqrt{1-\\vartheta}\\left(\\frac{2}{\\sqrt{1-\\rho^2}}-1\\right) > 1$. (The first constraint implies $\\vartheta\\geq$ some value, and the second one implies $\\vartheta\\leq$ some value.) The overall requirements are: $$\\sqrt{1-\\vartheta}\\left(\\frac{2}{\\sqrt{1-\\rho^2}}-1\\right) > 1,\\text{and }\\sqrt{\\frac{1 - 2\\vartheta}{1 -\\vartheta} } \\leq \\frac{2 \\sqrt{1 -\\rho^2} -(1 +|\\rho|)}{1 -|\\rho|}$$\n\nIf $\\frac{1}{1 -\\rho^2}\\left[(1 +|\\rho|) \\lambda' + (1 -\\rho^2)t'\\right]^2=\\vartheta+\\frac{1}{1 -\\rho^2}\\left[\\lambda'(1 -|\\rho|) + t'(1 -\\rho^2)\\right]^2=\\vartheta+\\frac{1}{1 -\\rho^2}\\left[(1 -\\rho^2)\\sqrt{r} - t'(1 -\\rho^2) -\\lambda'(1 -|\\rho|)\\right]^2= 1$, and other terms are greater than one, then we have $\\lambda' = \\frac{1 -\\rho^2}{2|\\rho|} \\cdot \\frac{1 - \\sqrt{1 -\\vartheta}}{\\sqrt{1 -\\rho^2}}$, $t' = \\frac{1}{\\sqrt{1 -\\rho^2}} -\\frac{\\lambda'}{1 -|\\rho|}$. In this case, $t' \\geq 0$ and $f_1,\\vartheta+ f_3,2\\vartheta+ f_4 \\geq 1$ implies $$\\sqrt{1 -\\vartheta} \\geq \\max \\left\\{ \\frac{1 -|\\rho|}{1 +|\\rho|},\\,\\frac{2 \\sqrt{1 -\\rho^2} -(1 -|\\rho|)}{1 +|\\rho|} ,\\,\\frac{1 -|\\rho|}{(3 - |\\rho|) - 2 \\sqrt{1 -\\rho^2}} \\right\\} =\\frac{2 \\sqrt{1 -\\rho^2} -(1 -|\\rho|)}{1 +|\\rho|}$$\n\nIf $(\\lambda' + t')^2=\\vartheta+\\frac{1}{1 -\\rho^2}\\left[\\lambda'(1 -|\\rho|) + t'(1 -\\rho^2)\\right]^2=\\vartheta+\\frac{1}{1 -\\rho^2}\\left[(1 -\\rho^2)\\sqrt{r} - t'(1 -\\rho^2) -\\lambda'(1 -|\\rho|)\\right]^2= 1$ and other terms are greater than one, then we have $\\lambda' = \\frac{1 +|\\rho|}{|\\rho|}\\left(1 - \\sqrt{\\frac{1 -\\vartheta}{1 -\\rho^2}} \\right)$ and $t' = 1 -\\lambda'$. In this case, $\\lambda' \\geq 0$ and $t' \\geq 0$ implies $\\frac{\\sqrt{1 -\\rho^2}}{1 +|\\rho|} \\leq \\sqrt{1 -\\vartheta} \\leq \\sqrt{1 -\\rho^2 }$, which is weaker than the requirements from $f_1,\\vartheta+ f_3$ and $2\\vartheta+ f_4$: $$\\left(\\frac{2}{\\sqrt{1 -\\rho^2}} - 1\\right)^{ - 1} \\leq \\sqrt{1 -\\vartheta} \\leq  \\frac{2 \\sqrt{1 -\\rho^2} -(1 -|\\rho|)}{1 +|\\rho|},\\text{and}\\;\\; \\frac{2 \\sqrt{1 -\\rho^2}}{1 -|\\rho|} + \\sqrt{1 - 2\\vartheta} \\leq \\frac{3 -|\\rho|}{1 -|\\rho|} \\sqrt{1 -\\vartheta}$$\n\nIf $\\vartheta+\\frac{1}{1 -\\rho^2}\\left[\\lambda'(1 -|\\rho|) + t'(1 -\\rho^2)\\right]^2=\\vartheta+\\frac{1}{1 -\\rho^2}\\left[(1 -\\rho^2)\\sqrt{r} - t'(1 -\\rho^2) -\\lambda'(1 -|\\rho|)\\right]^2=2\\vartheta+\\frac{1}{1 -\\rho^2}\\left[(1 -\\rho^2)\\sqrt{r} - t'(1 -\\rho^2) -\\lambda'(1 +|\\rho|)\\right]^2= 1$ and other terms are greater than one, then we have $\\lambda' = \\frac{1 -\\rho^2}{2|\\rho|}\\frac{1}{\\sqrt{1 -\\rho^2}}\\left(\\sqrt{1 -\\vartheta} - \\sqrt{1 - 2\\vartheta}\\right)$ and $t' = \\sqrt{\\frac{1 -\\vartheta}{1 -\\rho^2}} -\\frac{\\lambda'}{1 +|\\rho|}$. In this case, $\\lambda' \\geq 0$ and $t' \\geq 0$ implies $\\sqrt{\\frac{1 - 2\\vartheta}{1 -\\vartheta}} \\geq 1 - \\frac{2|\\rho|}{1 -|\\rho|}$, which is still weaker than the requirements from $f_1,\\vartheta+ f_3 \\geq 1$: $$\\sqrt{\\frac{1 - 2\\vartheta}{1 -\\vartheta}} \\geq \\frac{2 \\sqrt{1 -\\rho^2} -(1 +|\\rho|)}{1 -|\\rho|},\\text{and}\\;\\;\\frac{2 \\sqrt{1 -\\rho^2}}{1 -|\\rho|} + \\sqrt{1 - 2\\vartheta} \\leq \\frac{3 -|\\rho|}{1 -|\\rho|} \\sqrt{1 -\\vartheta}$$\n\nTaking the union over all the four cases of curve $\\sqrt{r} = 2 \\sqrt{\\frac{1 -\\vartheta}{1 -\\rho^2}}$, it exists for $\\vartheta$ satisfying $\\sqrt{1 -\\vartheta} \\geq \\left(\\frac{2}{\\sqrt{1 -\\rho^2}} - 1\\right)^{ - 1}$ and $\\frac{2 \\sqrt{1 -\\rho^2}}{1 -|\\rho|} + \\sqrt{1 - 2\\vartheta} \\leq \\frac{3 -|\\rho|}{1 -|\\rho|} \\sqrt{1 -\\vartheta}$, which is equivalent to $$\\sqrt{r} = 2 \\sqrt{\\frac{1 -\\vartheta}{1 -\\rho^2}} \\geq \\max \\left\\{ 1 + \\sqrt{1 -\\vartheta},\\;1+\\frac{1+|\\rho|}{2} \\sqrt{\\frac{1-\\vartheta}{1-\\rho^{2}}}+\\frac{1-|\\rho|}{2} \\sqrt{\\frac{1-2 \\vartheta}{1-\\rho^{2}}}   \\right\\}$$\n\n*Second*, we study the curve $\\sqrt{r} = 1 + \\sqrt{1 -\\vartheta}$, which is given by $(\\lambda' + t')^2=\\vartheta+(\\sqrt{r} -\\lambda' - t')_ +^2= 1$. We also need $f_1,\\vartheta+f_2,\\vartheta+f_3 \\geq 1$ and one more equality. When $\\rho \\geq 0$, we have discussed two cases; now we discuss the two old cases with the additional requirement $2\\vartheta+ f_4 \\geq 1$, and two additional cases.\n\nIf $\\frac{1}{1 -\\rho^2}\\left[(1 +|\\rho|) \\lambda' + (1 -\\rho^2)t'\\right]^2= (\\lambda' + t')^2=\\vartheta+(\\sqrt{r} -\\lambda' - t')_ +^2= 1$ and other terms are greater than one, we have already considered this in a previous section. Now we only add the requirement $2\\vartheta+\\frac{1}{1 -\\rho^2}\\left[(1 -\\rho^2)\\sqrt{r} - t'(1 -\\rho^2) -\\lambda'(1 +|\\rho|)\\right]^2\\geq 1$. The final requirement on $\\vartheta$ is $\\sqrt{1 -\\vartheta} \\leq \\frac{1 -|\\rho|}{1 +|\\rho|}$ and $(1 - \\sqrt{1 -\\rho^2}) + \\sqrt{1 - 2\\vartheta} \\leq \\sqrt{1 - \\rho^2}\\sqrt{1 - \\vartheta}$.\n\nIf $(\\lambda' + t')^2=\\vartheta+(\\sqrt{r} -\\lambda' - t')_ +^2= \\vartheta+\\frac{1}{1 -\\rho^2}\\left[(1 -\\rho^2)\\sqrt{r} - t'(1 -\\rho^2) -\\lambda'(1 -|\\rho|)\\right]^2= 1$ and other terms are greater than one, we have already considered this in a previous section. Now we only add the requirement $2\\vartheta+\\frac{1}{1 -\\rho^2}\\left[(1 -\\rho^2)\\sqrt{r} - t'(1 -\\rho^2) -\\lambda'(1 +|\\rho|)\\right]^2\\geq 1$. The final requirement on $\\vartheta$ is $\\frac{1 -|\\rho|}{1 + |\\rho|} \\leq \\sqrt{1 -\\vartheta} \\leq \\left(\\frac{2}{\\sqrt{1 -\\rho^2}} - 1\\right)^{ - 1}$ and $\\sqrt{\\frac{1 - 2\\vartheta}{1 -\\vartheta}} \\leq \\frac{2 \\sqrt{1 -\\rho^2} -(1 +|\\rho|)}{1 -|\\rho|}$\n\nIf $\\vartheta+\\frac{1}{1 -\\rho^2}\\left[\\lambda'(1 -|\\rho|) + t'(1 -\\rho^2)\\right]^2=(\\lambda' + t')^2=\\vartheta+(\\sqrt{r} -\\lambda' - t')_ +^2= 1$ and other terms are greater than one, this is a new case, and we have $\\lambda' = \\frac{1 +|\\rho|}{|\\rho|}\\left(1 - \\sqrt{\\frac{1 -\\vartheta}{1 -\\rho^2} }\\right)$, $t' = 1 -\\lambda'$. $\\lambda',t' \\geq 0$ requires $\\frac{\\sqrt{1 -\\rho^2}}{1 +|\\rho|} \\leq \\sqrt{1 -\\vartheta} \\leq \\sqrt{1 -\\rho^2}$, and the requirements from $f_1,\\vartheta+ f_3,2\\vartheta+ f_4 \\geq 1$ are $\\sqrt{1 -\\vartheta} \\leq \\min \\left\\{ \\frac{2 \\sqrt{1 -\\rho^2} -(1 -|\\rho|)}{1 +|\\rho|},\\,\\left(\\frac{2}{\\sqrt{1 -\\rho^2}} - 1\\right)^{ - 1} \\right\\}$ and $\\frac{1 +|\\rho|}{1 -|\\rho|}\\sqrt{1 -\\rho^2} + \\sqrt{1 - 2\\vartheta} \\leq \\left(\\frac{1 +|\\rho|}{1 -|\\rho|} +\\sqrt{1 -\\rho^2}\\right)\\sqrt{1 -\\vartheta}$. Taking the intersection, the overall requirement on $\\vartheta$ is $$\\frac{\\sqrt{1 -\\rho^2}}{1 +|\\rho|} \\leq \\sqrt{1 -\\vartheta} \\leq \\left(\\frac{2}{\\sqrt{1 -\\rho^2}} - 1\\right)^{ - 1}\\text{and}\\;\\frac{1 +|\\rho|}{1 -|\\rho|}\\sqrt{1 -\\rho^2} + \\sqrt{1 - 2\\vartheta} \\leq \\left(\\frac{1 +|\\rho|}{1 -|\\rho|} +\\sqrt{1 -\\rho^2}\\right)\\sqrt{1 -\\vartheta}$$\n\nIf $(\\lambda' + t')^2=\\vartheta+(\\sqrt{r} -\\lambda' - t')_ +^2=2\\vartheta+\\frac{1}{1 -\\rho^2}\\left[(1 -\\rho^2)\\sqrt{r} - t'(1 -\\rho^2) -\\lambda'(1 +|\\rho|)\\right]^2= 1$ and other terms are greater than one, then we have $\\lambda' = \\frac{1 -|\\rho|}{|\\rho|} \\left(\\sqrt{1 -\\vartheta} - \\sqrt{\\frac{1 - 2\\vartheta}{1 -\\rho^2}}\\right)$, $t' = 1 -\\lambda'$. The requirements from $\\lambda',t' \\geq 0$ and $f_1,\\vartheta+ f_2,\\vartheta+ f_3 \\geq 1$ are: $$\\begin{aligned}\n  \\sqrt{\\frac{1 - 2\\vartheta}{1 -\\vartheta}} \\leq&~ \\frac{2 \\sqrt{1 -\\rho^2} -(1 +|\\rho|)}{1 -|\\rho|} \\\\\n  \\frac{|\\rho| \\sqrt{1 -\\rho^2}}{1 -|\\rho|} + \\sqrt{1 - 2\\vartheta} \\geq&~ \\sqrt{1 -\\rho^2} \\cdot \\sqrt{1 -\\vartheta} \\\\\n  (1 - \\sqrt{1 -\\rho^2}) + \\sqrt{1 - 2\\vartheta} \\leq&~  \\sqrt{1 -\\rho^2} \\cdot \\sqrt{1 -\\vartheta} \\\\\n  \\frac{1 +|\\rho|}{1 -|\\rho|}\\sqrt{1 -\\rho^2} + \\sqrt{1 - 2\\vartheta} \\geq&~ \\left(\\frac{1 +|\\rho|}{1 -|\\rho|} +\\sqrt{1 -\\rho^2}\\right)\\sqrt{1 -\\vartheta}\n\\end{aligned}$$ Taking intersection, the first and the last can imply the other two, so the final requirements are just the first and the last one.\n\nTaking the union over all the four cases of $\\sqrt{r} = 1 + \\sqrt{1 -\\vartheta}$, we have $\\sqrt{1 -\\vartheta} \\leq \\left(\\frac{2}{\\sqrt{1 -\\rho^2}} - 1\\right)^{ - 1}$ and $\\sqrt{\\frac{1 - 2\\vartheta}{1 -\\vartheta}} \\leq \\frac{2 \\sqrt{1 -\\rho^2} -(1 +|\\rho|)}{1 -|\\rho|}$, which is equivalent to $$\\sqrt{r} = 1 + \\sqrt{1 -\\vartheta} \\geq \\max \\left\\{ 2 \\sqrt{\\frac{1 -\\vartheta}{1 -\\rho^2}},\\,1+\\frac{1+|\\rho|}{2} \\sqrt{\\frac{1-\\vartheta}{1-\\rho^{2}}}+\\frac{1-|\\rho|}{2} \\sqrt{\\frac{1-2 \\vartheta}{1-\\rho^{2}}} \\right\\}$$\n\n*Third*, we study the curve $\\sqrt{r} =\\sqrt{\\frac{1 - 2\\vartheta}{1 -\\rho^2}} +\\frac{1}{\\sqrt{1 -\\rho^2}}$ given by $\\frac{1}{1 -\\rho^2}\\left[(1 +|\\rho|) \\lambda' + (1 -\\rho^2)t'\\right]^2=2\\vartheta+\\frac{1}{1 -\\rho^2}\\left[(1 -\\rho^2)\\sqrt{r} - t'(1 -\\rho^2) -\\lambda'(1 +|\\rho|)\\right]^2= 1$. We need one more equality constraint and other terms greater than one. We will see this curve does not exist for ant $\\vartheta$ at all.\n\nIf the additional equality is $(\\lambda' + t')^2= 1$, then we have $\\lambda' = \\frac{\\sqrt{1 -\\rho^2} -(1 -\\rho^2)}{|\\rho|(1 +|\\rho|)}$ and $t' = \\frac{(1 +|\\rho|) - \\sqrt{1 -\\rho^2}}{|\\rho|(1 +|\\rho|)}$. However, this case admits no $\\vartheta$, because $\\vartheta+\\frac{1}{1 -\\rho^2}\\left[\\lambda'(1 -|\\rho|) + t'(1 -\\rho^2)\\right]^2\\geq 1$ requires $\\sqrt{1 -\\vartheta} \\leq \\frac{2 \\sqrt{1 -\\rho^2} -(1 -|\\rho|)}{1 +|\\rho|}$ and $\\vartheta+(\\sqrt{r} -\\lambda' - t')_ +^2\\geq 1$ requires $(1 - \\sqrt{1 -\\rho^2}) + \\sqrt{1 - 2\\vartheta} \\geq~  \\sqrt{1 -\\rho^2} \\cdot \\sqrt{1 -\\vartheta}$; these two requirements have no intersection.\n\nIf the additional equality is $\\vartheta+\\frac{1}{1 -\\rho^2}\\left[\\lambda'(1 -|\\rho|) + t'(1 -\\rho^2)\\right]^2= 1$, then we have $\\lambda' = \\frac{1 -\\rho^2}{2 |\\rho|}\\frac{1}{\\sqrt{1 -\\rho^2}}\\left(1 -\\sqrt{1 -\\vartheta}\\right)$ and $t' = \\frac{1}{\\sqrt{1 -\\rho^2}} -\\frac{\\lambda'}{1 -|\\rho|}$. However, this case admits no $\\vartheta$, because $t' \\geq 0$ requires $\\sqrt{1 -\\vartheta} > \\frac{1 -|\\rho|}{1 +|\\rho|}$ and $\\vartheta+\\frac{1}{1 -\\rho^2}\\left[(1 -\\rho^2)\\sqrt{r} - t'(1 -\\rho^2) -\\lambda'(1 -|\\rho|)\\right]^2\\geq 1$ requires $1 + \\sqrt{1 - 2\\vartheta} \\geq 2 \\sqrt{1 -\\vartheta}$; these two requirements have no intersection.\n\nIf the additional equality is $\\vartheta+(\\sqrt{r} -\\lambda' - t')_ +^2= 1$, then $\\lambda' = \\frac{1 -|\\rho|}{|\\rho|} \\left(\\sqrt{1 -\\vartheta} - \\sqrt{\\frac{1 - 2\\vartheta}{1 -\\rho^2}}\\right)$ and $t' = \\frac{1}{\\sqrt{1 -\\rho^2}} -\\frac{\\lambda'}{1 -|\\rho|}$. However, this case admits no $\\vartheta$, because $(\\lambda' + t')^2\\geq 2$ requires $(1 -\\sqrt{1 -\\rho^2}) + \\sqrt{1 - 2\\vartheta} \\geq \\sqrt{1 -\\rho^2}\\sqrt{1 -\\vartheta}$ and $\\vartheta+\\frac{1}{1 -\\rho^2}\\left[(1 -\\rho^2)\\sqrt{r} - t'(1 -\\rho^2) -\\lambda'(1 -|\\rho|)\\right]^2\\geq 1$ requires $\\sqrt{\\frac{1 - 2\\vartheta}{1 -\\vartheta}} \\leq \\frac{2 \\sqrt{1 -\\rho^2} -(1 +|\\rho|)}{1 -|\\rho|}$; these two requirements have no intersection.\n\nIf the additional equality is $\\vartheta+\\frac{1}{1 -\\rho^2}\\left[(1 -\\rho^2)\\sqrt{r} - t'(1 -\\rho^2) -\\lambda'(1 -|\\rho|)\\right]^2= 1$, then $\\lambda' = \\frac{1 -\\rho^2}{2|\\rho|}\\frac{1}{\\sqrt{1 -\\rho^2}}\\left(\\sqrt{1 -\\vartheta} - \\sqrt{1 - 2\\vartheta}\\right)$ and $t' = \\frac{1}{\\sqrt{1 -\\rho^2}} -\\frac{\\lambda'}{1 -|\\rho|}$. However, this case admits no $\\vartheta$, because $t' \\geq 0$, $(\\lambda' + t')^2\\geq 1$ and $\\vartheta+\\frac{1}{1 -\\rho^2}\\left[\\lambda'(1 -|\\rho|) + t'(1 -\\rho^2)\\right]^2\\geq 1$ respectively requires $$\\begin{aligned}\n&\\frac{2|\\rho|}{1 +|\\rho|} + \\sqrt{1 - 2\\vartheta} \\geq \\sqrt{1 -\\vartheta} \\\\\n& \\frac{2(1 - \\sqrt{1 -\\rho^2})}{1 +|\\rho|} + \\sqrt{1 - 2\\vartheta} \\geq \\sqrt{1 -\\vartheta}\\\\\n&1 + \\sqrt{1 - 2\\vartheta} \\geq 2 \\sqrt{1 -\\vartheta}\n\\end{aligned}$$ These three requirements admit no $\\vartheta\\in(0,1)$.\n\nTo sum up, the *third* surve does not show up in the phase diagram.\n\n*Fourth*, we study the curve $\\sqrt{r} =\\sqrt{\\frac{1-2 \\vartheta}{1-\\rho^{2}}}+\\frac{2(1-|\\rho|)-\\sqrt{(1-\\vartheta)\\left(1-\\rho^{2}\\right)}}{(1-|\\rho|)^{2}}$ given by $$(\\lambda' + t')^2= \\vartheta+\\frac{1}{1 -\\rho^2}\\left[\\lambda'(1 -|\\rho|) + t'(1 -\\rho^2)\\right]^2= 2\\vartheta+\\frac{1}{1 -\\rho^2}\\left[(1 -\\rho^2)\\sqrt{r} - t'(1 -\\rho^2) -\\lambda'(1 +|\\rho|)\\right]^2= 1.$$ We have $\\lambda' = \\frac{1 +|\\rho|}{|\\rho|}\\left(1 - \\sqrt{\\frac{1 -\\vartheta}{1 -\\rho^2} }\\right)$ and $t' = 1 -\\lambda'$. $\\lambda',t' \\geq 0$ requires $\\frac{\\sqrt{1 -\\rho^2}}{1 +|\\rho|} \\leq \\sqrt{1 -\\vartheta} \\leq \\sqrt{1 -\\rho^2}$, and we also need the requirements from $f_1,\\vartheta+f_3 \\geq 1$. The overall requirement on $\\vartheta$ is: $$\\begin{aligned}\n  \\frac{2 \\sqrt{1 -\\rho^2} -(1 -|\\rho|)}{1 +|\\rho|} \\leq&~ \\sqrt{1 -\\vartheta} \\leq\\sqrt{1 -\\rho^2} \\\\\n  \\frac{1 +|\\rho|}{1 -|\\rho|}\\sqrt{1 -\\rho^2} + \\sqrt{1 - 2\\vartheta} \\geq&~ \\left(\\frac{1 +|\\rho|}{1 -|\\rho|} +\\sqrt{1 -\\rho^2}\\right)\\sqrt{1 -\\vartheta}\\\\\n  \\frac{2 \\sqrt{1 -\\rho^2}}{1 -|\\rho|} + \\sqrt{1 - 2\\vartheta} \\geq&~ \\frac{3 -|\\rho|}{1 -|\\rho|} \\sqrt{1 -\\vartheta}\n\\end{aligned}$$ This is also an empty set, and this curve can never be present in the phase diagram.\n\nTo prove this, we note that $\\phi(\\vartheta) = \\frac{2 \\sqrt{1 -\\rho^2}}{1 -|\\rho|} + \\sqrt{1 - 2\\vartheta} - \\frac{3 -|\\rho|}{1 -|\\rho|} \\sqrt{1 -\\vartheta}$ is a \"first increasing, then decreasing\" function of $\\vartheta\\in[0,\\frac{1}{2}]$. When $|\\rho| \\geq \\frac{1 + 2 \\sqrt{2}}{7}$, the maximum of $\\frac{2 \\sqrt{1 -\\rho^2}}{1 -|\\rho|} + \\sqrt{1 - 2\\vartheta} - \\frac{3 -|\\rho|}{1 -|\\rho|} \\sqrt{1 -\\vartheta}$ is not positive, and does not admit a curve. When $|\\rho| < \\frac{1 + 2 \\sqrt{2}}{7}$, we add the requirement of $\\sqrt{1 -\\vartheta} \\geq \\frac{2 \\sqrt{1 -\\rho^2} -(1 -|\\rho|)}{1 +|\\rho|}$, even the largest $\\vartheta$ is still on the left side of the peak of the maximum point of $\\phi(\\vartheta)$ and still makes it negative.\n\n*Fifth*, we study the curve $\\sqrt{r} =\\left(\\frac{2}{1+|\\rho|}+\\frac{1}{\\sqrt{1-\\rho^{2}}}\\right) \\sqrt{1-\\vartheta}-\\frac{1-|\\rho|}{1+|\\rho|} \\sqrt{\\frac{1-2 \\vartheta}{1-\\rho^{2}}}$, which is given by $$\\vartheta+\\frac{1}{1 -\\rho^2}\\left[\\lambda'(1 -|\\rho|) + t'(1 -\\rho^2)\\right]^2=\\vartheta+(\\sqrt{r} -\\lambda' - t')_ +^2=2\\vartheta+\\frac{1}{1 -\\rho^2}\\left[(1 -\\rho^2)\\sqrt{r} - t'(1 -\\rho^2) -\\lambda'(1 +|\\rho|)\\right]^2= 1.$$ Now we have $\\lambda' = \\frac{1 -|\\rho|}{|\\rho|} \\left(\\sqrt{1 -\\vartheta} - \\sqrt{\\frac{1 - 2\\vartheta}{1 -\\rho^2}}\\right)$ and $t' = \\frac{1}{\\sqrt{1 -\\rho^2}} -\\frac{\\lambda'}{1 -|\\rho|}$. The requirements of $\\lambda',t' \\geq 0$ and $f_1,\\vartheta+f_3 \\geq 0$ are $$\\begin{aligned}\n  \\sqrt{1 -\\rho^2} - \\frac{|\\rho|(1 +|\\rho|)}{1 -|\\rho|} \\leq \\sqrt{\\frac{1 - 2\\vartheta}{1 -\\vartheta}} \\leq&~ \\frac{2 \\sqrt{1 -\\rho^2} -(1 +|\\rho|)}{1 -|\\rho|} \\\\\n  \\frac{1 +|\\rho|}{2} + \\sqrt{1 - 2\\vartheta} \\leq&~ \\left(\\frac{1 +|\\rho|}{2} + \\sqrt{1 -\\rho^2}\\right)\\sqrt{1 -\\vartheta} \\\\\n  \\frac{1 +|\\rho|}{1 -|\\rho|}\\sqrt{1 -\\rho^2} + \\sqrt{1 - 2\\vartheta} \\leq&~ \\left(\\frac{1 +|\\rho|}{1 -|\\rho|} +\\sqrt{1 -\\rho^2}\\right)\\sqrt{1 -\\vartheta}\n\\end{aligned}$$\n\nThis may not be an empty set. However, even when it is not an empty set, the curve is actually either greater than $\\sqrt{r} = 1 + \\sqrt{1 -\\vartheta}$ or $\\sqrt{r} = 2 \\sqrt{\\frac{1 -\\vartheta}{1 -\\rho^2}}$. To see this:\n\n-   In terms of the existence of $\\sqrt{r} = 1 + \\sqrt{1 -\\vartheta}$ and $\\sqrt{r} = 2 \\sqrt{\\frac{1 -\\vartheta}{1 -\\rho^2}}$: When $\\frac{1 -|\\rho|}{1 + |\\rho|} \\leq \\sqrt{1 -\\vartheta} \\leq \\left(\\frac{2}{\\sqrt{1 -\\rho^2}} - 1\\right)^{ - 1}$ and $\\sqrt{\\frac{1 - 2\\vartheta}{1 -\\vartheta}} \\leq \\frac{2 \\sqrt{1 -\\rho^2} -(1 +|\\rho|)}{1 -|\\rho|}$, we have proven that $\\sqrt{r} = 1 + \\sqrt{1 -\\vartheta}$ is one segment of the phase diagram. When $\\sqrt{1 -\\vartheta} \\geq \\left(\\frac{2}{\\sqrt{1 -\\rho^2}} - 1\\right)^{ - 1}$ and $\\sqrt{\\frac{1 - 2\\vartheta}{1 -\\vartheta}} \\leq \\frac{2 \\sqrt{1 -\\rho^2} -(1 +|\\rho|)}{1 -|\\rho|}$, we have proven that $\\sqrt{r} = 2 \\sqrt{\\frac{1 -\\vartheta}{1 -\\rho^2}}$ is one segment of the phase diagram. We can prove that $$\\begin{aligned}\n        &\\sqrt{1 -\\rho^2} - \\frac{|\\rho|(1 +|\\rho|)}{1 -|\\rho|} \\leq \\sqrt{\\frac{1 - 2\\vartheta}{1 -\\vartheta}} \\leq~ \\frac{2 \\sqrt{1 -\\rho^2} -(1 +|\\rho|)}{1 -|\\rho|}\\\\ \n        \\implies & \\begin{cases} \n        \\text{either }\\frac{1 -|\\rho|}{1 + |\\rho|} \\leq \\sqrt{1 -\\vartheta} \\leq \\left(\\frac{2}{\\sqrt{1 -\\rho^2}} - 1\\right)^{ - 1} \\text{ and }\\sqrt{\\frac{1 - 2\\vartheta}{1 -\\vartheta}} \\leq \\frac{2 \\sqrt{1 -\\rho^2} -(1 +|\\rho|)}{1 -|\\rho|} \\\\\n        \\text{or }\\sqrt{1 -\\vartheta} \\geq \\left(\\frac{2}{\\sqrt{1 -\\rho^2}} - 1\\right)^{ - 1} \\text{ and }\\sqrt{\\frac{1 - 2\\vartheta}{1 -\\vartheta}} \\leq \\frac{2 \\sqrt{1 -\\rho^2} -(1 +|\\rho|)}{1 -|\\rho|} \n        \\end{cases}\n      \n    \\end{aligned}$$ so one of $\\sqrt{r} = 1 + \\sqrt{1 -\\vartheta}$ and $\\sqrt{r} = 2 \\sqrt{\\frac{1 -\\vartheta}{1 -\\rho^2}}$ exists as long as the *fifth* curve exists.\n\n-   In the latter case, it is greater than $\\sqrt{r} = 2 \\sqrt{\\frac{1 -\\vartheta}{1 -\\rho^2}}$ which exists in the same region. (can be easily verified)\n\n-   In the former case, we can assume $|\\rho| \\leq 3 - 2 \\sqrt{2}$ because we need $\\sqrt{1 -\\vartheta} \\geq \\frac{1 -|\\rho|}{1 + |\\rho|}$ to hold for some $\\vartheta\\in(0,\\frac{1}{2})$. Using $|\\rho| \\leq 3 - 2 \\sqrt{2}$, we can prove $\\sqrt{\\frac{1 - 2\\vartheta}{1 -\\vartheta}} \\geq \\sqrt{1 -\\rho^2} - \\frac{|\\rho|(1 +|\\rho|)}{1 -|\\rho|} \\implies \\sqrt{1 -\\vartheta} \\geq \\frac{1 -|\\rho|}{1 + |\\rho|}$. Now we can easily verify the curve is greater than $\\sqrt{r} = 1 + \\sqrt{1 -\\vartheta}$ which exists in the same region\n\nAs a result, this curve does not play a part in the final phase diagram either.\n\n*Sixth*, we study the curve $\\sqrt{r} =1+\\frac{1+|\\rho|}{2} \\sqrt{\\frac{1-\\vartheta}{1-\\rho^{2}}}+\\frac{1-|\\rho|}{2} \\sqrt{\\frac{1-2 \\vartheta}{1-\\rho^{2}}}$, given by $$(\\lambda' + t')^2=\\vartheta+\\frac{1}{1 -\\rho^2}\\left[(1 -\\rho^2)\\sqrt{r} - t'(1 -\\rho^2) -\\lambda'(1 -|\\rho|)\\right]^2=2\\vartheta+\\frac{1}{1 -\\rho^2}\\left[(1 -\\rho^2)\\sqrt{r} - t'(1 -\\rho^2) -\\lambda'(1 +|\\rho|)\\right]^2= 1$$ We get $\\lambda' =\\frac{1 -\\rho^2}{2|\\rho|}\\frac{1}{\\sqrt{1 -\\rho^2}}\\left(\\sqrt{1 -\\vartheta} - \\sqrt{1 - 2\\vartheta}\\right)$ and $t' = 1 -\\lambda'$. The requirements from $t' \\geq 0$, $\\frac{1}{1 -\\rho^2}\\left[(1 +|\\rho|) \\lambda' + (1 -\\rho^2)t'\\right]^2\\geq 1$, $\\vartheta+\\frac{1}{1 -\\rho^2}\\left[\\lambda'(1 -|\\rho|) + t'(1 -\\rho^2)\\right]^2\\geq 1$ and $\\vartheta+(\\sqrt{r} -\\lambda' - t')_ +^2\\geq 1$ are respectively $$\\begin{aligned}\n &  \\frac{2|\\rho|}{\\sqrt{1 -\\rho^2}} +\\sqrt{1 - 2\\vartheta}\\geq \\sqrt{1 -\\vartheta} \\\\\n & \\frac{2(1 - \\sqrt{1 -\\rho^2})}{1 +|\\rho|} +\\sqrt{1 - 2\\vartheta} \\leq \\sqrt{1 -\\vartheta} \\\\\n & \\frac{2 \\sqrt{1 -\\rho^2}}{1 -|\\rho|} + \\sqrt{1 - 2\\vartheta} \\geq \\frac{3 -|\\rho|}{1 -|\\rho|} \\sqrt{1 -\\vartheta} \\\\\n & \\sqrt{\\frac{1 - 2\\vartheta}{1 -\\vartheta}} \\geq \\frac{2 \\sqrt{1 -\\rho^2} -(1 +|\\rho|)}{1 -|\\rho|}\n\\end{aligned}$$\n\nTaking the intersection, the last two inequalities can imply the rest, and it is equivalent to $$\\sqrt{r} =1+\\frac{1+|\\rho|}{2} \\sqrt{\\frac{1-\\vartheta}{1-\\rho^{2}}}+\\frac{1-|\\rho|}{2} \\sqrt{\\frac{1-2 \\vartheta}{1-\\rho^{2}}}  \\geq \\max \\left\\{1 + \\sqrt{1 -\\vartheta},\\, 2 \\sqrt{\\frac{1 -\\vartheta}{1 -\\rho^2}} \\right\\}.$$\n\nNow we have studied all the curves for $\\rho < 0$. To sum up, the phase curve is $$\\sqrt{r} =  \\max \\left\\{1 + \\sqrt{1 -\\vartheta},\\, 2 \\sqrt{\\frac{1 -\\vartheta}{1 -\\rho^2}},\\,1+\\frac{1+|\\rho|}{2} \\sqrt{\\frac{1-\\vartheta}{1-\\rho^{2}}}+\\frac{1-|\\rho|}{2} \\sqrt{\\frac{1-2 \\vartheta}{1-\\rho^{2}}} \\right\\}$$\n\n## Proof of Lemma [3](#supplem:sol.path.thresLasso){reference-type=\"ref\" reference=\"supplem:sol.path.thresLasso\"} {#subsec:proof-thresLasso-solution}\n\nRecall the optimization in [\\[enproof-optimization\\]](#enproof-optimization){reference-type=\"eqref\" reference=\"enproof-optimization\"}; the solution $b =(b_1,b_2)$ has to set the sub-gradient of the objective function to zero. As a result, the equation of the sub-gradient for $b =(b_1,b_2)$ is: $$\\begin{bmatrix} \n  1 & \\rho \\\\\n  \\rho & 1\n  \\end{bmatrix} \\begin{bmatrix} b_1 \\\\ b_2 \\end{bmatrix} \n  + \\lambda' \\begin{bmatrix} \n  \\mathop{\\mathrm{sgn}}(b_1) \\\\\n  \\mathop{\\mathrm{sgn}}(b_2)\n  \\end{bmatrix} \n   = \\begin{bmatrix} h_1 \\\\ h_2 \\end{bmatrix}$$\n\nNow we begin to find out the solution path. Thresholded Lasso has two steps: First, we run Lasso to select variables from $(x_j,x_{j + 1})$; second, a thresholding step with $t = t'\\sqrt{2\\log(p)}$ is further performed, and the surviving variables of the two steps are the finally selected ones. Also note that we have required $\\rho \\geq 0$.\n\nFirst, we study the behavior of Lasso, and decrease $\\lambda'$ from a sufficiently large value to see when the variables enter the model. We assume $h_1 > 0$ and $0 <\\abs{h_2} < h_1$.\n\nThe procedure is just setting $\\mu = 0$ in the proof of Lemma [2](#suppthm:sol.path.en){reference-type=\"ref\" reference=\"suppthm:sol.path.en\"}, and we summarise the results below:\n\n-   When $\\lambda' \\geq h_1$, we have $\\hat{b}_1 = \\hat{b}_2 = 0$.\n\n-   If $h_2\\geq \\rho h_1$, when $\\frac{{h_2 -\\rho h_1}}{1 -\\rho}\\leq \\lambda' <h_1$, we have $\\hat{b}_1 ={h_1 -\\lambda'}$, and $\\hat{b}_2 = 0$;\n\n    When $\\lambda'<\\frac{{h_2 -\\rho h_1}}{1 -\\rho}$, we have $$\\hat{b}_1 =\\frac{(h_1 -\\rho h_2) -(1 -\\rho)\\lambda'}{1 -\\rho^2},\\qquad \n        \\hat{b}_2 =\\frac{(h_2 -\\rho h_1) -(1 -\\rho)\\lambda'}{1 - \\rho^2};$$\n\n-   if $h_2<\\rho h_1$, when $\\frac{{-h_2 +\\rho h_1}}{1 +\\rho}\\leq \\lambda' <h_1$, we have $\\hat{b}_1 ={h_1 -\\lambda'}$, and $\\hat{b}_2 = 0$;\n\n    When $\\lambda'< \\frac{{-h_2 +\\rho h_1}}{1 +\\rho}$, we have $$\\hat{b}_1 =\\frac{(h_1 -\\rho h_2) -(1 +\\rho)\\lambda'}{1 - \\rho^2},\\quad \n      \\hat{b}_2 =\\frac{(h_2 -\\rho h_1) +(1 +\\rho)\\lambda'}{1 - \\rho^2}.$$\n\nWhen $\\lambda' \\geq h_1$, for any $t'$, we have $\\hat b_1 =\\hat b_2 = 0$.\n\nFor $h_2 \\geq \\rho h_1$, when $\\frac{{h_2 -\\rho h_1}}{1 -\\rho }\\leq \\lambda' <h_1$, if $h_1 \\leq \\lambda' + t'$, then we still selected neither of $(x_j,x_{j + 1})$ in the end. If $h_1 >\\lambda' + t'$, then we will select only $x_j$. When $\\lambda'<\\frac{{h_2 -\\rho h_1}}{1 -\\rho}$, we have $\\hat b_1 >\\hat b_2 > 0$ and it depends on whether $t \\geq \\hat b_1$, $\\hat b_2 \\leq  t < b_1$ or $\\hat b_2 > t$ how $(x_j,x_{j + 1})$ are selected in the end.\n\nFor $h_2 < \\rho h_1$, when $\\frac{{ -h_2 +\\rho h_1}}{1 +\\rho }\\leq \\lambda' <h_1$, if $h_1 \\leq \\lambda' + t'$, then we still selected neither of $(x_j,x_{j + 1})$ in the end. If $h_1 >\\lambda' + t'$, then we will select only $x_j$. When $\\lambda'<\\frac{{-h_2 +\\rho h_1}}{1 -\\rho}$, we have $\\hat b_1 >-\\hat b_2 > 0$ and it depends on whether $t \\geq \\hat b_1$, $-\\hat b_2 \\leq  t < b_1$ or $-\\hat b_2 > t$ how $(x_j,x_{j + 1})$ are selected in the end."},{"heading":"Proof of Theorem [5](#thm:forward){reference-type=\"ref\" reference=\"thm:forward\"} (Forward Selection)","text":"# Proof of Theorem [5](#thm:forward){reference-type=\"ref\" reference=\"thm:forward\"} (Forward Selection) {#suppsec:forward}\n\nThe proof for Forward Selection still consists of three parts: (a) deriving the rejection region, (b) obtaining the rate of convergence of $\\mathbb{E}[H(\\hat\\beta,\\beta)]$, and (c) calculating the phase diagram.\n\nFor Forward Selection, we first write $X=[x_1,x_2,\\ldots,x_p]$, where $x_i\\in \\mathbb{R}^n$ for $1\\leq i\\leq p$. For any subset $A\\subset\\{1,2,\\ldots,p\\}$, let $P^{\\bot}_{A}$ be the projection onto the orthogonal complement of the linear space spanned by $\\{ x_i:i\\in  A \\}$. Before the first part, we formally define Forward Selection in Algorithm [\\[alg:forward.selection\\]](#alg:forward.selection){reference-type=\"ref\" reference=\"alg:forward.selection\"}.\n\n::: algorithm\n::: algorithmic\nInput $X$ and $y$ (generated with our own setting.) Fix $t > 0$. Initialize $S^{(0)} =\\emptyset, \\hat\\beta^{(0)} = 0$, $\\hat r^{(0)} = y$. Initialize $k = 0$.   $k \\gets k + 1$ $\\hat r^{(k - 1)}\\gets P_{k - 1}^{\\bot}y$ $i^* \\gets \\mathop{\\mathrm{arg\\,max}}_{i\\notin S^{(k - 1)}}\\abs{x_i'\\hat r^{(k - 1)}}$[]{#step:entry.rule label=\"step:entry.rule\"} $\\delta^ + \\gets  \n       \\frac{ \\abs{x_{i^* }\\hat r^{(k - 1)} }}{\\norm{ P^{\\bot}_{k - 1} x_{i^*}}}$ Break. $S^{(k)}\\gets S^{(k - 1)}\\cup \\{ i^* \\}$ $k\\gets k - 1$\n\n$\\hat\\beta =\\hat\\beta^{\\text{ols}}(S^{(k)})$\n:::\n:::\n\n::: remark\n**Remark 3**. *The stopping rule is equivalent to measuring the decrease in the residual sum of squares. To see this, suppose $i\\in \\{ 1,2,\\dots ,p \\}$ is enrolled at step $k$, ans $S^{(k)} = S^{(k - 1)}\\cup \\{ i \\}$. Then $||y - X_{S^{(k)}}\\hat\\beta^{\\text{ols}}(S^{(k)})||^2 = ||P_k^{\\bot} y||^2$ and $||y - X_{S^{(k - 1)}}\\hat\\beta^{\\text{ols}}(S^{(k - 1)})||^2 = ||P_{k - 1}^{\\bot} y||^2$. By adding variable $i$ into $S^{(k - 1)}$, the decrease $||P_{k - 1}^{\\bot} y||^2 -||P_{k}^{\\bot} y||^2$ is equal to the squared norm of the projection of $P_{k - 1}^{\\bot} y$ onto the direction of $P_{k - 1}^{\\bot} x_i$, which is $\\left(\\frac{ \\abs{x_{i }\\hat r^{(k - 1)} }}{\\norm{ P^{\\bot}_{k - 1} x_{i}} } \\right)^2$ where $\\hat r^{(k - 1)} = P_{k - 1}^{\\bot}y$.*\n:::\n\n#### Part 1: Deriving the rejection region.\n\nForward selection is a sequential method, and we first need to show it can be decomposed into bivariate sub-problems. The main reason is that whether some variable $x_j$ is selected in the end only depends on $(x_j,x_{j + 1})$, and has nothing to do with other variables, or the number of steps $k$. We still use $(x_j,x_{j + 1})$ to denote an arbitrary pair of correlated variables:\n\nIn terms of forward gain, whenever $x_{j + 1}$ is not in $\\hat S^{(k - 1)}$ for arbitrary $k$, $P^{\\bot}_{k - 1}x_j = x_j$, and $\\frac{\\abs{x_j' \\hat r^{(k - 1)}} }{\\norm{ P^{\\bot}_{k - 1} x_j}} = \\abs{x'_j y} = |h_1|\\sqrt{2\\log(p)}$. When $x_{j + 1}$ is already in $\\hat S^{(k - 1)}$ for arbitrary $k$, $P^{\\bot}_{k - 1}x_j = x_j -\\rho x_{j + 1}$, and $\\frac{\\abs{x_j' \\hat r^{(k - 1)}} }{\\norm{ P^{\\bot}_{k - 1} x_j}} = \\frac{\\abs{(x'_j -\\rho x'_{j + 1}) y}} {\\sqrt{1 -\\rho^2}} = \\frac{\\abs{h_1 -\\rho h_2}}{\\sqrt{1 -\\rho^2}}\\sqrt{2\\log(p)}$.\n\nIn terms of the entry rule (\"$i^* \\gets \\mathop{\\mathrm{arg\\,max}}_{i\\notin S^{(k - 1)}}\\abs{x_i'\\hat r^{(k - 1)}}$\" in Algorithm [\\[alg:forward.selection\\]](#alg:forward.selection){reference-type=\"ref\" reference=\"alg:forward.selection\"}), since $x_j'\\hat r^{(k - 1)} = x_j'P^\\bot_{k - 1}y$, it is still $x_j'y$ or $(x_j'y -\\rho x_{j + 1}y)$ depending on whether $x_{j + 1}\\in S^{(k - 1)}$. It has nothing to do with specific $k$ or other variables than $(x_j,x_{j + 1})$.\n\nAs a result, Algorithm [\\[alg:forward.selection\\]](#alg:forward.selection){reference-type=\"ref\" reference=\"alg:forward.selection\"} under the block-wise diagonal design can be viewed as many bivariate sub-problems going on simultaneously for each pair of correlated variables. In each \"while\" loop, variables from different pairs may be enrolled, but (i) the order of $(x_j,x_{j + 1})$ does not depend on $k$ or other variables, and (ii) the bivariate problem must have terminated when the whole algorithm terminates, and the result of the bivariate problem does not depend on $k$ or other variables.\n\nOf course, we have assumed that Algorithm [\\[alg:forward.selection\\]](#alg:forward.selection){reference-type=\"ref\" reference=\"alg:forward.selection\"} will always terminate in finite steps, which is true, because each bivariate sub-problem always terminates as we will see in the proof of Lemma [3](#suppthm:forward.path){reference-type=\"ref\" reference=\"suppthm:forward.path\"}.\n\nWorking on a bivariate problem, we can scale everything down by $\\sqrt{2\\log(p)}$ and define $t' = t/\\sqrt{2\\log(p)}$. Then the solution path can be described in Lemma [3](#suppthm:forward.path){reference-type=\"ref\" reference=\"suppthm:forward.path\"}.\n\n::: {#suppthm:forward.path .lem}\n**Lemma 3** (Solution path of Forward Selection). *Consider the bivariate problem of running Algorithm [\\[alg:forward.selection\\]](#alg:forward.selection){reference-type=\"ref\" reference=\"alg:forward.selection\"} with $y$ and $(x_j,x_{j + 1})$. Suppose $h_1 > \\abs{h_2} \\geq 0$, and $\\rho > 0$.*\n\n-   *When $t' \\geq  h_1$, none of $(x_j,x_{j + 1})$ will get selected when the algorithm ends.*\n\n-   *If $- h_1 < h_2 \\leq (\\rho - \\sqrt{1 -\\rho^2}) h_1$, when $t' <  h_1$, both $(x_j,x_{j + 1})$ will get selected when the algorithm ends.*\n\n-   *If $(\\rho - \\sqrt{1 -\\rho^2}) h_1 < h_2 < h_1$, when $t\\in\\left[\\frac{\\abs{h_2 -\\rho h_1}}{\\sqrt{1 -\\rho^2}},h_1\\right)$, only $x_j$ is in the model before the algorithm ends.*\n\n-   *If $(\\rho - \\sqrt{1 -\\rho^2}) h_1 < h_2 < h_1$, when $t <\\frac{\\abs{h_2 -\\rho h_1}}{\\sqrt{1 -\\rho^2}}$, both $(x_j,x_{j + 1})$ will get selected when the algorithm ends.*\n:::\n\n::: proof\n*Proof of Lemma [3](#suppthm:forward.path){reference-type=\"ref\" reference=\"suppthm:forward.path\"}.* Note that we have required $\\rho > 0$ to avoid unnecessary discussion. Since $h_1 >\\abs{h_2}$, at any step $k$ when neither of $(x_j,x_{j + 1})$ is in the model, we have $|x_j' \\hat r_{k - 1}| = |h_1| > |x_{j + 1}' \\hat r_{k - 1}|=\\abs{h_2}$. As a result, if $t \\geq h_1$, then the algorithm will terminate without selecting either of $(x_j,x_{j + 1})$. If $t > h_1$, it will select $x_j$ at some step and proceed to the next \"while\" loop.\n\nAfter $x_j$ has been selected, if $h_2 \\geq (\\rho + \\sqrt{1 -\\rho^2})h_1$ or $h_2 \\leq (\\rho - \\sqrt{1 -\\rho^2})h_1$, we have $\\frac{\\abs{x_{j + 1}' \\hat r^{(k - 1)}} }{\\norm{ P^{\\bot}_{k - 1} x_{j + 1}}} = \\frac{\\abs{h_2 -\\rho h_1}}{\\sqrt{1 -\\rho^2}} \\geq h_1 > t$, and $x_{j + 1}$ will be selected at some later step. However, since $\\rho > 0$, we have $\\rho + \\sqrt{1 -\\rho^2} > 1$, so we can have $\\frac{\\abs{x_{j + 1}' \\hat r^{(k - 1)}} }{\\norm{ P^{\\bot}_{k - 1} x_{j + 1}}} = \\frac{\\abs{h_2 -\\rho h_1}}{\\sqrt{1 -\\rho^2}} \\geq h_1$ only when $h_2 \\geq (\\rho - \\sqrt{1 -\\rho^2})h_1$.\n\nIf $(\\rho - \\sqrt{1 -\\rho^2})h_1 < h_2 < h_1$, we have $\\frac{\\abs{h_2 -\\rho h_1}}{\\sqrt{1 -\\rho^2}} <h_1$. When $t <\\frac{\\abs{h_2 -\\rho h_1}}{\\sqrt{1 -\\rho^2}}$, both of the variables will be selected; when $t\\in\\left[\\frac{\\abs{h_2 -\\rho h_1}}{\\sqrt{1 -\\rho^2}},h_1\\right)$, only $x_j$ will be selected. ◻\n:::\n\nWe use Lemma [3](#suppthm:forward.path){reference-type=\"ref\" reference=\"suppthm:forward.path\"} to write down the rejection region, as in Figure [11](#fig:rej){reference-type=\"ref\" reference=\"fig:rej\"}, still for $\\rho > 0$. $$\\begin{aligned}\n \\label{suppeq:forward.rjRegion}\n  {\\cal R} &= \\{(h_1,h_2): h_1-\\rho h_2> t' \\sqrt{1 -\\rho^2},\\, h_1> \\frac{t' \\sqrt{1 -\\rho^2}}{1 -\\rho} \\}\\cr\n  &\\;\\; \\cup \\{(h_1, h_2): h_1 > t',\\, h_1> h_2\\} \\cup \\{(h_1, h_2):  h_2<- t,\\,h_1-\\rho h_2> t' \\sqrt{1 -\\rho^2} \\}\\cr\n  &\\;\\; \\cup \\{(h_1,h_2): -h_1 +\\rho h_2> t' \\sqrt{1 -\\rho^2},\\, h_1 <- \\frac{t' \\sqrt{1 -\\rho^2}}{1 -\\rho} \\}\\cr\n  &\\;\\; \\cup \\{(h_1, h_2): h_1 <- t',\\, h_1 < h_2\\} \\cup \\{(h_1, h_2):  h_2 > t,\\, -h_1 +\\rho h_2> t' \\sqrt{1 -\\rho^2} \\}\n  \n\\end{aligned}$$\n\n#### Part 2. Analyzing the Hamming error.\n\n![the rejection region of forward selection ($\\rho \\geq 0$) ](Figure-supp/forward_rejection_region_plus_new.pdf){#fig:rej width=\"85%\"}\n\n::: {#suppthm:hamm.forward .theorem}\n**Theorem 14**. *Suppose the conditions of Theorem [5](#thm:forward){reference-type=\"ref\" reference=\"thm:forward\"} holds. Let $t' = t /\\sqrt{2\\log(p)}$ and $h_1 = x_j' y /\\sqrt{2\\log(p)}$, $h_2 = x_{j + 1}' y /\\sqrt{2\\log(p)}$. As $p\\to\\infty$, $$\\ensuremath{\\mathrm{FP}}_p=L_p p^{1- \\min\\bigl\\{ t'^2, \\;\\; \\vartheta+ f_1(\\sqrt{r}, t')\\bigr\\}}, \\qquad \\ensuremath{\\mathrm{FN}}_p = L_p p^{1-\\min\\bigl\\{\\vartheta+ f_2(\\sqrt{r}, t'),\\;\\; 2\\vartheta+ f_3(\\sqrt{r}, t')\\bigr\\}},$$ where (below, $d^2_{|\\rho|}(u,v)$ is as in Definition [1](#def:EllipsDistance){reference-type=\"ref\" reference=\"def:EllipsDistance\"}), $$\\begin{aligned}\n    f_1(\\sqrt{r},t') & = \\begin{cases} \n      (t' -|\\rho| \\sqrt{r})^2 & \\text{ if } \\sqrt{r} \\leq \\frac{t'}{1 +|\\rho|}\\\\\n      \\frac{1}{1 -\\rho^2}d_{\\abs{\\rho}}^2((t',t'), (|\\rho| \\sqrt{r},\\sqrt{r})) & \\text{ if } \\frac{t'}{1 +|\\rho|} <\\sqrt{r} \\leq \\frac{2t'}{1+|\\rho|} \\\\\n      \\min\\left\\{ \\frac{1}{2}(1 -|\\rho|) r,\\ t'^2 \\right\\} & \\text{ if } \\sqrt{r} > \\frac{2t'}{1+|\\rho|}\n    \\end{cases}  \\cr\n   f_2(\\sqrt{r}, t') &= \\begin{cases} \n    \\min \\left\\{ (\\sqrt{r} - t')_ + ^2,\\  \\frac{1}{2}(1 -|\\rho|) r  \\right\\} & \\text{ if } \\sqrt{r} \\leq \\frac{2t'}{\\sqrt{1-\\rho^2}} \\\\\n    \\min \\left\\{ (\\sqrt{r} - t')_ + ^2,\\ \\frac{1}{1 -\\rho^2}d^2_{|\\rho|}(B,(\\sqrt{r},|\\rho| \\sqrt{r}))  \\right\\}& \\text{ if }\\frac{2t'}{\\sqrt{1-\\rho^2}} < \\sqrt{r} \\leq \\frac{t'\\sqrt{1-\\rho^2}}{|\\rho|(1-|\\rho|)} \\\\\n    \\left[ \\sqrt{1 -\\rho^2}\\sqrt{r} - t' \\right]^2 & \\text{ if } \\sqrt{r}>\\frac{t' \\sqrt{1-\\rho^2}}{|\\rho|(1-|\\rho|)}\n  \\end{cases}\n\\end{aligned}$$ The definition of $f_3(\\sqrt{r},t')$ depends on the sign of $\\rho$. When $\\rho > 0$, $$f_3(\\sqrt{r}, t') = \\left[ \\sqrt{1 -\\rho^2}\\sqrt{r} - t  \\right]^2$$ When $\\rho < 0$, $$\\begin{aligned}\n      f_3(\\sqrt{r},t') =&~ \\min \\left\\{  \\left[ \\sqrt{1 -\\rho^2}\\sqrt{r} - t  \\right]^2,\\ d^2_{|\\rho|}(C,((1 -|\\rho|)\\sqrt{r}, -(1 -|\\rho|)\\sqrt{r})) \\right\\} \\\\\n      =&~ \\min \\left\\{  \\left[ \\sqrt{1 -\\rho^2}\\sqrt{r} - t  \\right]^2,\\ \\frac{2}{1 -|\\rho|}\\left[ (1 -|\\rho|)\\sqrt{r} - t\n       \\right]^2 \\right\\}\n    \n\\end{aligned}$$*\n:::\n\nThe calculation of the elliptical distances are easy given Lemma [1](#supplem:distance){reference-type=\"ref\" reference=\"supplem:distance\"}.\n\n#### Part 3. Calculating the phase diagram.\n\nThe computation of the boundary between Alomst Full Recovery and No Recovery is almost the same for every method, so we omit the details and conclude that such boundary is $r =\\vartheta$.\n\nThen we set out to calculate the boundary between Alomst Full Recovery and Exact Recovery. As usual, we have four cases respectively for $\\rho > 0$ and $\\rho < 0$. However, unlike previous methods, forward selection is very easy, so we combine $\\rho > 0$ and $\\rho < 0$ in our discussion and use $\\abs{rho}$ all along.\n\nWhen $\\rho > 0$, the phase curve is $$\\sqrt{r} = \\max \\left\\{ 1 + \\sqrt{1 -\\vartheta}, \\sqrt{\\frac{2(1 -\\vartheta)}{1 -\\rho}}, \\sqrt{\\frac{1 - 2\\vartheta}{1 -\\rho^2}} + \\frac{1}{\\sqrt{1 -\\rho^2}}\\right\\}.$$ When $\\rho < 0$, the phase curve is $$\\sqrt{r} =\\max \\left\\{ 1 + \\sqrt{1 -\\vartheta}, \\sqrt{\\frac{2(1 -\\vartheta)}{1 -|\\rho|}},\\sqrt{\\frac{1 - 2\\vartheta}{1 -\\rho^2}} + \\frac{1}{\\sqrt{1 -\\rho^2}}, \\sqrt{\\frac{1 - 2\\vartheta}{2(1 -|\\rho|)}} + \\frac{1}{1 -|\\rho|}\\right\\}$$\n\n*First*, if $t'^2 = \\vartheta+ f_2(\\sqrt{r},t') = 1$ and $\\vartheta+ f_1(\\sqrt{r},t') \\geq 1$, $2\\vartheta+ f_3(\\sqrt{r},t') \\geq 1$, we discuss the conditional expression of $f_2(\\sqrt{r},t')$:\n\nWhen $\\sqrt{r} \\leq \\frac{2t'}{\\sqrt{1 -\\rho^2}}$ in $f_3(\\sqrt{r},t')$: Now $\\sqrt{r} =\\max \\left\\{ 1 + \\sqrt{1 -\\vartheta},\\, \\sqrt{\\frac{2(1 -\\vartheta)}{1 -|\\rho|}} \\right\\}.$ We need the following requirements: First, $\\sqrt{r} \\leq \\frac{2t'}{\\sqrt{1 -\\rho^2}} = \\frac{2}{\\sqrt{1 -\\rho^2}}$ itself. It is not restrictive, because $1 + \\sqrt{1 -\\vartheta} \\leq 2 \\leq \\frac{2}{\\sqrt{1 -\\rho^2}}$ and $\\sqrt{\\frac{2(1 -\\vartheta)}{1 -|\\rho|}} \\leq \\sqrt{\\frac{2}{1 -|\\rho|}} \\leq \\frac{2}{\\sqrt{1 -\\rho^2}}$. Second, $\\vartheta+ f_2(\\sqrt{r},t') \\geq 1$, which is still not restrictive. When $\\sqrt{r} \\geq \\frac{2t'}{1 +|\\rho|}$, we know $FP_2 = o(1)$ from $\\sqrt{r} \\geq \\sqrt{\\frac{2(1 -\\vartheta)}{1 -|\\rho|}}$ and $t' = 1$.When $1 < \\sqrt{r} <  \\frac{2t'}{1 +|\\rho|}$, we know $FP_2 = o(1)$ from $d^2(B,(\\sqrt{r},|\\rho| \\sqrt{r})) \\geq  \\frac{1}{2}(1 +|\\rho|)(1 -|\\rho|)^2 r$. Third, $2\\vartheta+ f_3(\\sqrt{r},t') \\geq 1$, which requires $\\sqrt{r} \\geq \\sqrt{\\frac{1 - 2\\vartheta}{1 -\\rho^2}} + \\frac{1}{\\sqrt{1 -\\rho^2}}$ when the correlation is positive, and $\\sqrt{r} \\geq \\max \\left\\{\\sqrt{\\frac{1 - 2\\vartheta}{1 -\\rho^2}} + \\frac{1}{\\sqrt{1 -\\rho^2}},\\,\\sqrt{\\frac{1 - 2\\vartheta}{2(1 -|\\rho|)}} + \\frac{1}{1 -|\\rho|}  \\right\\}.$ when the correlation is negative.\n\nWhen $\\frac{2t'}{\\sqrt{1-\\rho^2}} < \\sqrt{r} \\leq \\frac{t' \\sqrt{1 -\\rho^2}}{|\\rho|(1-|\\rho|)}$ in $f_3(\\sqrt{r},t')$: If $(1 -\\vartheta)(1 -\\rho^2) =  d^2(B,(\\sqrt{r},|\\rho| \\sqrt{r}))$, then since $d^2(B,(\\sqrt{r},|\\rho| \\sqrt{r})) \\geq  \\frac{1}{2}(1 +|\\rho|)(1 -|\\rho|)^2 r$, we have $\\sqrt{r} \\leq \\sqrt{\\frac{2(1 -\\vartheta)}{1 -|\\rho|}}.$ However, we also need $\\sqrt{r} >\\frac{2}{\\sqrt{1-\\rho^2}}$, which gives a contradiction. This case does not exist. If $\\sqrt{r} = 1 + \\sqrt{1 -\\vartheta}$, then it also contradicts $\\sqrt{r} >\\frac{2}{\\sqrt{1-\\rho^2}}$.\n\nWhen $\\sqrt{r}>\\frac{t' \\sqrt{1 -\\rho^2}}{|\\rho|(1-|\\rho|)}$ in $f_3(\\sqrt{r},t')$: $\\sqrt{r} = \\sqrt{\\frac{1 -\\vartheta}{1 -\\rho^2}} + \\frac{1}{\\sqrt{1 -\\rho^2}}$. It cannot meet the requirement $\\sqrt{r} > \\frac{t' \\sqrt{1 -\\rho^2}}{|\\rho|(1-|\\rho|)}$, so this case does not exist.\n\nTo sum up, the *first* case gives $$\\sqrt{r} =\\max \\left\\{ 1 + \\sqrt{1 -\\vartheta},\\, \\sqrt{\\frac{2(1 -\\vartheta)}{1 -|\\rho|}} \\right\\}.$$ which exists in the region:\n\n-   $\\left(\\frac{1}{2},1\\right]\\cup \\{ \\vartheta\\leq \\frac{1}{2}: \\sqrt{r} \\geq \\sqrt{\\frac{1 - 2\\vartheta}{1 -\\rho^2}} + \\frac{1}{\\sqrt{1 -\\rho^2}}\\}$ for $\\rho > 0$ .\n\n-   $\\left(\\frac{1}{2},1\\right]\\cup \\left\\{ \\vartheta\\leq \\frac{1}{2}: \\sqrt{r} \\geq \\max \\left\\{\\sqrt{\\frac{1 - 2\\vartheta}{1 -\\rho^2}} + \\frac{1}{\\sqrt{1 -\\rho^2}},\\,\\sqrt{\\frac{1 - 2\\vartheta}{2(1 -|\\rho|)}} + \\frac{1}{1 -|\\rho|}  \\right\\}\\right\\}$ for $\\rho < 0$ .\n\n*Second*, if $\\vartheta+ f_1(\\sqrt{r},t') = \\vartheta+ f_2(\\sqrt{r},t') = 1$ and $t' \\geq 1$, $2\\vartheta+ f_3(\\sqrt{r},t') \\geq 1$, then we will get nothing in this case. To see this, we first list a few requirements:\n\n-   We know that $t' \\geq 1$;\n\n-   We know from $\\vartheta+ f_2(\\sqrt{r},t') = 1$ that $\\sqrt{r} \\geq \\max \\left\\{ t' + \\sqrt{1 -\\vartheta},\\, \\sqrt{\\frac{2(1 -\\vartheta)}{1 -|\\rho|}} \\right\\}$;\n\n-   We know from $2\\vartheta+ f_3(\\sqrt{r},t') \\geq 1$ that, when $\\vartheta\\leq \\frac{1}{2}$, we need\n\n    -   $\\sqrt{r} \\geq \\sqrt{\\frac{1 - 2\\vartheta}{1 -\\rho^2}} + \\frac{t'}{\\sqrt{1 -\\rho^2}}$ for positive correlation;\n\n    -   $\\sqrt{r} \\geq \\max \\left\\{\\sqrt{\\frac{1 - 2\\vartheta}{1 -\\rho^2}} + \\frac{t'}{\\sqrt{1 -\\rho^2}},\\,\\sqrt{\\frac{1 - 2\\vartheta}{2(1 -|\\rho|)}} + \\frac{t'}{1 -|\\rho|}  \\right\\}$ for negative correlation.\n\nFrom these requirements, even if this case does admit some curve, it can only be higher than the one in the previous *first* case, and exist in a smaller region.\n\n*Third*, if $t'^2 = 2\\vartheta+ f_3(\\sqrt{r},t') = 1$ and $\\vartheta+ f_1(\\sqrt{r},t') \\geq  1$, $\\vartheta+ f_2(\\sqrt{r},t') \\geq  1$, then:\n\nWhen the correlation is positive, we have only one possible curve $\\sqrt{r} =\\sqrt{\\frac{1 - 2\\vartheta}{1 -\\rho^2}} + \\frac{1}{\\sqrt{1 -\\rho^2}}.$ From $\\vartheta+ f_2(\\sqrt{r},t') \\geq  1$, we get the requirement $\\sqrt{r} \\geq \\max \\left\\{ 1 + \\sqrt{1 -\\vartheta},\\, \\sqrt{\\frac{2(1 -\\vartheta)}{1 -\\rho}} \\right\\}$. For $\\vartheta+ f_1(\\sqrt{r},t') \\geq  1$, since we already have $\\sqrt{r} \\geq \\sqrt{\\frac{2(1 -\\vartheta)}{1 -\\rho}}$, we know $\\vartheta+ f_1(\\sqrt{r},t') \\geq  1$ always holds.\n\nWhen the correlation is negative, we have $\\sqrt{r} =\\max \\left\\{\\sqrt{\\frac{1 - 2\\vartheta}{1 -\\rho^2}} + \\frac{1}{\\sqrt{1 -\\rho^2}},\\,\\sqrt{\\frac{1 - 2\\vartheta}{2(1 -|\\rho|)}} + \\frac{1}{1 -|\\rho|}  \\right\\}.$ From $\\vartheta+ f_2(\\sqrt{r},t') \\geq  1$, we get the requirement $\\sqrt{r} \\geq \\max \\left\\{ 1 + \\sqrt{1 -\\vartheta},\\, \\sqrt{\\frac{2(1 -\\vartheta)}{1 -|\\rho|}} \\right\\}$. For $\\vartheta+ f_1(\\sqrt{r},t') \\geq  1$, since we already have $\\sqrt{r} \\geq \\sqrt{\\frac{2(1 -\\vartheta)}{1 -|\\rho|}}$, we know $FP_2 = o(1)$ always holds.\n\n*Fourth*, if $\\vartheta+ f_1(\\sqrt{r},t') = 2\\vartheta+ f_3(\\sqrt{r},t') = 1$ and $t' \\geq  1$, $\\vartheta+ f_2(\\sqrt{r},t') \\geq  1$, then we will get nothing from this case. To see this, we still list a few requirement:\n\n-   We know that $t' \\geq 1$.\n\n-   From $\\vartheta+ f_2(\\sqrt{r},t') \\geq  1$, we know that $\\sqrt{r} \\geq \\max \\left\\{ t' + \\sqrt{1 -\\vartheta},\\, \\sqrt{\\frac{2(1 -\\vartheta)}{1 -|\\rho|}} \\right\\}$.\n\n-   From $2\\vartheta+ f_3(\\sqrt{r},t') = 1$, we know that $\\sqrt{r} = \\sqrt{\\frac{1 - 2\\vartheta}{1 -\\rho^2}} + \\frac{t'}{\\sqrt{1 -\\rho^2}}$ for $\\rho > 0$ and $\\sqrt{r} =\\max \\left\\{\\sqrt{\\frac{1 - 2\\vartheta}{1 -\\rho^2}} + \\frac{t'}{\\sqrt{1 -\\rho^2}},\\,\\sqrt{\\frac{1 - 2\\vartheta}{2(1 -|\\rho|)}} + \\frac{t'}{1 -|\\rho|}  \\right\\}$ for $\\rho < 0$.\n\nEven if this case admits any curve, that curve would be above the curve in the previous *third* case, and exist within a smaller region of $\\vartheta$.\n\nTo sum up, we have the following results:\n\n-   Phae diagram when the correlation is positive: $$\\sqrt{r} = \\max \\left\\{ 1 + \\sqrt{1 -\\vartheta}, \\sqrt{\\frac{2(1 -\\vartheta)}{1 -\\rho}}, \\sqrt{\\frac{1 - 2\\vartheta}{1 -\\rho^2}} + \\frac{1}{\\sqrt{1 -\\rho^2}}\\right\\}.$$\n\n-   Phae diagram when the correlation is negative: $$\\sqrt{r} =\\max \\left\\{ 1 + \\sqrt{1 -\\vartheta}, \\sqrt{\\frac{2(1 -\\vartheta)}{1 -|\\rho|}},\\sqrt{\\frac{1 - 2\\vartheta}{1 -\\rho^2}} + \\frac{1}{\\sqrt{1 -\\rho^2}}, \\sqrt{\\frac{1 - 2\\vartheta}{2(1 -|\\rho|)}} + \\frac{1}{1 -|\\rho|}\\right\\}$$"},{"heading":"Proof of Theorem [6](#thm:forward-backward){reference-type=\"ref\" reference=\"thm:forward-backward\"} (Forward Backward Selection)","text":"# Proof of Theorem [6](#thm:forward-backward){reference-type=\"ref\" reference=\"thm:forward-backward\"} (Forward Backward Selection) {#suppsec:forward.backward}\n\nThe proof for Forward Selection still has three tasks: (a) deriving the rejection region, (b) obtaining the rate of convergence of $\\mathbb{E}[H(\\hat\\beta,\\beta)]$, and (c) calculating the phase diagram. However, as we will see later, forward backward selection has six cases, each of which has a different shape of the rejection region. After deriving the rejection region, we consider the $\\mathbb{E}[H(\\hat\\beta,\\beta)]$ and phase curves of the six cases one by one, and summarise the results at the end.\n\nDefore deriving the rejection region in the first part, we need some clarification about the definition of the forward backward selection we have investigated. More precisely, we have simplified the backward step into one thresholding step after the forward selection algorithm, so it is more precisely \"thresholded forward selection\".\n\nThe reason why we have not used a sequential algorithm with alternating forward and backward steps, like FoBa defined in @zhang2011adaptive, is not compuational simplicity, but to avoid degeneration. We explain briefly why any sequential algorithm with alternating forward and backward steps will either have nonfunctional backward steps, or be unable to terminate at a finite step.\n\nTo see this, we review the setting of Lemma [3](#suppthm:forward.path){reference-type=\"ref\" reference=\"suppthm:forward.path\"} about the solution path of forward selection, in which $h_1 >\\abs{h_2}$ and we only consider a bivariate problem. Using the same argument, some version of FoBa can also be decomposed into bivariate subproblems, and it is equivalent to running the algorithm only on $y$ and $(x_j,x_{j + 1})$.\n\nIn brief, in such a bivariate problem with $h_1 >\\abs{h_2}$, if either or both of $(x_j,x_{j + 1})$ ever get selected and then deleted at some backward step, then they will be selected back again because they still meet the requirements for a variable to get enrolled. When $h_1 >\\abs{h_2}$, the case of deleting $x_j$ while leaving $x_{j + 1}$ still in the model cannot happen, because no deletion rule based on $(x_j'y,x_{j + 1}'y)$ can delete $x_j$ without touching $x_{j +1}$. As a result, the algorithm cannot terminate at a finite step.\n\nIf the algorithm termininates at a finite step, then the backward step much have not deleted any of $(x_j,x_{j + 1})$, and such algorithm performs the same as forward selection.\n\nWe have explained the degeneration of Foba [@zhang2011adaptive], but we still want to implement some kind of backward step additional to forward selection, because the problem with forward selection is inability to correct the mistakes made in the early steps. Thus it is natural to use one thresholding step at the end.\n\n#### Part 1: Deriving the rejection region.\n\nWe first work on the solution path, and then compute the rejection region. The forward selection part has been discussed before, and we recall the results in Lemma [3](#suppthm:forward.path){reference-type=\"ref\" reference=\"suppthm:forward.path\"} (re-iterated in an equivalent way):\n\n1.  When $t \\geq  h_1$, neither is selected.\n\n2.  When $t < h_1$, and $\\rho h_1 - t \\sqrt{1 -\\rho^2} \\leq h_2 \\leq \\rho h_1 + t  \\sqrt{1 -\\rho^2}$, only $x_j$ is selected.\n\n3.  When $t < h_1$, and $\\begin{cases} \n      \\text{either }h_2 >  \\rho h_1 + t  \\sqrt{1 -\\rho^2} \\\\\n      \\text{or }- h_1 < h_2 < \\rho h_1 - t  \\sqrt{1 -\\rho^2}\n      \\end{cases}$, both $x_j$ and $x_{j + 1}$ are selected.\n\nNow this is followed by a thresholding step. Before using $v$ to threshold the results, we note that:\n\n1.  When $x_{j + 1}$ is not selected, $\\hat\\beta_j = h_1$;\n\n2.  When both $(x_j,x_{j + 1})$ are selected, $\\hat\\beta_j = \\frac{h_1 -\\rho h_2}{1 -\\rho^2}$ and $\\hat\\beta_{j + 1} =\\frac{h_2 -\\rho h_1}{1 -\\rho^2}$.\n\nWhen the thresholding is performed, we can naturally describe the solution path of thresholded forward selection as:\n\n::: {#supplem:sol.path.thres.forward .lemma}\n**Lemma 4**. *Consider the bivariate problem of running Algorithm [\\[alg:forward.selection\\]](#alg:forward.selection){reference-type=\"ref\" reference=\"alg:forward.selection\"} with $y$ and $(x_j,x_{j + 1})$, followed by thresholding $(\\hat\\beta_j,\\hat\\beta_{j + 1})$ with $v$. Define $h_1 = x_j' y /\\sqrt{2\\log(p)}$, $h_2 = x_{j + 1}' y /\\sqrt{2\\log(p)}$ and $t' = t/\\sqrt{2\\log(p)}$, $v' = v/\\sqrt{2\\log(p)}$. Suppose $h_1 > \\abs{h_2} \\geq 0$, and $\\rho > 0$. Then*\n\n-   *When $t' \\geq h_1$, neither is selected.*\n\n-   *When $t' < h_1$ and $\\rho h_1 - t'  \\sqrt{1 -\\rho^2} \\leq h_2 \\leq \\rho h_1 + t'  \\sqrt{1 -\\rho^2}$,*\n\n    -   *If $v' \\geq h_1$, neither is selected.*\n\n    -   *If $v' < h_1$, only $x_j$ is selected.*\n\n-   *When $t' < h_1$, and $\\begin{cases} \n          \\text{either }h_2 >  \\rho h_1 + t'  \\sqrt{1 -\\rho^2} \\\\\n          \\text{or }- h_1 < h_2 < \\rho h_1 - t'  \\sqrt{1 -\\rho^2}\n          \\end{cases}$,*\n\n    -   *If $h_2 >  \\rho h_1 + t'   \\sqrt{1 -\\rho^2}$, and $h_2 -\\rho h_1 >  v'(1 -\\rho^2)$, both $(x_j,x_{j + 1})$ are selected.*\n\n    -   *If $h_2 >  \\rho h_1 + t'   \\sqrt{1 -\\rho^2}$, and $h_2 -\\rho h_1 \\leq   v'(1 -\\rho^2) < h_1 -\\rho h_2$, only $x_j$ is selected.*\n\n    -   *If $h_2 >  \\rho h_1 + t'   \\sqrt{1 -\\rho^2}$, and $h_1 -\\rho h_2 \\leq  v'(1 -\\rho^2)$, neither is selected.*\n\n    -   *If $h_2 <  \\rho h_1 - t'   \\sqrt{1 -\\rho^2}$, and $\\rho h_1 - h_2 >  v'(1 -\\rho^2)$, both $(x_j,x_{j + 1})$ are selected.*\n\n    -   *If $h_2 <  \\rho h_1 - t'   \\sqrt{1 -\\rho^2}$, and $\\rho h_1 - h_2 \\leq   v'(1 -\\rho^2) < h_1 -\\rho h_2$, only $x_j$ is selected.*\n\n    -   *If $h_2 <  \\rho h_1 - t' \\sqrt{1 -\\rho^2}$, and $h_1 -\\rho h_2 \\leq  v'(1 -\\rho^2)$, neither is selected.*\n:::\n\nThe rejection region can be complicated, and it has many cases visually. See Figure [18](#fig:rjRegion.thres.forward){reference-type=\"ref\" reference=\"fig:rjRegion.thres.forward\"}. $$\\begin{aligned}\n \\label{suppeq:foba.rjRegion}\n  {\\cal R} &= \\{(h_1,h_2): h_1-\\rho h_2> \\max\\{t' \\sqrt{1 -\\rho^2},v'(1 -\\rho^2)\\},\\, h_2 >\\rho h_1 + t\\sqrt{1 -\\rho^2} \\}\\cr\n  &\\;\\; \\cup \\{(h_1,h_2):  h_1> \\max\\{t',v'\\},\\, h_2 \\leq \\rho h_1 + t\\sqrt{1 -\\rho^2},\\,h_2 \\geq \\rho h_1 - t\\sqrt{1 -\\rho^2} \\}\\cr\n  &\\;\\; \\cup \\{(h_1, h_2): h_1> h_2,\\,h_1 >\\max\\{t',v'\\},\\, h_2 >\\rho h_1 - t\\sqrt{1 -\\rho^2}\\} \\cr\n  &\\;\\; \\cup \\{(h_1, h_2): h_1 > t',\\, h_2 \\leq \\rho h_1 - t\\sqrt{1 -\\rho^2},\\, h_1 -\\rho h_2 > v'(1 -\\rho^2) \\} \\cr\n  &\\;\\; \\cup \\{(h_1, h_2):  h_2<- t,\\,h_1-\\rho h_2> \\max\\{t' \\sqrt{1 -\\rho^2},v'(1 -\\rho^2)\\} \\}\\cr\n  &\\;\\; \\cup \\{(h_1,h_2): -h_1 +\\rho h_2> \\max\\{t' \\sqrt{1 -\\rho^2},v'(1 -\\rho^2)\\},\\, h_2 <\\rho h_1 - t\\sqrt{1 -\\rho^2} \\}\\cr\n  &\\;\\; \\cup \\{(h_1,h_2):  h_1 <- \\max\\{t',v'\\},\\, h_2 \\geq \\rho h_1 - t\\sqrt{1 -\\rho^2},\\,h_2 \\leq \\rho h_1 + t\\sqrt{1 -\\rho^2} \\}\\cr\n  &\\;\\; \\cup \\{(h_1, h_2): h_1 < h_2,\\,h_1 >\\max\\{t',v'\\},\\, h_2 <\\rho h_1 + t\\sqrt{1 -\\rho^2}\\} \\cr\n  &\\;\\; \\cup \\{(h_1, h_2): h_1 < -t',\\, h_2 \\geq \\rho h_1 + t\\sqrt{1 -\\rho^2},\\, -h_1 +\\rho h_2 > v'(1 -\\rho^2) \\} \\cr\n  &\\;\\; \\cup \\{(h_1, h_2):  h_2 > t,\\, -h_1 +\\rho h_2> \\max\\{t' \\sqrt{1 -\\rho^2},v'(1 -\\rho^2)\\} \\}\n  \n\\end{aligned}$$\n\n<figure id=\"fig:rjRegion.thres.forward\">\n<figure id=\"subfig:small.v\">\n<embed src=\"Figure-supp/rejection_region_small_v2.pdf\" />\n<figcaption>When <span class=\"math inline\"><em>v</em>′ ≤ <em>t</em>′</span></figcaption>\n</figure>\n<figure id=\"subfig:middle.v\">\n<embed src=\"Figure-supp/rejection_region_middle_v.pdf\" />\n<figcaption>When <span class=\"math inline\">$t' &lt;v' \\leq  \\frac{t}{\\sqrt{1 -\\rho^2}}$</span></figcaption>\n</figure>\n<figure id=\"subfig:large.v\">\n<embed src=\"Figure-supp/rejection_region_large_v.pdf\" />\n<figcaption>When <span class=\"math inline\">$\\frac{t'}{\\sqrt{1 -\\rho^2}} &lt; v' \\leq t'\\left( 1 +\\frac{\\rho}{\\sqrt{1 -\\rho^2}} \\right)$</span></figcaption>\n</figure>\n<figure id=\"subfig:large.plus.v\">\n<embed src=\"Figure-supp/rejection_region_large_plus_v.pdf\" />\n<figcaption>When <span class=\"math inline\">$t'\\left( 1 +\\frac{\\rho}{\\sqrt{1 -\\rho^2}} \\right)\\leq v' \\leq\\frac{t' \\sqrt{1 -\\rho^2}}{1 -\\rho}$</span></figcaption>\n</figure>\n<figure id=\"subfig:large.plus.plus.v\">\n<embed src=\"Figure-supp/rejection_region_large_plus_plus_v.pdf\" />\n<figcaption>When <span class=\"math inline\">$\\frac{t' \\sqrt{1 -\\rho^2}}{1 -\\rho}\\leq v' \\leq \\frac{t'}{1 -\\rho}$</span></figcaption>\n</figure>\n<figure id=\"subfig:large.plus.plus.plus.v\">\n<embed src=\"Figure-supp/rejection_region_large_plus_plus_plus_v.pdf\" />\n<figcaption>When <span class=\"math inline\">$v' &gt; \\frac{t'}{1 -\\rho}$</span> </figcaption>\n</figure>\n<figcaption>The rejection region of thresholded forward selection has many cases <span class=\"math inline\">(<em>ρ</em>≥0)</span>.</figcaption>\n</figure>\n\nDue to the many cases of thresholded forward selection, we structure the rest of the proof in a different way: We discuss the six cases shown in Figure [18](#fig:rjRegion.thres.forward){reference-type=\"ref\" reference=\"fig:rjRegion.thres.forward\"} in the next six parts, and summarise the results for $\\rho \\geq 0$ and $\\rho < 0$ respectively, at the end of the proof. In other words, each of the six cases has its phase curves, and we take the minimum of all the curves to be the final phase curve.\n\n#### Case 1: When $v' \\leq t'$.\n\nFrom the rejection region defined in Equation [\\[suppeq:foba.rjRegion\\]](#suppeq:foba.rjRegion){reference-type=\"eqref\" reference=\"suppeq:foba.rjRegion\"}, and Figure [12](#subfig:small.v){reference-type=\"ref\" reference=\"subfig:small.v\"}, we know that thresholding does not have any effects in this case. Everything can be copied from forward selection:\n\nThe curve between Almost Full Recovery and No Recovery is $r =\\vartheta$. The curve between Almost Full Recovery and Exact Recovery is: When $\\rho \\geq  0$, $$\\label{eq:boundary.small.v.positive}\n  \\sqrt{r} =\\max \\left\\{ 1 + \\sqrt{1 -\\vartheta},\\sqrt{\\frac{2(1 -\\vartheta)}{1 -\\rho}},\\sqrt{\\frac{1 - 2\\vartheta}{1 -\\rho^2}} + \\frac{1}{\\sqrt{1 -\\rho^2}} \\right\\}.$$ When $\\rho < 0$, $$\\label{eq:boundary.small.v.negative}\n  \\sqrt{r} =\\max \\left\\{ 1 + \\sqrt{1 -\\vartheta},\\sqrt{\\frac{2(1 -\\vartheta)}{1 - |\\rho|}},\\sqrt{\\frac{1 - 2\\vartheta}{1 - \\rho^2}} + \\frac{1}{\\sqrt{1 - \\rho^2}},\\sqrt{\\frac{1 - 2\\vartheta}{2(1 - |\\rho|)}} + \\frac{1}{1 - |\\rho|} \\right\\}.$$\n\n#### Case 2: When $t' \\leq v' \\leq \\frac{t'}{\\sqrt{1 -\\rho^2}}$.\n\n::: {#suppthm:hamm.middle.v .theorem}\n**Theorem 15** (The Hamming error rate When $t' \\leq v' \\leq \\frac{t'}{\\sqrt{1 -\\rho^2}}$ ). *Suppose the conditions of Theorem [6](#thm:forward-backward){reference-type=\"ref\" reference=\"thm:forward-backward\"} holds. Let $h_1 = x_j' y /\\sqrt{2\\log(p)}$, $h_2 = x_{j + 1}' y /\\sqrt{2\\log(p)}$ and $v' = v/\\sqrt{2\\log(p)}$, $t' = t /\\sqrt{2\\log(p)}$. We require $t' \\leq v' \\leq \\frac{t'}{\\sqrt{1 -\\rho^2}}$. As $p\\to\\infty$, $$\\ensuremath{\\mathrm{FP}}_p=L_p p^{1- \\min\\bigl\\{ \\min\\{v'^2,2t'^2\\}, \\;\\; \\vartheta+ f_1(\\sqrt{r}, t',v')\\bigr\\}}, \\qquad \\ensuremath{\\mathrm{FN}}_p = L_p p^{1-\\min\\bigl\\{\\vartheta+ f_2(\\sqrt{r}, t',v'),\\;\\; 2\\vartheta+ f_3(\\sqrt{r}, t',v')\\bigr\\}},$$ where (below, $d^2_{|\\rho|}(u,v)$ is as in Definition [1](#def:EllipsDistance){reference-type=\"ref\" reference=\"def:EllipsDistance\"}), $$\\begin{aligned}\n    f_1(\\sqrt{r},t',v') & = \\begin{cases} \n      (v' - |\\rho| \\sqrt{r})^2 & \\text{ if } \\sqrt{r} \\leq \\frac{v'}{1 +|\\rho|}\\\\\n      \\frac{1}{1 -\\rho^2}d^2_{|\\rho|}((v',v'), (|\\rho| \\sqrt{r},\\sqrt{r})) & \\text{ if } \\frac{v'}{1 +|\\rho|} <\\sqrt{r} \\leq \\frac{2v'}{1+|\\rho|} \\\\\n      \\min\\left\\{ \\frac{1}{2}(1 -|\\rho|) r,\\ t'^2 \\right\\} & \\text{ if } \\sqrt{r} > \\frac{2v'}{1+|\\rho|}\n    \\end{cases}  \\cr\n   f_2(\\sqrt{r}, t',v') &= \\begin{cases} \n    \\min \\left\\{ (\\sqrt{r} - t')_ + ^2,\\  \\frac{1}{2}(1 -|\\rho|) r  \\right\\} & \\text{ if } \\sqrt{r} \\leq \\frac{2t'}{\\sqrt{1-\\rho^2}} \\\\\n    \\min \\left\\{ (\\sqrt{r} - t')_ + ^2,\\ \\frac{1}{1 -\\rho^2}d^2_{|\\rho|}(B,(\\sqrt{r},|\\rho| \\sqrt{r}))  \\right\\}& \\text{ if }\\frac{2t'}{\\sqrt{1-\\rho^2}} < \\sqrt{r} \\leq \\frac{t'\\sqrt{1-\\rho^2}}{|\\rho|(1-|\\rho|)} \\\\\n    \\left[ \\sqrt{1 -\\rho^2}\\sqrt{r} - t' \\right]^2 & \\text{ if } \\sqrt{r}>\\frac{t' \\sqrt{1-\\rho^2}}{|\\rho|(1-|\\rho|)}\n  \\end{cases}\n\\end{aligned}$$ The definition of $f_3(\\sqrt{r},t')$ depends on the sign of $\\rho$. When $\\rho > 0$, $$f_3(\\sqrt{r}, t') = \\left[ \\sqrt{1 -\\rho^2}\\sqrt{r} - t  \\right]^2$$ When $\\rho < 0$, $$\\begin{aligned}\n      f_3(\\sqrt{r},t') =&~ \\min \\left\\{  \\left[ \\sqrt{1 -\\rho^2}\\sqrt{r} - t  \\right]^2,\\ d^2_{|\\rho|}(C,((1 -|\\rho|)\\sqrt{r}, -(1 -|\\rho|)\\sqrt{r})) \\right\\} \\\\\n      =&~ \\min \\left\\{  \\left[ \\sqrt{1 -\\rho^2}\\sqrt{r} - t  \\right]^2,\\ \\frac{2}{1 -|\\rho|}\\left[ (1 -|\\rho|)\\sqrt{r} - t\n       \\right]^2 \\right\\}\n    \n\\end{aligned}$$*\n:::\n\n::: remark\n**Remark 4**. *The proof of Theorem [15](#suppthm:hamm.middle.v){reference-type=\"ref\" reference=\"suppthm:hamm.middle.v\"} is easy, but we need to emphasize one thing: For $FP_1$, whose ellipsoid is centered at $(0,0)$, it may be tangent to $h_1 = v'$ at $(v',|\\rho| v')$; or, it may intersect the rejection region at the corner $(t,t(\\rho - \\sqrt{1 -\\rho^2}))$.*\n:::\n\n::: {#suppthm:phase.middle.v .theorem}\n**Theorem 16** (The phase diagram when $t \\leq v \\leq \\frac{t}{\\sqrt{1 -\\rho^2}}$ ). *Suppose the conditions of Theorem [6](#thm:forward-backward){reference-type=\"ref\" reference=\"thm:forward-backward\"} holds. The boundary between Exact Recovery and and Almost Full Recovery is Equation [\\[suppeq:boundary.middle.v.positive\\]](#suppeq:boundary.middle.v.positive){reference-type=\"ref\" reference=\"suppeq:boundary.middle.v.positive\"} when the correlation is positive, and Equation [\\[suppeq:boundary.middle.v.negative\\]](#suppeq:boundary.middle.v.negative){reference-type=\"ref\" reference=\"suppeq:boundary.middle.v.negative\"} when the correlation is negative. When $\\rho \\geq  0$, $$\\label{suppeq:boundary.middle.v.positive}\n  \\sqrt{r} =\\max \\left\\{ 1 + \\sqrt{1 -\\vartheta},\\sqrt{\\frac{2(1 -\\vartheta)}{1 -\\rho}},\\sqrt{\\frac{1 - 2\\vartheta}{1 -\\rho^2}} + \\sqrt{\\frac{1 -\\vartheta}{1 -\\rho^2}} \\right\\}.$$ When $\\rho < 0$, $$\\begin{aligned}\n\\label{suppeq:boundary.middle.v.negative}\n  \\sqrt{r} =  \\max \\Bigg\\{&~ 1 + \\sqrt{1 -\\vartheta},\\sqrt{\\frac{2(1 -\\vartheta)}{1 -|\\rho|}},\\sqrt{\\frac{1 - 2\\vartheta}{1 -\\rho^2}} + \\sqrt{\\frac{1 -\\vartheta}{1 -\\rho^2}},\\nonumber\\\\ \n  & \\sqrt{\\frac{1 - 2\\vartheta}{2(1 -|\\rho|)}} + \\frac{\\sqrt{1 -\\vartheta}}{1 -|\\rho|},\\sqrt{\\frac{1 - 2\\vartheta}{2(1 -|\\rho|)}} + \\frac{\\sqrt{1 -\\rho^2}}{1 -|\\rho|} \\Bigg\\}.\n\\end{aligned}$$*\n:::\n\n::: proof\n*Proof of Theorem [16](#suppthm:phase.middle.v){reference-type=\"ref\" reference=\"suppthm:phase.middle.v\"}.* Like the proof of forward selection and other methods, We still discuss the $2\\times 2 = 4$ cases. For brevity, we use $f_1,f_2,f_3$ as shorthand of $f_1(\\sqrt{r},t',v'),f_2(\\sqrt{r},t',v'),f_3(\\sqrt{r},t',v')$.\n\n*First*, if $\\min(v'^2,2t'^2) =\\vartheta+ f_2 = 1$, we have $\\sqrt{r} =\\max \\left\\{ v' + \\sqrt{1 -\\vartheta}, \\sqrt{\\frac{2(1 -\\vartheta)}{1 -|\\rho|}} \\right\\}$. To ensure $\\vartheta+ f_1 \\geq 1$, we need $t' \\geq \\sqrt{1 -\\vartheta}$. For $f_2$, it need to meet the requirement $\\sqrt{r} \\leq \\frac{2t'}{\\sqrt{1 -\\rho^2}}$. This is not restrictive, because: $$\\begin{cases} \n    t' \\geq \\sqrt{1 -\\vartheta} \\\\\n    1 \\leq  v' \\leq \\frac{t'}{\\sqrt{1 -\\rho^2}} \\implies t' \\geq v' \\sqrt{1 -\\rho^2}\n    \\end{cases} \\implies 2t' \\geq \\sqrt{1 -\\vartheta} + v'\\sqrt{1 -\\rho^2}.$$ (For the conditional expression of $f_2$, $\\sqrt{r} \\leq \\frac{2t'}{\\sqrt{1 -\\rho^2}}$ is the only possibility here; to see this, we can just refer to the same part of proof for forward selection.) Finally, $2\\vartheta+ f_4 \\geq 1$ requires $\\sqrt{r} \\geq \\sqrt{\\frac{1 - 2\\vartheta}{1 -\\rho^2}} + \\frac{t'}{\\sqrt{1 -\\rho^2}}$ when $\\rho \\geq  0$, and $\\sqrt{r} \\geq \\max \\left\\{ \\sqrt{\\frac{1 - 2\\vartheta}{1 -\\rho^2}} + \\frac{t'}{\\sqrt{1 -\\rho^2}}, \\sqrt{\\frac{1 - 2\\vartheta}{2(1 -|\\rho|)}} + \\frac{t'}{1 -|\\rho|} \\right\\}$ when $\\rho \\leq 0$.\n\nIn the above discussion, the $(v',t')$ refers to any admissible $t'$ in this case, so we choose $v'_{\\min} = 1$ and $t'_{\\min} =\\min\\left\\{ \\sqrt{1 -\\vartheta},\\sqrt{1 -\\rho^2}, \\frac{\\sqrt{2}}{2}  \\right\\}$. To sum up, $\\sqrt{r} = \\max \\left\\{ v'_{\\min} + \\sqrt{1 -\\vartheta}, \\sqrt{\\frac{2(1 -\\vartheta)}{1 -|\\rho|}} \\right\\}$, and we require $\\sqrt{r} \\geq \\sqrt{\\frac{1 - 2\\vartheta}{1 -\\rho^2}} + \\frac{t'_{\\min}}{\\sqrt{1 -\\rho^2}}$ when $\\rho \\geq  0$, and $\\sqrt{r} \\geq \\max \\left\\{ \\sqrt{\\frac{1 - 2\\vartheta}{1 -\\rho^2}} + \\frac{t'_{\\min}}{\\sqrt{1 -\\rho^2}}, \\sqrt{\\frac{1 - 2\\vartheta}{2(1 -|\\rho|)}} + \\frac{t'_{\\min}}{1 -|\\rho|} \\right\\}$ when $\\rho \\leq 0$.\n\n*Second*, if $\\vartheta+ f_1 =\\vartheta+ f_2 = 1$, this case will not give us any curve. We first need $v' \\geq 1$ and $t' \\geq \\frac{\\sqrt{2}}{2}$, and $\\sqrt{r} = \\max \\left\\{ v' + \\sqrt{1 -\\vartheta},\\sqrt{\\frac{2(1 -\\vartheta)}{1 -\\rho}} \\right\\}$. For $\\vartheta+ f_1 \\geq 1$, we already have $\\sqrt{r} \\geq \\sqrt{\\frac{2(1 -\\vartheta)}{1 -\\rho}}$, so we only need $t' \\geq \\sqrt{1 -\\vartheta}$. The requirement from $2\\vartheta+ f_3 \\geq 1$ is still the same as that of the *first* case.\n\nWe notice that even if this case admits any curve, it is strictly above the curve yielded by $FP_1 = FN_1$, and it exists in a smaller interval of $\\vartheta$. As a result, we need not discuss this case any further.\n\n*Third*, if $\\min\\left\\{ v'^2,2t'^2 \\right\\} =\\vartheta+ f_2 = 1$, then we immdiately have $v' \\geq 1$, $t' \\geq \\frac{\\sqrt{2}}{2}$, and we can limit ourselves to consider $\\vartheta\\leq \\frac{1}{2}$. From $\\vartheta+ f_1 \\geq 1$ and $\\vartheta+ f_2 \\geq 1$, we have the requirement $\\sqrt{r} \\geq  \\max \\left\\{ v' + \\sqrt{1 -\\vartheta},\\sqrt{\\frac{2(1 -\\vartheta)}{1 -\\rho}} \\right\\}$ and $t' \\geq \\sqrt{1 -\\vartheta}$. Also, since $v' \\leq t'/\\sqrt{1 -\\rho^2}$, we need $t' \\geq \\sqrt{1 -\\rho^2}$.\n\nWhen $\\rho \\geq 0$, we have $\\sqrt{r} = \\sqrt{\\frac{1 - 2\\vartheta}{1 -\\rho^2}} + \\frac{t'}{\\sqrt{1 -\\rho^2}}$; when $\\rho < 0$, we have $\\sqrt{r} = \\max \\left\\{ \\sqrt{\\frac{1 - 2\\vartheta}{1 -\\rho^2}} + \\frac{t'}{\\sqrt{1 -\\rho^2}}, \\sqrt{\\frac{1 - 2\\vartheta}{2(1 -\\rho)}} + \\frac{t'}{1 -\\rho} \\right\\}$. Just like the *first* case, we can take $v'_{\\min} = 1$ and $t'_{\\min} =\\min\\left\\{ \\sqrt{1 -\\vartheta},\\sqrt{1 -\\rho^2}, \\frac{\\sqrt{2}}{2}  \\right\\}$ in the expression of $\\sqrt{r}$.\n\n*Fourth*, if $\\vartheta+ f_1 = 2\\vartheta+ f_3 = 1$, this case does not give any curve. The discussion is exactly the same as the *second* case: even if this case gives us any curve, it would be strictly above the curve in the *third* case.\n\nTo sum up, define $v'_{\\min} = 1$ and $t'_{\\min} =\\min\\left\\{ \\sqrt{1 -\\vartheta},\\sqrt{1 -\\rho^2}, \\frac{\\sqrt{2}}{2}  \\right\\}$: When $\\rho \\geq  0$: $$\\sqrt{r} =\\max \\left\\{v'_{\\min} + \\sqrt{1 -\\vartheta}, \\sqrt{\\frac{2(1 -\\vartheta)}{1 -|\\rho|}},\\sqrt{\\frac{1 - 2\\vartheta}{1 -\\rho^2}} + \\frac{t'_{\\min}}{\\sqrt{1 -\\rho^2}} \\right\\}$$ and add $\\sqrt{\\frac{1 - 2\\vartheta}{2(1 -\\rho)}} + \\frac{t'_{\\min}}{1 -\\rho}$ into the maximum when $\\rho < 0$.\n\nWe can simplify the expression of the curve above, by deleting a few curves in the maximum:\n\n-   the curve $\\sqrt{r} =\\sqrt{\\frac{1 - 2\\vartheta}{1 -\\rho^2}} + \\frac{\\sqrt{2}/2}{\\sqrt{1 -\\rho^2}}$ is always below other curves, and can be omitted. This is because when $\\vartheta\\leq \\frac{1}{2}$, $\\sqrt{1 -\\vartheta} \\geq \\frac{\\sqrt{2}}{2}$, which implies $\\sqrt{r} =\\sqrt{\\frac{1 - 2\\vartheta}{1 -\\rho^2}} + \\frac{\\sqrt{2}/2}{\\sqrt{1 -\\rho^2}} \\leq \\sqrt{\\frac{1 - 2\\vartheta}{1 -\\rho^2}} + \\frac{\\sqrt{1 -\\vartheta}}{\\sqrt{1 -\\rho^2}}$.\n\n-   for the same reason, the curve $\\sqrt{r} =\\sqrt{\\frac{1 - 2\\vartheta}{2(1 -\\rho)}} + \\frac{\\sqrt{2}/2}{1 -\\rho}$ is also always below other curves, and can be omitted.\n\n-   the curve $\\sqrt{r} =\\sqrt{\\frac{1 - 2\\vartheta}{1 -\\rho^2}} + \\frac{\\sqrt{1 -\\rho^2}}{\\sqrt{1 -\\rho^2}}$ is always below other curves, and can be omitted.\n\n    -   When $\\rho \\geq \\frac{\\sqrt{2}}{2}$, $\\sqrt{1 -\\rho^2} \\leq \\sqrt{1 -\\vartheta}$ for all $\\vartheta\\leq \\frac{1}{2}$. Thus $\\sqrt{\\frac{1 - 2\\vartheta}{1 -\\rho^2}} + \\frac{\\sqrt{1 -\\rho^2}}{\\sqrt{1 -\\rho^2}} \\leq \\sqrt{\\frac{1 - 2\\vartheta}{1 -\\rho^2}} + \\frac{\\sqrt{1 -\\vartheta}}{\\sqrt{1 -\\rho^2}}$\n\n    -   When $\\rho \\geq \\frac{\\sqrt{2}}{2}$: If $\\vartheta\\leq \\rho^2$, we still have $\\sqrt{\\frac{1 - 2\\vartheta}{1 -\\rho^2}} + \\frac{\\sqrt{1 -\\rho^2}}{\\sqrt{1 -\\rho^2}} \\leq \\sqrt{\\frac{1 - 2\\vartheta}{1 -\\rho^2}} + \\frac{\\sqrt{1 -\\vartheta}}{\\sqrt{1 -\\rho^2}}$. If $\\rho^2 < \\vartheta\\leq \\frac{1}{2}$, it can be verified that $\\sqrt{\\frac{1 - 2\\vartheta}{1 -\\rho^2}} + \\frac{\\sqrt{1 -\\rho^2}}{\\sqrt{1 -\\rho^2}} \\leq 1 + \\sqrt{1 -\\vartheta}$.\n\nNow we have arrived at the conclusion of Theorem [16](#suppthm:phase.middle.v){reference-type=\"ref\" reference=\"suppthm:phase.middle.v\"}. ◻\n:::\n\n#### Case 3: When $\\frac{t'}{\\sqrt{1 -\\rho^2}}\\leq v' \\leq t'( 1 +\\frac{|\\rho|}{\\sqrt{1 -\\rho^2}} )$.\n\n::: {#suppthm:hamm.large.v .theorem}\n**Theorem 17** (The Hamming error rate When $\\frac{t'}{\\sqrt{1 -\\rho^2}}\\leq v' \\leq t'( 1 +\\frac{|\\rho|}{\\sqrt{1 -\\rho^2}} )$ ). *Suppose the conditions of Theorem [6](#thm:forward-backward){reference-type=\"ref\" reference=\"thm:forward-backward\"} holds. Let $h_1 = x_j' y /\\sqrt{2\\log(p)}$, $h_2 = x_{j + 1}' y /\\sqrt{2\\log(p)}$ and $v' = v/\\sqrt{2\\log(p)}$, $t' = t /\\sqrt{2\\log(p)}$. As shorthand notation, define the points $A(v',v')$, $B(\\frac{t'\\sqrt{1 -\\rho^2}}{1 -|\\rho|},\\frac{t'\\sqrt{1 -\\rho^2}}{1 -|\\rho|})$, and $D(v' + \\frac{\\rho t'}{\\sqrt{1 -\\rho^2}},\\rho v' + \\frac{ t'}{\\sqrt{1 -\\rho^2}})$ as marked in Figure [14](#subfig:large.v){reference-type=\"ref\" reference=\"subfig:large.v\"}. We require $t'/\\sqrt{1 -\\rho^2}\\leq v' \\leq t'( 1 + |\\rho|/ \\sqrt{1 -\\rho^2} )$. As $p\\to\\infty$, $$\\ensuremath{\\mathrm{FP}}_p=L_p p^{1- \\min\\bigl\\{ \\min\\{v'^2,2t'^2\\}, \\;\\; \\vartheta+ f_1(\\sqrt{r}, t',v')\\bigr\\}}, \\qquad \\ensuremath{\\mathrm{FN}}_p = L_p p^{1-\\min\\bigl\\{\\vartheta+ f_2(\\sqrt{r}, t',v'),\\;\\; 2\\vartheta+ f_3(\\sqrt{r}, t',v')\\bigr\\}},$$ where (below, $d^2_{|\\rho|}(u,v)$ is as in Definition [1](#def:EllipsDistance){reference-type=\"ref\" reference=\"def:EllipsDistance\"}), $$\\begin{aligned}\n    f_1(\\sqrt{r},t',v') & = \\begin{cases} \n      (v' -|\\rho| \\sqrt{r})^2 & \\text{ if } \\sqrt{r} \\leq \\frac{v'}{1 +\\rho}\\\\\n      \\frac{1}{1 -\\rho^2}d_{|\\rho|}^2(A, (|\\rho| \\sqrt{r},\\sqrt{r})) & \\text{ if } \\frac{v'}{1 +\\rho} <\\sqrt{r} \\leq \\frac{2v'}{1+\\rho} \\\\\n      \\min\\left\\{ k(v',t'),\\ v'^2(1 -\\rho^2) \\right\\} & \\text{ if } \\sqrt{r} > \\frac{2v'}{1+\\rho}\n    \\end{cases}\n  \n\\end{aligned}$$ where $k(v',t')$ is defined like: $$k(v',t')\\stackrel{\\mathrm{def}}{=}\\begin{cases} \n      \\frac{1}{2}(1 -|\\rho|) r & \\text{ if } \\frac{2v'}{1+|\\rho|} \\leq \\sqrt{r} \\leq \\frac{2t'}{\\sqrt{1-\\rho^2}}\\\\\n      \\frac{1}{1 -\\rho^2}d^2_{|\\rho|}\\left(B,(|\\rho| \\sqrt{r},\\sqrt{r})\\right) & \\text{ if }  \\frac{2t'}{\\sqrt{1-\\rho^2}} \\leq \\sqrt{r} \\leq \\frac{t \\sqrt{1-\\rho^2}}{|\\rho|(1-|\\rho|)} \\\\\n      \\left[ \\sqrt{1 -\\rho^2}\\sqrt{r} - t' \\right]^2& \\text{ if } \\sqrt{r} \\geq \\frac{t \\sqrt{1-\\rho^2}}{|\\rho|(1-|\\rho|)}\n    \\end{cases}$$ $$\\begin{aligned}\n   f_2(\\sqrt{r}, t',v') &= \\begin{cases} \n    \\min \\left\\{(\\sqrt{r} - v')_ + ^2,\\, \\frac{1}{2}(1 -|\\rho|) r,\\, t'^2  \\right\\} \\qquad\\qquad\\text{ if } \\sqrt{r} \\leq v' +|\\rho| \\frac{t'}{\\sqrt{1-\\rho^2}} \\\\\n    \\min \\{ (\\sqrt{r} - v')_ + ^2,\\,\\frac{1}{2}(1 -|\\rho|) r,\\,\\frac{1}{1 -\\rho^2}d^2_{|\\rho|}(D,(\\sqrt{r},|\\rho| \\sqrt{r})) \\}\\\\\n \\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad\\qquad \\text{ if } v' +|\\rho| \\frac{t'}{\\sqrt{1-\\rho^2}} \\leq \\sqrt{r} \\leq \\min\\{\\sqrt{r_2(v',t')},v' + \\frac{t'}{|\\rho|\\sqrt{1-\\rho^2}}\\} \\\\\n    \\min \\left\\{ (\\sqrt{r} - v')_ + ^2,\\ \\frac{1}{1 -\\rho^2} d^2_{|\\rho|}(D,(\\sqrt{r},|\\rho| \\sqrt{r}))\\right\\}   \\text{ if } \\min\\{\\sqrt{r_2},v' + \\frac{t'}{|\\rho|\\sqrt{1-\\rho^2}}\\} \\leq \\sqrt{r} \\leq v' + \\frac{t'}{|\\rho|\\sqrt{1-\\rho^2}} \\\\\n    (1 -\\rho^2)\\left[ \\sqrt{r} - v' \\right]^2  \\qquad\\qquad\\qquad\\qquad\\qquad\\quad  \\text{if }  \\sqrt{r} \\geq  v' + \\frac{t'}{|\\rho|\\sqrt{1-\\rho^2}}\n  \\end{cases} \n\\end{aligned}$$ where $r_2 =r_2(v',t')$ is the larger root of the quadratic equation $$\\begin{aligned}\n      \\frac{1}{1 - \\rho^2}d^2_{|\\rho|}(D,(\\sqrt{r},|\\rho| \\sqrt{r})) =&~ \\frac{1}{2}(1 -|\\rho|) r\\cr \n      \\Leftrightarrow \\frac{1 +|\\rho|}{2}r - 2 \\left( v' +\\frac{|\\rho| t'}{\\sqrt{1 -\\rho^2}}\\right)  \\sqrt{r} +&~ \\left( v'^2 + \\frac{t'^2}{1 -\\rho^2} +\\frac{2|\\rho| v' t'}{\\sqrt{1 -\\rho^2}} \\right) = 0 \\label{eq:r_2}\n    \n\\end{aligned}$$ and the explicitely form of $r_2(v',t')$ is $$\\begin{aligned}\n      \\sqrt{r_2(v',t')} =&~ \\frac{1}{1 +|\\rho|} \\left[ 2 \\left( v' +\\frac{|\\rho| t'}{\\sqrt{1 -\\rho^2}}\\right) + \\sqrt{2(1 -|\\rho|) \\left( v' - \\frac{t'}{\\sqrt{1 -\\rho^2}}\\right) \\left( v' + (1 + 2|\\rho|) \\frac{t'}{\\sqrt{1 -\\rho^2}} \\right)} \\right].\n    \n\\end{aligned}$$*\n\n*The definition of $f_3(\\sqrt{r},t')$ depends on the sign of $\\rho$. When $\\rho > 0$, $$f_3(\\sqrt{r}, t') = \\left[ \\sqrt{1 -\\rho^2}\\sqrt{r} - t  \\right]^2$$ When $\\rho < 0$, $$\\begin{aligned}\n      f_3(\\sqrt{r},t') =&~ \\min \\left\\{  \\left[ \\sqrt{1 -\\rho^2}\\sqrt{r} - t  \\right]^2,\\ d^2_{|\\rho|}(C,((1 -|\\rho|)\\sqrt{r}, -(1 -|\\rho|)\\sqrt{r})) \\right\\} \\\\\n      =&~ \\min \\left\\{  \\left[ \\sqrt{1 -\\rho^2}\\sqrt{r} - t  \\right]^2,\\ \\frac{2}{1 -|\\rho|}\\left[ (1 -|\\rho|)\\sqrt{r} - t\n       \\right]^2 \\right\\}\n    \n\\end{aligned}$$*\n:::\n\n::: proof\n*Proof of Theorem [17](#suppthm:hamm.large.v){reference-type=\"ref\" reference=\"suppthm:hamm.large.v\"}.* We explain one detail, about why $\\sqrt{r_2(v',t')}$ is introduced in $f_2(\\sqrt{r},t',v')$, which corresponds to the ellipsoid centered at $\\mu_{10} =(\\sqrt{r},|\\rho| \\sqrt{r})$. Recall that the point $D$ as noted in in Figure [14](#subfig:large.v){reference-type=\"ref\" reference=\"subfig:large.v\"} has cooredinate $x_D =v' + \\frac{|\\rho| t'}{\\sqrt{1 -\\rho^2}},y_D =|\\rho| v' + \\frac{ t'}{\\sqrt{1 -\\rho^2}}$. Suppose $\\rho \\geq  0$, as the case of $\\rho < 0$ can be obtained with symmetry.\n\nWhen $\\sqrt{r} \\leq x_D = v' + \\frac{\\rho t'}{\\sqrt{1 -\\rho^2}}$, the ellipsoid can be tangent to any one among the three line segments: (i) $h_1 = v'$, (ii) $h_2 = h_1$ or (iii) $h_2 = \\rho h_1 + t' \\sqrt{1 -\\rho^2}$.\n\nWhen $\\rho \\sqrt{r} \\geq y_D =\\rho v' + \\frac{ t'}{\\sqrt{1 -\\rho^2}}$, the ellipsoid can either be tangent to the red line $h_1 =\\rho h_2 + v'(1 -\\rho^2)$, or the blue line $h_1 = v'$ in Figure [14](#subfig:large.v){reference-type=\"ref\" reference=\"subfig:large.v\"}.\n\nWhen $v' + \\frac{\\rho t'}{\\sqrt{1 -\\rho^2}} \\leq  \\sqrt{r} \\leq  v' + \\frac{ t'}{\\rho\\sqrt{1 -\\rho^2}}$, thing are more tricky:\n\n-   The ellipsoid can possibly be tangent to $h_1 = v'$, or it may intersect point $D$ and rotate around it.\n\n-   However, it is uncertain whether we should include the segment $h_2 = h_1$ into the form of the Hamming error. This is because when $\\sqrt{r}$ is large, the ellipsoid is at the upper right side of point $D$, where $h_2 = h_1$ does not form the boundary of the rejection region. If we still include it, the final phase diagram will be worse than it actually is.\n\n-   To exclude $h_2 = h_1$ when it is unwanted, we require $\\sqrt{r} > \\sqrt{r_2(v',t')}$.\n\n-   The place of $\\sqrt{r_2(v',t')}$ is exchangable to the symmetric axis of the quadratic equation [\\[eq:r_2\\]](#eq:r_2){reference-type=\"eqref\" reference=\"eq:r_2\"}, which is $\\frac{2}{1 +\\rho}v' +\\frac{2\\rho t'}{(1 +\\rho)\\sqrt{1 -\\rho^2}}$.\n\n$\\sqrt{r_2(v',t')}$ can be greater than $v' +\\frac{t'}{\\rho \\sqrt{1 -\\rho^2}}$, so it is taken minimum with $v' +\\frac{t'}{\\rho \\sqrt{1 -\\rho^2}}$ in the definition of $f_2$. ◻\n:::\n\nBefore moving on to the phase diagram, we first introduce two terms to simplify notation:\n\n::: {#def:min.min .definition}\n**Definition 3**. *Define $v'_{\\min}$ and $t'_{\\min}$, both as functions of $\\vartheta$ and $\\rho$. $v'_{\\min} =\\max \\left\\{ 1,\\sqrt{\\frac{1 -\\vartheta}{1 -\\rho^2}},\\frac{\\sqrt{2}/2}{\\sqrt{1 -\\rho^2}} \\right\\}$ and $t'_{\\min} = \\max \\left\\{ \\frac{\\sqrt{2}}{2}, \\frac{\\max\\{1,\\sqrt{\\frac{1 -\\vartheta}{1 -\\rho^2}}\\}}{1 +|\\rho|/\\sqrt{1 -\\rho^2}}\\right\\}$.*\n:::\n\nFigure [19](#suppfig:def.minmin){reference-type=\"ref\" reference=\"suppfig:def.minmin\"} gives an explanation of how $v'_{\\min}$ and $t'_{\\min}$ are defined.\n\n![the definition of $v'_{\\min}$ and $t'_{\\min}$](Figure-supp/minmin.JPG){#suppfig:def.minmin width=\"60%\"}\n\n::: {#suppthm:phase.large.v .theorem}\n**Theorem 18** (The phase diagram when $\\frac{t'}{\\sqrt{1 -\\rho^2}}\\leq v' \\leq t'( 1 +\\frac{|\\rho|}{\\sqrt{1 -\\rho^2}} )$ ). *Suppose the conditions of Theorem [6](#thm:forward-backward){reference-type=\"ref\" reference=\"thm:forward-backward\"} holds. The boundary between Exact Recovery and and Almost Full Recovery is Equation [\\[suppeq:boundary.middle.v.positive\\]](#suppeq:boundary.middle.v.positive){reference-type=\"ref\" reference=\"suppeq:boundary.middle.v.positive\"} when the correlation is positive, and Equation [\\[suppeq:boundary.middle.v.negative\\]](#suppeq:boundary.middle.v.negative){reference-type=\"ref\" reference=\"suppeq:boundary.middle.v.negative\"} when the correlation is negative. When $\\rho \\geq  0$, $$\\begin{aligned}\n\\label{eq:boundary.large.v.positive}\n    \\sqrt{r} = \\max \\left\\{ v'_{\\min} + \\sqrt{1 -\\vartheta},\\, \\sqrt{\\frac{2(1 -\\vartheta)}{1 -|\\rho|}},\\, \\sqrt{\\frac{1 - 2\\vartheta}{1 -\\rho^2}} + v'_{\\min} \\right\\}\n  \n\\end{aligned}$$ When the actual correlation is negative, $$\\begin{aligned}\n\\label{eq:boundary.large.v.negative}\n      \\sqrt{r} =  \\max \\Bigg\\{&~ v'_{\\min} + \\sqrt{1 -\\vartheta},\\, \\sqrt{\\frac{2(1 -\\vartheta)}{1 -|\\rho|}},\\, \\sqrt{\\frac{1 - 2\\vartheta}{1 -\\rho^2}} + v'_{\\min},\\sqrt{\\frac{1 - 2\\vartheta}{2(1 -|\\rho|)}} + \\frac{t'_{\\min}}{1 -|\\rho|} \\Bigg\\}.\n    \n\\end{aligned}$$*\n:::\n\n::: proof\n*Proof of Theorem [18](#suppthm:phase.large.v){reference-type=\"ref\" reference=\"suppthm:phase.large.v\"}.* We start by considering the case of $\\rho \\geq 0$. The other case of $\\rho < 0$ can be prove in a very similar way by adding one more curve. In the discussion below, we do not work with $|\\rho|$, but $\\rho \\geq 0$ itself.\n\nIn this particular proof, we do not limit ourselves to discuss the four cases as usual. Instead, we just think about the conditions for $\\min\\{v'^2,2t'^2\\},\\vartheta+ f_1,\\vartheta+ f_2,2\\vartheta+ f_3 \\geq 1$, and take the smallest $\\sqrt{r}$ possible. Also, remember the important fact that at least two of these requirements should be tight.\n\nFor fixed $(v',t')$, the necessary and sufficient condition for $\\min\\{v'^2,2t'^2\\} \\geq 1$ is $v' \\geq  1$ and $t' \\geq \\frac{\\sqrt{2}}{2}$. The necessary and sufficient condition for $2\\vartheta+ f_3 \\geq 0$ is $\\sqrt{r} \\geq \\sqrt{\\frac{1 - 2\\vartheta}{1 -\\rho^2}} + v'$. (If $\\rho < 0$, the condition should be $\\sqrt{r} \\geq \\max\\left\\{\\sqrt{\\frac{1 - 2\\vartheta}{1 -\\rho^2}} + v', \\sqrt{\\frac{1 -\\vartheta}{1 -\\rho^2}} \\right\\}$.) From $\\vartheta+ f_2 \\geq 1$, we know a necessary condition, $\\sqrt{r} \\geq v' + \\sqrt{1 -\\vartheta}$. From the discussion above, we already know $$\\begin{aligned}\n  & \\sqrt{r} \\geq  \\max \\left\\{ v' + \\sqrt{1 -\\vartheta},\\, \\sqrt{\\frac{1 - 2\\vartheta}{1 -\\rho^2}} + v' \\right\\} \n\\end{aligned}$$\n\nIn terms of admissible $(v',t')$, we already have $v' \\geq 1$ and $t' \\geq \\frac{\\sqrt{2}}{2}$. Additionally, note that $f_1(\\sqrt{r},t',v')$ as a function of $\\sqrt{r}$ takes its maximum when $f_1 =(1 -\\rho^2)v'^2$, so $\\vartheta+f_1 \\geq 1$ implies $v' \\geq \\sqrt{\\frac{1 -\\vartheta}{1 -\\rho^2}}$. Under the conditions $v' \\geq \\max \\{1,\\sqrt{\\frac{1 -\\vartheta}{1 -\\rho^2}}\\}$, $t' \\geq \\frac{\\sqrt{2}}{2}$, and $\\frac{t'}{\\sqrt{1 -\\rho^2}}\\leq v' \\leq t'( 1 +\\frac{|\\rho|}{\\sqrt{1 -\\rho^2}} )$, the smallest admissible $(v',t')$ are precisely defined by Definition [3](#def:min.min){reference-type=\"ref\" reference=\"def:min.min\"} as $(v'_{\\min},t'_{\\min})$.\n\nWe consider the two cases: $\\rho \\leq 0.576$ and $\\rho \\geq 0.576$. The point 0.576 is important, because when $\\rho \\leq 0.576$, since $v'_{\\min} \\geq \\max \\left\\{ 1,\\sqrt{\\frac{1 -\\vartheta}{1 -\\rho^2}} \\right\\}$, we can prove $$\\sqrt{r} \\geq \\max \\left\\{ v'_{\\min} + \\sqrt{1 -\\vartheta},\\,  \\sqrt{\\frac{1 - 2\\vartheta}{1 -\\rho^2}} + v'_{\\min} \\right\\} \\geq \\sqrt{\\frac{2(1 -\\vartheta)}{1 -\\rho}}.$$\n\nSince now $\\sqrt{r} \\geq \\sqrt{\\frac{2(1 -\\vartheta)}{1 -\\rho}}$ is already implied by other necessary conditions, we only need to check the sufficiency of $\\sqrt{r} = \\max \\left\\{  v'_{\\min} + \\sqrt{1 -\\vartheta},\\,  \\sqrt{\\frac{1 - 2\\vartheta}{1 -\\rho^2}} + v'_{\\min}\\right\\}$ to prove Theorem [18](#suppthm:phase.large.v){reference-type=\"ref\" reference=\"suppthm:phase.large.v\"} for $0 \\leq \\rho < 0.576$.\n\nWhen $0 \\leq \\rho \\leq 0.576$, we already have $\\min\\{v'^2,2t^2\\} \\geq 1$, $2\\vartheta+ f_3 \\geq 1$ and $\\sqrt{r} \\geq \\sqrt{\\frac{2(1 -\\vartheta)}{1 -\\rho}}$. we need to check $\\vartheta+ f_1 \\geq 1$ and $\\vartheta+ f_2 \\geq 1$.\n\nFor $\\vartheta+ f_1 \\geq 1$, because $\\frac{1}{1 -\\rho^2} d^2_{|\\rho|}(A, (|\\rho| \\sqrt{r},\\sqrt{r})) \\geq \\frac{1}{2}(1 -\\rho)r$ and $\\frac{1}{1 -\\rho^2} d^2_{|\\rho|}(B, (|\\rho| \\sqrt{r},\\sqrt{r})) \\geq \\frac{1}{2}(1 -\\rho)r$, we only need to check $\\sqrt{r} \\geq \\sqrt{\\frac{1 -\\vartheta}{1 -\\rho^2}} + \\frac{t'}{\\sqrt{1 -\\rho^2}}$ when $\\sqrt{r} \\geq \\frac{t \\sqrt{1 -\\rho^2}}{\\rho(1 -\\rho)}$. Since $t' \\geq \\frac{v'}{1 +\\frac{\\rho}{\\sqrt{1 -\\rho^2}}} \\geq \\frac{\\sqrt{1 -\\vartheta}}{\\rho + \\sqrt{1 - \\rho^2}} \\geq \\rho \\sqrt{1 -\\vartheta}$ for all $\\rho \\leq \\frac{\\sqrt{2}}{2}$, we have $t' \\geq  \\rho \\sqrt{1 -\\vartheta}$, and now actually $\\frac{t \\sqrt{1 -\\rho^2}}{\\rho(1 -\\rho)} \\geq \\sqrt{\\frac{1 -\\vartheta}{1 -\\rho^2}} + \\frac{t'}{\\sqrt{1 -\\rho^2}}$, so this case is not restrictive at all.\n\nFor $\\vartheta+ f_2 \\geq 1$, when $\\sqrt{r} \\leq \\sqrt{r_2(v',t')}$, it is sufficient to have $\\sqrt{r} \\geq \\sqrt{\\frac{2(1 -\\vartheta)}{1 -\\rho}}$. We only need to check the rest two cases in which $\\sqrt{r}$ is large:\n\n-   When $\\sqrt{r_2(v',t')} \\leq \\sqrt{r} \\leq v' + \\frac{t'}{\\rho\\sqrt{1-\\rho^2}}$, we need $d^2(D,(\\sqrt{r},\\rho \\sqrt{r})) \\geq (1 -\\rho^2)(1 -\\vartheta)$. This trivially holds, because $\\sqrt{r} \\geq \\sqrt{r_2(v',t')}$. From Equation [\\[eq:r_2\\]](#eq:r_2){reference-type=\"eqref\" reference=\"eq:r_2\"}: $$r - 2 \\left( v' +\\frac{\\rho t'}{\\sqrt{1 -\\rho^2}}\\right)  \\sqrt{r} + \\left( v'^2 + \\frac{t'^2}{1 -\\rho^2} +\\frac{2\\rho v' t'}{\\sqrt{1 -\\rho^2}} \\right) \\geq \\frac{1 -\\rho}{2}r \\geq 1 -\\vartheta$$ whose last inequality is because $\\sqrt{r} \\geq \\sqrt{\\frac{2(1 -\\vartheta)}{1 -\\rho}}$.\n\n-   When $\\sqrt{r} \\geq v' + \\frac{t'}{\\rho\\sqrt{1-\\rho^2}}$, we need $\\sqrt{r} \\geq \\sqrt{\\frac{1 -\\vartheta}{1 -\\rho^2}} + v'$. Since $t' \\geq \\rho\\sqrt{1 -\\vartheta}$, it trivially holds.\n\nWe have proved Theorem [18](#suppthm:phase.large.v){reference-type=\"ref\" reference=\"suppthm:phase.large.v\"} for $\\rho \\leq 0.576$. Now we move on to the next case of $\\rho > 0.576$.\n\nFor $\\rho > 0. 576$, our task is to prove it is necessary and sufficient to have $\\sqrt{r} \\geq \\sqrt{\\frac{2(1 -\\vartheta)}{1 -\\rho}}$. Reviewing Theorem [18](#suppthm:phase.large.v){reference-type=\"ref\" reference=\"suppthm:phase.large.v\"}, as long as $\\sqrt{r} \\leq \\sqrt{r_2(v',t')}$ and $\\sqrt{r} \\leq \\frac{2t'}{\\sqrt{1-\\rho}^2}$ for fixed admissible $(v',t')$, then $\\sqrt{r} \\geq \\sqrt{\\frac{2(1 -\\vartheta)}{1 -\\rho}}$ is also sufficient for $\\sqrt{r}$. In other words, we could simply set $$\\sqrt{r} =\\max \\left\\{  v'_{\\min} + \\sqrt{1 -\\vartheta},\\,\\sqrt{\\frac{2(1 -\\vartheta)}{1 -\\rho}},\\, \\sqrt{\\frac{1 - 2\\vartheta}{1 -\\rho^2}} + v'_{\\min} \\right\\}.$$\n\nWe are left to eliminate other cases, i.e. $\\sqrt{r} \\leq \\sqrt{r_2(v',t')}$ or $\\sqrt{r} \\leq \\frac{2t'}{\\sqrt{1-\\rho}^2}$ for fixed admissible $(v',t')$. They can either be impossible, or only produce a curve greater than the one in Equation [\\[eq:boundary.large.v.positive\\]](#eq:boundary.large.v.positive){reference-type=\"ref\" reference=\"eq:boundary.large.v.positive\"}. The rest of the proof focuses on the elimination of other cases.\n\nTo prepare for such work, we take a closer look at the definition of $f_2$. We point out that when $v' \\geq \\frac{\\sqrt{2(1 +\\rho)} - 1}{\\rho} \\cdot \\frac{t}{\\sqrt{1 -\\rho^2}}$, the case of $$\\begin{cases} \n  (\\sqrt{r} - v')_ + ^2 \\\\\n  \\frac{1}{1 -\\rho^2}d^2_{|\\rho|} (D, (\\sqrt{r},\\rho \\sqrt{r}))\n  \\end{cases} \\text{ if } \\min\\{\\sqrt{r_2},v' + \\frac{t'}{|\\rho|\\sqrt{1-\\rho^2}}\\} \\leq \\sqrt{r} \\leq v' + \\frac{t'}{|\\rho|\\sqrt{1-\\rho^2}}$$ does not exist, and the degenerated $f_2$ is just $$f_2(\\sqrt{r},t',v') =\n  \\begin{cases} \n    \\min \\{(\\sqrt{r} - v')_ + ^2,\\frac{1}{2}(1 -\\rho) r,t'^2\\}\n &\\text{ if } \\sqrt{r} \\leq v' +\\rho \\frac{t'}{\\sqrt{1-\\rho^2}} \\\\\n    \\min\\{(\\sqrt{r} - v')_ + ^2,\\frac{1}{2}(1 -\\rho) r,\\frac{1}{1 -\\rho^2}d^2_{|\\rho|}(D,(\\sqrt{r},\\rho \\sqrt{r}))\\}\n & \\text{ if } v' +\\rho \\frac{t'}{\\sqrt{1-\\rho^2}} \\leq \\sqrt{r} \\leq v' + \\frac{t'}{\\rho\\sqrt{1-\\rho^2}} \\\\\n    (1 -\\rho^2)\\left[ \\sqrt{r} - v' \\right]^2 & \\text{ if }  \\sqrt{r} \\geq  v' + \\frac{t'}{\\rho\\sqrt{1-\\rho^2}}.\n  \\end{cases}$$ When $v' < \\frac{\\sqrt{2(1 +\\rho)} - 1}{\\rho} \\cdot \\frac{t}{\\sqrt{1 -\\rho^2}}$, the case of $$\\begin{cases} \n  (1 -\\rho^2)(\\sqrt{r} - v')_ + ^2 \\\\\n  d^2 (D, (\\sqrt{r},\\rho \\sqrt{r}))\n  \\end{cases} \\text{ if } \\sqrt{r_2(v',t')} \\leq \\sqrt{r} \\leq v' + \\frac{t'}{\\rho\\sqrt{1-\\rho^2}}$$ in $f_2$ does exist.\n\nNow we are ready to eliminate the unwanted cases.\n\nWhen the case of $\\frac{2t'}{\\sqrt{1-\\rho^2}} \\leq \\sqrt{r} \\leq \\frac{t \\sqrt{1-\\rho^2}}{\\rho(1-\\rho)}$ in $\\vartheta+f_1 \\geq 1$ is tight and active, we have $\\sqrt{r} =\\frac{(1 + \\rho)t'}{\\sqrt{1 -\\rho^2}} - \\sqrt{1 -\\vartheta- t'^2}.$ There is one important fact: $v' +\\frac{t'}{\\rho \\sqrt{1 -\\rho^2}} \\geq \\frac{2}{1 +\\rho} v' + \\frac{2\\rho t'}{(1 +\\rho) \\sqrt{1-\\rho^2}} \\geq \\frac{2t'}{\\sqrt{1-\\rho^2}}$. The middle term is the symmetric axis of Equation [\\[eq:r_2\\]](#eq:r_2){reference-type=\"eqref\" reference=\"eq:r_2\"}, and can be used exchangably with $\\sqrt{r_2(v',t')}$ as we have noted in the proof of Theorem [17](#suppthm:hamm.large.v){reference-type=\"ref\" reference=\"suppthm:hamm.large.v\"}.\n\n-   If $\\sqrt{r} \\leq v' +\\frac{t'}{\\rho \\sqrt{1 -\\rho^2}}$, we need $d^2_{|\\rho|}(D, (\\sqrt{r},\\rho \\sqrt{r})) \\geq (1 -\\rho^2)(1 -\\vartheta)$, i.e. $\\sqrt{r} \\geq v' + \\frac{\\rho t'}{\\sqrt{1 -\\rho^2}} + \\sqrt{1 -\\vartheta- t'^2}$, which gives a contradiction.\n\n-   If $\\sqrt{r} > v' +\\frac{t'}{\\rho \\sqrt{1 -\\rho^2}}$, because $d^2_{|\\rho|}(D, (\\sqrt{r},\\rho \\sqrt{r})) \\geq (1 -\\rho^2)^2\\left[ \\sqrt{r} - v' \\right]^2$, it is the same contradiction.\n\nWhen the case of $\\sqrt{r} > \\frac{t \\sqrt{1-\\rho^2}}{\\rho(1-\\rho)}$ in $\\vartheta+f_1 \\geq 1$ is tight and active, we have $\\sqrt{r} = \\sqrt{\\frac{1-\\vartheta}{1 -\\rho^2}} + \\frac{t'}{\\sqrt{1 -\\rho^2}}$. Then we discuss the conditional expression of $f_2$ in $\\vartheta+ f_2 \\geq 1$; given the intractability of $\\sqrt{r_2(\\vartheta,\\rho)}$, we work with the alternative $\\frac{2}{1 +\\rho} v' + \\frac{2\\rho t'}{(1 +\\rho) \\sqrt{1-\\rho^2}}$ instead.\n\n-   When $\\sqrt{r} \\geq v' +\\frac{t'}{\\rho \\sqrt{1 -\\rho^2}}$ in $f_2$, we need $\\sqrt{r} \\geq \\sqrt{\\frac{1-\\vartheta}{1 -\\rho^2}} + v'$ which gives a contradiction.\n\n-   When $\\frac{2}{1 +\\rho} v' + \\frac{2\\rho t'}{(1 +\\rho) \\sqrt{1-\\rho^2}} \\leq \\sqrt{r} < v' +\\frac{t'}{\\rho \\sqrt{1 -\\rho^2}}$ in $f_2$, we actually cannot have $\\sqrt{\\frac{1-\\vartheta}{1 -\\rho^2}} + \\frac{t'}{\\sqrt{1 -\\rho^2}} \\geq \\frac{2}{1 +\\rho} v' + \\frac{2\\rho t'}{(1 +\\rho) \\sqrt{1-\\rho^2}}$, because it means $\\left( \\sqrt{\\frac{1-\\vartheta}{1 -\\rho^2}} - v' \\right) +\\frac{1 -\\rho}{1 +\\rho}\\frac{t'}{\\sqrt{1 -\\rho^2}} \\geq \\frac{1 -\\rho}{1 +\\rho}v'$.\n\nWhen the case of $\\sqrt{r} \\geq  v' + \\frac{t'}{\\rho\\sqrt{1-\\rho^2}}$ in $FN_1$ is tight and active, we have $\\sqrt{r} = \\sqrt{\\frac{1-\\vartheta}{1 -\\rho^2}} + v'_{\\min}$. This case does not have any problem or contradiction itself. However, with $v' \\geq \\max \\{1,\\sqrt{\\frac{1 -\\vartheta}{1 -\\rho^2}} \\}$, it is too large, much larger than $\\sqrt{r} = \\max \\left\\{ v'_{\\min} + \\sqrt{1 -\\vartheta},\\, \\sqrt{\\frac{2(1 -\\vartheta)}{1 -\\rho}},\\, \\sqrt{\\frac{1 - 2\\vartheta}{1 -\\rho^2}} + v'_{\\min} \\right\\}$ whose sufficiency has been proven.\n\nWhen $v \\geq  \\frac{\\sqrt{2(1 +\\rho)} - 1}{\\rho} \\cdot \\frac{t}{\\sqrt{1 -\\rho^2}}$, there is no more cases in $f_2$, and our discussion is finished. When $v < \\frac{\\sqrt{2(1 +\\rho)} - 1}{\\rho} \\cdot \\frac{t}{\\sqrt{1 -\\rho^2}}$, we need to look at the last case of $\\sqrt{r_2(v',t')} \\leq \\sqrt{r} \\leq v' + \\frac{t'}{\\rho\\sqrt{1-\\rho^2}}$ in $FN_1$. If this case is tight and active, we have $\\sqrt{r} = v' + \\frac{\\rho t'}{\\sqrt{1 -\\rho^2}} + \\sqrt{1 -\\vartheta- t'^2}$.\n\n-   It can be verified that $\\sqrt{r_2(v',t')} \\geq \\frac{t \\sqrt{1-\\rho^2}}{\\rho(1-\\rho)}$, which is equivalent to $$-\\frac{1 +\\rho}{2}v'^2 + \\frac{2v't'}{\\sqrt{1 -\\rho^2}} \\left( 1 +\\frac{1 -\\rho^3}{2\\rho} \\right) \\geq \\frac{t'^2}{1 -\\rho^2} \\left[ \\frac{3}{2} + \\frac{1}{\\rho} -\\frac{\\rho}{2} -\\rho^2 - \\frac{(1 -\\rho^2)^2}{4\\rho^2} \\right]$$ $(RHS-LHS)$ is decreasing in $\\frac{t'}{v'}$, and the inequality holds as $\\frac{t'}{v'} =\\frac{\\rho \\sqrt{1 -\\rho^2}}{\\sqrt{2(1 +\\rho)} - 1}$.\n\n-   As a result, we need to verify $v' + \\frac{\\rho t'}{\\sqrt{1 -\\rho^2}} + \\sqrt{1 -\\vartheta- t'^2} \\geq \\sqrt{\\frac{1 -\\vartheta}{1 -\\rho^2}} + \\frac{t'}{\\sqrt{1 -\\rho^2}}$ implied by $\\vartheta+ f_1 \\geq 1$. This would give us a contradiction, because actually $v' + \\frac{\\rho t'}{\\sqrt{1 -\\rho^2}} + 1.02 \\cdot \\sqrt{1 -\\vartheta- t'^2} \\leq  \\sqrt{\\frac{1 -\\vartheta}{1 -\\rho^2}} + \\frac{t'}{\\sqrt{1 -\\rho^2}}$. In fact, We only need to prove $\\left[ \\frac{\\sqrt{2(1+\\rho)} - 1}{\\rho} +\\rho - 1 \\right]\\frac{t'}{\\sqrt{1 -\\rho^2}} + 1.02 \\cdot \\sqrt{1 -\\vartheta- t'^2} \\leq \\sqrt{\\frac{1 -\\vartheta}{1 -\\rho^2}}$. The coefficient $1.02$ as to make the LHS decreasing in $t'$ for $\\rho \\geq 0.576$. Taking $t =\\frac{\\rho \\sqrt{1 -\\vartheta}}{\\sqrt{2(1 +\\rho)} - 1}$ proves the inequality.\n\nSo far our discussion is finally finished, and we have proven the phase curve to be $$\\sqrt{r} =\\max \\left\\{  v'_{\\min} + \\sqrt{1 -\\vartheta},\\,\\sqrt{\\frac{2(1 -\\vartheta)}{1 -\\rho}},\\, \\sqrt{\\frac{1 - 2\\vartheta}{1 -\\rho^2}} + v'_{\\min} \\right\\}.$$ where $v'_{\\min}$ is defined in Defition [3](#def:min.min){reference-type=\"ref\" reference=\"def:min.min\"}.\n\nReviewing the proof for $\\rho \\geq 0$, we notice that $2\\vartheta+ f_3(\\sqrt{r},t'v') \\geq 1$ is only used at the very start of the proof, and does not change the bulk of the discussion. It can be proved with vitually the same proof, that when $\\rho < 0$, the phase curve is $$\\begin{aligned}\n  \\sqrt{r} =  \\max \\Bigg\\{&~ v'_{\\min} + \\sqrt{1 -\\vartheta},\\, \\sqrt{\\frac{2(1 -\\vartheta)}{1 -|\\rho|}},\\, \\sqrt{\\frac{1 - 2\\vartheta}{1 -\\rho^2}} + v'_{\\min},\\sqrt{\\frac{1 - 2\\vartheta}{2(1 -|\\rho|)}} + \\frac{t'_{\\min}}{1 -|\\rho|} \\Bigg\\}.\n\\end{aligned}$$ ◻\n:::\n\n*Summarising the first three cases:* When $\\rho \\geq 0$, among the first three cases, we can take the minimum over Equation [\\[eq:boundary.small.v.positive\\]](#eq:boundary.small.v.positive){reference-type=\"eqref\" reference=\"eq:boundary.small.v.positive\"},[\\[suppeq:boundary.middle.v.positive\\]](#suppeq:boundary.middle.v.positive){reference-type=\"eqref\" reference=\"suppeq:boundary.middle.v.positive\"},[\\[eq:boundary.large.v.positive\\]](#eq:boundary.large.v.positive){reference-type=\"eqref\" reference=\"eq:boundary.large.v.positive\"}. In fact, the minimum is just Equation [\\[suppeq:boundary.middle.v.positive\\]](#suppeq:boundary.middle.v.positive){reference-type=\"eqref\" reference=\"suppeq:boundary.middle.v.positive\"}, which is $$\\label{suppeq:boundary.firstthree.positive}\n  \\sqrt{r} =\\max \\left\\{ 1 + \\sqrt{1 -\\vartheta},\\sqrt{\\frac{2(1 -\\vartheta)}{1 -\\rho}},\\sqrt{\\frac{1 - 2\\vartheta}{1 -\\rho^2}} + \\sqrt{\\frac{1 -\\vartheta}{1 -\\rho^2}} \\right\\}.$$ When $\\rho < 0$, we also take the minimum over Equation [\\[eq:boundary.small.v.negative\\]](#eq:boundary.small.v.negative){reference-type=\"eqref\" reference=\"eq:boundary.small.v.negative\"},[\\[suppeq:boundary.middle.v.negative\\]](#suppeq:boundary.middle.v.negative){reference-type=\"eqref\" reference=\"suppeq:boundary.middle.v.negative\"},[\\[eq:boundary.large.v.negative\\]](#eq:boundary.large.v.negative){reference-type=\"eqref\" reference=\"eq:boundary.large.v.negative\"}. In fact, in the region $\\vartheta\\in(\\frac{1}{2},1)$, Equation [\\[suppeq:boundary.middle.v.negative\\]](#suppeq:boundary.middle.v.negative){reference-type=\"eqref\" reference=\"suppeq:boundary.middle.v.negative\"} is the minimum, but when $\\vartheta\\leq \\frac{1}{2}$, Equation [\\[eq:boundary.large.v.negative\\]](#eq:boundary.large.v.negative){reference-type=\"eqref\" reference=\"eq:boundary.large.v.negative\"} is the minimum. As a result, we have an upper bound on the final phase curve, which can be expressed as: $$\\begin{aligned}\n\\label{suppeq:boundary.firstthree.negative}\n  \\sqrt{r} =  \\max \\Bigg\\{&~ v'_{\\min} + \\sqrt{1 -\\vartheta},\\, \\sqrt{\\frac{2(1 -\\vartheta)}{1 -|\\rho|}},\\, \\sqrt{\\frac{1 - 2\\vartheta}{1 -\\rho^2}} + v'_{\\min},\\sqrt{\\frac{1 - 2\\vartheta}{2(1 -|\\rho|)}} + \\frac{t'_{\\min}}{1 -|\\rho|} \\Bigg\\}.\n\\end{aligned}$$ where we define $v'_{\\min} =\\max \\{1,\\sqrt{\\frac{1 -\\vartheta}{1 -\\rho^2}}\\}$ and $t'_{\\min} = \\max \\left\\{ \\frac{\\sqrt{2}}{2}, \\frac{v'_{\\min}}{1 +|\\rho|/\\sqrt{1 -\\rho^2}}\\right\\}$.\n\n::: remark\n**Remark 5**. *Equation [\\[suppeq:boundary.firstthree.positive\\]](#suppeq:boundary.firstthree.positive){reference-type=\"eqref\" reference=\"suppeq:boundary.firstthree.positive\"} and [\\[suppeq:boundary.firstthree.negative\\]](#suppeq:boundary.firstthree.negative){reference-type=\"eqref\" reference=\"suppeq:boundary.firstthree.negative\"} is the result we presented as Theorem [6](#thm:forward-backward){reference-type=\"ref\" reference=\"thm:forward-backward\"} in the main text.*\n\n*When $\\rho \\geq 0$, since we used to define $v'_{\\min} =\\max \\left\\{ 1,\\sqrt{\\frac{1 -\\vartheta}{1 -\\rho^2}},\\frac{\\sqrt{2}/2}{\\sqrt{1 -\\rho^2}} \\right\\}$ in the third case, Equation [\\[eq:boundary.large.v.negative\\]](#eq:boundary.large.v.negative){reference-type=\"eqref\" reference=\"eq:boundary.large.v.negative\"} is strictly above Equation [\\[suppeq:boundary.middle.v.negative\\]](#suppeq:boundary.middle.v.negative){reference-type=\"eqref\" reference=\"suppeq:boundary.middle.v.negative\"}.*\n\n*When $\\rho < 0$, in Equation [\\[eq:boundary.large.v.negative\\]](#eq:boundary.large.v.negative){reference-type=\"eqref\" reference=\"eq:boundary.large.v.negative\"}, we used to define $v'_{\\min} =\\max \\left\\{ 1,\\sqrt{\\frac{1 -\\vartheta}{1 -\\rho^2}},\\frac{\\sqrt{2}/2}{\\sqrt{1 -\\rho^2}} \\right\\}$. When $\\vartheta\\leq  \\frac{1}{2}$, it is equivalent to $v'_{\\min} =\\max \\{1,\\sqrt{\\frac{1 -\\vartheta}{1 -\\rho^2}}\\}$, which agrees with Equation [\\[suppeq:boundary.firstthree.negative\\]](#suppeq:boundary.firstthree.negative){reference-type=\"eqref\" reference=\"suppeq:boundary.firstthree.negative\"} for $\\vartheta\\leq \\frac{1}{2}$ and $\\rho \\leq 0$.*\n\n*When $\\rho < 0$ and $\\vartheta> \\frac{1}{2}$, Equation [\\[eq:boundary.large.v.negative\\]](#eq:boundary.large.v.negative){reference-type=\"eqref\" reference=\"eq:boundary.large.v.negative\"} is not the minimum among the three, mainly because $\\frac{\\sqrt{2}/2}{\\sqrt{1 -\\rho^2}} > 1$ for $|\\rho| >\\frac{\\sqrt{2}}{2}$. The lowest phase curve for $\\vartheta> \\frac{1}{2}$ and $\\rho\\in( - 1,1)$ should be $\\max \\{1 + \\sqrt{1 -\\vartheta},\\sqrt{\\frac{2(1 -\\vartheta)}{1 -|\\rho|}}\\}$, but we can also write it equivalently as $\\max \\left\\{ \\max \\{1,\\sqrt{\\frac{1 -\\vartheta}{1 -\\rho^2}}\\} + \\sqrt{1 -\\vartheta},\\sqrt{\\frac{2(1 -\\vartheta)}{1 -|\\rho|}}\\right\\}$, because $1 + \\sqrt{1 -\\vartheta} \\geq \\sqrt{\\frac{2(1 -\\vartheta)}{1 -|\\rho|}}$ and $\\vartheta> \\frac{1}{2}$ together imply $\\sqrt{\\frac{1 -\\vartheta}{1 -\\rho^2}} \\leq 1$.*\n:::\n\n#### The last three cases:\n\nWe are still left to discuss the rest three cases: (i) $t'( 1 +\\frac{|\\rho|}{\\sqrt{1 -\\rho^2}} )\\leq v' \\leq \\frac{t' \\sqrt{1 -\\rho^2}}{1 -|\\rho|}$, (ii) $\\frac{t' \\sqrt{1 -\\rho^2}}{1 -|\\rho|}\\leq v' \\leq \\frac{t'}{1 -|\\rho|}$, (iii) $v' \\geq \\frac{t'}{1 -|\\rho|}$.\n\nWhen $\\rho \\geq  0$, we can prove that Equation [\\[suppeq:boundary.firstthree.positive\\]](#suppeq:boundary.firstthree.positive){reference-type=\"eqref\" reference=\"suppeq:boundary.firstthree.positive\"} is already the best, and there is no need to discuss the rest three cases for $\\rho \\geq 0$. This is because in Figure [15](#subfig:large.plus.v){reference-type=\"ref\" reference=\"subfig:large.plus.v\"}, [16](#subfig:large.plus.plus.v){reference-type=\"ref\" reference=\"subfig:large.plus.plus.v\"} and [17](#subfig:large.plus.plus.plus.v){reference-type=\"ref\" reference=\"subfig:large.plus.plus.plus.v\"}, we need $v' \\geq \\max \\left\\{ 1,\\sqrt{\\frac{1 -\\vartheta}{1 -\\rho^2}} \\right\\}$. We also need at least that $\\sqrt{r} \\geq v' + \\sqrt{1 -\\vartheta}$, $\\sqrt{r} \\geq \\sqrt{\\frac{2(1 -\\vartheta)}{1 -\\rho}}$ and $\\sqrt{r} \\geq \\sqrt{\\frac{1 - 2\\vartheta}{1 -\\rho^2}} + v'$, so it cannot be any better than Equation [\\[suppeq:boundary.firstthree.positive\\]](#suppeq:boundary.firstthree.positive){reference-type=\"eqref\" reference=\"suppeq:boundary.firstthree.positive\"}.\n\nWhen $\\rho < 0$, the optimal phase curve for $\\vartheta\\leq  \\frac{1}{2}$ may still be one of the last three cases, but the discussion is too difficult. Even the expression of the phase curves is very complicated. We present the phase curves of the rest three cases without proof:\n\n*Case 4. When $t'\\left( 1 +\\frac{|\\rho|}{\\sqrt{1 -\\rho^2}} \\right)\\leq v' \\leq \\frac{t' \\sqrt{1 -\\rho^2}}{1 -|\\rho|}$*, the phase curve is $$\\begin{aligned}\n\\label{eq:boundary.large.large.v.negative}\n  \\sqrt{r} =  \\max \\Bigg\\{&~ v'_{\\min} + \\sqrt{1 -\\vartheta},\\, \\sqrt{\\frac{2(1 -\\vartheta)}{1 -|\\rho|}},\\, \\sqrt{\\frac{1 - 2\\vartheta}{1 -\\rho^2}} + v'_{\\min}(\\vartheta), \\sqrt{\\frac{1 - 2\\vartheta}{2(1 -|\\rho|)}} + \\frac{t'_{\\min}(\\vartheta)}{1 -|\\rho|}\\Bigg\\}.\n\\end{aligned}$$ and the definition of $(v'_{\\min},t'_{\\min})$ is specific to this case. $$v_{\\min}(\\vartheta) =\\max\\left\\{ 1,\\sqrt{\\frac{1 -\\vartheta}{1 -\\rho^2}},\\frac{\\sqrt{2}}{2} \\left( 1 +\\frac{|\\rho|}{\\sqrt{1 -\\rho^2}} \\right) \\right\\},\\quad t_{\\min}(\\vartheta) =\\max \\left\\{ \\frac{1 -|\\rho|}{\\sqrt{1 -\\rho^2}} v_{\\min}(\\vartheta),\\, f(|\\rho|),g(\\vartheta) \\right\\}$$ where $$f(|\\rho|) = \\begin{cases} \n  \\sqrt{\\frac{1 -\\rho^2}{2 -\\rho^2}} & \\text{ if } |\\rho| \\leq (\\sqrt{5}-1)/2 \\\\\n  \\frac{1}{1 +|\\rho|} & \\text{ if } (\\sqrt{5}-1)/2 \\leq |\\rho| \\leq \\frac{1}{3} \\left[ - 2 + (19+3 \\sqrt{33})^{1/3} + (19-3 \\sqrt{33})^{1/3}\\right]\\\\\n  \\frac{|\\rho|}{\\sqrt{1 +(1 +|\\rho|)^2 - 2(1 +|\\rho|)\\sqrt{1 -\\rho^2}}} & \\text{ if } |\\rho| \\geq \\frac{1}{3} \\left[ - 2 + (19+3 \\sqrt{33})^{1/3} + (19-3 \\sqrt{33})^{1/3}\\right]\n  \\end{cases}$$ and $$g(\\vartheta) = \\begin{cases} \n  g_1(\\vartheta) & \\vartheta \\geq \\vartheta^* \\\\\n  g_2(\\vartheta) & \\vartheta< \\vartheta^*\n  \\end{cases}$$ where $\\vartheta^*$, $g_1(\\vartheta)$ and $g_2(\\vartheta)$ are respectively the roots of $\\vartheta$ of the following three equations: $$\\begin{aligned}\n  \\vartheta=\\vartheta^* : &~ ~ \n  \\frac{|\\rho|}{1 -|\\rho|} + \\sqrt{\\frac{1 - 2\\vartheta}{2(1 -|\\rho|)}} -\\frac{1 +\\rho^2}{\\sqrt{1 -\\rho^2}} - \\sqrt{1 -\\vartheta-\\rho^2} = 0\n  \\\\\n  t =g_1(\\vartheta): & ~~ t \\left( \\frac{1}{1 -|\\rho|} - \\frac{2|\\rho|}{\\sqrt{1 -\\rho^2}} \\right) + \\sqrt{\\frac{1 - 2\\vartheta}{2(1 -|\\rho|)}} - \\sqrt{1 -t^2} - \\sqrt{1 -\\vartheta- t^2} = 0\\\\\n  t =g_2(\\vartheta): & ~~  t \\left( \\frac{1}{1 -|\\rho|} - \\frac{|\\rho|}{\\sqrt{1 -\\rho^2}} \\right) + \\sqrt{\\frac{1 - 2\\vartheta}{2(1 -|\\rho|)}} - \\frac{1}{\\sqrt{1 -\\rho^2}} - \\sqrt{1 -\\vartheta- t^2} = 0\n\\end{aligned}$$ All three equations can be solved easily with bi-section methods.\n\n*Case 5. When $\\frac{t' \\sqrt{1 -\\rho^2}}{1 -|\\rho|}\\leq v' \\leq \\frac{t'}{1 -|\\rho|}$*: We first define $v_{\\min}(\\vartheta) = \\max\\left\\{ 1, \\sqrt{\\frac{1 -\\vartheta}{1 -\\rho^2}}, \\frac{\\sqrt{1 -\\rho^2}}{1 -|\\rho|} \\cdot f(|\\rho|),\\right\\}$ in which $f(|\\rho|)$ has the same definition from *Case 4*.\n\nWhen $|\\rho| \\leq \\frac{\\sqrt{2}}{2}$, the boundary is $$\\sqrt{r} =\\max \\left\\{ v_{\\min}(\\vartheta) + \\sqrt{1 -\\vartheta}, h_1(\\vartheta) \\right\\}$$ where $h_1(\\vartheta) =\\min\\left\\{ \\textit{Slope}(\\vartheta) \\cdot \\sqrt{1 -\\vartheta},\\,h_2(\\vartheta),\\,  \\max \\left\\{ \\sqrt{\\frac{1 - 2\\vartheta}{2(1 -|\\rho|)}} + \\frac{g_1(\\vartheta)}{1 -|\\rho|},\\,\\frac{1+2|\\rho|}{\\sqrt{2-\\rho^2}}+\\sqrt{1-\\vartheta-\\frac{1-\\rho^2}{2-\\rho^2}} \\right\\} \\right\\}$, in which $$\\begin{gathered}\n  \\textit{Slope}(\\vartheta) = 1 + \\frac{\\sqrt{1-\\rho^2}}{1-|\\rho|} \\cdot t^*, \\text{ where \\( t^*\\in(0,1) \\) solves } \\frac{|\\rho| t}{\\sqrt{1-\\rho^2}} + \\sqrt{1 -t^2}= 1,\n\\end{gathered}$$ and $$h_2(\\vartheta) = \\begin{cases} \n    \\sqrt{\\frac{1 - 2\\vartheta}{1 -\\rho^2}} + \\frac{1}{\\sqrt{1 -\\rho^2}} & \\text{ if } \\vartheta\\leq 1 -\\frac{1}{\\rho^2(1+|\\rho|)^2} \\\\\n    \\frac{1 + 2|\\rho|}{(1 +|\\rho|)\\sqrt{1 -\\rho^2}} + \\sqrt{1 -\\vartheta-\\frac{1}{(1 +|\\rho|)^2}} & \\text{ if } \\vartheta> 1 -\\frac{1}{\\rho^2(1+|\\rho|)^2}\n  \\end{cases}$$ and $g_1(\\vartheta)$ is the same one in *Case 4*.\n\nWe define a numerical special numerical value for $|\\rho|$: $|\\rho| = 0.7544$. It is the value which makes $\\sqrt{r} = \\frac{1 + 2|\\rho|}{(1 +|\\rho|)\\sqrt{1 -\\rho^2}} + \\sqrt{1 -\\vartheta-\\frac{1}{(1 +|\\rho|)^2}}$ and $\\sqrt{r} =\\left( 1 + \\frac{\\sqrt{1-\\rho^2}}{1-|\\rho|}  \\right)\\sqrt{1 -\\vartheta}$ intersect at $\\vartheta= \\frac{1}{2}$.\n\nWhen $\\frac{\\sqrt{2}}{2} < |\\rho| \\leq 0.7544$, the boundary is $$\\sqrt{r} =\\max \\left\\{ v_{\\min}(\\vartheta) + \\sqrt{1 -\\vartheta}, h_3(\\vartheta) \\right\\},$$ where the definiton of $v_{\\min}$ is unchanged; $h_3(\\vartheta) =\\min \\left\\{ \\left( 1 + \\frac{\\sqrt{1-\\rho^2}}{1-|\\rho|}  \\right)\\sqrt{1 -\\vartheta},\\,h_2(\\vartheta) \\right\\}$.\n\nWhen $|\\rho| > 0.7544$, the boundary is: $$\\sqrt{r} = \\begin{cases} \n  \\max \\left\\{ v_{\\min}(\\vartheta) + \\sqrt{1 -\\vartheta}, \\, \\left( 1 + \\frac{\\sqrt{1-\\rho^2}}{1-|\\rho|}  \\right)(1 -\\vartheta) \\right\\} & \\text{ if }\\vartheta\\leq \\frac{1}{2} \\\\\n  h_2(\\vartheta) & \\text{ if }\\vartheta>\\frac{1}{2}\n  \\end{cases}$$\n\n*Case 6. When $v' \\geq \\frac{t'}{1 -|\\rho|}$* $$\\sqrt{r} = \\max \\left\\{ \\sqrt{1 -\\vartheta} + \\frac{1}{\\sqrt{1 -\\rho^2}},\\,  \\sqrt{\\frac{1 - 2\\vartheta}{1 -\\rho^2}} + \\frac{1}{\\sqrt{1 -\\rho^2}},\\,h(\\vartheta) \\right\\}$$ where the curve $h(\\vartheta)$ is defined as $$h(\\vartheta) = \\begin{cases} \n    \\sqrt{\\frac{1 - \\vartheta}{1 -\\rho^2}} + \\frac{1}{\\sqrt{1 -\\rho^2}} & \\text{  if }\\vartheta\\leq 1 - \\frac{1-|\\rho|}{\\rho^2(1+|\\rho|)} \\\\\n    \\frac{1}{\\sqrt{1 -\\rho^2}} +\\frac{|\\rho|}{1 +|\\rho|} + \\sqrt{1 -\\vartheta-\\frac{1 -|\\rho|}{1 +|\\rho|}} & \\text{  if }\\vartheta> 1 - \\frac{1-|\\rho|}{\\rho^2(1+|\\rho|)}.\n  \\end{cases}$$\n\nTo sum up all the six cases, the lowest phase curve over the six cases in given in Equation [\\[suppeq:boundary.firstthree.positive\\]](#suppeq:boundary.firstthree.positive){reference-type=\"eqref\" reference=\"suppeq:boundary.firstthree.positive\"} when $\\rho \\geq  0$. When $\\rho< 0$, the optimal curve is too complicated, but an upper bound is given by Equation [\\[suppeq:boundary.firstthree.negative\\]](#suppeq:boundary.firstthree.negative){reference-type=\"eqref\" reference=\"suppeq:boundary.firstthree.negative\"}."},{"heading":"Proof of Theorem [7](#thm:equivalence)","text":"# Proof of Theorem [7](#thm:equivalence){reference-type=\"ref\" reference=\"thm:equivalence\"} {#proof-of-theorem-thmequivalence}\n\nThe key is to analyze the random-design setting and show that its minimax rate of Hamming error is only determined by $\\mathbb{E}[X'X]=\\Sigma$. Then, when we switch to the fixed-design case of $X'X=\\Sigma$, the same minimax rate holds. For the random-design setting, we proceed by deriving a lower bound and an upper bound of the minimax Hamming error separately.\n\nFirst, we derive a lower bound for the minimax Hamming error. Let $G=X'X$ denote the Gram matrix of the random-design model. Fixing any two subsets $V_0, V_1\\subset\\{1,2,\\ldots,p\\}$, we write $V=V_0\\cup V_1$. Let $\\eta\\in \\{0,1\\}^p$ be an arbitrary binary vector. We consider two binary vectors $\\mu^{(0)}, \\mu^{(1)}\\in\\{0,1\\}^p$ where $\\mu^{(0)}_j=\\mu^{(1)}_j=\\eta_j$, for $j\\notin V$, and restricted on $V$, $\\mathrm{Supp}(\\mu^{(0)}_V)=V_0$ and $\\mathrm{Supp}(\\mu_V^{(1)})=V_1$. Let $\\tau_p=\\sqrt{2r\\log(p)}$. Consider the testing problem $$\\label{proof-equivalent-1}\nH_0: \\beta = \\tau_p\\mu^{(0)}, \\qquad v.s. \\qquad H_1: \\beta=\\tau_p \\mu^{(1)}.$$ For a test $T$, let $R(T)$ be the sum of type I and type II errors. Any selector $\\hat{\\beta}$ can be converted to a test $T(\\hat{\\beta})$, where we reject the null hypothesis if $\\mathrm{Supp}(\\hat{\\beta})\\neq V_0$. It is seen that $R(T(\\hat{\\beta}))=\\mathbb{P}\\{\\beta=\\tau_p\\mu^{(0)}, \\mathrm{Supp}(\\hat{\\beta})\\neq V_0\\} +\\mathbb{P}\\{\\beta=\\tau_p\\mu^{(1)}, \\mathrm{Supp}(\\hat{\\beta})= V_0) \\} \\leq \\sum_{j\\in V}\\{\\mathbb{P}(\\beta_j=0, \\hat{\\beta}_j\\neq \\tau_p)+ \\mathbb{P}(\\beta_j=\\tau_p, \\hat{\\beta}_j= 0)\\}$. It follows that $$\\label{proof-equivalent-2}\n\\mathbb{E}[H(\\hat{\\beta}_V, \\beta_V)|X]\\geq R(T(\\hat{\\beta}))\\geq \\inf_{T}R(T)\\equiv R^*(V_0, V_1; \\eta, X).$$ We can compute the right hand side using the Neyman-Pearson lemma. Define $$\\label{proof-equivalent-Def-a}\na=a(V_0, V_1, X) = (\\mu^{(0)}-\\mu^{(1)})'G(\\mu^{(0)}-\\mu^{(1)}).$$ The likelihood ratio test for [\\[proof-equivalent-1\\]](#proof-equivalent-1){reference-type=\"eqref\" reference=\"proof-equivalent-1\"} is equivalent to using the test statistic $Z=a^{-1/2}(\\mu^{(1)}-\\mu^{(0)})'X'(y-\\tau_p X\\mu^{(0)})$. Then, $Z\\sim {\\cal N}(0, 1)$ under $H_0$, and $Z\\sim {\\cal N}(a^{1/2}\\tau_p, 1)={\\cal N}(\\sqrt{2ar\\log(p)}, 1)$, under $H_1$. By Neyman-Pearson lemma, $$\\begin{aligned}\n  \\label{proof-equivalent-3}\nR^*(V_0, V_1; \\eta, X) &=\\inf_t\\Bigl\\{\\epsilon_p^{|V_0|}\\cdot \\mathbb{P}\\bigl({\\cal N}(0, 1)>t\\bigr) +  \\epsilon_p^{|V_1|}\\cdot \\mathbb{P}\\bigl( {\\cal N}(\\sqrt{2ar\\log(p)}, 1) <t\\bigr)\\Bigr\\}\\cr\n&= \\inf_{t=\\sqrt{2q\\log(p)}}\\Bigl\\{ L_p p^{-|V_0|\\vartheta - q} + L_p p^{-|V_1|\\vartheta - (\\sqrt{ar}-\\sqrt{q})_+^2}\\Bigr\\}\\cr\n&= L_p p^{-h(V_0, V_1, X)}, \n\\end{aligned}$$ where $$h(V_0,V_1, X)=\\max_{q>0}\\Bigl(\\min\\Bigl\\{ |V_0|\\vartheta+q, \\; |V_1|\\vartheta +  (\\sqrt{ar}-\\sqrt{q})_+^2\\Bigr\\}\\Bigr).$$ In the second line of [\\[proof-equivalent-3\\]](#proof-equivalent-3){reference-type=\"eqref\" reference=\"proof-equivalent-3\"}, we have used the Mills' ratio of $N(0,1)$ (e.g., see [@ke2014covariance] for a similar use of the Mills' ratio). Let $\\Sigma$ be the covariance matrix, parameterized by $\\rho$. We define the following quantities: $$\\begin{aligned}\n \\label{proof-equivalent-h-star}\na^*(V_0, V_1,\\rho)& = (\\mu^{(0)}-\\mu^{(1)})'\\Sigma(\\mu^{(0)}-\\mu^{(1)}), \\cr\nh^*(V_0, V_1, \\rho) &= \\max_{q>0}\\Bigl(\\min\\Bigl\\{ |V_0|\\vartheta+q, \\; |V_1|\\vartheta +  (\\sqrt{a^*r}-\\sqrt{q})_+^2\\Bigr\\}\\Bigr). \n\\end{aligned}$$ Below, we show that $h(V_0, V_1, X)$ is sufficiently close to $h^*(V_0, V_1, \\rho)$. The key is showing that $\\Sigma$ and $G$ are sufficiently close on the diagonal block restricted to $V$. We use Theorem 5.39 and Remark 5.40 of [@Vershynin] with $t=O(\\sqrt{|V|\\log(p)})$. It follows that, when $|V|\\ll n$, with probability $1-o(p^{-3-|V|})$, $$\\|G_{V,V}-\\Sigma_{V,V}\\|\\leq C\\|\\Sigma_{V,V}\\|\\sqrt{n^{-1}|V|\\log(p)};\\quad \\mbox{here, $C$ a constant independent of $|V|$}.$$ We note that $\\|\\Sigma_{V,V}\\|\\leq \\|\\Sigma\\|\\leq C$. For any finite integer $m\\geq 1$, the total number of size-$m$ subset $V$ is ${p\\choose m}=O(p^m)$. We then apply the probability union bound to get that, with probability $1-O(p^{-3})$, $$\\label{proof-equivalent-4}\n\\max_{V: |V|\\leq m} \\|G_{V,V}-\\Sigma_{V,V}\\|\\leq C\\sqrt{n^{-1}\\log(p)}.$$ Since $|a(V_0, V_1, X)-a^*(V_0, V_1, \\rho)|\\leq \\|G_{V,V}-\\Sigma_{V,V}\\|\\cdot\\|\\mu^{(1)}-\\mu^{(0)}\\|^2\\leq \\|G_{V,V}-\\Sigma_{V,V}\\|\\cdot |V|$, we immediately know that $$\\label{proof-equivalent-a-bound}\n|a(V_0, V_1, X)-a^*(V_0, V_1, \\rho)|\\leq C\\sqrt{n^{-1}\\log(p)} \\quad \\mbox{here, $C$ depends on $m$}.$$ Write $h=h(V_0,V_1,X)$ and $h^*=h^*(V_0,V_1,\\rho)$ for short, and let $(h^*, a^*)$ be the shorthand notations defined similarly. Then, $h=\\max_{q}g(q, a)$ and $h^* =\\max_q f(q, a^*)$, for $f(q,a)=\\min\\{ |V_0|\\vartheta+q,  |V_1|\\vartheta +  (\\sqrt{ar}-\\sqrt{q})_+^2\\}$. Let $\\tilde{q}$ and $\\tilde{q}^*$ be the two maximizers. It is seen that $h=f(\\tilde{q}, a)\\leq f(\\tilde{q}, a^*)+\\max_{q}|f(q,a)-f(q,a^*)|\\leq h^*+\\max_{q}|f(q,a)-f(q,a^*)|$. Similarly, we can also derive that $h\\leq h^*+\\max_q|f(q, a^*)-f(q,a )|$. Combining them gives $|h-h^*|\\leq \\max_q |f(q, a^*)-f(q,a)|$. We plug in the expression of $f(q,a)$ to get $$|h(V_0, V_1, X)-h^*(V_0, V_1, \\rho)|\\leq |\\sqrt{ar}-\\sqrt{a^*r}|\\leq C\\sqrt{n^{-1}\\log(p)}.$$ We now combine all the results, and note that [\\[proof-equivalent-4\\]](#proof-equivalent-4){reference-type=\"eqref\" reference=\"proof-equivalent-4\"} has a maximum over all $V=V_0\\cup V_1$. It follows that, with probability $1-O(p^{-3})$, $$\\label{proof-equivalent-5}\n\\max_{(V_0, V_1): |V_0\\cup V_1|\\leq m}|h(V_0, V_1, X)-h^*(V_0, V_1, \\rho)|\\leq C\\sqrt{n^{-1}\\log(p)}.$$ We plug it into [\\[proof-equivalent-3\\]](#proof-equivalent-3){reference-type=\"eqref\" reference=\"proof-equivalent-3\"}. Note that $L_p p^{-h}=L_p p^{-h^*}\\cdot p^{h^*-h}$. In line of [\\[proof-equivalent-5\\]](#proof-equivalent-5){reference-type=\"eqref\" reference=\"proof-equivalent-5\"}, $p^{h^*-h}$ is a multi-$\\log(p)$ term, i.e., $L_pp^{-h}=L_pp^{-h^*}$. We then combine it with [\\[proof-equivalent-2\\]](#proof-equivalent-2){reference-type=\"eqref\" reference=\"proof-equivalent-2\"}. It yields that, with probability $1-O(p^{-3})$, $$\\label{proof-equivalent-6}\n\\mathbb{E}[H(\\hat{\\beta}_V, \\beta_V)|X]\\geq L_p p^{-h^*(V_0, V_1,\\rho)}, \\mbox{simultaneously for all $(V_0,V_1)$ with $|V_0\\cup V_1|\\leq m$}.$$ Given $V$, we further take a maximum over $(V_0, V_1)$ on the right hand side. It follows that $$\\label{proof-equivalent-h-star-star}\n\\mathbb{E}[H(\\hat{\\beta}_V, \\beta_V)|X]\\geq L_p p^{-h^{**}(V,\\rho)}, \\qquad\\mbox{where}\\quad h^{**}(V, \\rho) = \\min_{\\substack{(V_0,V_1): V_0\\neq V_1,\\\\V_0\\cup V_1=V}}h^*(V_0, V_1, \\rho).$$ Write $\\{1,2,\\ldots,p\\}=\\cup_{j=1}^{\\lceil p/2\\rceil} V_j$, where $V_j=\\{2j-1, 2j\\}$ for $j\\leq p/2$ and $V_j=\\{p\\}$ for $j>p/2$ (this happens only if $p$ is odd). It follows that, with probability $1-O(p^{-3})$, $$\\mathbb{E}[H(\\hat{\\beta},\\beta)|X]=\\sum_{1\\leq j\\leq \\lceil p/2\\rceil}\\mathbb{E}[H(\\hat{\\beta}_{V_j}, \\beta_{V_j})|X]\\geq \n\\sum_{1\\leq j\\leq \\lceil p/2\\rceil}L_p p^{-h^{**}(V_j,\\rho)}.$$ When $p$ is even, $h^{**}(V_j,\\rho)$ are all equal. When $p$ is odd, $h^{**}(V_j,\\rho)$ are all equal, except for one $V_j$; but this one has a negligible effect on the right hand side above. Let $h^{**}(\\rho)$ be the common value of $h^{**}(V_j,\\rho)$. Since $h^{**}(\\rho)$ also depends on $(\\vartheta,r)$, we write it as $h^{**}(\\rho; \\vartheta,r)$ to reflect this dependence. We immediately have that, with probability $1-O(p^{-3})$, $$\\mathbb{E}[H(\\hat{\\beta},\\beta)|X] \\geq L_p p^{1-h^{**}(\\rho;\\vartheta,r)}.$$ On the event that the above inequality does not hold, the Hamming error is at most $p$. The contribution of this event to the expected Hamming error is at most $p\\cdot O(p^{-3})=O(p^{-2})$, which is negligible to $L_p p^{1-h^*(\\rho;\\vartheta,r)}$. It follows that $$\\label{proof-equivalent-7}\n\\mathbb{E}[H(\\hat{\\beta},\\beta)]\\geq L_p p^{1-h^{**}(\\rho;\\vartheta,r)}, \\qquad\\mbox{for any method $\\hat{\\beta}$}.$$ This gives a lower bound for the minimax Hamming error.\n\nNext, we give an upper bound for the minimax Hamming error. We will consider a specific $\\hat{\\beta}$. Let the partition $\\{1,2,\\ldots,p\\}=\\cup_{j=1}^{\\lceil p/2\\rceil} V_j$ be the same as above. For any subset $U\\subset\\{1,2,\\ldots,p\\}$, let ${\\bf1}_U$ be the binary vector such that its $j$th entry is $1$ if $j\\in U$ and $0$ otherwise. Additionally, let $X_U$ be the submatrix of $X$ restricted to columns in $U$. For each $V_j$, define $$\\label{proof-equivalent-8}\n\\hat{U}_j = \\mathop{\\mathrm{arg\\,min}}_{U\\subset V_j}\\Bigl\\{ \\frac{1}{2}\\|y-\\tau_p X{\\bf 1}_U\\|^2 + \\vartheta \\log(p)|U| \\Bigr\\}.$$ Define $\\hat{\\mu}\\in\\{0,1\\}^p$ such that for any $i\\in V_j$, $\\hat{\\mu}_i=1$ if $i\\in \\hat{U}_j$, and $\\hat{\\mu}_i=0$ otherwise. The estimator is $\\hat{\\beta}=\\tau_p \\hat{\\mu}$. We now calculate the expected Hamming error of this estimator. Let $S$ be the support of $\\beta$. Fix $V_j$ and write $V=V_j$ for short. Given any two subsets $U_0$ and $U_1$ of $V$ such that $U_0\\neq U_1$, we consider the event $$\\label{proof-equivalent-event}\n\\mathrm{Supp}(\\beta_V)=U_0, \\qquad \\mathrm{Supp}(\\hat{\\beta}_V)=U_1, \\qquad |S|\\leq 2p^{1-\\vartheta}.$$ On this event, it is true that $$\\label{proof-equivalent-9}\n\\frac{1}{2}\\|y-\\tau_p X{\\bf 1}_{U_0}\\|^2 + \\vartheta \\log(p)|U_0|\\geq  \\frac{1}{2}\\|y-\\tau_p X {\\bf 1}_{U_1}\\|^2 + \\vartheta \\log(p)|U_1|.$$ Note that $y=X\\beta+z=\\tau_p X{\\bf 1}_{U_0}+\\tau_p X{\\bf 1}_{S\\cap V^c}+z$. We can re-write [\\[proof-equivalent-9\\]](#proof-equivalent-9){reference-type=\"eqref\" reference=\"proof-equivalent-9\"} as $$\\frac{1}{2}\\|z+\\tau_p X{\\bf 1}_{S\\cap V^c}\\|^2 + \\vartheta \\log(p)|U_0|\\geq  \\frac{1}{2}\\|(z+\\tau_p X{\\bf 1}_{S\\cap V^c})-\\tau_p X({\\bf 1}_{U_1}-{\\bf 1}_{U_0})\\|^2 + \\vartheta \\log(p)|U_1|.$$ Let $F=U_0\\cap U_1$, $E_0=U_0\\setminus F$ and $E_1=U_1\\setminus F$. Then, $|U_0|=|F|+|E_0|$ and $|U_1|=|F|+|E_1|$. We plug it into the above inequality and re-arrange the terms. It gives $$z'X({\\bf 1}_{U_1}-{\\bf 1}_{U_0})\\geq \\frac{\\tau_p}{2}({\\bf 1}_{U_1}-{\\bf 1}_{U_0})'G({\\bf 1}_{U_1}-{\\bf 1}_{U_0}) -  {\\bf 1}_{S\\cap V^c}'G({\\bf 1}_{U_1}-{\\bf 1}_{U_0})+\\frac{\\vartheta\\log(p)}{\\tau_p}(|E_1|-|E_0|).$$ Let $a=a(U_0, U_1, X)=({\\bf 1}_{U_1}-{\\bf 1}_{U_0})'G({\\bf 1}_{U_1}-{\\bf 1}_{U_0})$. We note that this definition is indeed the same as that in [\\[proof-equivalent-Def-a\\]](#proof-equivalent-Def-a){reference-type=\"eqref\" reference=\"proof-equivalent-Def-a\"}. Let $\\tilde{z}=z'X({\\bf 1}_{U_1}-{\\bf 1}_{U_0})/\\sqrt{a}$. The above can be written equivalently as $$\\label{proof-equivalent-10}\n\\tilde{z}\\geq \\sqrt{2\\log(p)} \\biggl[ \\frac{\\sqrt{ra}}{2} - \\frac{{\\bf 1}_{S\\cap V^c}'G({\\bf 1}_{U_1}-{\\bf 1}_{U_0})}{\\sqrt{a}} + \\frac{\\vartheta(|E_1|-|E_0|)}{2\\sqrt{ra}}\\biggr], \\quad\\mbox{where}\\;\\; \\tilde{z}|(X,\\beta)\\sim N(0,1).$$ We bound $|{\\bf 1}_{S\\cap V^c}'G({\\bf 1}_{U_1}-{\\bf 1}_{U_0})|$. Since $\\Sigma_{V^cV}$ is a zero matrix, we immediately have ${\\bf 1}_{S\\cap V^c}'\\Sigma({\\bf 1}_{U_1}-{\\bf 1}_{U_0})=0$. It follows by the triangle inequality that $$\\begin{aligned}\n  \\label{proof-equivalent-11}\n|{\\bf 1}_{S\\cap V^c}'G({\\bf 1}_{U_1}-{\\bf 1}_{U_0})| & \\leq |{\\bf 1}_{S\\cap V^c}'(G-\\Sigma)({\\bf 1}_{U_1}-{\\bf 1}_{U_0})|\\cr\n&\\leq |V|\\cdot \\max_{k\\in V}|e_k'(G-\\Sigma){\\bf 1}_{S\\cap V^c}|. \n\\end{aligned}$$ For $k\\in V$, $e_k'(G-\\Sigma){\\bf 1}_{S\\cap V^c}=\\sum_{i=1}^n \\sum_{\\ell \\in S\\cap V^c}\\{X(i,k)X(i,\\ell)-\\mathbb{E}[X(i,k)X(i,\\ell)]\\}$. We recall that $\\{1,2,\\ldots,p\\}=\\cup_{m=1}^{\\lceil p/2\\rceil}V_m$ is a partition. It induces a partition on $S\\cap V^c$, which we denote by $S\\cap V^c=\\cup_{m=1}^{N_p}S_m$. Each $S_m$ contains at most 2 indices and $|S\\cap V^c|/2\\leq N_p\\leq |S\\cap V^c|$. Write $$e_k'(G-\\Sigma){\\bf 1}_{S\\cap V^c} = \\sum_{i=1}^n \\sum_{m=1}^{N_p}\\Bigl[ \\sum_{\\ell\\in S_m}\\{X(i,k)X(i,\\ell)-\\mathbb{E}[X(i,k)X(i,\\ell)]\\}\\Bigr].$$ The right hand side is a sum of $nN_p$ independent variables, where each variable has a zero mean and a sub-exponential norm bounded by $n^{-1}K$, for a constant $K>0$. We apply the Bernstein inequality [@Vershynin Proposition 5.16] to get that, for every $t>0$, $$\\mathbb{P}\\bigl(|e_k'(G-\\Sigma){\\bf 1}_{S\\cap V^c}|>t\\bigr)\\leq 2\\exp\\Bigl(-c\\min\\Bigl\\{\\frac{nt^2}{K^2N_p}, \\frac{nt}{K}\\Bigr\\}\\Bigr),$$ where $c>0$ is a universal constant. By letting $t=C\\sqrt{n^{-1}N_p\\log(p)}$ for a properly large constant $C$, we have that, with probability $1-O(p^{-3})$, $$|e_k'(G-\\Sigma){\\bf 1}_{S\\cap V^c}|\\leq C\\sqrt{n^{-1}N_p\\log(p)}\\leq C\\sqrt{n^{-1}|S|\\log(p)}.$$ We plug it into [\\[proof-equivalent-10\\]](#proof-equivalent-10){reference-type=\"eqref\" reference=\"proof-equivalent-10\"} and apply the probability union bound. We also note that $|S|=O(p^{1-\\vartheta})$ on the event [\\[proof-equivalent-event\\]](#proof-equivalent-event){reference-type=\"eqref\" reference=\"proof-equivalent-event\"}; also, $n=p^{\\omega}$ with $\\omega>1-\\vartheta$. It follows that, on this event, with probability $1-O(p^{-3})$, $$\\label{proof-equivalent-12}\n|{\\bf 1}_{S\\cap V^c}'G({\\bf 1}_{U_1}-{\\bf 1}_{U_0})|\\leq Cp^{-(\\omega-1+\\vartheta)/2}\\sqrt{\\log(p)}.$$ We plug [\\[proof-equivalent-12\\]](#proof-equivalent-12){reference-type=\"eqref\" reference=\"proof-equivalent-12\"} into [\\[proof-equivalent-10\\]](#proof-equivalent-10){reference-type=\"eqref\" reference=\"proof-equivalent-10\"} to get: $$\\tilde{z}\\geq \\sqrt{2b_p\\log(p)},\\quad \\mbox{where } b_p= \\Bigl[\\frac{\\sqrt{ra}}{2} + \\frac{\\vartheta(|E_1|-|E_0|)}{2\\sqrt{ra}}-L_p p^{-\\frac{\\omega-1+\\vartheta}{2}}\\Bigr]^2.$$ Moreover, let $a^*=a^*(U_0, U_1, \\rho)=({\\bf 1}_{U_1}-{\\bf 1}_{U_0})'\\Sigma({\\bf 1}_{U_1}-{\\bf 1}_{U_0})$, which is the same as the definition in [\\[proof-equivalent-h-star\\]](#proof-equivalent-h-star){reference-type=\"eqref\" reference=\"proof-equivalent-h-star\"}. By [\\[proof-equivalent-a-bound\\]](#proof-equivalent-a-bound){reference-type=\"eqref\" reference=\"proof-equivalent-a-bound\"}, the replacement of $a$ by $a^*$ only yields a difference of $L_p p^{-\\frac{\\omega-1+\\vartheta}{2}}$ in the expression of $b_p$. We further have: $$\\label{proof-equivalent-13}\n\\tilde{z}\\geq \\sqrt{2b_p\\log(p)},\\quad \\mbox{where }\\tilde{z}|(X,\\beta)\\sim N(0,1) \\mbox{ and } b_p= \\Bigl[\\frac{\\sqrt{ra^*}}{2} + \\frac{\\vartheta(|E_1|-|E_0|)}{2\\sqrt{ra^*}}+L_p p^{-\\frac{\\omega-1+\\vartheta}{2}}\\Bigr]^2.$$ First, what [\\[proof-equivalent-13\\]](#proof-equivalent-13){reference-type=\"eqref\" reference=\"proof-equivalent-13\"} says is that, conditioning on $(X,\\beta_{V}, \\beta_{V^c})$, if $\\|\\beta\\|_0\\leq Cp^{1-\\vartheta}$, then except for an event of probability $O(p^{-3})$, $\\mathrm{Supp}(\\hat{\\beta}_V)=U_1$ implies $\\tilde{z}>\\sqrt{2b_p\\log(p)}$. Second, under our model, $(X,\\beta_{V^c})$ are independent of $\\beta_V$, and $\\mathbb{P}(\\mathrm{Supp}(\\beta_V)=U_0)=L_pp^{-\\vartheta |U_0|}=L_pp^{-\\vartheta(|F|+|E_0|)}$. Last, $\\mathbb{P}(\\|\\beta_{V^c}\\|_0\\leq 2p^{1-\\vartheta})=1-O(p^{-3})$ (this is seen by noticing that $\\|\\beta_{V^c}\\|_0$ is the sum of independent Bernoulli variables and by applying the Bernstein's inequality). We combine the above to get $$\\begin{aligned}\n& \\mathbb{P}\\bigl(\\mathrm{Supp}(\\beta_V)=U_0,\\, \\mathrm{Supp}(\\hat{\\beta}_V)=U_1, \\, |S|\\leq 2p^{1-\\vartheta}\\bigr)\\cr\n \\leq \\;\\; &L_pp^{-\\vartheta|U_0|}\\cdot \\mathbb{P}\\biggl(\\tilde{z}\\geq \\sqrt{2b_p\\log(p)}\\biggr) + O(p^{-3})\\cr\n \\leq \\;\\; & L_p p^{-\\vartheta( |F| +|E_0|)- \\bigl[\\frac{\\sqrt{ra^*}}{2} + \\frac{\\vartheta(|E_1|-|E_0|)}{2\\sqrt{ra^*}}\\bigr]_+^2}. \n\\end{aligned}$$ By elementary calculations, we have $$\\begin{aligned}\n & \\vartheta(|F|+ |E_0|)+ \\Bigl[\\frac{\\sqrt{ra^*}}{2} + \\frac{\\vartheta(|E_1|-|E_0|)}{2\\sqrt{ra^*}}\\Bigr]_+^2\\cr\n \\geq \\quad & \\vartheta|F|+\\max\\{|E_0|, |E_1|\\}\\vartheta +\\frac{1}{4}\\Bigl(\\sqrt{ra^*}-\\frac{|(|E_1|-|E_0|)|}{\\sqrt{ra^*}}\\Bigr)_+^2\\cr\n=\\quad &  \\max\\{|U_0|, |U_1|\\}\\vartheta +\\frac{1}{4}\\Bigl(\\sqrt{ra^*}-\\frac{|(|U_1|-|U_0|)|}{\\sqrt{ra^*}}\\Bigr)_+^2\\quad =\\quad h^*(U_0, U_1, \\rho), \n\\end{aligned}$$ where $h^*(U_0, U_1, \\rho)$ is the same as that defined in [\\[proof-equivalent-h-star\\]](#proof-equivalent-h-star){reference-type=\"eqref\" reference=\"proof-equivalent-h-star\"} (the last equality above follows by solving $q$ in [\\[proof-equivalent-h-star\\]](#proof-equivalent-h-star){reference-type=\"eqref\" reference=\"proof-equivalent-h-star\"}). We combine the above to get $$\\label{proof-equivalent-14}\n\\mathbb{P}\\bigl(\\mathrm{Supp}(\\beta_V)=U_0,\\, \\mathrm{Supp}(\\hat{\\beta}_V)=U_1, \\, |S|\\leq 2p^{1-\\vartheta}\\bigr)\\leq L_p p^{-h^*(U_0, U_1, \\rho)}.$$ On the above event, the Hamming error contributed by $\\hat{\\beta}_V$ is $|E_0|+|E_1|\\leq |V|\\leq 2$. Moreover, $h^*(U_0, U_1,\\rho)\\geq h^{**}(\\rho;\\vartheta,r)$, where the latter is defined in [\\[proof-equivalent-h-star-star\\]](#proof-equivalent-h-star-star){reference-type=\"eqref\" reference=\"proof-equivalent-h-star-star\"}. It follows that $$\\begin{aligned}\n\\mathbb{E}[H(\\hat{\\beta}_V, \\beta_V)] &= \\sum_{(U_0, U_1)}2\\cdot \\mathbb{P}\\bigl(\\mathrm{Supp}(\\beta_V)=U_0,\\, \\mathrm{Supp}(\\hat{\\beta}_V)=U_1, \\, |S|\\leq 2p^{1-\\vartheta}\\bigr) +O(p^{-3})\\cr\n&\\leq L_p \\sum_{(U_0, U_1)} p^{-h^*(U_0, U_1, \\rho)} \\leq  L_p p^{-h^{**}(\\rho;\\vartheta,r)}. \n\\end{aligned}$$ The above is true for every $V=V_j$ in the partition $\\{1,2,\\ldots,p\\}=\\cup_{j=1}^{\\lceil p/2\\rceil}$ (except for the last $V_j$ in the case that $p$ is odd; but this single $V_j$ has a negligible effect on the rate of the Hamming error). We immediately have $$\\label{proof-equivalent-15}\n\\mathbb{E}[H(\\hat{\\beta}, \\beta)]=\\sum_{1\\leq j\\leq \\lceil p/2\\rceil}\\mathbb{E}[H(\\hat{\\beta}_V, \\beta_V)]\\leq L_p p^{1-h^{**}(\\rho;\\vartheta,r)}, \\quad\\mbox{for the $\\hat{\\beta}$ in \\eqref{proof-equivalent-8}}.$$ This gives an upper bound for the minimax Hamming error.\n\nLast, we use [\\[proof-equivalent-7\\]](#proof-equivalent-7){reference-type=\"eqref\" reference=\"proof-equivalent-7\"} and [\\[proof-equivalent-15\\]](#proof-equivalent-15){reference-type=\"eqref\" reference=\"proof-equivalent-15\"} to show the claim. By combining these two inequalities, we know that, for the random design, $$\\inf_{\\hat{\\beta}} \\mathbb{E}[H(\\hat{\\beta}, \\beta)] = L_p p^{1-h^{**}(\\rho;\\vartheta,r)}.$$ A key observation is that the exponent $h^{**}(\\rho; \\vartheta,r)$ is only related to $\\Sigma$, not the realization of $X'X$. Now, we can mimic all the above derivations to get the same conclusion when the Gram matrix is fixed at $\\Sigma$ (the proof is very similar, except that we now have $G=\\Sigma$). Therefore, the minimax rates of the Hamming errors under two settings are exactly the same. \u00070◻\n\n[^1]: In [\\[linearM\\]](#linearM){reference-type=\"eqref\" reference=\"linearM\"}, we assume that each column of $X$ is standardized to have a unit $\\ell^2$-norm and that the order of nonzero $\\beta_j$ is $\\sqrt{\\log(n)}$. Alternatively, many works assume that each column of $X$ is standardized to have an $\\ell^2$-norm of $\\sqrt{n}$ and that the order for nonzero $\\beta_j$ is $n^{-1/2}\\sqrt{\\log(p)}$. These are two equivalent parameterizations."}],"approval":true,"conference":"iclr","rating":3,"year":2022,"id":"913ffd750d5ee3f5851be72d6bda20be932839a30844f9935da73e80eaa84a5f","y_true":1,"y_pred":1,"rationale_true":"","rationale_pred":"Paper Summary: This paper compares Lasso with five other variable selection methods (Elastic net, SCAD, thresholded Lasso, forward selection, and forward backward selection) by analyzing their expected Hamming errors under a theoretical framework. It derives rates of convergence for these methods, providing insights into their performance in high-dimensional settings.\n\nSupporting Evidence:\n- The paper provides a comprehensive review of LASSO and its extensions, emphasizing the importance of regularization and variable selection in high-dimensional regression models, which aligns with the Main Paper's focus. (from: High-Dimensional LASSO-Based Computational Regression Models: Regularization, Shrinkage, and Selection)\n- The paper highlights the importance of penalization methods, specifically the elastic net, which is also discussed in the Main Paper as an alternative to Lasso for variable selection, reinforcing the relevance of advanced penalization techniques. (from: A Computational Note on the Graphical Ridge in High-dimension)\n- The paper provides practical implementations of several variable selection methods discussed in the Main Paper, emphasizing the importance of model selection in high-dimensional settings, which aligns with the Main Paper's focus. (from: lassopack: Model selection and prediction with regularized regression in Stata)\n- The paper highlights the advantages of nonconvex penalty methods, such as SCAD, which are compared to Lasso in the context of variable selection, reinforcing the argument that nonconvex penalties are viable options for variable selection. (from: COORDINATE DESCENT ALGORITHMS FOR NONCONVEX PENALIZED REGRESSION, WITH APPLICATIONS TO BIOLOGICAL FEATURE SELECTION.)\n- The paper demonstrates that multi-step thresholding procedures can effectively estimate sparse vectors in high-dimensional settings, aligning with the Main Paper's exploration of alternative variable selection methods. (from: Thresholding Procedures for High Dimensional Variable Selection and Statistical Estimation)\n- The paper emphasizes the importance of variable selection in high-dimensional statistical modeling and introduces penalized likelihood approaches that address challenges faced by traditional methods like Lasso, reinforcing the Main Paper's findings. (from: Variable Selection via Nonconcave Penalized Likelihood and its Oracle Properties)\n- The paper provides a thorough analysis of two-stage variable selection techniques, which includes bridge estimators that can enhance the performance of variable selection methods like Lasso, reinforcing the Main Paper's conclusions. (from: Which bridge estimator is optimal for variable selection?)\n- The paper provides a crucial reference that supports the Main Paper's discussion on Lasso's solution path, strengthening the Main Paper's claims about Lasso's performance in variable selection. (from: Addendum: Regularization and variable selection via the elastic net)\n- The paper highlights the advantages of the Lasso method in variable selection, reinforcing the Main Paper's findings by discussing the favorable properties of Lasso, such as its stability and interpretability. (from: Regression Shrinkage and Selection via the Lasso)\n\nContradictory Evidence:\n- The paper emphasizes the limitations of Lasso in variable selection, advocating for bi-level selection methods that may offer improved accuracy, challenging the efficacy of Lasso in certain contexts. (from: SPARSE REGULARIZATION FOR BI-LEVEL VARIABLE SELECTION)\n- The paper focuses on the efficiency of Lasso solvers through screening rules, suggesting that the challenges faced by Lasso can be mitigated through innovative solver strategies, contrasting the Main Paper's findings. (from: Screening Rules for Lasso with Non-Convex Sparse Regularizers)\n- The paper critiques the Lasso method for its limitation of using a single tuning parameter, introducing the Variable Inclusion and Shrinkage Algorithms (VISA) as a more effective alternative, diverging from the Main Paper's focus. (from: Variable Inclusion and Shrinkage Algorithms)\n- The paper emphasizes the computational challenges associated with Lasso in high-dimensional settings, suggesting that marginal regression offers a faster alternative, questioning the efficiency of Lasso in practical applications. (from: A Comparison of the Lasso and Marginal Regression)\n- The paper highlights the computational challenges of traditional variable selection methods, suggesting that the methods compared in the Main Paper may not be suitable for ultra-high dimensional problems, which is a key point of divergence. (from: A Selective Overview of Variable Selection in High Dimensional Feature Space.)\n- The paper critiques the Main Paper's reliance on Lasso for variable selection, emphasizing that Lasso's effectiveness is contingent upon specific conditions, highlighting a potential limitation in the Main Paper's conclusions. (from: On Model Selection Consistency of Lasso)\n- The paper highlights the limitations of Lasso, particularly its bias in variable selection, suggesting that the MC+ method outperforms Lasso and other methods discussed in the Main Paper, presenting a significant divergence. (from: Nearly unbiased variable selection under minimax concave penalty)\n- The paper emphasizes the effectiveness of likelihood-based methods for feature selection, arguing for the superiority of nonconvex constrained likelihood approaches, which highlights a potential limitation in the Main Paper's reliance on traditional methods. (from: Likelihood-Based Selection and Sharp Parameter Estimation)\n\nKey Comparisons:\n- The paper's theoretical framework for comparing variable selection methods is unique in its focus on Hamming errors, which is not extensively covered in the existing literature.\n- The derivation of convergence rates for the methods compared provides a novel perspective on their performance, particularly in high-dimensional settings.\n\nConclusion: The paper presents a novel theoretical comparison of variable selection methods, particularly focusing on Hamming errors and convergence rates, which has not been extensively addressed in prior works. While there are critiques of Lasso and alternative methods proposed, the unique approach and insights provided by the paper contribute to the field of variable selection in a meaningful way.","structured_evaluation":{"paper_summary":"This paper compares Lasso with five other variable selection methods (Elastic net, SCAD, thresholded Lasso, forward selection, and forward backward selection) by analyzing their expected Hamming errors under a theoretical framework. It derives rates of convergence for these methods, providing insights into their performance in high-dimensional settings.","supporting_evidence":[{"text":"The paper provides a comprehensive review of LASSO and its extensions, emphasizing the importance of regularization and variable selection in high-dimensional regression models, which aligns with the Main Paper's focus.","paper_id":"S1","paper_title":"High-Dimensional LASSO-Based Computational Regression Models: Regularization, Shrinkage, and Selection","source":"citations"},{"text":"The paper highlights the importance of penalization methods, specifically the elastic net, which is also discussed in the Main Paper as an alternative to Lasso for variable selection, reinforcing the relevance of advanced penalization techniques.","paper_id":"S2","paper_title":"A Computational Note on the Graphical Ridge in High-dimension","source":"citations"},{"text":"The paper provides practical implementations of several variable selection methods discussed in the Main Paper, emphasizing the importance of model selection in high-dimensional settings, which aligns with the Main Paper's focus.","paper_id":"S3","paper_title":"lassopack: Model selection and prediction with regularized regression in Stata","source":"citations"},{"text":"The paper highlights the advantages of nonconvex penalty methods, such as SCAD, which are compared to Lasso in the context of variable selection, reinforcing the argument that nonconvex penalties are viable options for variable selection.","paper_id":"S4","paper_title":"COORDINATE DESCENT ALGORITHMS FOR NONCONVEX PENALIZED REGRESSION, WITH APPLICATIONS TO BIOLOGICAL FEATURE SELECTION.","source":"citations"},{"text":"The paper demonstrates that multi-step thresholding procedures can effectively estimate sparse vectors in high-dimensional settings, aligning with the Main Paper's exploration of alternative variable selection methods.","paper_id":"S5","paper_title":"Thresholding Procedures for High Dimensional Variable Selection and Statistical Estimation","source":"citations"},{"text":"The paper emphasizes the importance of variable selection in high-dimensional statistical modeling and introduces penalized likelihood approaches that address challenges faced by traditional methods like Lasso, reinforcing the Main Paper's findings.","paper_id":"S6","paper_title":"Variable Selection via Nonconcave Penalized Likelihood and its Oracle Properties","source":"citations"},{"text":"The paper provides a thorough analysis of two-stage variable selection techniques, which includes bridge estimators that can enhance the performance of variable selection methods like Lasso, reinforcing the Main Paper's conclusions.","paper_id":"S7","paper_title":"Which bridge estimator is optimal for variable selection?","source":"citations"},{"text":"The paper provides a crucial reference that supports the Main Paper's discussion on Lasso's solution path, strengthening the Main Paper's claims about Lasso's performance in variable selection.","paper_id":"S8","paper_title":"Addendum: Regularization and variable selection via the elastic net","source":"citations"},{"text":"The paper highlights the advantages of the Lasso method in variable selection, reinforcing the Main Paper's findings by discussing the favorable properties of Lasso, such as its stability and interpretability.","paper_id":"S9","paper_title":"Regression Shrinkage and Selection via the Lasso","source":"citations"}],"contradictory_evidence":[{"text":"The paper emphasizes the limitations of Lasso in variable selection, advocating for bi-level selection methods that may offer improved accuracy, challenging the efficacy of Lasso in certain contexts.","paper_id":"C1","paper_title":"SPARSE REGULARIZATION FOR BI-LEVEL VARIABLE SELECTION","source":"citations"},{"text":"The paper focuses on the efficiency of Lasso solvers through screening rules, suggesting that the challenges faced by Lasso can be mitigated through innovative solver strategies, contrasting the Main Paper's findings.","paper_id":"C2","paper_title":"Screening Rules for Lasso with Non-Convex Sparse Regularizers","source":"citations"},{"text":"The paper critiques the Lasso method for its limitation of using a single tuning parameter, introducing the Variable Inclusion and Shrinkage Algorithms (VISA) as a more effective alternative, diverging from the Main Paper's focus.","paper_id":"C3","paper_title":"Variable Inclusion and Shrinkage Algorithms","source":"citations"},{"text":"The paper emphasizes the computational challenges associated with Lasso in high-dimensional settings, suggesting that marginal regression offers a faster alternative, questioning the efficiency of Lasso in practical applications.","paper_id":"C4","paper_title":"A Comparison of the Lasso and Marginal Regression","source":"citations"},{"text":"The paper highlights the computational challenges of traditional variable selection methods, suggesting that the methods compared in the Main Paper may not be suitable for ultra-high dimensional problems, which is a key point of divergence.","paper_id":"C5","paper_title":"A Selective Overview of Variable Selection in High Dimensional Feature Space.","source":"citations"},{"text":"The paper critiques the Main Paper's reliance on Lasso for variable selection, emphasizing that Lasso's effectiveness is contingent upon specific conditions, highlighting a potential limitation in the Main Paper's conclusions.","paper_id":"C6","paper_title":"On Model Selection Consistency of Lasso","source":"citations"},{"text":"The paper highlights the limitations of Lasso, particularly its bias in variable selection, suggesting that the MC+ method outperforms Lasso and other methods discussed in the Main Paper, presenting a significant divergence.","paper_id":"C7","paper_title":"Nearly unbiased variable selection under minimax concave penalty","source":"citations"},{"text":"The paper emphasizes the effectiveness of likelihood-based methods for feature selection, arguing for the superiority of nonconvex constrained likelihood approaches, which highlights a potential limitation in the Main Paper's reliance on traditional methods.","paper_id":"C8","paper_title":"Likelihood-Based Selection and Sharp Parameter Estimation","source":"citations"}],"key_comparisons":["The paper's theoretical framework for comparing variable selection methods is unique in its focus on Hamming errors, which is not extensively covered in the existing literature.","The derivation of convergence rates for the methods compared provides a novel perspective on their performance, particularly in high-dimensional settings."],"conclusion":"The paper presents a novel theoretical comparison of variable selection methods, particularly focusing on Hamming errors and convergence rates, which has not been extensively addressed in prior works. While there are critiques of Lasso and alternative methods proposed, the unique approach and insights provided by the paper contribute to the field of variable selection in a meaningful way.","label":1,"novel":"yes","probability":0.999999997664407,"rationale":"Paper Summary: This paper compares Lasso with five other variable selection methods (Elastic net, SCAD, thresholded Lasso, forward selection, and forward backward selection) by analyzing their expected Hamming errors under a theoretical framework. It derives rates of convergence for these methods, providing insights into their performance in high-dimensional settings.\n\nSupporting Evidence:\n- The paper provides a comprehensive review of LASSO and its extensions, emphasizing the importance of regularization and variable selection in high-dimensional regression models, which aligns with the Main Paper's focus. (from: High-Dimensional LASSO-Based Computational Regression Models: Regularization, Shrinkage, and Selection)\n- The paper highlights the importance of penalization methods, specifically the elastic net, which is also discussed in the Main Paper as an alternative to Lasso for variable selection, reinforcing the relevance of advanced penalization techniques. (from: A Computational Note on the Graphical Ridge in High-dimension)\n- The paper provides practical implementations of several variable selection methods discussed in the Main Paper, emphasizing the importance of model selection in high-dimensional settings, which aligns with the Main Paper's focus. (from: lassopack: Model selection and prediction with regularized regression in Stata)\n- The paper highlights the advantages of nonconvex penalty methods, such as SCAD, which are compared to Lasso in the context of variable selection, reinforcing the argument that nonconvex penalties are viable options for variable selection. (from: COORDINATE DESCENT ALGORITHMS FOR NONCONVEX PENALIZED REGRESSION, WITH APPLICATIONS TO BIOLOGICAL FEATURE SELECTION.)\n- The paper demonstrates that multi-step thresholding procedures can effectively estimate sparse vectors in high-dimensional settings, aligning with the Main Paper's exploration of alternative variable selection methods. (from: Thresholding Procedures for High Dimensional Variable Selection and Statistical Estimation)\n- The paper emphasizes the importance of variable selection in high-dimensional statistical modeling and introduces penalized likelihood approaches that address challenges faced by traditional methods like Lasso, reinforcing the Main Paper's findings. (from: Variable Selection via Nonconcave Penalized Likelihood and its Oracle Properties)\n- The paper provides a thorough analysis of two-stage variable selection techniques, which includes bridge estimators that can enhance the performance of variable selection methods like Lasso, reinforcing the Main Paper's conclusions. (from: Which bridge estimator is optimal for variable selection?)\n- The paper provides a crucial reference that supports the Main Paper's discussion on Lasso's solution path, strengthening the Main Paper's claims about Lasso's performance in variable selection. (from: Addendum: Regularization and variable selection via the elastic net)\n- The paper highlights the advantages of the Lasso method in variable selection, reinforcing the Main Paper's findings by discussing the favorable properties of Lasso, such as its stability and interpretability. (from: Regression Shrinkage and Selection via the Lasso)\n\nContradictory Evidence:\n- The paper emphasizes the limitations of Lasso in variable selection, advocating for bi-level selection methods that may offer improved accuracy, challenging the efficacy of Lasso in certain contexts. (from: SPARSE REGULARIZATION FOR BI-LEVEL VARIABLE SELECTION)\n- The paper focuses on the efficiency of Lasso solvers through screening rules, suggesting that the challenges faced by Lasso can be mitigated through innovative solver strategies, contrasting the Main Paper's findings. (from: Screening Rules for Lasso with Non-Convex Sparse Regularizers)\n- The paper critiques the Lasso method for its limitation of using a single tuning parameter, introducing the Variable Inclusion and Shrinkage Algorithms (VISA) as a more effective alternative, diverging from the Main Paper's focus. (from: Variable Inclusion and Shrinkage Algorithms)\n- The paper emphasizes the computational challenges associated with Lasso in high-dimensional settings, suggesting that marginal regression offers a faster alternative, questioning the efficiency of Lasso in practical applications. (from: A Comparison of the Lasso and Marginal Regression)\n- The paper highlights the computational challenges of traditional variable selection methods, suggesting that the methods compared in the Main Paper may not be suitable for ultra-high dimensional problems, which is a key point of divergence. (from: A Selective Overview of Variable Selection in High Dimensional Feature Space.)\n- The paper critiques the Main Paper's reliance on Lasso for variable selection, emphasizing that Lasso's effectiveness is contingent upon specific conditions, highlighting a potential limitation in the Main Paper's conclusions. (from: On Model Selection Consistency of Lasso)\n- The paper highlights the limitations of Lasso, particularly its bias in variable selection, suggesting that the MC+ method outperforms Lasso and other methods discussed in the Main Paper, presenting a significant divergence. (from: Nearly unbiased variable selection under minimax concave penalty)\n- The paper emphasizes the effectiveness of likelihood-based methods for feature selection, arguing for the superiority of nonconvex constrained likelihood approaches, which highlights a potential limitation in the Main Paper's reliance on traditional methods. (from: Likelihood-Based Selection and Sharp Parameter Estimation)\n\nKey Comparisons:\n- The paper's theoretical framework for comparing variable selection methods is unique in its focus on Hamming errors, which is not extensively covered in the existing literature.\n- The derivation of convergence rates for the methods compared provides a novel perspective on their performance, particularly in high-dimensional settings.\n\nConclusion: The paper presents a novel theoretical comparison of variable selection methods, particularly focusing on Hamming errors and convergence rates, which has not been extensively addressed in prior works. While there are critiques of Lasso and alternative methods proposed, the unique approach and insights provided by the paper contribute to the field of variable selection in a meaningful way."},"arxiv_id":"2203.15075"},"terms":{"tasks":["variable selection in linear models","comparing methods for variable selection","measuring performances of variable selection methods"],"methods":["Lasso","Elastic net","SCAD","forward selection","thresholded Lasso","forward backward selection"],"metrics":["expected Hamming error"],"resources":[],"relations":[{"head":"Lasso","tail":"variable selection in linear models"},{"head":"Elastic net","tail":"variable selection in linear models"},{"head":"SCAD","tail":"variable selection in linear models"},{"head":"forward selection","tail":"variable selection in linear models"},{"head":"thresholded Lasso","tail":"variable selection in linear models"},{"head":"forward backward selection","tail":"variable selection in linear models"},{"head":"expected Hamming error","tail":"measuring performances of variable selection methods"}]},"background":"Lasso is a celebrated method for variable selection in linear models, but it faces challenges when the covariates are moderately or strongly correlated. This motivates alternative approaches such as using a non-convex penalty, adding a ridge regularization, or conducting a post-Lasso thresholding.","target":"In this paper, we compare Lasso with 5 other methods: Elastic net, SCAD, forward selection, thresholded Lasso, and forward backward selection. We measure their performances theoretically by the expected Hamming error, assuming that the regression coefficients are ${\rm iid}$ drawn from a two-point mixture and that the Gram matrix is block-wise diagonal. By deriving the rates of convergence of Hamming errors and the phase diagrams, we obtain useful conclusions about the pros and cons of different methods."}]
