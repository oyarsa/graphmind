# Generative SFT config for Qwen3-32B with graph context - full-length (A100 required)
# Requires A100 80GB GPU - use fleche run train_gen_graph --constraint a100

[model]
name = "Qwen/Qwen3-32B"
quantisation_enabled = true
input_mode = "graph"

[lora]
r = 8
alpha = 16
dropout = 0.1
target_modules = ["q_proj", "k_proj", "v_proj"]

[training]
batch_size = 1  # Minimal batch size for large model with long sequences
learning_rate = 2e-5  # Lower LR for larger model
num_epochs = 1  # Match Llama Gen Graph
warmup_steps = 50
weight_decay = 0.01
max_length = 2000  # Match Llama Gen Graph config for fair comparison
logging_steps = 10
max_new_tokens = 256

[generation]
max_new_tokens = 256
temperature = 0.1
do_sample = true
top_p = 0.9
