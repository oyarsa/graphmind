[
  {
    "title": "One Pass Streaming Algorithm for Super Long Token Attention Approximation in Sublinear Space",
    "abstract": "Attention computation takes both the time complexity of $O(n^2)$ and the space complexity of $O(n^2)$ simultaneously, which makes deploying Large Language Models (LLMs) in streaming applications that involve long contexts requiring substantial computational resources. In recent OpenAI DevDay (Nov 6, 2023), OpenAI released a new model that is able to support a 128K-long document, in our paper, we focus on the memory-efficient issue when context length $n$ is much greater than 128K ($n \\gg 2^d$). Considering a single-layer self-attention with Query, Key, and Value matrices $Q, K, V \\in \\mathbb{R}^{n \\times d}$, the polynomial method approximates the attention output $T \\in \\mathbb{R}^{n \\times d}$. It accomplishes this by constructing $U_1, U_2 \\in \\mathbb{R}^{n \\times t}$ to expedite attention ${\\sf Attn}(Q, K, V)$  computation within $n^{1+o(1)}$ time executions. Despite this, computing the approximated attention matrix $U_1U_2^\\top \\in \\mathbb{R}^{n \\times n}$ still necessitates $O(n^2)$ space, leading to significant memory usage. In response to these challenges, we introduce a new algorithm that only reads one pass of the data in a streaming fashion. This method employs sublinear space  $o(n)$ to store three sketch matrices, alleviating the need for exact $K, V$ storage. Notably, our algorithm exhibits exceptional memory-efficient performance with super-long tokens. As the token length $n$ increases, our error guarantee diminishes while the memory usage remains nearly constant. This unique attribute underscores the potential of our technique in efficiently handling LLMs in streaming applications.",
    "text": "",
    "rationale": "Summary: In this work, the authors study improving the time and space complexity of the attention mechanism in the non-causal setting. They give a single pass streaming algorithm that given the query, key and value vectors approximates the attention output using tools from randomized linear algebra and compressed sensing. At a high level, the algorithm computes sketches of low rank matrices $U_1, U_2 \\in \\mathbb{R}^{n \\times k}$ so that sparse approximations for the columns of the attention output $\\exp(QK^T / d)V$ can be computed using the sketches of the low rank matrices. The space required by the algorithm is $n^{o(1)}$ and the time to process the streams is $n^{1+o(1)}$.\n\nStrengths: --\n\nWeaknesses: 1. Attention computation can be done without using $O(n^2)$ space. See Rabe and Staats \"Self-Attention does not need $O(n^2)$ memory\".\n2. The version of the problem being studied i.e., without using causal masking is not super relevant to the current LLMs which all use causal masking.\n3. As stated, the results only output at most $k \\cdot d$ nonzero entries for the entire matrix $V$ which has $n$ rows. Since we expect each of the rows in the full attention output to have similar norms, many useful rows are completely marked to zero by this algorithm unless $k \\ge n/d$ at which point the sketches stored by the algorithm are linear in size, at which point the claim of streaming algorithm does not make sense and the algorithm is essentially no different from Alman and Song.\n4. No experimental verification of the ideas. At least showing how the algorithm works on small instances, even without implementing the additional sparse recovery steps would have been useful.\n\nQuestions: --",
    "rating": 1,
    "label": false
  },
  {
    "title": "TIES-Merging: Resolving Interference When Merging Models",
    "abstract": "Transfer learning \u2013 i.e., further fine-tuning a pre-trained model on a downstream task \u2013 can confer significant advantages, including improved downstream performance, faster convergence, and better sample efficiency. These advantages have led to a proliferation of task-specific fine-tuned models, which typically can only perform a single task and do not benefit from one another. Recently, model merging techniques have emerged as a solution to combine multiple task-specific models into a single multitask model without performing additional training. However, existing merging methods often ignore the interference between parameters of different models, resulting in large performance drops when merging multiple models. In this paper, we demonstrate that prior merging techniques inadvertently lose valuable information due to two major sources of interference: (a) interference due to redundant parameter values and (b) disagreement on the sign of a given parameter\u2019s values across models. To address this, we propose our method, TrIm, Elect Sign & Merge (TIES-Merging), which introduces three novel steps when merging models: (1) resetting parameters that only changed a small amount during fine-tuning, (2) resolving sign conflicts, and (3) merging only the parameters that are in alignment with the final agreed-upon sign. We find that TIES-Merging outperforms existing methods in diverse settings covering a range of modalities, domains, number of tasks, model sizes, architectures, and fine-tuning settings. We further analyze the impact of different types of interference on model parameters, highlight the importance of signs, and show that estimating the signs using the validation data could further improve performance.",
    "text": "",
    "rationale": "Summary: The paper presents a novel method, TIES-MERGING, to merge models in the weight space for multitask learning. It observes an interference problem when linearly interpolating weights, and proposes a simple yet effective two-step solution: parameter trimming for small changes during fine-tuning and sign conflict resolution. The experiments in multitask shows that TIES improves performances, making it a notable (experimental) contribution to the literature of model merging.\n\nStrengths: * The paper's main strengths lie in its simplicity, as highlighted by its clear description and illustration.\n* The paper successfully builds upon the \"task arithmetic\" task vector approach to report a new interference phenomenon, and then enhances performance with simple yet important modifications. Model merging is an important topic in multitask, and this paper fills a crucial gap in the current literature.\n* The experimental framework is robust, with a focus on significant large-scale tasks across CV and NLP domains using recent architectures and fair benchmarks.\n\nWeaknesses: Despite its strengths, some areas require attention.\n* The contributions, though valuable, are incremental, and the observed gains in multitask learning are consistent but arguably marginal, and the trimming/sign play only a marginal role in this gain.\n* The experiments focus solely on models trained on different tasks. Yet, weight averaging is also useful to merge models trained on a single target task; on this model soups setup, I speculate that sign interference is less an issue, and that TIES would actually be detrimental as it would increase variance, thus limiting the benefits from combining multiple models, in particular under distribution shifts. As a minimum fix, the title should reflect this specificity, as the current one does not adequately reflect this scope limitation. A (naive) suggestion could be \"Resolving Multitask Interference When Merging Models\".\n* Even whithin the multitask challenge, the experiments do not cover two important scenarios. First, multitask as better pretraining before fine-tuning on a target task (as in \"Fusing finetuned models for better pretraining\"). Second, multitask in the sequential patching scenario (as in \"Patching open-vocabulary models by interpolating weight\").\n* Lack of analysis and clarity of the interpolating coefficient, for TIES and for the baselines. Specifically, without validation samples, could you clarify which values $\\lambda$ is used: it seems that it's $\\lambda=1/|num tasks|$ for weight averaging, $\\lambda=0.4$ for task arithmetic (could you please point where you found this value), but $\\lambda=1$ for TIES. Therefore, is the difference between task arithmetic/weight averaging in Table 1 simply due to the use of different $\\lambda$? Thus (as further suggested from Table 3), scaling is an important factor, these different values of $\\lambda$ hidden the true impact of your core contributions.\n* Similarly, the ablation study in Table 3 could be made clearer. It is not clear whether the ablations are done one at a time or sequentially all together. If the latter is true, then why do we recover 65.5 on T5-base (when all 4 components are removed), while weight averaging performs 65.9? Moreover, what does it mean to remove \"elect\" while keeping \"disjoint mean\"? what does it mean to remove \"scaling\" (is it $\\lambda=1/|num tasks|$ or something different)?\n\nQuestions: * What is the impact of task similarity on the number of sign conflicts? Does the number of sign conflicts decrease when two models are fine-tuned on a shared task?\n* Can you visualize the number of sign conflicts for different trimming ratios?\n* Can TIES improves downstream transfer learning performances?\n* Could you plot the curve performance while varying the coefficients given to different tasks: see Pareto curves in \"Patching open-vocabulary models by interpolating weight\", but also in \"Pareto manifold learning: tackling multiple tasks via ensembles of single-task models\", a missing yet important related work.\n* Could you enrich the ablation study to further clarify/highlight the different impacts of the key contributions.\n* How does ensembling of predictions behave in comparison with weight interpolation?\n\nLimitations: The authors have highlighted in Sections 7.3 and 7.4 that a key limitation is to accurately elect the sign.\nYet, the paper would benefit from a dedicated limitation section. It should ideally include that:\n- weight interpolation lacks proper theoretical understanding,\n- their approach is only (verified) for multitask learning,\n- the averageability relies on a large pretraining and \"good\" hyperparams,\n- TIES may be less impactful in a \"sequential patching\" scenario,\n- they still lag behind the simultaneous multitask learning.",
    "rating": 2,
    "label": false
  },
  {
    "title": "Boosting Backdoor Attack with A Learnable Poisoning Sample Selection Strategy",
    "abstract": "Data-poisoning based backdoor attacks aim to inject backdoor into models by manipulating training datasets without controlling the training process of the target model. Existing backdoor attacks mainly focus on designing diverse triggers or fusion strategies to generate poisoned samples. However, all these attacks randomly select samples from the benign dataset to be poisoned, disregarding the varying importance of different samples. In order to select important samples to be poisoned from a global perspective, we first introduce a learnable poisoning mask into the regular backdoor training loss. Then we propose a Learnable Poisoning sample Selection (LPS) strategy to learn the mask through a min-max optimization. During the two-player game, considering hard samples contribute more to the training process, the inner optimization maximizes loss  w.r.t. the mask to identify hard poisoned samples by impeding the training objective, while the outer optimization minimizes the loss w.r.t. the model\u2019s weight to train the surrogate model. After several rounds of adversarial training,  we finally select poisoned samples with high contribution. Extensive experiments on benchmark datasets demonstrate the effectiveness and efficiency of our LPS strategy in boosting the performance of various data-poisoning based backdoor attacks.",
    "text": "",
    "rationale": "Summary: The authors present a sample selection method for data poisoning aimed at enhancing backdoor attacks. A min-max optimization technique is employed to learn a poisoning mask for selecting the appropriate samples.\n\nStrengths: Pros:\n- The manuscript is well-organized and easy to follow.\n- Although the idea of sample selection for poisoning is conceptually similar to the FUS method, the two approaches diverge in their perspectives. While FUS focuses on local optimization, the proposed method aims for global sample selection.\n- The empirical results are robust and substantiate the paper's claims effectively.\n\nWeaknesses: Cons:\n- The code for replication is not provided, limiting the paper's reproducibility.\n- The significant training loss gap between poisoned and clean samples might make the attack easily detectable by potential victims.\n- Ethic statement is missing.\n\nQuestions: - Does the threshold \"T\" vary across different datasets and model architectures?\n- Is the proposed approach effective for the combination of CNN-based surrogate models and attention-based target models?\n- Could the authors clarify why the method underperforms when the poisoning rate is low?\n- For the ablation study, could the authors provide results of LPS\\PC?",
    "rating": 3,
    "label": true
  },
  {
    "title": "Stochastic Taylor Derivative Estimator: Efficient amortization for arbitrary differential operators",
    "abstract": "Optimizing neural networks with loss that contain high-dimensional and high-order differential operators\n  is expensive to evaluate with back-propagation due to $\\mathcal{O}(d^{k})$ scaling of the derivative tensor size and the $\\mathcal{O}(2^{k-1}L)$ scaling in the computation graph, where $d$ is the dimension of the domain, $L$ is the number of ops in the forward computation graph, and $k$ is the derivative order. In previous works, the polynomial scaling in $d$ was addressed by amortizing the computation over the optimization process via randomization. Separately, the exponential scaling in $k$ for univariate functions ($d=1$) was addressed with high-order auto-differentiation (AD). In this work, we show how to efficiently perform arbitrary contraction of the derivative tensor of arbitrary order for multivariate functions, by properly constructing the input tangents to univariate high-order AD, which can be used to efficiently randomize any differential operator.\n  When applied to Physics-Informed Neural Networks (PINNs), our method provides >1000$\\times$ speed-up and >30$\\times$ memory reduction over randomization with first-order AD, and we can now solve 1-million-dimensional PDEs in 8 minutes on a single NVIDIA A100 GPU. This work opens the possibility of using high-order differential operators in large-scale problems.",
    "text": "",
    "rationale": "Summary: The paper addresses the computational challenges of optimizing neural networks with loss functions that include high-dimensional and high-order differential operators. These challenges arise due to the scaling of the derivative tensor size with the dimension of the domain (d) and the computational graph's size with the number of operations (L) and the order of the derivative (k). Traditional methods either amortize the computational cost over the optimization process via randomization or use high-order auto-differentiation (AD) for univariate functions to tackle these issues.\n\nIn this work, the authors propose a method to efficiently perform arbitrary contraction of the derivative tensor for multivariate functions. This is achieved by constructing input tangents to univariate high-order AD, enabling efficient randomization of any differential operator. When applied to Physics-Informed Neural Networks (PINNs), this approach provides significant speed and memory efficiency improvements, achieving over 1000 times speed-up and 30 times memory reduction compared to randomization with first-order AD. The method allows solving 1-million-dimensional partial differential equations (PDEs) in just 8 minutes on a single NVIDIA A100 GPU, opening up the possibility of using high-order differential operators in large-scale problems.\n\nStrengths: The paper introduces STDE, a general method for constructing stochastic estimators for arbitrary differential operators, which can be efficiently evaluated using Taylor mode auto-differentiation (AD). When evaluated on Physics-Informed Neural Networks (PINNs), a specific optimization problem where the loss function includes differential operators, STDE significantly outperforms baseline methods. Furthermore, STDE's applicability extends beyond PINNs to arbitrarily high-order and high-dimensional AD-based PDE solvers, making it more general than related methods.\n\nThe strengths of this paper are as follows:\n\nGenerality: STDE can be applied to a wide range of problems, including those involving arbitrarily high-order and high-dimensional differential operators. This broad applicability distinguishes STDE from other methods, which are often restricted to specific forms of second-order PDEs.\n\nEfficiency: The method enables efficient evaluation of stochastic estimators through Taylor mode AD, providing significant computational benefits.\n\nPerformance: In practical evaluations on PINNs, STDE outperforms baseline methods in terms of both speed and memory efficiency. It demonstrates over 1000 times speed-up and 30 times memory reduction compared to first-order AD randomization.\n\nScalability: STDE allows for the solution of extremely large-scale problems, such as 1-million-dimensional PDEs, in a matter of minutes on advanced hardware like the NVIDIA A100 GPU.\n\nVersatility: Beyond PINNs, STDE can be applied to various AD-based PDE solvers, making it a versatile tool for tackling a broad spectrum of differential operator-based optimization problems.\n\nOverall, STDE's generality, efficiency, performance, scalability, and versatility make it a powerful method for addressing high-dimensional and high-order differential operator challenges in neural network optimization.\n\nWeaknesses: While the paper demonstrates the significant strengths and broad applicability of STDE, it is important to acknowledge some limitations and areas for future improvement. As a general method, STDE may not leverage the specific optimization possibilities that are available for particular operators. Additionally, the study did not explore variance reduction techniques, which could potentially enhance the method's performance and could be a promising area for future research.\n\nAnother observation is that while reducing the randomization batch size improves both the speed and memory profile of STDE, this comes with a trade-off in the form of increased computational variance. Further analysis is required to understand and optimize this balance between computational efficiency and variance.\n\nLooking ahead, the paper identifies an intriguing connection between the fields of automatic differentiation (AD) and randomized numerical linear algebra, highlighting the potential for future work at this intersection. Such research could lead to significant advancements in large-scale scientific modeling with neural networks.\n\nIn summary, while there are areas for refinement, the contributions of this paper are substantial. The development of STDE as a general and efficient method for constructing stochastic estimators for arbitrary differential operators is a notable achievement, offering substantial benefits for high-dimensional and high-order differential operator problems in neural network optimization. The identified limitations and future research directions provide a clear path for further enhancing this already impressive work.\n\nQuestions: Given the importance of various other complex equations in scientific modeling, I am curious about the applicability of STDE to equations such as the Nonlinear Schr\u00f6dinger Equation (NLS), the fourth-order NLS, and the Navier-Stokes equations. Could you elaborate on how STDE might perform or be adapted for these specific cases? Additionally, are there any preliminary results or theoretical considerations you could share regarding the application of STDE to these important equations?\n\nLimitations: Yes, they have.",
    "rating": 4,
    "label": true
  }
]
