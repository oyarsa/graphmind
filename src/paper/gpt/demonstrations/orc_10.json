[
  {
    "title": "dMel: Speech Tokenization made Simple",
    "abstract": "Large language models have revolutionized natural language processing by leveraging self-supervised pretraining on vast textual data.\n  Inspired by this success, researchers have investigated complicated speech tokenization methods to discretize continuous speech signals so that language modeling techniques can be applied to speech data.\n  However, existing approaches either model semantic (content) tokens, potentially losing acoustic information, or model acoustic tokens, risking the loss of semantic (content) information. \n  Having multiple token types also complicates the architecture and requires additional pretraining.\n  Here we show that discretizing mel-filterbank channels into discrete intensity bins produces a simple representation (dMel), that performs better than other existing speech tokenization methods.\n  Using an LM-style transformer architecture for speech-text modeling, we comprehensively evaluate different speech tokenization methods on speech recognition (ASR) and speech synthesis (TTS).\n  Our results demonstrate the effectiveness of dMel in achieving high performance on both tasks within a unified framework, paving the way for efficient and effective joint modeling of speech and text. The code is available at anonymous_url, while generation samples are in the supplementary materials.",
    "text": "# Introduction\n\nLarge language models (LLMs) have achieved remarkable success in various natural language processing tasks by leveraging self-supervised pretraining on massive amounts of textual data [@brown2020language]. Inspired by this success, numerous works [@borsos2023audiolm; @rubenstein2023audiopalm; @zhang2023speechgpt; @wang2023neural] have sought to extend the language modeling approach to speech processing, aiming to build unified models capable of both speech understanding and generation tasks. However, a key challenge lies in the continuous nature of speech signals, necessitating effective tokenization methods to discretize the input for language model-based processing.\n\nCurrent speech tokenization approaches can be broadly categorized into two types: semantic (content) tokens and acoustic tokens[^1]. Semantic tokens, extracted from self-supervised (SSL) pretrained speech models [@baevski2020wav2vec; @hsu2021hubert], where the speech signal is first encoded into speech representations and then clustered into semantic tokens with $k$-means method. However, such SSL pretrained models are not useful for high fidelity speech synthesis as speaker identity and other details of raw speech are lost in training [@borsos2023audiolm]. Conversely, acoustic tokens can be obtained from audio compression models that are trained to compress the speech signal into codebook indices with residual vector quantization (RVQ) and reconstruction objectives [@zeghidour2021soundstream; @defossez2022high]. These tokens prioritize acoustic reconstruction but lose semantic information which can lead to poorer results in generating audio [@wang2023neural].\n\nTo combine the advantages of both semantic and acoustic tokens, AudioLM [@borsos2023audiolm] proposed to model both semantic tokens and acoustic tokens with 3 stages: semantic modeling, coarse acoustic modeling, and fine acoustic modeling. The coarse-to-fine modeling strategy is designed to match the residual structure of RVQ based acoustic tokens. This solution addresses both content and speech quality, but its multi-stage hierarchical structure complicates the model and can lead to slower training and inference. Another solution is to combine the semantic and acoustic features together. @zhang2023speechtokenizer proposed to distill the semantic tokens into the acoustic token's first residual channel during the training of the RVQ model in a teacher-student manner. In this way, the new feature can preserve the semantic information better and also reconstruct high quality speech signals.\n\nIn this paper, we raise the following fundamental question -- ***do we really need to separate speech into semantic and acoustic tokens first, and process them with idiosyncratic architectures?*** We propose a simple alternative called `dMel`(see Figure [1](#fig:dmel){reference-type=\"ref\" reference=\"fig:dmel\"}) that discretizes log mel-filterbanks (Mel) energies directly into ordinal bins. Intriguingly, we find that discretizing Mel has little impact on the ability of off-the-shelf Mel vocoders to reconstruct waveforms[^2]. In Table [\\[tab:teaser\\]](#tab:teaser){reference-type=\"ref\" reference=\"tab:teaser\"} we show different vocoders to reconstruct waveforms from Mel and discretized Mel (`dMel`) computed on them, as well as ASR models trained on Mel and `dMel`.\n\n::: wraptable\nr7.5cm\n\n  -------------- ------------------------ ---------------- --------------------- ------------\n                  Reconstruction WER (%)                    Recognition WER (%)  \n                     **P-WaveGAN^1^**      **HifiGAN^2^**     **Seq2seq^3^**      **CTC^4^**\n  Ground-truth             2.02                                     \\-           \n  Mel                      2.13                 2.08                2.4              2.1\n  `dMel`                   2.23                 2.11                2.5              2.1\n  -------------- ------------------------ ---------------- --------------------- ------------\n\n^\\*^ [@yamamoto2020parallel]^1^, [@kong2020hifi]^2^, [@dong2018speech]^3^, [@graves2006connectionist]^4^ Configurations are detailed in Sec. [3](#sec:exp){reference-type=\"ref\" reference=\"sec:exp\"}.\n:::\n\nWe find that the word error rate (WER) of an ASR system run on the reconstructed waveforms, is quite similar to the WER of the same system run on the ground-truth audio, showing that `dMel` captures the acoustic information needed to reconstruct good waveforms. Similarly, we find that the WER of ASR models trained on Mel and `dMel` are similar, indicating that `dMel` are good at preserving semantic content that can easily be found by ASR models. This shows that discretizing Mel has limited impact on information content.\n\nBy operating on the log mel-filterbanks and preserving the frequency and intensity information (with some loss of resolution from discretization), `dMel`*inherently preserves both semantic and acoustic information in a unified representation*, without the need for separate tokenization or additional pretraining of a tokenization model. There are many advantages to discretizing log mel-filterbanks:\n\n-   Log mel-filterbanks is an interpretable representation of speech signal, where both the semantic and acoustic information is preserved. As discretization has little impact, `dMel` inherits the properties.\n\n-   `dMel` is a *model-free representation grounded in raw acoustic space*. As a result it can be converted to waveforms by any mel-filterbank vocoder, unlike other tokenization schemes that have feature representations that are intricately coupled to both the encoder and the decoder.\n\n-   Different channels of `dMel` do not have the complex hierarchical dependencies on each other that is typical of coarse-to-fine acoustic tokens; we find that they can be ***modeled independently*** in each frame using a simple decoder-only (LM-style) transformer architecture.\n\nThrough comprehensive evaluations, we show that using `dMel` allows us to use a single decoder-only model, and achieve high performance on both automatic speech recognition (ASR) and text-to-speech (TTS) tasks. The ASR task validates that `dMel` preserves semantic information, while the TTS task shows that `dMel` are useful for high-fidelity acoustic reconstruction of speech. We also compare `dMel` to other tokenization methods and find that `dMel` achieves the best WER for ASR task, which indicates that the semantic information is well preserved. Also, `dMel` achieves the lower WER score for TTS task when using WhisperX [@bain2022whisperx] for automatic evaluation and we find model trained with `dMel` can generate long and natural speech samples (see Supplementary Material).\n\n![Prior works on speech tokenization use either heavy self-supervised pretrained encoders [@baevski2020wav2vec; @hsu2021hubert] to extract semantic tokens (and train a separate decoder for it [@lakhotia2021generative]) or learn compression encoder-decoder models with residual vector quantizations [@zeghidour2021soundstream; @defossez2022high] to obtain acoustic tokens. By contrast we eliminate the encoder and simply discretize mel-filerbanks (`dMel`) to encode audio, and use a simple mel-filterbank vocoder [@yamamoto2020parallel] to reconstruct speech signals. ](figs/dmel.pdf){#fig:dmel width=\"80%\"}\n\n# Method\n\nIn this section, we first introduce our proposed `dMel` speech tokenization method, which discretizes log mel-filterbanks energies directly into bins. We then describe our unified LM-style transformer model for ASR and TTS tasks, which leverages `dMel` for speech tokenization. The model architecture is illustrated in Figure [2](#fig:arch){reference-type=\"ref\" reference=\"fig:arch\"}.\n\n## `dMel` Speech Tokenizer\n\nDifferent from existing VQ-VAE [@borsos2023audiolm; @zhang2023speechtokenizer; @kim2024clamtts; @zeghidour2021soundstream] based speech tokenizers, we propose a discretized log mel-filterbanks based speech tokenizer. The outline of the discretization method is shown in Figure [1](#fig:dmel){reference-type=\"ref\" reference=\"fig:dmel\"}. Later in the paper, we show that this tokenizer allows the model to process the input speech signal efficiently and capture the relevant acoustic features for both ASR and TTS tasks.\n\nWe denote tensors as $\\mathbf{X}$ while $\\mathbf{X}_{i, ...}$ denote the $(i, ...)$-th component of tensor $\\mathbf{X}$. First, the speech tokenizer takes the input speech signal $\\mathbf{x}$ and computes the log mel-filterbanks representation $\\mathbf{M}$: $$\\mathbf{M} = \\text{Mel}(\\mathbf{x}),$$ where $\\text{Mel}(\\cdot)$ represents the function that computes the log mel-filterbanks, $\\mathbf{M}\\in\\mathbb{R}^{T\\times N}$, $N$ is the number of log mel-filterbanks and $T$ is the number of frames in the spectrogram.\n\n#### Tokenization\n\nTo discretize the log mel-filterbanks representation $\\mathbf{M}$ into speech tokens, we adopt a codebook $\\mathbf{C}$. In this paper, we apply a simple linear discretization, so that the codebook $\\mathbf{C}\\in\\mathbb{R}^{2^K}$ and its values are evenly spaced in the range of the log mel-filterbanks values: $$m=\\min_{t, i}(\\mathbf{M}_{t, i}), \n   \\qquad\n   M=\\max_{t, i}(\\mathbf{M}_{t, i})\n   \\qquad\n   \\delta=\\frac{M - m}{2^K},$$ $$\\mathbf{C} = \\left[ m,\\, m + \\delta, \\, m + 2 \\delta,\\,  \\dots, \\, m + (2^K - 1) \\delta \\right].$$ In practice, we compute the minimum $m$ and maximum $M$ values of log mel-filterbanks across the entire dataset to define the codebook $\\mathbf{C}$. Then we map a magnitude $\\mathbf{M}_{t, i}$ of every frequency channel $i=1\\dots N$ for the time frame $t=1\\dots T$ into a bin index of the codebook $\\mathbf{C}$ in the following way: $$\\mathbf{S}_{t,i} = \\text{Discretize}(\\mathbf{M}_{t, i}) \n                = \\text{argmin}_j |\\mathbf{M}_{t, i} - \\mathbf{C}_j|$$ where $\\mathbf{S}\\in\\mathbf{B}^{T\\times N}$ represents the discretized log mel-filterbanks (`dMel`) with $\\mathbf{B}=\\{j|j=1,2,3, \\dots 2^K\\}$ and $\\mathbf{S}_{t}\\in\\mathbf{B}^{N}$ being the $t$-th speech token. As the codebook $\\mathbf{C}$ has $2^K$ distinct values and thus number of bins $|\\mathbf{B}|=2^K$, each speech token is represented by $N\\cdot K$ bits where every $K$ bits are used to represent one of $N$ frequency channels.\n\n#### Detokenization\n\nTo reconstruct the speech signal $\\mathbf{x}$ from the speech tokens $\\mathbf{S}$, we first transform bin indices back to the log mel-filterbanks representation via the codebook $\\mathbf{C}$: $$\\hat{\\mathbf{M}}_{t,i} = \\mathbf{C}_{\\mathbf{S}_{t,i}}.$$ Then, we apply a vocoder [@yamamoto2020parallel] to transform reconstructed log mel-filterbanks $\\hat{\\mathbf{M}}_{t,i}$ back into the time domain signal $\\mathbf{x}$. The vocoder is trained independently and is not part of the transformer decoder-based model.\n\n## Unified Speech-Text Transformer Decoder\n\n![Unified Speech-Text Transformer Decoder with speech tokens as `dMel`.](figs/decoder.pdf){#fig:arch width=\"88%\"}\n\nModeling speech and text sequences jointly is essential for a model to understand and generate both modalities. However, it is challenging to design a unified model that can handle both speech-to-text and text-to-speech effectively. In this work, we apply a unified LM-style transformer model that takes speech and text tokens as input and generates the output tokens in the target sequence. The model is trained in end-to-end on a combined dataset of speech and text pairs, enabling it to learn the joint representations for ASR and TTS tasks. As we show in the rest of the paper, the crucial part for the joint model training is the proper speech tokenization which `dMel` provides.\n\n#### Token Representation\n\nFor text data, we apply a character-level tokenizer to convert the input text into a sequence of text tokens. The text tokens are passed through an embedding layer, $\\text{Embed}(\\cdot): \\{j|j=1,2,3\\dots L\\} \\to \\mathbb{R}^{D}$, where $D$ is the embedding dimension and $L$ is the vocabulary size. The dimension of the speech token embedding is set to be the same as the text token embedding $D$ and no further mapping is required. The motivation for using a character-level tokenizer is to reduce the vocabulary size $L$ and improve the model's generalization ability. Also, character tokens can capture the fine-grained linguistic features that are essential for both ASR and TTS tasks.\n\nFor speech signal, we apply the `dMel` speech tokenizer to convert the input speech signal into a sequence of speech tokens. Then, the speech tokens $\\mathbf{S}\\in\\mathbf{B}^{T\\times N}$ are passed through a learnable embedding layer, $\\text{Embed}(\\cdot): \\mathbf{B} \\to \\mathbb{R}^{d}$, and a learnable linear layer, $\\text{Linear}(\\cdot): \\mathbb{R}^{N \\times d} \\to \\mathbb{R}^{D}$, to obtain the speech token representation $\\mathbf{E}\\in\\mathbb{R}^{T\\times D}$: $$\\begin{aligned}\n  \\mathbf{E}_{t}=\\text{Linear}(\\mathbf{E^{\\prime}}_{t}), \\, \\text{and}\\,\\,\\,\n\\mathbf{E^{\\prime}}_{t}=\\text{Concatenate}([\\text{Embed}(\\mathbf{S}_{t,1}), \\text{Embed}(\\mathbf{S}_{t,2}), \\ldots, \\text{Embed}(\\mathbf{S}_{t,N})]),\n\\end{aligned}$$ where $\\mathbf{E}_t\\in\\mathbb{R}^D$ is the speech token representation. Here, for every time frame $t$, a speech token $\\mathbf{S}_{t}$ is processed *in parallel and independently* for every frequency channel $i$ by $\\text{Embed}(\\mathbf{S}_{t,i})$ mapping, and then embeddings of all frequency channels are stacked together to form one vector representation $\\mathbf{E^{\\prime}}_{t}$ for the frame $t$. Finally, the speech token embeddings $\\mathbf{E}_t$ are fed into the LM-style transformer models for further processing.\n\nWe also implemented other popular speech tokenizers including HuBERT-KM [@lakhotia2021generative] and SpeechTokenizer [@zhang2023speechtokenizer] for comparison. The main difference among these speech tokenizers is the codebook size and codes dimension, shown in Table [1](#tab:tokenizer){reference-type=\"ref\" reference=\"tab:tokenizer\"}. For both HuBERT-KM and SpeechTokenizer the speech tokens are mapped via a learnable linear layer from their dimension to the text embedding dimension $D$ before feeding into the LM-style transformer model.\n\n::: {#tab:tokenizer}\n                    `dMel`   HuBERT-KM   SpeechTokenizer\n  ---------------- -------- ----------- -----------------\n  Codebook Size       16        200           1024\n  Code Dimension      80         1              8\n  Training-free?                        \n\n  : Comparison between different speech tokenizers: `dMel`(ours), HuBERT-LM and SpeechTokenizer. For `dMel` we use $N=80$ log mel-filterbanks and $2^K=16$ values of the codebook $\\mathbf{C}$.\n:::\n\n#### Speaker Representation\n\nTo properly model multi-speaker data, we also include speaker embeddings as input to the transformer decoder. The speaker embeddings are extracted from an independent dvector [@variani2014deep] model[^3]. We use a learnable linear layer to map the speaker embeddings to the same dimension as the speech and text token embeddings $D$. The speaker representation is optional for ASR task, but required for TTS task. Hence, during the training, it is applied for text-to-speech and ignored for speech-to-text.\n\n#### Transformer Decoder\n\nThe transformer decoder is trained end-to-end on a combined dataset of speech and text pairs. For TTS training, the input sequence is constructed by concatenating the speaker embedding (extracted from a random audio for the same speaker of the current sample), text tokens, and speech tokens. For ASR training, the input sequence is constructed by concatenating the speech tokens and text tokens. Both tasks are trained with causal masking, where the model is trained to predict the next token based on the previous tokens. The loss is calculated using the cross-entropy loss between the predicted tokens and the ground-truth tokens. Loss calculation is skipped on the speech tokens for ASR task and on the text tokens for TTS task. ***Note, that all frequency channels at time frame $t$ for `dMel` tokenizer are predicted independently and in parallel.***\n\nTo capture the relative distances between tokens in the input sequence, we apply multiplicative relative positional embedding RoPE [@su2024roformer]. This allows the model to learn the positional relationships between speech tokens, text tokens, and speaker embeddings, enhancing its ability to generate coherent output sequences. For positional embeddings we do not distinguish between text, speech and speaker tokens and thus having global positions notation across all of them, see Figure [2](#fig:arch){reference-type=\"ref\" reference=\"fig:arch\"}.\n\n#### Robust Training\n\nCompared to LMs, audio frames are highly redundant with strong local correlations. This makes longform generation difficult for models due to exposure bias [@scheduled_sampling]. To mitigate exposure bias during training, we apply span-masking [@raffel2020exploring] to the speech token context, masking out multiple random spans of speech frames. The model is trained to predict the next token based on the masked context. This context-masking strategy helps the model learn to generate accurate speech tokens in the presence of missing information, improving its robustness and generalization. It forces the model to attend to the text rather than copying previously inferred speech tokens due to learnt correlations. We also find that span-masking text tokens improves the ASR task.\n\n# Experiments {#sec:exp}\n\nIn this section, we begin by evaluating different speech tokenizers through a common practice in the literature: tokenizing speech into discrete units and then reconstructing the speech to assess the quality of the reconstruction. This approach helps gauge the effectiveness of various tokenization techniques. Following this, we present both TTS and ASR results using an LM-style (decoder-only) model with different speech tokens. While most related work focuses solely on speech synthesis, our study encompasses both speech generation and recognition, providing a more comprehensive evaluation of the tokenization methods. We evaluate the performance of our model mainly on the LibriSpeech dataset and compare it with state-of-the-art speech tokenizers, ASR and TTS models.\n\n## Training Data\n\nWe use several open-sourced datasets with paired speech and text transcription to conduct experiments: i) LibriSpeech [@panayotov2015librispeech] dataset (CC BY 4.0) consists of English speech recordings (960h, 16kHz) from various speakers ($\\sim$`<!-- -->`{=html}2k) and conditions; ii) LibriTTS [@zen2019libritts] (CC BY 4.0) dataset (500h) derived from LibriSpeech improves on it with the proper sentence split, text normalization and keeping samples 24kHz; iii) VCTK [@vctk2019] contains 44h of English speech (108 speakers); iv) LJSpeech [@ljspeech17] (public domain in US) is a single speaker English audio recordings of 16kHz with read speech from LibriVox[^4]. While LibriSpeech is used to train ASR and TTS models, LibriTTS, VCTK and LJSpeech are only used to train the TTS.\n\n## Training Configuration\n\nWe train the LM-style transformers in three different sizes: Small, Base, and Large (see Appendix Table [10](#tab:model-size){reference-type=\"ref\" reference=\"tab:model-size\"}). Unless stated otherwise, the Base model is used in all experiments if not stated otherwise. All models use pre-LayerNorm with dropout set to 0.1 for residual, attention and embedding layers and 0.3 for positional embedding. `dMel` uses 16 discrete bins for each channel while text is tokenized with a character vocabulary; the speaker embedding dvector has 512 dimensions (see Appendx [10](#app:training-details){reference-type=\"ref\" reference=\"app:training-details\"} for details). In all experiments, training data are sampled to 16kHz.\n\n## Main Results\n\n### Speech Reconstruction\n\nFollowing [@zhang2023speechtokenizer], we randomly sample 300 speech utterances and their ground truth transcriptions from the LibriSpeech *test-clean* dataset. We use the speech2unit and unit2speech modules to convert the speech signal to speech tokens and then reconstruct the speech signal from the speech tokens. We compute the WER between the ASR outputs from HuBERT-Large [@hsu2021hubert][^5] on the audio samples and their ground truth transcripts. We also report MOS-LQO (Mean Opinion Score -- Listening Quality Objective) score to measure the reconstruction quality using ViSQOL [@hines2012visqol]. Finally, we use human evaluation to measure the naturalness of the reconstructed speech using a MOS score with 95% confidence interval. We instruct the human evaluators to rate the naturalness of the reconstructed speech on a scale of 1 to 5, where 1 is the worst and 5 is the best. The results are shown in Table [2](#tab:reconstruction){reference-type=\"ref\" reference=\"tab:reconstruction\"}.\n\n::: {#tab:reconstruction}\n  **Tokenizer**                         **WER↓**   **MOS-LQO↑**  \n  ----------------- ----- ----- ------ ---------- -------------- --------------------------------\n  GroundTruth          \\-    \\-   \\-      2.02          \\-        3.91$\\pm$`<!-- -->`{=html}0.12\n  HuBERT-KM            95   111  50Hz     8.71         2.06       2.74$\\pm$`<!-- -->`{=html}0.14\n  EnCodec               7     7  75Hz     2.03         4.03       3.69$\\pm$`<!-- -->`{=html}0.13\n  SpeechTokenizer      65    34  50Hz     2.41         4.19       3.77$\\pm$`<!-- -->`{=html}0.13\n  Mel-HifiGAN         n/a    12  80Hz     2.08         4.52       3.80$\\pm$`<!-- -->`{=html}0.12\n  `dMel`-HifiGAN      n/a    12  80Hz     2.11         4.47       3.68$\\pm$`<!-- -->`{=html}0.13\n  Mel-PWG             n/a     1  80Hz     2.13         4.40       3.27$\\pm$`<!-- -->`{=html}0.14\n  `dMel`-PWG          n/a     1  80Hz     2.23         4.37       3.23$\\pm$`<!-- -->`{=html}0.14\n  Mel-PWG             n/a     1  40Hz     2.36         4.34       2.99$\\pm$`<!-- -->`{=html}0.15\n  `dMel`-PWG          n/a     1  40Hz     2.51         4.29       2.97$\\pm$`<!-- -->`{=html}0.15\n\n  : Speech reconstruction results on 300 random samples from LibriSpeech *test-clean* set.\n:::\n\nFrom Table [2](#tab:reconstruction){reference-type=\"ref\" reference=\"tab:reconstruction\"}, we can see that semantic tokenization (HuBERT-KM) is not good for speech reconstruction. Meanwhile, acoustic tokenizers that are optimized to reconstruct the signal directly (EnCodec and SpeechTokenizer) do well.\n\nWe apply different vocoders to reconstruct the speech signal from log mel-filterbanks, and find that the WER of the reconstructed speech signal is comparable to the acoustic tokenization methods with a fraction of the parameters. Also, log mel-filterbanks achieve a better MOS-LQO score, which indicates that the reconstructed audio is more similar to the original audio. By comparing Mel and `dMel`, we can see that discretization has little impact on WER and MOS-LQO scores. We also find that the exact vocoder matters much less than the frame rate of tokenization: the WER goes from 2.08 to 2.13 when switching from HifiGAN to ParallelWaveGAN, but it falls from 2.13 to 2.36 when the frame rate is changed from 80Hz to 40Hz. However, even a 1M parameter vocoder operating at a 40Hz frame rate is comparable to the much larger SpeechTokenizer on WER and MOS-LQO metrics.\n\n*Considering the efficiency and performance, we choose the `dMel` speech tokenizer in 40Hz with ParallelWaveGAN vocoder for the following experiments.*\n\n### LM-Style Text-to-Speech\n\nHere we compare the accuracy and naturalness of speech synthesized by LM-style text-to-speech (TTS) models trained on different tokenization methods. For TTS evaluation, we utilize WhisperX [@bain2022whisperx] (\"base.en\" from [@radford2023robust]) to transcribe our generated speech into text and calculate the WER and the character error rate (CER). We report both WER and CER to facilitate comparisons to prior works which have reported only one or the other.\n\nWe trained the TTS model using the same architecture but with three different tokenization methods: HuBERT+KM (with 200 clusters), SpeechTokenizer, and `dMel`. Additionally, we present the results from VOXTLM [@maiti2024voxtlm] and USLM [@zhang2023speechtokenizer] for comparison. VOXTLM is a larger model trained on more data that is initialized from a pretrained LLM (OPT) using HuBERT-KM as the speech tokenizer. USLM comprises an autoregressive (AR) model and a non-autoregressive (NAR) model, both trained with the SpeechTokenizer.\n\nAs shown in Table [3](#tab:main-tts){reference-type=\"ref\" reference=\"tab:main-tts\"} for training on LibriSpeech dataset, our LM-style model with `dMel` tokenization achieves a WER of 4.3 and a CER of 1.8, significantly outperforming the baseline methods. This indicates that our model can generate more accurate speech with less hallucination and distortion. Furthermore, we observed that the AR model trained on SpeechTokenizer tokens exhibits a much higher WER compared to the idiosyncratic coarse to fine models (labeled AR+NAR) developed for these residual tokenizers -- indicating that `dMel` lies on a simpler data manifold.\n\nGiven the success of our LM-style `dMel` TTS model, dubbed `RichTTS`, we further evaluate it on various datasets, including LJSpeech, VCTK, and LibriTTS, and compare it with popular open-sourced TTS models, including Tacotron2 [@shen2018natural], FastSpeech2 [@ren2020fastspeech], and VITS [@kim2021conditional]. We conduct human evaluation to measure the naturalness of 50 randomly sampled synthesized speech from VCTK test set. `RichTTS` achieves competitive performance on the TTS task in terms of both MOS and WER demonstrating its effectiveness in generating high-quality synthesized speech, see Table [4](#tab:tts){reference-type=\"ref\" reference=\"tab:tts\"}. Interestingly, we find that VITS performs poorly on the VCTK WER. We suspect this is because VITS tends to make more mistakes at the beginning of each sequence, and since VCTK comprises short sequences, even one or two word errors can lead to a high WER.\n\n::: {#tab:main-tts}\n  **Model**                                                       **WER↓** (%)   **CER↓** (%)  **Params**\n  ------------------------------------------------------------- -------------- -------------- ------------\n  VOXTLM (HuBERT+KM), [@maiti2024voxtlm]                                    \\-            3.5     350M\n  USLM (SpeechTokenizer), AR+NAR, [@zhang2023speechtokenizer]              6.5             \\-     356M\n  `RichTTS`(HuBERT+KM)                                                     9.5            4.3     258M\n  `RichTTS`(SpeechTokenizer), AR                                          11.4            5.9     258M\n  `RichTTS`(`dMel`)                                                    **4.3**        **1.8**     258M\n\n  : Text-to-speech results for different tokenizers. `RichTTS` is trained on LibriSpeech 960h. WER (%) and CER (%) are evaluated with WhisperX ASR (\"base.en\") and reported on *test-clean*.\n:::\n\n::: {#tab:tts}\n                                        **WER↓ (%)**                           \n  ----------------------------------- -------------- -------------- ---------- ------------------------------------\n  2-4 (l)5-5 **Model**                  **LJSpeech**   **LibriTTS**   **VCTK**               **VCTK**\n  GroundTruth                                    2.6            3.8        3.4    4.18$\\pm$`<!-- -->`{=html}0.10\n  Tacotron2, [@shen2018natural]                  4.4            7.3        4.2    2.91$\\pm$`<!-- -->`{=html}0.15\n  FastSpeech2, [@ren2020fastspeech]              6.1           10.2        3.8    3.03$\\pm$`<!-- -->`{=html}0.14\n  VITS, [@casanova2022yourtts]                   6.4            8.3       11.1  **3.56$\\pm$`<!-- -->`{=html}0.12**\n  `RichTTS`(`dMel`)                          **4.0**        **4.5**    **2.2**    3.34$\\pm$`<!-- -->`{=html}0.14\n\n  : WER (%) (evaluated with WhisperX ASR \"base.en\") and MOS of different TTS models' generations using transcriptions from each evaluation set that correponds to data used for training.\n:::\n\n::: {#tab:tts-length}\n  **Sequence Length**     **Tacotron2**   **FastSpeech2**   **VITS**   **`RichTTS`**\n  --------------------- --------------- ----------------- ---------- ---------------\n  Total WER↓ (%)                    4.4               6.1        6.4         **4.0**\n  10-20 words                       5.5           **3.1**        7.4             3.5\n  20+ words                         3.3               9.1        5.3         **3.0**\n\n  : results for TTS models trained on LibriSpeech 960h and evaluated on LJSpeech test set.\n:::\n\nFurthermore, we observed that our model with `dMel` tokenization can generate long audio sequences with high quality. Here, we evaluate the performance of our model on different lengths of text sequences using the LJSpeech test set. Table [5](#tab:tts-length){reference-type=\"ref\" reference=\"tab:tts-length\"} shows the WER results for our model on text sequences with 10-20 words and more than 20 words. We ignore text sequences with fewer than 10 words, as they are too short and not robust for WER evaluation. From Table [5](#tab:tts-length){reference-type=\"ref\" reference=\"tab:tts-length\"}, we observe that our model achieves competitive performance across different text lengths, demonstrating its robustness and generalization ability in generating synthesized speech for varying text inputs lengths. Additionally, we find that the non-autoregressive (NAR) model FastSpeech2 achieves the lowest WER on shorter sequences but the highest WER on longer sequences. This suggests that NAR models may not be well-suited for generating long audio sequences.\n\n### LM-Style Speech-to-Text\n\n::: {#tab:main-asr}\n  **Model**                                        **dev-clean↓**                      **dev-other↓**                    **test-clean↓**                     **test-other↓** **Params**\n  ---------------------------- ---------------------------------- ----------------------------------- ---------------------------------- ----------------------------------- ------------\n  `RichASR`(SpeechTokenizer)         6.5$\\pm$`<!-- -->`{=html}0.3       16.9$\\pm$`<!-- -->`{=html}0.7       6.9$\\pm$`<!-- -->`{=html}0.4       17.5$\\pm$`<!-- -->`{=html}0.5 258M\n  `RichASR`(HuBERT+KM)               5.3$\\pm$`<!-- -->`{=html}0.1       13.7$\\pm$`<!-- -->`{=html}0.2       5.8$\\pm$`<!-- -->`{=html}0.1       13.8$\\pm$`<!-- -->`{=html}0.1 258M\n  `RichASR`(`dMel`)              **3.8**$\\pm$`<!-- -->`{=html}0.1   **10.3**$\\pm$`<!-- -->`{=html}0.1   **4.2**$\\pm$`<!-- -->`{=html}0.2   **10.4**$\\pm$`<!-- -->`{=html}0.1 258M\n\n  : Speech recognition results for different tokenizers measured with WER (%). All models are trained on LibriSpeech 960h.\n:::\n\n::: {#tab:main-asr-sota}\n  **Model**            **Data** (h)    **dev-clean↓**   **dev-other↓**   **test-clean↓**   **test-other↓**  **Params**\n  ------------------- -------------- ---------------- ---------------- ----------------- ----------------- ------------\n  VOXTLM                   280k                    \\-               \\-               6.5              17.6     350M\n  VOXTLM                   280k                    \\-               \\-               4.6              12.1     1.3B\n  [@chen2024loss]          960                    3.6          **7.8**               3.8           **8.3**     355M\n  `RichASR`(`dMel`)        960                **3.1**              8.4           **3.4**               8.6     355M\n\n  : Comparison of WER (%) for best `RichASR` trained with `dMel` tokenization and prior work with LM-style ASR models and HuBERT+KM with subword modeling on top as tokenization.\n:::\n\nTraining an LM-style speech-to-text (ASR) model can test if the speech tokens can preserve the semantic information in the speech signal and support the speech content-based task. Table [6](#tab:main-asr){reference-type=\"ref\" reference=\"tab:main-asr\"} shows results of our model dubbed `RichASR`, trained with different tokenizations including `dMel` for the ASR task. Our LM-style model with `dMel` speech tokenization achieves 4.2% WER on the *test-clean* and 10.4% WER on the *test-other* sets outperforming both HuBERT-KM and SpeechTokenizer. We also observe that our model with HuBERT-KM [@lakhotia2021generative] outperforms the SpeechTokenizer [@zhang2023speechtokenizer] for ASR, which is reasonable as semantic tokens are more suitable for the ASR task.\n\nIn Table [7](#tab:main-asr-sota){reference-type=\"ref\" reference=\"tab:main-asr-sota\"}, we further compare `RichASR` with `dMel` speech tokenizer trained with GPT-2-meduim architecture [@radford2019language] on LibriSpeech 960h with prior work: VOXTLM [@maiti2024voxtlm] that uses larger model trained with more data and initialized from a pretrained LLM (OPT [@zhang2022opt]), and HuBERT-KM with additional subword modeling on top as the speech tokenizer; [@chen2024loss] that also uses GPT-2 architecture trained on LibriSpeech 960h and HuBERT-KM with additional subword modeling on top as the speech tokenizer[^6]. `RichASR` with `dMel` outperforms VOXTLM; it also outperforms [@chen2024loss] on clean sets and a bit behind it on other sets.\n\nThe ASR results clearly demonstrate the benefit of using our `dMel` speech tokenizer for the content-related tasks in speech, as it better preserves the semantic information in the speech signal. Further details and ablations can be found in Appendix [10](#app:training-details){reference-type=\"ref\" reference=\"app:training-details\"} and [11](#app:ablations){reference-type=\"ref\" reference=\"app:ablations\"}.\n\n### Ablations\n\nWe first investigate the impact of the codebook sizes, shown in Table [8](#tab:nbins-ablation){reference-type=\"ref\" reference=\"tab:nbins-ablation\"}. The 16-bin configuration used in the paper demonstrates the best overall performance across tasks. While the 32-bin setup slightly outperforms on the ASR *test-other* set, it shows degraded performance in TTS. This trade-off likely stems from the increased speech vocabulary size, which may pose challenges for accurate prediction. The results may get better with increased data and model size. And 8-bin configuration looses too much information with discretization.\n\nWe then ablate the ASR results to understand why ASR LM-style model is behind the state-of-the-art ASR results on LibriSpeech. We take two existing transformer ASR baselines, Seq2Seq and CTC, that use 80 log mel-filterbanks and characters as targets. We then modify these baselines by using `dMel` instead (the discretization, embedding layer and linear layer) while keeping all other hyper-parameters the same (we adjust only the SpecAugment time masking max width accordingly to keep total masking in *ms* the same). Our results (Table [9](#tab:asr-ablation){reference-type=\"ref\" reference=\"tab:asr-ablation\"}) suggest: i) `dMel` brings only small degradation compared to Mel; ii) additional discrepancy is coming from different hop distance in featurization; iii) ***the main and significant performance degradation is coming from switching to LM-style model.*** The latter is in line with [@maiti2024voxtlm] and [@chen2024loss], though was not discussed in detail by any prior work. We hypothesise this gap is due to observed overfitting of the LM-style models.\n\n::: {#tab:nbins-ablation}\n   **N-Bins**   **ASR test-clean↓**   **ASR test-other↓**   **TTS WER↓ (%)**\n  ------------ --------------------- --------------------- ------------------\n       8                6.6                  16.5                 7.3\n       16               4.4                  10.7                 4.8\n       32               4.7                  10.2                 5.7\n\n  : ASR and TTS results (WER, %) with `dMel` speech tokenizer and different number of bins (codebook size) for discretization in `dMel`. All models are trained on LibriSpeech 960h.\n:::\n\n::: {#tab:asr-ablation}\n  **Model**                                          **Features**  **dev-clean↓**   **dev-other↓**  \n  ------------------------------------------------ -------------- ---------------- ---------------- --\n  [@gulati2020conformer] (RNN-T -- Conformer)            Mel-10ms       1.9              4.1        \n  [@kim2022squeezeformer] (CTC -- Squeezeformer)         Mel-10ms       2.3              5.8        \n  Seq2Seq [@dong2018speech]                              Mel-10ms       2.4              5.4        \n                                                      `dMel`-10ms       2.5              5.9        \n                                                         Mel-25ms       2.8              6.5        \n                                                      `dMel`-25ms       2.7              6.2        \n  CTC [@graves2006connectionist]                         Mel-10ms       2.1              5.4        \n                                                      `dMel`-10ms       2.1              5.6        \n                                                         Mel-25ms       2.1              5.4        \n                                                      `dMel`-25ms       2.3              6.1        \n  LM-style                                            `dMel`-25ms       3.4              9.5        \n\n  : WER (%) comparison for CTC, Seq2Seq, and LM-style ASR models ($\\sim$`<!-- -->`{=html}260M) trained on LibriSpeech 960h with `dMel` and Mel features. We compute 80 log mel-filterbanks with 25ms (50ms) window and 10ms (25ms) hop distance, denoted as '10ms' ('25ms').\n:::\n\n### Unlocking Joint Speech-Text Modeling\n\nOur model design allows us to train a single model for both ASR and TTS tasks leading to a simpler setup. We train a single model with the same architecture and tokenization as `RichTTS`, by constructing the training data with \\<text, speech\\> and \\<speech, text\\> pairs for ASR and TTS tasks, respectively. By mixing these two types of data, we can train a single model for both tasks.\n\nTable [\\[tab:joint-asr\\]](#tab:joint-asr){reference-type=\"ref\" reference=\"tab:joint-asr\"} shows that the joint model is worse on both tasks, but ASR is affected more than TTS. Comparing our results to VOXTLM, which initializes its model from pretrained LLM (OPT) and finetunes it with multiple tasks and datasets, we speculate that our joint model needs text-only training to learn a good LM for better ASR performance. Our model structure trivially allows for this text-only training, but we leave those experiments for future work (for further discussion see Appendix [11.3](#app:joint){reference-type=\"ref\" reference=\"app:joint\"}).\n\n# Related Work\n\n#### Speech Tokenization\n\nRecent advancements in speech tokenization have primarily focused on two approaches: semantic tokens and acoustic tokens. This section examines these methods, their combinations, and their limitations, highlighting the need for more efficient and generalizable solutions. Semantic tokens, extracted from self-supervised pretrained speech models, have shown promise in capturing high-level content information. Methods like wav2vec [@baevski2020wav2vec] and HuBERT [@hsu2021hubert] employ $k$-means clustering on speech representations to generate these tokens. While effective in capturing semantic content, these approaches often struggle with preserving fine-grained acoustic details crucial for high-quality speech synthesis. In contrast, acoustic tokens, derived from pretrained audio compression models, excel at preserving low-level acoustic information. Techniques such as SoundStream [@zeghidour2021soundstream] and EnCodec [@defossez2022high] utilize residual vector quantization (RVQ) with reconstruction objectives. These methods achieve high-quality audio compression but may not capture higher-level semantic structures effectively.\n\nRecognizing the complementary nature of semantic and acoustic tokens, recent works have attempted to combine these approaches. AudioLM [@borsos2023audiolm] introduced a three-stage model: semantic modeling, coarse acoustic modeling, and fine acoustic modeling. While comprehensive, this approach introduces complexity and computational overhead. AudioPalm [@rubenstein2023audiopalm] further demonstrated the critical importance of large-scale training data and model parameters for effective multi-stage modeling, highlighting potential generalization issues in low-resource scenarios. An alternative hybrid approach, proposed by @zhang2023speechtokenizer, attempts to distill semantic information into acoustic tokens during RVQ model training. However, this method still requires additional pretraining and does not fully achieve a single-stage model architecture.\n\nDespite these advancements, several challenges persist in the field of speech tokenization: i) balancing semantic and acoustic information in a unified representation; ii) reducing model complexity and computational requirements; iii) improving generalization to low-resource / out-of-domain data e.g. with mixed speech from multiple speakers, or multiple languages, or changing characteristics of recording equipment/sampling rate etc.; iv) developing truly single-stage tokenizers. Our proposed method, `dMel`, addresses these challenges by offering a training-free speech tokenization approach. By directly discretizing log mel-filterbanks into bins, it inherently preserves both semantic and acoustic information in a unified representation, while significantly reducing computational complexity.\n\n#### Speech-Text Modeling\n\nModeling speech and text jointly is a challenging task, as speech signals are continuous and while text is discrete. Existing works have explored various approaches to address this challenge, including usage of separate encoders for different modalities [@ao2021speecht5; @bapna2021slam]. @bai20223 proposed an encoder-only model A3T for speech-text modeling, by introducing alignment embedding to encourage cross-modal transfer between text and speech. Although A3T achieved good performance on speech synthesis and editing tasks, it cannot generate text and cannot generalize to longform generation because of its encoder-only architecture and mask-reconstruction training strategy. VioLA [@wang2023viola] also targets a unified speech-text model which can generate speech and text with a single model, but it is specifically designed for the Encodec [@defossez2022high] style feature, and compelled to model speech tokens in a multi-stage hierarchical manner. @maiti2024voxtlm proposed a LM-style model VOXTLM, to model speech and text jointly. However, VOXTLM is only models the HuBERT semantic tokens, and relies on an external generation model to transform semantic tokens into waveform, but the speaker and acoustic information are lost. In comparison, the model architecture in this paper is a simple, single stage LM-style transformer model, and can handle both the speech generation and text generation tasks.\n\n# Conclusion\n\nIn this work, we proposed `dMel`, a novel train-free speech tokenization method that discretizes log mel-filterbank energies directly into bins. By operating on the authentic log mel-filterbank representation, `dMel` inherently preserves both semantic and acoustic information in a unified tokenized representation. Our key contribution is the evaluation of `dMel` within a unified LM-style transformer architecture for speech recognition (ASR) and speech synthesis (TTS) tasks. Our `dMel`-based ASR model, `RichASR`, achieved the lowest word error rate among tokenization methods, robustly preserving semantic content. For TTS, `dMel`'s generation yielded the lowest WER, accurately reconstructing speech waveforms. Our `dMel`-based TTS model, `RichTTS`, achieved competitive naturalness, lowest error rates, and long audio generation capabilities.\n\n`dMel`'s simplicity circumvents separate tokenizers or multi-stage modeling, reducing computational overhead and dependence on pretrained models. By unifying semantic and acoustic modeling, `dMel` enables efficient speech-text modeling frameworks. While initial joint TTS-ASR training showed promise, further work is needed. Our primary contribution demonstrates `dMel`'s effectiveness for high-performing separate TTS and ASR models within a unified LM-style architecture.\n\n# Ethics Statement {#ethic}\n\nThe development and deployment of speech technologies carry important ethical considerations. While our proposed `dMel` method aims to advance the state-of-the-art in speech-text modeling, it is crucial to highlight potential ethical risks and raise the awareness so that new methods may be developed to mitigate these risks.\n\nOur first main concern is the potential dual-use of speech synthesis technologies for nefarious purposes such as impersonation, misleading audio-visual content generation, or voice spoofing attacks. Proactive measures, including watermarking techniques and robust speaker verification methods, should be explored to counter such risks. The former attempts to build markers into the generated speech that make it easy to detect, while the latter focusses on distinguishing synthetic from real data. Prior work [@le2023voicebox] has shown that neural networks can be trained to distinguish speech synthesized from their model from real speech, probably because of artifacts from the use of mel spectral vocoders. While we did not train a network to do so in our work yet (we will create one before code release), the vocoders we use are similar to their work -- going from mel spectrogram to raw waveforms. Our model also does not use prosody, phoneme duration and other predictions that more sophisticated TTS systems use to allow the model to perform very well on imitating speaker styles in zero-shot settings. However our model can probably mimic the styles of training speakers very well. It is our hope that releasing our methods will facilitate more research on fake speech verification and watermarking techniques -- even if current classifiers are able to perform this detection, the quality of the generative models is improving. It is also our hope that future works will attempt to perform more credit assignment -- by providing metrics that show which real data samples a synthetic speech example copies its style and substance from.\n\nAnother concern is the perpetuation of societal biases encoded within training data. Speech datasets may exhibit biases along dimensions such as gender, race, age, or socioeconomic status, which could be propagated or amplified by trained models. Rigorous debiasing techniques and careful curation of representative training data are essential to mitigate these risks. On the mitigating side of this equation, we also hope that with better, more controllable TTS systems, ASR systems can improve because more data can be generated for underrepresented segments of the distribution from the TTS models.\n\nFurthermore, the development and deployment of speech technologies should prioritize accessibility and inclusivity. Models should be evaluated for performance across diverse demographics, accents, and language varieties to ensure equitable access and quality of service.\n\nFinally, it is important to foster transparency and accountability in the research and development process. Clear documentation of model capabilities, limitations, and potential failure modes should be provided to enable informed decision-making and responsible usage.\n\nAddressing these ethical considerations requires a multistakeholder approach involving researchers, developers, policymakers, and end-users. By prioritizing ethical principles such as fairness, privacy, and accountability, we can work towards realizing the benefits of speech technologies while mitigating potential risks and adverse societal impacts.\n\n# Limitations {#app:limitations}\n\nBecause TTS work is tremendously fragmented and clear protocols are not often available for training and evaluation, we reimplemented other tokenizers within our code base using publicly available, official implementations where available: e.g. we used Hubert-KM and speech tokenizer features extraction from the public codebases and pluged them into our LM-style model training. While we made the best effort to tune the tokenization methods and the models, there is always a possibility we missed some details. However, our results seem to tell a consistent story when viewed from multiple angles, and when viewed on multiple datasets. We also did not train on larger model sizes (\\>1B parameters), larger datasets (\\>1k hours), or using pretrained models.\n\nThe real challenge for modern multimodal LLMs is complex semantic understanding tasks. While our current experiments focus on text-to-speech and speech-to-text tasks, these encompass critical aspects of speech processing. `dMel`'s effective performance within a decoder-only architecture for both tasks suggests potential for broader applications. We recognize the importance of more sophisticated speech understanding tasks and view our work as a foundation for future research leaving other tasks out of scope of the paper. Scaling up pretraining and exploring complex semantic understanding tasks could further validate our approach's versatility across a wider range of multimodal language processing challenges.\n\nWe acknowledge that our current scope targets only speech on purpose, as indicated in our title. While `dMel` may potentially support non-speech tasks, ***our current exploration and verification focus solely on speech, not general audio.*** Regarding the \"speaker variations\" -- mel-spectrogram is used for speaker recognition widely, thus it preserves necessary speaker information on which we thus rely in `dMel` too.\n\n# Data, Code, Reproducibility {#app:repro}\n\nWe made the best effort to use publicly available data and official implementations of prior works where it is possible. All data we used are under permissive license for research. We provided as much as detail as is possible without code such as details on our model training and hyperparameters throughout the paper and in the Appendix. We plan to open-source our code upon paper acceptance.\n\nWe do not plan to open-source any pre-trained models for sake of privacy, safety and misuse.\n\n# Subjective Evaluation for TTS {#subjective_evals}\n\nWe use crowd-sourcing to collect subjective ratings to compare the naturalness of the reconstructed speech from the different tokenizers. We evaluate the quality of the same (randomly sampled) 50 utterances for each model by collecting around seven ratings per sample. Overall, we collect 3500 ratings from 65 raters. The raters were English-speaking and were paid at least the minimum wage.\n\nWe present the raters with a generated speech sample and instruct them to rate how natural it sounds on a five-point Likert scale, where 1 corresponds to very unnatural and 5 corresponds to very natural. Figure [3](#fig:crowd_screenshot){reference-type=\"ref\" reference=\"fig:crowd_screenshot\"} shows a screenshot of our subjective test as seen by the rater.\n\n![A screenshot of the assessment task, as the crowd-sourced rater sees it.](figs/crowdsource_screenshot.png){#fig:crowd_screenshot width=\"80%\"}\n\nWe noticed human annotators have bias over audio volume so we do volume normalization on top of all reconstructed or generated audio before giving them to human annotators.\n\nWe report Mean Opinion Score (MOS) results throughout the paper with confidence intervals calculated using bootstrap resampling with 1000 iterations, providing a reliable estimate of the variability MOS results.\n\n# Training Details {#app:training-details}\n\n## Baselines\n\nFor reproducibility, we provide the HuggingFace model cards used in our experiments in Table [4](#tab:tts){reference-type=\"ref\" reference=\"tab:tts\"}:\n\n-   Tacotron2 [@shen2018natural], <https://huggingface.co/espnet/espnet/kan-bayashi_vctk_tts_train_xvector_tacotron2_raw_phn_tacotron_g2p_en_no_space_train.loss.ave>\n\n-   FastSpeech2 [@ren2020fastspeech], <https://huggingface.co/espnet/kan-bayashi_vctk_gst_fastspeech2>\n\n-   VITS [@casanova2022yourtts], <https://huggingface.co/espnet/kan-bayashi_vctk_multi_spk_vits>\n\n## `RichASR` and `RichTTS`\n\nFor our LM-style model we stack together speaker embedding, speech tokens and text tokens. Both speech and text tokens have prepended begin of sentence token (\\<bos\\>) and appended end of sentence token (\\<eos\\>).\n\nWe train all models using the Adam optimizer with a learning rate of 1e-3, learning rate warmup of 4k steps for ASR and 5k for TTS, cosine learning rate schedule and gradient clipping of 1.0 for TTS and 0.1 for ASR and joint models. We use dynamic batching to optimize the data packing with total batch size of 1.4h/1.4h/0.7h for ASR training and 1h/2h/2h for TTS training for Small/Base/Large models. We train TTS models for 100k steps and ASR models 80k steps with mixed precision training and BF16 on A100 and H100 GPUs with 80GB. Both ASR models and TTS models are trained with 8GPUs for less than a day and for 2-4 days for ASR and TTS respectively.\n\n::: {#tab:model-size}\n                               **Small**   **Base**   **Large**\n  -------------------------- ----------- ---------- -----------\n  \\# of layers                        18         36          48\n  \\# of attention heads                2          4           8\n  \\# of hidden units ($D$)           512        768        1536\n  \\# of parameters                   59M       258M        1.3B\n\n  : LM-style transformer model configurations for ASR, TTS and joint models training.\n:::\n\n## LM-Style Speech-to-Text\n\nFor ASR training as an augmentation we apply SpecAugment [@DBLP:conf/interspeech/ParkCZCZCL19] with 2 frequency masks with max width 30 and 10 time masks with max width 50 and ratio 0.1. With ablations we found that SpecAugment masking with average value instead of zero is slightly better. Without applying SpecAugment performance of ASR is 7.3% WER on *dev-clean* and 20.3% WER on *dev-other*, which is further can be improved with usage of frequency masking only to 6.4% WER on *dev-clean* and 16.6% WER on *dev-other*. Usage of both frequency masking and time masking results in the best performance of Table [6](#tab:main-asr){reference-type=\"ref\" reference=\"tab:main-asr\"}.\n\nWe found that span masking is key part of model training to enforce slef-attention to attend to speech part as well as to reduce exposure bias. The masking strategy is similar to the one used for TTS training: for every training step with probability $p$ the sample in the minibatch is masked with the mean span of 3 tokens with masking ration of 0.5. We found that the mean span of 1 token or 5 tokens gives the same results; while the mask probability $p$ is the most important hyper-parameter. The optimimal value for ASR is found to be 0.8, which is used in all final models.\n\nAs we found one best model configuration for the Base model with `dMel` we then change only i) model size ii) speech tokenization iii) training data (here we increase model dropout to 0.3 for training on *train-clean-360* and to 0.5 for training on *train-clean-100* as otherwise models drastically overfit); the rest of hyper-parameters stay the same.\n\n# Ablations {#app:ablations}\n\n## LM-Style Text-to-Speech\n\nScaling results for `RichTTS` are shown in Table [11](#tab:abl-tts){reference-type=\"ref\" reference=\"tab:abl-tts\"}.\n\n::: {#tab:abl-tts}\n                               **WER↓ (%)** \n  -------------------------- -------------- --\n  `RichTTS`(`dMel`), Small              8.1 \n  `RichTTS`(`dMel`), Base               4.3 \n  `RichTTS`(`dMel`), Large              5.4 \n\n  : Text-to-speech results for different model sizes with `dMel`. All models are trained on LibriSpeech 960h dataset. Evaluation is done via speech generation on the full *test-clean* transcriptions and speakers, and then evaluated WER with WhisperX base.en.\n:::\n\n## LM-Style Speech-to-Text\n\nASR ablations for different model sizes, data sizes, and tokenizers are shown in Table [\\[tab:extended-asr\\]](#tab:extended-asr){reference-type=\"ref\" reference=\"tab:extended-asr\"}.\n\nWe noticd the results in [@chen2024loss] seems to be the SOTA for LM-style ASR model to the best of our knowledge. However, as many ablations are missed in [@chen2024loss], we took their open-sourced code and run ablations ourselves to have proper comparison with it. The final results, including ablation with `dMel` are shown in Table [\\[tab:chen-ablation\\]](#tab:chen-ablation){reference-type=\"ref\" reference=\"tab:chen-ablation\"}:\n\n-   We successfully reproduced [@chen2024loss] results (row 1 and 2).\n\n-   Without pretraining (rows 3, 4, 5):\n\n    `dMel` outperforms HuBERT-KM on both clean and other datasets; `dMel` surpasses BPE on top of HuBERT-KM on clean data, while BPE on HuBERT-KM performs better on other.\n\n-   Without pretraining and without speed perturbation (rows 6, 7, 8):\n\n    BPE on HuBERT-KM performance decreases significantly after diabling speed perturbation (compare rows 3 and 6), raising questions about its generalizability to other domains, given that BPE tokens are trained on speed-perturbed LibriSpeech data.\n\n    Our `dMel`(row 8) achieves substantially better results than both HuBERT-KM and BPE on HuBERT-KM (rows 7 and 6), demonstrating robust performance even without speed augmentation.\n\nNote that in `dMel`, we use SpecAugment (masking across time and channels) and [@chen2024loss] also use SpecAugment. According to their code, the time masking is 30%, while channel masking is impossible as there is only 1 channel).\n\nWe believe these results demonstrate the effectiveness, simplicity in use, and robustness of our `dMel` tokenization method, particularly in scenarios where extensive pretraining or domain-specific augmentations may not be feasible.\n\nNote that [@chen2024loss] did not show applicability of BPE on HuBERT-KM or HuBERT-KM to TTS task, while in VOXTLM (also uses BPE on HuBERT-KM) it is shown that this tokenization is not suited for TTS (the performance is poor). `dMel` in contrary is shown to perform well on TTS task too in addition to ASR.\n\n## Joint Speech-Text Modeling Discussion {#app:joint}\n\nWe found it to be challenging to train joint model for ASR and TTS, similar to observations as in [@maiti2024voxtlm] and e.g. [@shi2022learning; @anonymous2024avcpl] for joint audio-visual speech recognition. Also, there is a very recent research work [@toyin2024unified], that also shows training TTS and ASR jointly is challenging, and needs carefully designed model architecture and training loss fusion technique.\n\nOne of the reasons is the different pace of learning. Careful consideration of training strategies can mitigate some of the challenges in joint modeling of TTS and ASR tasks, highlighting the complexities inherent in combining these distinct but related tasks within a single model.\n\nAnother reason we suspect is the mismatch between train and test time, which is more pronounced for the joint modeling: if we compare individual validation losses per task in joint model to their one-task training counterparts we see they match each other (so training is fine), however the generation (test time which mismatches how the train loss is defined) for both tasks is broken: longer sequences has hallucination and high repetition issues. This could be due to different length of sequences between text and audio and thus learnt attention pattern could be different which creates longer sequences generation issue for the joint model.\n\nLast but not the least, the two tasks have opposite modalities in the input and output, making it rather difficult to model. Most previously researched multi-task work have the same modality in the output. The combination of ASR and TTS is a rather recent phenomenon, such as Viola and VOXTLM.\n\n[^1]: We use a word 'semantic' with the meaning of 'content' to keep prior work notation [@borsos2023audiolm].\n\n[^2]: We used vocoders from <https://github.com/kan-bayashi/ParallelWaveGAN.>\n\n[^3]: We use a pretrained model \"Speaker Encoder\" from the YourTTS [@casanova2022yourtts] repository <https://github.com/Edresson/YourTTS>.\n\n[^4]: <https://librivox.org/pages/public-domain/>.\n\n[^5]: We use checkpoint [https://huggingface.co/facebook/hubert-large-ls960-ft\n    ](https://huggingface.co/facebook/hubert-large-ls960-ft\n    ){.uri}.\n\n[^6]: We use official codebase to train this model w/o text pretraining as [@chen2024loss] report results only with text pretraining.",
    "rationale": "Summary: This work propose to solve the problem of codec: it is hard for one codebook to cover both semantic and acoustic information, but multiple codebook will complicate the architecture and require additional pretraining. Therefore, this work proposes to discretize mel-filterbank channels into discrete intensity bins, which produces a simple representation that outperforms existing speech\ntokenization methods.\n\nI believe this is a pioneering work that may open a potentially new track of TTS research --> use continous mel to replace discrete codec in lm base TTS.\n\nOne question:\n- How is it compared to another similar work MELL-E (https://arxiv.org/abs/2407.08551) that also use continous mel tokens for lm based TTS?\n\nStrengths: see above\n\nWeaknesses: see above\n\nQuestions: see above",
    "rating": 3,
    "label": true,
    "rationale_edited": "Summary: This work propose to solve the problem of codec: it is hard for one codebook to cover both semantic and acoustic information, but multiple codebook will complicate the architecture and require additional pretraining. Therefore, this work proposes to discretize mel-filterbank channels into discrete intensity bins, which produces a simple representation that outperforms existing speech\ntokenization methods.\n\nI believe this is a pioneering work that may open a potentially new track of TTS research --> use continous mel to replace discrete codec in lm base TTS.\n\n- How is it compared to another similar work MELL-E (https://arxiv.org/abs/2407.08551) that also use continous mel tokens for lm based TTS?",
    "chosen": true
  },
  {
    "title": "BioMaze: Benchmarking and Enhancing Large Language Models for Biological Pathway Reasoning",
    "abstract": "Large language models (LLMs) have demonstrated remarkable performance across various domains of biology, but their ability to reason about biological pathways remains underexplored. This includes reasoning about how perturbations in biological systems lead to various downstream effects through complex intermediate processes. Such reasoning is crucial for explaining and predicting biological phenomena, as well as for formulating hypotheses and designing experiments.\n\nIn this study, we investigate whether LLMs can effectively understand and reason about biological pathways by introducing BioMaze, a comprehensive benchmark focusing on reasoning about the effects and mechanisms of natural and synthetic interventions—such as mutations, infections, or treatments—on various downstream targets under different conditions through complex intermediate pathway processes. BioMaze spans multiple biological domains and is categorized along three reasoning dimensions, capturing various aspects of pathway reasoning.\n\nWe evaluate LLMs using the BioMaze benchmark with reasoning methods like Chain-of-Thought (CoT) and pathway graph-augmented approaches. Results show that while LLMs can understand mechanisms in natural organisms, they struggle with predicting phenomena after perturbations, highlighting their limitations in reasoning about biological pathways. To address these challenges, we propose PathSeeker, a novel LLM agent that interactively reasons through subgraph-based navigation within the pathway graph. This approach enhances LLMs' reasoning in biological pathways by leveraging pathway graph augmentation, particularly in cases involving perturbations, potentially bridging the gap between LLMs' current capabilities and the complexities of biological systems.",
    "text": "# Introduction\n\n<figure id=\"mainfig\">\n<p><br />\n</p>\n<figcaption>Illustration of BioMaze task and reasoning method with or without additional biological pathway graph data guidance. The task of BioMaze focuses on reasoning about the effects and mechanisms of natural components or synthetic interventions on various downstream targets under different conditions through complex intermediate pathway processes.</figcaption>\n</figure>\n\n# Related Work\n\n**Biological Scientific Question Answering** Previous studies have explored the potential of language models in the biological scientific domain [@lu2022learn; @vilares2019head; @jin2021disease; @pal2022medmcqa], such as biological scientific reading comprehension [@welbl2018constructing; @jin2019pubmedqa] and question-answering [@krithara2023bioasq]. A few studies have examined language models' ability to complete biological pathways [@li2023chatpathway; @park2023comparative; @azam2024comprehensive]. Different from previous tasks, this work introduces biological pathway reasoning in realistic research scenarios. See Appendix [7.12](#appendix related work){reference-type=\"ref\" reference=\"appendix related work\"} for a more detailed comparison.\n\n**Graph-augmented Language Model** Several studies have explored augmenting LLMs with graph data. In particular, some works enhance LLMs by encoding graph data as text [@ye2023natural; @wang2024can; @fatemi2023talk], or tuning LLMs specifically for graph-based tasks [@liu2023one; @tang2024graphgpt; @he2024g; @zhao2023gimlet; @he2024unigraph]. Augmented LLMs have been applied to knowledge-based QA [@sun2023think; @he2024g; @li2023chain; @jin2024graph; @cheng2024call; @edge2024local] and graph tasks like property prediction [@wang2024can; @he2023harnessing]. Some studies leverage graph structures for complex reasoning tasks [@jiang2023structgpt; @besta2024graph]. Most large graph databases use retrieval mechanisms [@he2024g; @li2023chain], while others employ LLMs as interactive agents for navigation [@sun2023think; @jin2024graph; @li2024graphreader]. This work introduces a more efficient agent-based approach using subgraph navigation and reasoning to improve pathway database exploration.\n\n# Benchmark: BioMaze\n\n## Dataset Creation\n\nBioMaze is created by generating question-answer pairs from biological pathway research papers, which are then checked and filtered through a combination of automated methods and expert human review. The dataset creation process involves prompting large language models, with GPT-4 and LLaMA3.1-405B [@dubey2024llama] being selected for data generation in this study.\n\nTo gather relevant biological pathway questions in realistic scientific research contexts, particularly those involving interventions, the data for BioMaze is sourced from over 6,000 biological pathway research papers. These studies include carefully designed experimental interventions supported by pathway mechanisms to observe biological system responses. After extracting detailed experimental observations and their contexts, we convert each one into either a True/False or open-ended question, depending on its content. Each question is paired with corresponding labeled answers.\n\nWe then apply multiple data filters and human expert reviews to ensure the accuracy and quality of the questions. The accuracy of each question is validated by comparing it with the content of the original paper. Question quality is ensured through several filters that remove questions that are poorly defined, ask for specific measurement values, query more than one fact, are trivial (with answers revealed in the question's context), or are unrelated to biological pathways.\n\nFinally, all questions are reviewed by human experts based on quality dimensions and their judgment to ensure overall question quality. The passing rate for expert review is approximately 40%. After applying all filters, BioMaze contains 5.1k high-quality questions. More details are provided in Appendix [7.2](#appendix: data creation){reference-type=\"ref\" reference=\"appendix: data creation\"}.\n\nThe questions of BioMaze cover a wide range of biological domains, as illustrated in Figure [2](#fig bio fan){reference-type=\"ref\" reference=\"fig bio fan\"} (left).\n\n## Reasoning Type Categories\n\n::: table*\n:::\n\nTo study various research scenarios in biological pathways, such as natural dynamic changes, disturbances and interventions, and additional intervention conditions, as well as a multi-scale understanding of single factors, action processes, and macroscopic functions, we classify BioMaze tasks from three dimensions, namely inquiry type, extra condition, and investigation target, as shown in Table [\\[table category example\\]](#table category example){reference-type=\"ref\" reference=\"table category example\"}. More full question cases are in Appendix [7.1](#appendix data cases){reference-type=\"ref\" reference=\"appendix data cases\"}. The distribution of the three dimensions' questions is shown in Figure [2](#fig bio fan){reference-type=\"ref\" reference=\"fig bio fan\"} (right). We introduce each category of the dimensions below:\n\n<figure id=\"fig bio fan\">\n\n<figcaption>Dataset biological domain and reasoning type distribution. Left: BioMaze covers six main domains: metabolism, genetic information processing, environmental information processing, cellular processes, organismal systems, and human diseases. Right: BioMaze is categorized along three dimensions of reasoning types: inquiry type, extra condition, and investigation target.</figcaption>\n</figure>\n\n**Dimension 1: Inquiry Type** is the independent variable studied, which can be either **Normal Source**, involving the prediction of the effects of natural components in their normal state within a biological pathway, or **Perturbed Source**, which deals with predicting the effects of external interventions or treatments---such as mutations, infections, or experimentally introduced elements---on downstream targets within pathways. Normal Source tasks focus on understanding the fundamental mechanisms and natural dynamics of pathways, while Perturbed Source tasks examine the phenomenon under perturbation.\n\n**Dimension 2: Extra Condition** refers to additional settings besides the independent variable. This could be the **Natural Condition**, where no additional treatments are applied, and the pathway operates under the organism's natural conditions, or the **Intervened Condition**, which assesses the impact of the inquiry source when the pathway has already been influenced by other factors, such as mutations or interventions. The Intervened Condition challenges the model by requiring it to deduce the system's behavior under unnatural conditions, thus increasing the reasoning difficulty.\n\n**Dimension 3: Investigation Target** refers to the dependent variable in the question, which could be **Single Component as Target**, focusing on the effect of the source on a specific component within the pathway; **Components Interaction as Target**, examining the effect of the source on interactions between components within the pathway; or **Function as Target**, evaluating the effect of the source on broader biological functions or macro-level phenomena. The multi-scale targets address the reasoning of single components, downstream processes, or organism-wide outcomes.\n\n## Pathway Graph Augmentated Reasoning\n\nText-only reasoning methods, such as Chain-of-Thought (CoT), generate reasoning based on the inherent knowledge of LLMs. However, biological pathways present unique challenges due to their graph-structured nature. Consequently, using graph-augmented LLM reasoning is a natural approach for BioMaze. Providing explicit pathway graphs can serve as structural blueprints, enhancing reasoning from both knowledge and planning perspectives. We formalize this problem as follows: $$\\begin{aligned}\na = G (\\mathcal{E}, o),\n\\end{aligned}$$ where $G$ represents the language model, $\\mathcal{E}$ denotes the task instruction (including the question), $o$ refers to the observation from the augment pathway graph database, and $a$ is the model output which could be the reasoning and answer.\n\n**Pathway Graph Database** To augment LLMs with reasoning in biological pathways, we created a pathway graph database based on KEGG [@kanehisa2000kegg], a collection of pathway maps on metabolism and various cellular and organismal functions widely-used resource among biologists. We compiled all available pathway networks and maps from KEGG and integrated all of them into a single pathway graph database. The statistics for the pathways are in Appendix [7.4](#appendix pathway statistic){reference-type=\"ref\" reference=\"appendix pathway statistic\"}. Each entry in the dataset is provided with a detailed description and function corpus. The graph is structured in triples as \\[, , [(, )]{style=\"color: gray\"}\\].\n\n**Pathway Graph Database API:** When the language model accesses the pathway database, it may need to retrieve relevant triples from the pathway graph using APIs like $\\operatorname{Search\\_Node}$, $\\operatorname{Search\\_Edge}$, $\\operatorname{Search\\_Triple}$, and $\\operatorname{Search\\_Subgraph}$ [@sun2023think; @li2023chain]. Our pathway database supports these core retrieval APIs, which are based on detailed descriptions and functional corpora. These APIs are essential for enabling various graph-augmented reasoning methods in LLMs. When the pathway graph $S$ is to be input to the LLM, they are encoded into text $o$ by $o=\\operatorname{TripleToText}(\\operatorname{DFSOrder}(S))$. The details of the API implementations are described in Appendix [7.5](#appendix subgraph retriever){reference-type=\"ref\" reference=\"appendix subgraph retriever\"}.\n\n# Method: Pathway Reasoning Agent [PathSeeker]{.smallcaps}\n\n<figure id=\"fig agent method\">\n\n<figcaption><span class=\"smallcaps\">PathSeeker</span> allows interactive browsing of the pathway graph database by navigating through subgraphs. At each step, <span class=\"smallcaps\">PathSeeker</span> can perform either a global subgraph search or a local search around a previously explored pathway step. This functionality enables <span class=\"smallcaps\">PathSeeker</span> to fully leverage the augmented pathway graph database during biological pathway reasoning.</figcaption>\n</figure>\n\nAs we evaluated several graph-augmented reasoning methods, we found that current graph-augmentation methods' performance is limited by their ineffective utilization of the pathway graph database for reasoning. Inspired by how scientists browse pathway networks during reasoning, we propose [PathSeeker]{.smallcaps}, a reasoning agent method that can interactively conduct reasoning and take actions to perceive and navigate pathways using a web-like engine, along with flexible reasoning in each step, as shown in Figure [3](#fig agent method){reference-type=\"ref\" reference=\"fig agent method\"}.\n\nAt each step, $t$, the language agent $G$ can conduct reasoning by natural language thought, and takes an action step $a_t$, based on problem $\\mathcal{E}$ (problem instructions) and previous observation-action trajectory $h_t=[o_1,a_1, \\dots, o_{t-1}, a_{t-1}, o_t]$, $$a_t =G(\\mathcal{E}, h_t)$$\n\n**Global and Local Subgraph Navigation** In addition to the global subgraph retriever $\\operatorname{Search\\_Subgraph}$, [PathSeeker]{.smallcaps} has access to an additional neighbor subgraph retriever, $\\operatorname{Neighbor\\_Subgraph}(\\texttt{line\\_id}, \\texttt{query}, N)$, which retrieves an optimal connected subgraph of target size from the multi-hop neighbors of a previously observed pathway step $\\texttt{line\\_id}$. $$\\small\n\\begin{aligned}\n\\operatorname{Neighbor\\_Subgraph}&(\\texttt{line\\_id}, \\texttt{query}, N)= \\\\\\underset{\\substack{S \\subseteq P_{id}, S \\text { is connected }, |S|=N}}{\\operatorname{argmax}} &\\sum_{i \\in V_S \\cup E_S} \\operatorname{score}(i, \\texttt{query})\n\\end{aligned}$$ Here, $P_{id}$ represents the multi-hop neighbors of the triple with $\\texttt{line\\_id}$. This allows [PathSeeker]{.smallcaps} to navigate the pathway graph database by either performing a global search or by exploring the multi-hop neighbors of an observed subgraph at each step. See Appendix for case [7.6](#appendix agent case){reference-type=\"ref\" reference=\"appendix agent case\"}.\n\n**Graph Encoding** In step $t$, the action taken by LLM agent get subgraph $S_t$ from environment, and the subgraph is encoded into text observation $o_t$ as following: $$\\small\n\\begin{aligned}\n\\hat{S}_t&=\\operatorname{DFSOrder}\\left(\\operatorname{RemoveSeen}(S_t,[S_1, \\dots, S_{t-1}])\\right) \\\\\no_t&=\\operatorname{TripleToOrderedText}\\left(\\hat{S}_t, \\operatorname{TotalNum}\\left([S_1, \\dots, S_{t-1}]\\right)\\right)\n\\end{aligned}$$ Function $\\operatorname{RemoveSeen}$ eliminates triples from the $t$-th turn's subgraph that have been observed in previous turns, ensuring that each triple appears in the LLM's observations only once when first retrieved. This approach enhances content length efficiency and encourages the LLM to understand the whole navigation history rather than focusing solely on the most recent turn.\n\nThe function $\\operatorname{TripleToOrderedText}$ convert ordered subgraph $\\hat{S}_t$ into text in the following format: $\\text{Line ID)}  \\text{ Head } |  \\text{ Tail } | \\text{ Relation and Biological Process}$. These global line IDs indicate the order of each triple across all turns, providing a unique reference for the LLM agent during local searches or reasoning. For the $t$-th turn's subgraph $S_t$, the ID starts at the total number of unique triples seen in previous history, given by $\\operatorname{TotalNum}([S_1, \\dots, S_{t-1}])$.\n\n**Final Reasoning** As graph data browsing finishes, the final reasoning is conducted based on all the navigation history: $$a_r =G(\\mathcal{E}_r, [o_1, \\dots, o_{T}])$$\n\n**Graph Navigation Capacity** The combination of global and local subgraph retrieval APIs empowers LLM agents to explore the entire network flexibly and efficiently. It allows the LLM to guide its exploration by adjusting both keywords and the root of the local subgraph, depending on the intermediate reasoning, offering stronger expressiveness than navigation methods like BFS, DFS, and various retrieval methods.\n\n# Experiment\n\n## Baseline and Metric\n\nWe evaluate the reasoning performance of LLMs on BioMaze in both the unaugmented step-by-step reasoning and the pathway graph-augmented methods. We adopt reasoning method without graph augmentation Chain-of-Thought (CoT) [@wei2022chain; @kojima2022large], and methods with pathway graph augmentation: Chain-of-Knowledge (CoK) [@li2023chain], Think-of-Graph (ToG) [@sun2023think], and G-Retriever [@he2024g]. Details of baselines are in Appendix [7.7](#appendix baseline){reference-type=\"ref\" reference=\"appendix baseline\"}.\n\nFor True/False tasks, we compute accuracy averaged across the True and False labels to account for label imbalance in the dataset. For open-ended tasks, the LLM is used to evaluate the accuracy of generated answers by comparing them to the ground truth and determining whether they are correct or incorrect. In this study, we use the LLaMA3.1-405B model as the evaluator, with five in-context examples. The performance of the evaluator is further analyzed in Appendix [\\[subsection evaluator quality\\]](#subsection evaluator quality){reference-type=\"ref\" reference=\"subsection evaluator quality\"}.\n\n## Main Result\n\n::: table*\n:::\n\n::: table*\n:::\n\nWe evaluate [PathSeeker]{.smallcaps} and baseline methods on BioMaze, presenting results in Tables [\\[table judge performance\\]](#table judge performance){reference-type=\"ref\" reference=\"table judge performance\"} and [\\[table generative performance\\]](#table generative performance){reference-type=\"ref\" reference=\"table generative performance\"}. The comparison covers task dimensions including signal source, additional conditions, and target. The results lead to the following conclusions:\n\n**LLMs struggle with biological pathway reasoning.** Pathway reasoning tasks in BioMaze are tough for LLMs, with True/False accuracy slightly above random. Both CoT and graph-augmented reasoning reveal the gap between LLM capabilities and the complexity of biological systems.\n\n**Perturbation query in BioMaze presents significant challenges.** LLMs perform worse with perturbed inquiry settings than normal ones, especially in True/False and open-ended formats. This suggests that reasoning about biological pathways is harder in intervention scenarios, where the events are less aligned with common biological knowledge. answerable using established biological knowledge about how typical pathways work.\n\n**Intervened conditions present greater reasoning challenges.** Like perturbations, intervention condition complicate reasoning by disrupting the biological system. These scenarios require more deductive reasoning, as they rely less on typical biological knowledge.\n\n**Reasoning target brings diverse challenges for reasoning.** The Investigation target creates varied difficulties, causing inconsistent performance across models and reasoning methods. \\\"Function as target\\\" is the most difficult category.\n\n**Pathway augmentation can enhance reasoning in biological systems, especially for intervention cases.** As shown in Tables [\\[table judge performance\\]](#table judge performance){reference-type=\"ref\" reference=\"table judge performance\"} and [\\[table generative performance\\]](#table generative performance){reference-type=\"ref\" reference=\"table generative performance\"}, reasoning methods with pathway augmentation, especially [PathSeeker]{.smallcaps}, outperform non-augmented approaches. [PathSeeker]{.smallcaps} consistently exceeds CoT across most question types and categories, regardless of the backbone model, highlighting the value of integrating biological pathways to enhance reasoning in biological systems. Additionally, [PathSeeker]{.smallcaps} outperforms other graph augmentation methods, demonstrating the effectiveness of its subgraph-based navigation approach. Notably, it reduces the performance gap between natural and intervened/perturbed groups, helping bridge the gap in pathway causal reasoning.\n\n## Analysis\n\n<figure id=\"fig model comp\">\n<p><br />\n</p>\n<figcaption>Comparison of the reasoning abilities of different LLMs for biological pathways. While overall performance improves with larger and more powerful models, a consistent gap exists between normal/perturbed and natural/intervened settings. This highlights the inherent limitations of LLMs in reasoning about the causal relationships within biological pathways.</figcaption>\n</figure>\n\n**Backbone Ability for Pathway Reasoning** We compare the performance of different backbones in Figure [4](#fig model comp){reference-type=\"ref\" reference=\"fig model comp\"}. As the model size and capacity increase, overall performance improves, indicating a strong correlation between an LLM's general reasoning ability and its performance in pathway reasoning. However, a consistent gap remains between the normal/perturbed and natural/intervened settings across different backbones. This underscores the inherent limitations of LLMs in reasoning about causal relationships within biological pathways.\n\n**Reasoning Difficulty with Steps** To explore the relationship between task difficulty and reasoning steps in BioMaze, we prompted LLaMA3.1-405B to explain its reasoning process based on the correct answer and pathway to get the step numbers.\n\nFigure [5](#fig reasoning step){reference-type=\"ref\" reference=\"fig reasoning step\"} shows that Chain-of-Thought (CoT) performance declines as reasoning steps increase, suggesting that more steps lead to higher reasoning difficulty. This supports our hypothesis that the complexity of the pathways is one factor of challenges in biological pathway reasoning.\n\nNotably, [PathSeeker]{.smallcaps}'s performance remains more consistent across different reasoning step counts. This suggests that augmenting LLMs with biological pathway information can mitigate the challenges of pathway reasoning, particularly when dealing with intricate intermediate processes.\n\n<figure id=\"fig reasoning step\">\n\n<figcaption>Performance versus reasoning steps. LLMs face increasing difficulty in reasoning about biological systems as task complexity rises and requires more reasoning steps. In contrast, pathway augmentation significantly mitigates the drop of performance for tasks that involve more steps.</figcaption>\n</figure>\n\n**Failure Reasons Statistics** We analyze failed cases in biological pathway reasoning, covering CoT and [PathSeeker]{.smallcaps}, and classify the failures into: (1) **Unresolved Conclusion (UC)** For cases where the model fails to provide a definitive answer, indicating uncertainty or belief that the answer is unknown. (2) **Incomplete Answer (IA)** When the response lacks essential details, such as missing the requested effects or other key elements. (3) **Omission in Reasoning (OR)** For errors where critical pathway steps in the question's biological process are left out, causing the final answer to be incorrect. (4) **Faulty in Reasoning (FR)** When the reasoning path is correct, but there are significant errors in deducing the events within that pathway. We manually classify 200 random samples from these error cases to approximate the overall error cases, with a professional biology Ph.D. student.\n\nThe results in Figure [6](#fig error analysis){reference-type=\"ref\" reference=\"fig error analysis\"} show that in both True/False and open-ended tasks, the main error in CoT reasoning is faulty reasoning, where LLMs correctly identify the biological pathway but misinterpret the events within it. Another key error is omission, where critical steps or branches of the pathway are overlooked. This highlights the challenges LLMs face in reasoning about biological pathways, due to both knowledge gaps and difficulties in deductive reasoning.\n\n[PathSeeker]{.smallcaps} reduces faulty reasoning by providing pathway graphs, improving accuracy. However, omissions remain a challenge, often due to limitations in the pathway database and browsing issues. With pathways available, LLMs are more confident and less fail in drawing conclusions.\n\n<figure id=\"fig error analysis\">\n<p><span><embed src=\"fig/judge_error_ratio_GPT3.5_y.pdf\" style=\"width:45.0%\" /></span></p>\n<figcaption>Error analysis for CoT reasoning and reasoning with pathway augmentation (our method <span class=\"smallcaps\">PathSeeker</span>). The primary cause of errors in (CoT) reasoning for biological systems is due to both faulty reasoning and omissions in reasoning. When pathway augmentation is applied, omissions in reasoning become the predominant issue, but the rate of faulty reasoning is significantly reduced, thereby improving the overall reasoning accuracy of LLMs in biological systems.</figcaption>\n</figure>\n\n::::: minipage\n::: minipage\n  Agent #Steps    1-4     4-6     6-8    8-10    $\\ge$`<!-- -->`{=html}10\n  -------------- ------ ------- ------- ------- --------------------------\n  True / False    0.91   50.14   26.58   12.66             9.70\n  Open-Ended      1.45   52.44   25.69   13.97             6.46\n:::\n\n::: minipage\n:::\n:::::\n\n## Method Analysis and Ablation Study\n\n**API Usage and Step Distribution** We analyze [PathSeeker]{.smallcaps}'s agent behavior by reviewing task steps and API usage. Tables [\\[table agent steps\\]](#table agent steps){reference-type=\"ref\" reference=\"table agent steps\"} and [\\[table API usage\\]](#table API usage){reference-type=\"ref\" reference=\"table API usage\"} show that most tasks are completed in six or fewer steps, though some require over ten due to missing pathway data. On average, the agent performs 1.5 global searches and over three local navigations per task, indicating frequent subgraph exploration.\n\n**Ablation Study** To evaluate [PathSeeker]{.smallcaps}'s components, we perform ablation studies, with results for LLaMA3-8B in Table [\\[table ablation\\]](#table ablation){reference-type=\"ref\" reference=\"table ablation\"}. The most important component is FinalReaser; without it, the agent's answers degrade due to long task history. The local search API is also crucial for efficient graph navigation, and the graph encoding method improves performance, highlighting the value of encoding graph data for sequential models.\n\n::: table*\n:::\n\n# Conclusion\n\nIn this study, we introduce BioMaze, a benchmark designed to evaluate LLMs' ability to understand and reason about biological pathways. Extensive evaluations using BioMaze, incorporating advanced methods like CoT and graph-augmented approaches, show that LLMs struggle with understanding pathway mechanisms. We also propose [PathSeeker]{.smallcaps}, a novel LLM agent that uses interactive subgraph exploration to enhance reasoning in biological pathways.\n\n# Appendix\n\n## Dataset Case Illustration {#appendix data cases}\n\n**Dimension 1: Inquiry Type**\n\n**Category 1: Normal Source**\n\nQuestion: In the context of YopT-induced cleavage of Rho GTPases, is carboxyl methylation a necessary post-translational modification for the cysteine protease activity of YopT to occur?\n\nAnswer: No\n\nQuestion: In U2OS-hGR osteosarcoma cells, does the glucocorticoid receptor (GR)-dependent mRNA expression of ladinin 1 require the presence of both Mediator subunits MED1 and MED14 for transcriptional regulation in response to dexamethasone treatment?\n\nAnswer: Yes\n\nQuestion: In pancreatic acinar cells, How does the sensitivity to nicotinic acid adenine dinucleotide phosphate (NAADP) compare to the sensitivity to cyclic adenosine 5'-diphosphate-ribose (cADPR) and inositol trisphosphate (InsP3) in terms of triggering Ca2+ release?\n\nAnswer: Pancreatic acinar cells are significantly more sensitive to NAADP than to either cyclic adenosine 5'-diphosphate-ribose (cADPR) or inositol trisphosphate (InsP3).\n\nQuestion: In the context of Mycobacterium tuberculosis signal transduction, what is the effect of TrcS-phosphate and calcium ions (Ca2+) on the transphosphorylation of the response regulator protein TrcR?\n\nAnswer: TrcS-phosphate and Ca2+ are required for the transphosphorylation of TrcR.\n\n**Category 2: Perturbed Source**\n\nQuestion: In the context of human papillomavirus type 16 (HPV16) E7 gene product's role in cellular transformation, can a transcriptionally inactive c-Jun deletion mutant that retains E7 binding capability interfere with the E7-induced transformation of rat embryo fibroblasts when co-expressed with an activated ras oncogene?\n\nAnswer: Yes\n\nQuestion: In HTLV-1-infected T-cell lines, does the application of short interfering RNA (siRNA) targeting JunD result in an increase in matrix metalloproteinase-7 (MMP-7) mRNA expression?\n\nAnswer: No\n\nQuestion: In the context of opioid withdrawal, how does blocking A1-adenosine receptors with 8-cyclopentyl-1, 3-dipropylxantine affect the response to mu-opioid receptor activation on the amplitude of GABAB-mediated IPSPs in slices taken from morphine-treated guinea pigs?\n\nAnswer: Blocking A1-adenosine receptors with 8-cyclopentyl-1, 3-dipropylxantine allows mu-opioid receptor activation to inhibit the IPSP at all concentrations and increases the maximal inhibition in withdrawn slices.\n\nQuestion: In 3T3-L1 adipocytes, What is the effect of platelet-derived growth factor treatment on the tyrosine phosphorylation of APS?\n\nAnswer: Platelet-derived growth factor treatment results in no APS phosphorylation.\n\n**Dimension 2: Extra Condition**\n\n**Category 1: Natural Condition**\n\nQuestion: In the context of Kaposi's sarcoma-associated herpesvirus (KSHV) infection, can viral interleukin-6 (vIL-6) induce the up-regulation of DNA methyltransferase 1 (DNMT1) in endothelial cells independently of signal transducer and activator of transcription 3 (STAT3) activation?\n\nAnswer: No\n\nQuestion: In BJAB cells, does the expression of the K5 protein, encoded by the Kaposi's sarcoma-associated herpesvirus (KSHV), lead to a reduction in the surface expression of intercellular adhesion molecule 1 (ICAM-1)?\n\nAnswer: Yes\n\nQuestion: In human hepatoma HuH-7 cells, What is the effect of transient expression of the Hepatitis C virus (HCV) core protein on the trans-activation ability of the cellular putative RNA helicase CAP-Rf?\n\nAnswer: Transient expression of HCV core protein in human hepatoma HuH-7 cells potentiates the trans-activation effect of CAP-Rf on gene expression.\n\nQuestion: In Caenorhabditis elegans, What is the effect of being heterozygous for the cet-1 gene on body length compared to wild-type and null mutant individuals?\n\nAnswer: Heterozygotes for cet-1 display body lengths ranging between null mutant and wild type.\n\n**Category 2: Intervened Condition**\n\nQuestion: In the Edar signalling pathway, can NF-kappaB activation still be induced by Edar-associated death domain (Edaradd) in the presence of dominant negative forms of TNF-receptor-associated factor 6 (TRAF6)?\n\nAnswer: No\n\nQuestion: In mice treated with the angiotensin-converting enzyme inhibitor ramipril, do those lacking the specific PACAP receptor (PAC1-/-) exhibit lower plasma renin concentrations compared to their wild-type littermates?\n\nAnswer: Yes\n\nQuestion: In human lung cancer cell lines with methylated retinoic acid receptor-beta (RARbeta) P2 promoter, What is the effect of demethylation by 5-aza-2'-deoxycytidine on the expression of RARbeta2 and RARbeta4 isoforms?\n\nAnswer: Demethylation by exposure to 5-aza-2'-deoxycytidine restores the expression of RARbeta2 and RARbeta4 in methylated tumor lines.\n\nQuestion: In the context of hematopoietic progenitor cell proliferation, What is the effect of Flt3 ligand (FL) on progenitor cells from mice deficient in the signal transducer and activator of transcription 5a (Stat5a)?\n\nAnswer: Flt3 ligand (FL) does not act on progenitors from marrows of Stat5a(-/-) mice.\n\n**Dimension 3: Investigation Target**\n\n**Category 1: Single Component as Target**\n\nQuestion: In MELN cells, which are derived from MCF-7 breast cancer cells and stably express estrogen receptor alpha (ERalpha), does exposure to mono-n-butyl ester phthalic acid result in an increase in vascular endothelial growth factor (VEGF) secretion?\n\nAnswer: No\n\nQuestion: In the context of non-alcoholic fatty liver disease (NAFLD) induced by a choline-deficient, ethionine-supplemented (CDE) diet in C57BL/6J mice, is the protein level of peroxisome-proliferator-activated receptor-gamma coactivator 1alpha (PGC1alpha) decreased in comparison to control livers?\n\nAnswer: Yes\n\nQuestion: In the context of Mycobacterium tuberculosis signal transduction, what is the effect of TrcS-phosphate and calcium ions (Ca2+) on the transphosphorylation of the response regulator protein TrcR?\n\nAnswer: TrcS-phosphate and Ca2+ are required for the transphosphorylation of TrcR.\n\nQuestion: In the context of myeloma cells, What is the effect of elevated heparanase expression on the expression levels of urokinase-type plasminogen activator (uPA) and the uPA receptor?\n\nAnswer: Elevation in heparanase expression in myeloma cells increases urokinase-type plasminogen activator (uPA) and uPA receptor expression levels.\n\n**Category 2: Components Interaction as Target**\n\nQuestion: In the context of growth hormone-releasing hormone (GHRH) stimulation of growth hormone (GH) gene expression in anterior pituitary somatotrophs, does the CBP-Pit-1 pathway, which involves the interaction between CREB binding protein (CBP) and the pituitary-specific transcription factor Pit-1, require the presence of cAMP-response element binding protein (CREB) to mediate its effects on the human GH promoter?\n\nAnswer: No\n\nQuestion: In the context of mitophagy, does the ubiquitin-binding protein RABGEF1 play a role in recruiting the downstream Rab GTPases, RAB5 and RAB7A, to damaged mitochondria that have been ubiquitinated by Parkin?\n\nAnswer: Yes\n\nQuestion: In human lung cancer cells treated with the nitrosamine 4-(methylnitrosamino)-1-(3-pyridyl)-1-butanone (NNK), What is the role of Bcl2 phosphorylation at Ser(70) in the interaction between Bcl2 and c-Myc oncogenic proteins?\n\nAnswer: Phosphorylation of Bcl2 at Ser(70) promotes a direct interaction between Bcl2 and c-Myc, enhancing the half-life of the c-Myc protein.\n\nQuestion: In cells infected with Epstein-Barr virus (EBV), How does the BGLF4 protein kinase affect the recruitment of active interferon regulatory factor 3 (IRF3) to the IRF3-responsive element containing the interferon-beta (IFN-beta) promoter region?\n\nAnswer: BGLF4 reduces the amount of active IRF3 recruited to the IRF3-responsive element containing the IFN-beta promoter region.\n\n**Category 3: Function as Target**\n\nQuestion: In human hepatoma cells (Huh-7) and normal human hepatocytes (Hc) infected with an adenovirus expressing a mutated form of IkappaBalpha (Ad5IkappaB), does pretreatment with N,N-dimethylsphingosine (DMS), an inhibitor of sphingosine kinase (SphK), lead to an increase in the number of apoptotic cells induced by tumor necrosis factor-alpha (TNF-alpha) stimulation?\n\nAnswer: Yes\n\nQuestion: In Escherichia coli, do mutants lacking acyl-acyl carrier protein (acyl-ACP) synthetase activity (aas mutants) retain the ability to incorporate exogenous fatty acids into the major outer membrane lipoprotein through an acyl-CoA-independent pathway?\n\nAnswer: No\n\nQuestion: In the context of myocytes expressing adiponectin receptor 1 (AdipoR1), What is the effect of adiponectin on extracellular calcium (Ca2+) influx?\n\nAnswer: Adiponectin induces extracellular Ca(2+) influx by adiponectin receptor 1 (AdipoR1) in myocytes.\n\nQuestion: In the context of lung inflammation resolution, What is the role of cholesterol 25-hydroxylase (Ch25h) in alveolar macrophages (AM) during the resolution phase of inflammation?\n\nAnswer: Ch25h is induced in macrophages upon their encounter with apoptotic cells and is required for LXR-dependent prevention of AM lipid overload, induction of Mertk, efferocytic resolution of airway neutrophilia, and induction of TGF-beta.\n\n## Data Creation and Filter Pipeline {#appendix: data creation}\n\n<figure id=\"dataset_pipeline\">\n\n<figcaption>Dataset BioMaze creation pipeline.</figcaption>\n</figure>\n\nThe overall dataset creation pipeline is shown in Figure [7](#dataset_pipeline){reference-type=\"ref\" reference=\"dataset_pipeline\"}.\n\nTo ensure question quality, we employ a two-step process. First, we create and filter questions using an advanced language model ( GPT-4 and LLaMa 3.1-405B) to assess their relevance and clarity. Subsequently, each question undergoes a final quality check by human experts.\n\nThe well-define filter removes questions that are poorly defined, unpredictable (e.g., asking for specific measurement values), or require more than one prediction, and the nontrivial filter removes the data that answers revealed in the question's context.\n\nTo validate the answer quality, we require the LLM to answer the questions based on the original paper's content. The model is explicitly instructed to respond with Undetermined if it cannot confidently generate an answer. Each question is tested five times, and only questions that are consistently answered correctly (i.e., aligned with the intended label) and not marked as Undetermined in any of the trials are retained. This process helps eliminate questions with incorrect labels, ambiguous phrasing, or poor structure.\n\nIn the final stage, human experts perform the quality check to further refine the questions, excluding data that are poorly defined, overly complex (e.g., asking for multiple facts), incorrectly labeled, or of any other low quality, ensuring the dataset's overall reliability and usability. Approximately 60% of the data is filtered out at this stage.\n\n## Question key words Distribution\n\nWe present the distribution of question keywords in Figure [8](#fig key words fan){reference-type=\"ref\" reference=\"fig key words fan\"}. While these keywords do not directly correspond to the three main categories we primarily use, they offer an additional perspective on the dataset. Below are explanations of the keywords:\n\nGeneral Influence Inquiry: Can x influence y or not?\n\nActivation Inquiry: Can x activate y?\n\nInhibition Inquiry: Can x inhibit y?\n\nDependency Inquiry: Is y dependent on x?\n\nInduction Question: Can x induce y?\n\nRelief Inquiry: Can x relieve y?\n\nMechanism Question: Does x influence y via a specific mechanism?\n\nExclusive Mechanism Question: Is a specific mechanism the only mechanism for process z?\n\nSignificance Inquiry: Will x cause a significant/insignificant phenomenon y?\n\nBaseline Comparison Question: Is x different from the baseline?\n\nExperimental Observation Question: Will a specific detailed phenomenon be observed in the experiment?\n\nPhysiological Observation Question: Will a specific phenomenon be observed in the body?\n\n<figure id=\"fig key words fan\">\n\n<figcaption>Dataset key words distribution.</figcaption>\n</figure>\n\n## Pathway Graph Database Statistics {#appendix pathway statistic}\n\n::: {#table pathway statistic}\n  --------- ------- -----------------------------\n   Entries   Edges   Involved biological process\n    8939     15131              2265\n  --------- ------- -----------------------------\n\n  : Data statistic of our pathway network database.\n:::\n\n## Implementation of Subgraph Retriever {#appendix subgraph retriever}\n\nSince the connectivity of the pathway graph is crucial for enhancing reasoning in biological systems, we developed the retrieval API designed to find the optimal connected subgraph $S^*=\\operatorname{Search\\_Subgraph}(\\texttt{query}, N)$, where $S^*$ is the retrieved subgraph, $\\texttt{query}$ represents the query content, such as keywords, and $N$ is the target size. The goal is to match a given target size as closely as possible while maximizing the matching score. This is formulated as a optimization problem: $$\\begin{aligned}\nS^*&=\\operatorname{Search\\_Subgraph}(\\texttt{query}, N)\\\\&= \\underset{\\substack{S \\subseteq P, \\\\ |S|=N,\\\\S \\text { is connected}}}{\\operatorname{argmax}} \\sum_{i \\in V_S \\cup E_S} \\operatorname{score}(i, \\texttt{query})\\\\\n\\end{aligned}$$ Here, the overall pathway network is denoted by $P$, with $V_S$ and $E_S$ representing the node and edge sets of subgraph $S$.\n\nThe optimization problem is hard to solve directly for huge graph databse. Inspire by [@he2024g] that convert optimal subgraph problem into a Prize-Collecting Steiner Tree (PCST) problem, we solve the problem as a bi-level optimization as follows:\n\n$$\\small\n\\begin{aligned}\n&S(C_e)=\\underset{\\substack{S \\subseteq P, \\\\ S \\text { is connected }}}{\\operatorname{argmax}} \\sum_{i \\in V_S \\cup E_S} \\operatorname{score}(i, \\texttt{query})-\\left|E_S\\right| \\times C_e\\\\\n& C_e^* = \\underset{\\substack{C_e}}{\\operatorname{argmin}}  | |S(C_e)|-N|\\\\\n\\end{aligned}$$\n\nThen $$\\operatorname{Search\\_Subgraph}(\\texttt{query}, N)= S(C_e^*)\\\\\n$$\n\nThe inner optimization problem, a Prize-Collecting Steiner Tree (PCST) problem, seeks to identify a connected subgraph that maximizes the total score of its nodes and edges while minimizing the edge costs, as utilized in [@he2024g]. The outer optimization problem minimizes the difference between the optimal subgraph size and the target size. In practice, we apply a fast-solving method for the inner problem proposed by [@hegde2015nearly] and employ a binary root search algorithm for the outer problem.\n\n**Graph-to-Text Encoder** For retrieved results which is a list of triples (e.g., the output of $\\operatorname{Search\\_Triple}$ or $\\operatorname{Search\\_Subgraph}$), the result $S$ is encoded into text $o$ using the following process: $$\\begin{aligned}\no&=\\operatorname{TripleToText}(\\operatorname{DFSOrder}(S))\n\\end{aligned}$$ Function $\\operatorname{DFSOrder}$ arranges the triples in depth-first search (DFS) order. Unlike other methods, such as relevance scoring, DFS order more closely aligns with the reasoning process through the subgraph, reflecting the natural progression of biological functions. The ordered subgraph $\\hat{S}$ is converted into text format for the LLM by encoding each triple as a string in the following format: $\\text{ Head } |  \\text{ Tail } | \\text{ Relation and Biological Process}$.\n\n## [PathSeeker]{.smallcaps} Case {#appendix agent case}\n\n``` {style=\"QuestionType\"}\nPlease explore pathways to find relevant information regarding the following question: Worldwide, one of the most prevalent forms of chronic disease is alcoholic fatty liver, which may progress to more severe forms of liver injury. Chronic ethanol exposure is thought to cause enhanced hepatic lipogenesis and impaired fatty acid oxidation. Adiponectin is an adipose-derived hormone that has been associated with the development of alcoholic liver steatosis in rodent models. Does adiponectin have a protective role against alcoholic liver steatosis?\n```\n\n``` {style=\"AnswerType\"}\nAction: Thought: To find information regarding the role of adiponectin in protecting against alcoholic liver steatosis, I can use the search_biopathway_subgraph_global tool to explore relevant biological pathways. I should convert all the question content into keywords to enhance retrieval quality. Action:\n     search_biopathway_subgraph_global(['Adiponectin, adipose-derived hormone, associated with the development of alcoholic liver steatosis in rodent models, protective role against alcoholic liver steatosis, chronic ethanol exposure, enhanced hepatic lipogenesis, impaired fatty acid oxidation, liver injury'])\n     End Action\n```\n\n``` {style=\"QuestionType\"}\nState: 0) C00469: Ethanol; Ethyl alcohol; Methylcarbinol | 406999: microRNA 217 MIR217 MIRN217 mir-217 | PCrel indirect effect activation | hsa04936: Alcoholic liver disease\n1) 406999: microRNA 217 MIR217 MIRN217 mir-217 | 23411: sirtuin 1 SIRT1 SIR2 SIR2L1 SIR2alpha | PPrel inhibition indirect effect | hsa04936: Alcoholic liver disease\n2) 406999: microRNA 217 MIR217 MIRN217 mir-217 | 29896 6434: TRA2A AWMS1 HSU53209 | PPrel inhibition indirect effect | hsa04936: Alcoholic liver disease\n3) 29896 6434: TRA2A AWMS1 HSU53209 | 23175 64900 9663: LPIN1 PAP1 | PPrel activation | hsa04936: Alcoholic liver disease\n4) 23175 64900 9663: LPIN1 PAP1 | 6720: sterol regulatory element binding transcription factor 1 SREBF1 HMD IFAP2 SREBP1 bHLHd1 | PPrel inhibition | hsa04936: Alcoholic liver disease\n5) 6720: sterol regulatory element binding transcription factor 1 SREBF1 HMD IFAP2 SREBP1 bHLHd1 | 6319 79966: SCD FADS5 MSTP008 SCD1 SCDOS hSCD1 | GErel expression | hsa04152: AMPK signaling pathway hsa04936: Alcoholic liver disease\n6) 6720: sterol regulatory element binding transcription factor 1 SREBF1 HMD IFAP2 SREBP1 bHLHd1 | 23175 64900 9663: LPIN1 PAP1 | GErel expression | hsa04936: Alcoholic liver disease\n7) C00469: Ethanol; Ethyl alcohol; Methylcarbinol | 51094 79602: ADIPOR1 ACDCR1 CGI-45 CGI45 PAQR1 TESBP1A | PCrel inhibition indirect effect | hsa04936: Alcoholic liver disease\n8) C00469: Ethanol; Ethyl alcohol; Methylcarbinol | 51422 53632 5562 5563 5564 5565 5571: PRKAG2 AAKG AAKG2 CMH6 H91620p WPWS | PCrel inhibition indirect effect | hsa04936: Alcoholic liver disease\n9) C00469: Ethanol; Ethyl alcohol; Methylcarbinol | 50507: NADPH oxidase 4 NOX4 KOX KOX-1 RENOX | PCrel activation | hsa04936: Alcoholic liver disease\n10) 50507: NADPH oxidase 4 NOX4 KOX KOX-1 RENOX | C22381: Reactive oxygen species; ROS | PCrel activation | hsa04936: Alcoholic liver disease\n11) C22381: Reactive oxygen species; ROS | 51422 53632 5562 5563 5564 5565 5571: PRKAG2 AAKG AAKG2 CMH6 H91620p WPWS | PCrel inhibition indirect effect | hsa04936: Alcoholic liver disease\n12) C22381: Reactive oxygen species; ROS | 10000 207 208: AKT3 MPPH MPPH2 PKB-GAMMA PKBG PRKBG RAC-PK-gamma RAC-gamma STK-2 | PCrel inhibition | hsa04936: Alcoholic liver disease\n13) 10000 207 208: AKT3 MPPH MPPH2 PKB-GAMMA PKBG PRKBG RAC-PK-gamma RAC-gamma STK-2 | 1499: catenin beta 1 CTNNB1 CTNNB EVR7 MRD19 NEDSDV armadillo | PPrel activation phosphorylation | hsa04936: Alcoholic liver disease\n14) C22381: Reactive oxygen species; ROS | 5606 5608: MAP2K3 MAPKK3 MEK3 MKK3 PRKMK3 SAPKK-2 SAPKK2 | PCrel activation | hsa04936: Alcoholic liver disease\n15) C00469: Ethanol; Ethyl alcohol; Methylcarbinol | 712 713 714: C1QA C1QD1 | PCrel activation | hsa04936: Alcoholic liver disease\n16) C00469: Ethanol; Ethyl alcohol; Methylcarbinol | 3605: interleukin 17A IL17A CTLA-8 CTLA8 IL-17 IL-17A IL17 ILA17 | PCrel indirect effect activation | hsa04936: Alcoholic liver disease\n17) C00084: Acetaldehyde; Ethanal | 51422 53632 5562 5563 5564 5565 5571: PRKAG2 AAKG AAKG2 CMH6 H91620p WPWS | PCrel inhibition indirect effect | hsa04936: Alcoholic liver disease\n18) 9370: adiponectin C1Q and collagen domain containing ADIPOQ ACDC ACRP30 ADIPQTL1 ADPN APM-1 APM1 GBP28 | 51094 79602: ADIPOR1 ACDCR1 CGI-45 CGI45 PAQR1 TESBP1A | PPrel activate activation | hsa04152: AMPK signaling pathway hsa04211: Longevity regulating pathway hsa04936: Alcoholic liver disease hsa04920: Adipocytokine signaling pathway N01788: ADIPOQ-ADIPOR signaling pathway\n19) 7132: TNF receptor superfamily member 1A TNFRSF1A CD120a FPF TBP1 TNF-R TNF-R-I TNF-R55 TNFAR TNFR1 TNFR55 TNFR60 p55 p55-R p60 | 6720: sterol regulatory element binding transcription factor 1 SREBF1 HMD IFAP2 SREBP1 bHLHd1 | PPrel indirect effect activation | hsa04936: Alcoholic liver disease\n```\n\n``` {style=\"AnswerType\"}\nAction: Thought: I have gathered a large number of pathways related to the question. To better understand the role of adiponectin in protecting against alcoholic liver steatosis, I will check the neighboring pathways of Pathway 18, which is directly related to adiponectin. I will use the function search_biopathway_triple_N_hop_subgraph with history_line_id set to 18. The keywords are the same as previously used.\n     Action:\n     search_biopathway_triple_N_hop_subgraph(18, ['Adiponectin, adipose-derived hormone, associated with the development of alcoholic liver steatosis in rodent models, protective role against alcoholic liver steatosis, chronic ethanol exposure, enhanced hepatic lipogenesis, impaired fatty acid oxidation, liver injury'])\n     End Action\n```\n\n``` {style=\"QuestionType\"}\nState: 20) C00219: Arachidonate; Arachidonic acid; (5Z8Z11Z14Z)-Icosatetraenoic acid; cis-581114-Eicosatetraenoic acid; 5Z8Z11Z14Z-Eicosatetraenoic acid; | 2180 2181 2182 23205 23305 51703: ACSL1 ACS1 FACL1 FACL2 LACS LACS1 LACS2 | pathway bind to | hsa04216: Ferroptosis N01590: Arachidonate/Adrenic acid metabolism\n21) 2180 2181 2182 23205 23305 51703 81616: ACSL1 ACS1 FACL1 FACL2 LACS LACS1 LACS2 | C00162: Fatty acid | PCrel  | hsa04920: Adipocytokine signaling pathway\n22) 1376: carnitine palmitoyltransferase 2 CPT2 CPT1 CPTASE IIAE4 | 51 8310: ACOX1 ACOX AOX MITCH PALMCOX SCOX | ECrel compound | hsa00071: Fatty acid degradation\n23) 1376: carnitine palmitoyltransferase 2 CPT2 CPT1 CPTASE IIAE4 | 126129 1374 1375: CPT1C CATL1 CPT1-B CPT1P CPTI-B CPTIC SPG73 | ECrel compound | hsa00071: Fatty acid degradation\n24) 126129 1374 1375: CPT1C CATL1 CPT1-B CPT1P CPTI-B CPTIC SPG73 | 2180 2181 2182 23205 23305 51703 81616: ACSL1 ACS1 FACL1 FACL2 LACS LACS1 LACS2 | ECrel compound | hsa00071: Fatty acid degradation hsa04920: Adipocytokine signaling pathway\n25) 4217: mitogen-activated protein kinase kinase kinase 5 MAP3K5 ASK1 MAPKKK5 MEKK5 | 5609 6416: MAP2K7 JNKK2 MAPKK7 MEK MEK_7 MKK7 PRKMK7 SAPKK-4 SAPKK4 | PPrel activate activation phosphorylation | hsa05418: Fluid shear stress and atherosclerosis hsa04668: TNF signaling pathway hsa04936: Alcoholic liver disease hsa05208: Chemical carcinogenesis - reactive oxygen species N01407: Metals to JNK signaling pathway\n26) 5609 6416: MAP2K7 JNKK2 MAPKK7 MEK MEK_7 MKK7 PRKMK7 SAPKK-4 SAPKK4 | 5599 5601 5602: MAPK8 JNK JNK-46 JNK1 JNK1A2 JNK21B1/2 PRKM8 | PPrel activate activation phosphorylation | hsa05418: Fluid shear stress and atherosclerosis hsa05135: Yersinia infection hsa05417: Lipid and atherosclerosis hsa05167: Kaposi sarcoma-associated herpesvirus infection hsa04620: Toll-like receptor signaling pathway hsa04668: TNF signaling pathway\n27) 5599 5601 5602: MAPK8 JNK JNK-46 JNK1 JNK1A2 JNK21B1/2 PRKM8 SAPK1 SAPK1c | 3667 8471 8660: IRS1 HIRS-1 | PPrel inhibition phosphorylation | hsa04910: Insulin signaling pathway hsa05010: Alzheimer disease hsa04930: Type II diabetes mellitus hsa04920: Adipocytokine signaling pathway\n28) 5599 5601 5602: MAPK8 JNK JNK-46 JNK1 JNK1A2 JNK21B1/2 PRKM8 SAPK1 SAPK1c | C00162: Fatty acid | PCrel  | hsa04930: Type II diabetes mellitus\n29) 4790 5970: NFKB1 CVID12 EBP-1 KBF1 NF-kB NF-kB1 NF-kappa-B1 NF-kappaB NF-kappabeta NFKB-p105 | 2919 2920 2921: CXCL1 FSP GRO1 GROa MGSA MGSA-a NAP-3 SCYB1 | PPrel activation expression | hsa04621: NOD-like receptor signaling pathway hsa05167: Kaposi sarcoma-associated herpesvirus infection hsa05417: Lipid and atherosclerosis hsa05120: Epithelial cell signaling in Helicobacter pylori infection hsa04936: Alcoholic liver disease\n30) 4790 5970: NFKB1 CVID12 EBP-1 KBF1 NF-kB NF-kB1 NF-kappa-B1 NF-kappaB NF-kappabeta NFKB-p105 NFKB-p50 | 4792: NFKB inhibitor alpha NFKBIA EDAID2 IKBA MAD-3 NFKBI | PPrel missing interaction dissociation | hsa05215: Prostate cancer hsa05167: Kaposi sarcoma-associated herpesvirus infection hsa05161: Hepatitis B hsa05220: Chronic myeloid leukemia hsa05160: Hepatitis C hsa04936: Alcoholic liver disease hsa04657: IL-17 signaling pathway\n31) 4790 5970: NFKB1 CVID12 EBP-1 KBF1 NF-kB NF-kB1 NF-kappa-B1 NF-kappaB NF-kappabeta NFKB-p105 NFKB-p50 NFkappaB | 9021: suppressor of cytokine signaling 3 SOCS3 ATOD4 CIS3 Cish3 SOCS-3 SSI-3 SSI3 | GErel expression | hsa04920: Adipocytokine signaling pathway\n32) 9021: suppressor of cytokine signaling 3 SOCS3 ATOD4 CIS3 Cish3 SOCS-3 SSI-3 SSI3 | 3667 8471 8660: IRS1 HIRS-1 | PPrel inhibition | hsa04920: Adipocytokine signaling pathway\n33) 51094 79602: ADIPOR1 ACDCR1 CGI-45 CGI45 PAQR1 TESBP1A | C00076: Calcium cation; Ca2+; Calcium(2+); Calcium ion | PCrel indirect effect activation | hsa04152: AMPK signaling pathway hsa04936: Alcoholic liver disease\n34) C00076: Calcium cation; Ca2+; Calcium(2+); Calcium ion | 10645: calcium/calmodulin dependent protein kinase kinase 2 CAMKK2 CAMKK CAMKKB | PCrel activation | hsa04152: AMPK signaling pathway hsa04211: Longevity regulating pathway hsa04936: Alcoholic liver disease hsa04920: Adipocytokine signaling pathway\n35) 10645: calcium/calmodulin dependent protein kinase kinase 2 CAMKK2 CAMKK CAMKKB | 51422 53632 5562 5563 5564 5565 5571: PRKAG2 AAKG AAKG2 CMH6 H91620p WPWS | PPrel activation phosphorylation | hsa04152: AMPK signaling pathway hsa04921: Oxytocin signaling pathway hsa04211: Longevity regulating pathway hsa04936: Alcoholic liver disease hsa04920: Adipocytokine signaling pathway\n36) 51094 79602: ADIPOR1 ACDCR1 CGI-45 CGI45 PAQR1 TESBP1A | 5465: peroxisome proliferator activated receptor alpha PPARA NR1C1 PPAR PPAR-alpha PPARalpha hPPAR | PPrel indirect effect activation | hsa04920: Adipocytokine signaling pathway\n37) 5465: peroxisome proliferator activated receptor alpha PPARA NR1C1 PPAR PPAR-alpha PPARalpha hPPAR | 126129 1374 1375: CPT1C CATL1 CPT1-B CPT1P CPTI-B CPTIC SPG73 | GErel expression | hsa04920: Adipocytokine signaling pathway\n38) 5465: peroxisome proliferator activated receptor alpha PPARA NR1C1 PPAR PPAR-alpha PPARalpha hPPAR | C00638: Long-chain fatty acid; Higher fatty acid | PCrel  | hsa04920: Adipocytokine signaling pathway\n```\n\n``` {style=\"AnswerType\"}\nAction: Thought: I have gathered a large number of pathways related to the question. To better understand the role of adiponectin in protecting against alcoholic liver steatosis, I will check the neighboring pathways of Pathway 18, which is directly related to adiponectin. I will use the function search_biopathway_triple_N_hop_subgraph with history_line_id set to 18. The keywords are the same as previously used.\n     Action:\n     search_biopathway_triple_N_hop_subgraph(18, ['Adiponectin, adipose-derived hormone, associated with the development of alcoholic liver steatosis in rodent models, protective role against alcoholic liver steatosis, chronic ethanol exposure, enhanced hepatic lipogenesis, impaired fatty acid oxidation, liver injury'])\n     End Action\n```\n\n``` {style=\"QuestionType\"}\nState: No new pathways were found besides those previously seen.\n```\n\n``` {style=\"AnswerType\"}\nAction: Thought: Since no new pathways were found, I will finish the exploration.\nAnswer:\nFinished.\n```\n\n``` {style=\"QuestionType\"}\nState: You finished the task.\n```\n\n## Baseline Details {#appendix baseline}\n\n**Chain-of-Thought (CoT)** [@wei2022chain; @kojima2022large] prompting encourages language models to generate step-by-step reasoning to solve tasks. We adopt CoT as the unaugmented standard method for LLM reasoning.\n\n**Chain-of-Knowledge (CoK)** [@li2023chain] enhances chain-of-thought reasoning through knowledge verification, enabling language models to handle knowledge-intensive tasks. CoK first generates reasoning using chain-of-thought processes, then employs knowledge triples to verify the accuracy of the reasoning. While the reasoning of CoK is primarily driven by the language model, graph-based information is used for fact verification.\n\n**Think-of-Graph (ToG)** [@sun2023think] is an interactive reasoning method designed to actively navigate knowledge graphs for question solving. It primarily uses large language models to prune knowledge graph edges, thereby enabling efficient knowledge acquisition from complex graphs. The reasoning process in ToG is guided by graph navigation.\n\n**G-Retriever** [@he2024g] is a graph retriever-augmented generation method that retrieves relevant subgraphs from a database and generates answers based on the retrieved subgraphs. While the original model in their work uses a graph encoder to encode graph data as a separate modality, in this work, we directly implement the graph-to-text encoder for improved versatility and better comparability with other methods.\n\n## Failure Reason Category Cases\n\nWe classify the error reason of biological pathway reasoning into four types: (1) **Unresolved Conclusion** For cases where the model fails to provide a definitive answer, indicating uncertainty or belief that the answer is unknown. (2) **Incomplete Answer** When the response lacks essential details, such as missing the requested effects or other key elements. (3) **Omission in Reasoning** For errors where critical pathway steps in the question's biological process are left out, causing the final answer to be incorrect. (4) **Faulty in Reasoning** When the reasoning path is correct, but there are significant errors in deducing the events within that pathway.\n\nIn this subsection, we provide examples of each failure reason type. The 'Question' and 'Answer' represent the input question and its labeled correct answer. The 'Wrong Answer by Model' refers to the incorrect response generated by the model. Additionally, we include a 'Reference Reasoning Process', which outlines the reasoning process leading to the correct final answer by LLaMA3.1 405B. This helps readers understand how errors occur during the reasoning process.\n\nFor the error types 'Omission in Reasoning' and 'Faulty Reasoning,' we present cases for both CoT and PathSeeker individually. These examples demonstrate how LLM reasoning fails either on its own or when augmented with a pathway graph.\n\n**Category 1: Unresolved Conclusion**\n\n<figure>\n\n<figcaption>The Unresolved Conclusion error occurs when the model cannot provide a definitive answer, indicating uncertainty or the belief that the answer is unknown. In this case, the model is unable to determine the specific effect of the fusion protein on WNT-16 expression.</figcaption>\n</figure>\n\n**Category 2: Incomplete Answer**\n\n<figure>\n\n<figcaption>The Incomplete Answer error occurs when the response lacks essential details, such as missing the requested effects or other key elements. In this case, the model’s response suggests that continual efferocytosis is enhanced in macrophages through metabolic reprogramming but fails to mention the metabolism of apoptotic cell-derived arginine and ornithine to putrescine, which is a key part of the labeled answer.</figcaption>\n</figure>\n\n**Category 3: Omission in Reasoning**\n\n**Omission in Reasoning of CoT**\n\n<figure>\n\n<figcaption>Omission in Reasoning refers to errors where critical steps in the biological pathway described in the question are omitted, leading to an incorrect final answer. In this case, the model’s reasoning process only considered that BAMBI has been identified as a target of beta-catenin but overlooked the pathway in which BAMBI interacts with LRP6, FZD10, and DVL1, all of which are crucial components of the Wnt signaling pathway. As a result, the reasoning led to an incorrect conclusion.</figcaption>\n</figure>\n\n**Omission in Reasoning of PathSeeker**\n\n<figure>\n\n<figcaption>Omission in Reasoning refers to errors where critical steps in the biological pathway involved in the question are omitted, leading to an incorrect final answer. In this case, the model’s reasoning process considered the pathway involving the degradation of Ang II but missed the more important pathway of the conversion of Angiotensin I to Angiotensin II. This resulted in reasoning that led to an incorrect conclusion. </figcaption>\n</figure>\n\n**Category 4: Faulty in Reasoning**\n\n**Faulty in Reasoning of CoT**\n\n<figure>\n\n<figcaption>Faulty Reasoning occurs when the reasoning path is correct, but there are significant errors in deducing the events within that pathway. In this case, the model’s reasoning step proposes that EspF binds to Abcf2 and activates it, which is incorrect, as it actually inhibits Abcf2. As a result, the reasoning leads to an incorrect conclusion.</figcaption>\n</figure>\n\n**Faulty in Reasoning of PathSeeker**\n\n<figure>\n\n<figcaption>Faulty Reasoning occurs when the reasoning path is correct, but there are significant errors in deducing the events within that pathway. In this case, the model reasoned along the NF-kappaB signaling pathway but failed to deduce that a GogB deficiency leads to an increase in inflammation. Consequently, the final conclusion is incorrect.</figcaption>\n</figure>\n\n## Uncertainty Measure\n\n**Uncertainty** We investigate whether graph augmentation can reduce the uncertainty in model responses. We measure this uncertainty in the discriminant task by calculating five times the entropy of the final results, as shown in Figure [9](#fig entropy){reference-type=\"ref\" reference=\"fig entropy\"}. Notably, graph augmentation reduces prediction uncertainty for LLaMA3-8B but not for GPT-3.5. This discrepancy may arise because GPT-3.5 tends to have fewer hallucinations, whereas LLaMA3-8B may exhibit overconfidence in some generations where it is uncertain.\n\n<figure id=\"fig entropy\">\n\n<figcaption>Uncertainty measured by entropy.</figcaption>\n</figure>\n\n[]{#subsection evaluator quality label=\"subsection evaluator quality\"}\n\n## Evaluation of the Evaluator Quality\n\nAs the generation task involves LLMs as evaluators, we assess the quality of the evaluation method by comparing the result with the human manual annotation score. The accuracy of LLaMA3.1 405B with human annotation is $96\\%$, while the inconsistency lines in the case that the answer is close to the ground truth but the expression is general and missing details.\n\n## Introduction of Pathway\n\nUnderstanding biological systems is inherently complex due to the numerous interacting molecules, processes, and environmental factors involved. These systems operate with intricate interactions that result in non-linear, multi-layered, and dynamic behaviors. To address this complexity, biological researchers use pathway graphs as structured blueprints to simplify these systems into organized structures that consist of basic interactions. The linear reactions, cyclical relationships, or the local network of pathways offer snapshots of how a system behaves under specific conditions and enable researchers to predict how changes in one molecule or interaction can affect the entire system. Pathway graphs also provide a structured, static representation of dynamic processes, helping researchers understand the sequence of events even as the system changes over time.\n\n## Detailed Related Work {#appendix related work}\n\n**Biological Scientific Question Answering** Previous studies have explored the potential of language models in the biological scientific domain. MEDHOP [@welbl2018constructing] and PubMedQA [@jin2019pubmedqa] investigated biological scientific question answering in the form of reading comprehension. BioASQ-QA [@krithara2023bioasq] proposed a realistic question-answering benchmark for the actual information needs of biomedical experts. Beyond textual QA, several works have also studied multimodal scientific ability [@lu2022learn]. Additionally, other studies have explored biomedical domain tasks [@vilares2019head; @jin2021disease; @pal2022medmcqa]. Most existing tasks in the biological sciences concentrate on knowledge probing, assessing how well models understand biological information. In contrast, our work, BioMaze, is the first to focus on models' reasoning abilities within the biological scientific domain, specifically targeting phenomena observed in experiments about biological pathways.\n\nA few studies have examined language models' ability to understand biological pathways. Chatpathway [@li2023chatpathway] and [@azam2024comprehensive] specifically investigated language models' capacity for completing biological pathways. However, these studies mainly focus on probing biological pathway knowledge, i.e., determining whether language models possess the relevant pathway information. In contrast, this work introduces a novel task that employs pathway models for practical biological pathway phenomenon reasoning, bridging the gap between pathway network knowledge and its implications. We compare this work with previous biological datasets in Table [\\[appendix related work dataset table\\]](#appendix related work dataset table){reference-type=\"ref\" reference=\"appendix related work dataset table\"}.\n\n**Graph-augmented Language Model** Several studies have explored augmenting large language models (LLMs) with graph data. In particular, some works enhance LLMs by encoding graph data as text [@ye2023natural; @wang2024can; @fatemi2023talk], or tuning LLMs specifically for graph-based tasks [@liu2023one; @tang2024graphgpt; @he2024g; @zhao2023gimlet; @he2024unigraph]. By augmenting LLMs with graph data, they have been applied to knowledge-based QA [@sun2023think; @he2024g; @li2023chain; @jin2024graph; @cheng2024call], and to graph-oriented tasks like graph property prediction [@wang2024can; @he2023harnessing]. A few other studies leverage graph structures during LLM reasoning to tackle complex tasks [@jiang2023structgpt; @besta2024graph].\n\nUnlike tasks in previous works, this study addresses whether reasoning in biological systems can be enhanced by pathway graphs, which act as a *structured blueprint* for reasoning about the system's states. It is not sufficient to simply identify the correct paths in the pathway graph to find the answer. Instead, it is necessary to perform deductive reasoning about the events that occur when the system is intervened upon under specific conditions and to predict the resulting states and mechanisms of the intervened system.\n\nFor large graph databases, most works enable LLMs to access graph data through retrieval mechanisms [@he2024g; @li2023chain], while a few studies have explored using LLMs as interactive agents [@yao2023react; @shinn2023reflexion; @zhao2024empowering] to navigate and explore vast graph databases [@sun2023think; @jin2024graph]. In this work, we introduce an agent-based interactive graph exploration approach using subgraph navigation-based browsing, which is more efficient and offers enhanced navigation capabilities for pathway database.",
    "rationale": "Summary: This study explores the under-examined ability of LLMs to reason about biological pathways, particularly focusing on how system perturbations affect downstream biological processes. The authors introduce the BioMaze dataset, a benchmark designed to assess LLMs’ reasoning on how various interventions, like mutations, infections, or treatments, impact downstream targets through complex pathway mechanisms across different biological contexts. With this dataset, the authors then test LLMs with reasoning techniques such as Chain-of-Thought (CoT) and graph-augmented methods, and they find that while LLMs can understand basic biological mechanisms, they struggle with predicting effects after perturbations. To enhance the reasoning ability of LLMs, the authors also developed PathSeeker. In this novel approach, the LLM agent navigates pathway subgraphs to improve performance in pathway reasoning, particularly in scenarios with biological perturbations.\n\nStrengths: 1. BioMaze benchmark for biological pathway reasoning: The authors present BioMaze, a benchmark dataset designed to evaluate LLMs’ reasoning abilities within a biological context. BioMaze focuses on assessing how well LLMs comprehend and reason about complex biological pathway phenomena, including cause-effect relationships in natural and perturbed conditions. Curated from the literature, this dataset includes high-quality questions and answers generated with Llama 3.1405B and GPT-4. Covering multiple biology subfields, BioMaze undergoes extensive filtering and validation to ensure relevance, accuracy, and diversity of pathway scenarios.\n2. Pathway graph augmentation via PATHSEEKER agent model: Given that biological pathways are naturally structured as networks, the authors incorporate pathway graph data to improve LLM reasoning. They introduce PATHSEEKER, a novel graph-augmented agent that navigates pathway subgraphs to enrich LLM understanding and support reasoning in complex pathway contexts. This approach allows LLMs to access and utilize structural information essential for nuanced pathway reasoning, particularly in scenarios involving biological interventions.\n3. Comprehensive evaluation and analysis: The paper conducts a thorough evaluation across multiple LLM models and experimental settings, systematically analyzing LLM performance with and without pathway graph augmentation. Additionally, the ablation study of PATHSEEKER explores its effectiveness by examining API usage, step distribution, and performance impact. These analyses further strengthen the value of pathway augmentation, validating the importance of PATHSEEKER in enhancing LLMs’ reasoning capabilities in biological pathway contexts.\n\nWeaknesses: 1. Limited evaluation method for open-ended questions: outputs from different LLMs are evaluated by another LLM, specifically using the Llama 3.1 405B model, which is considerably powerful but would be costly to replicate the results. It would be more helpful if the authors could consider some alternatives, such as using rule-based keyword-matching or for example, using ROUGE score or embedding-based summarization methods to compare how similar or dissimilar answers from LLMs are to the ground truth answers. Another alternative could be to construct different evaluation methods based on the failure modes discovered later from the error analysis study. \n2. see questions\n\nQuestions: 1. Pathway graph limitations: This paper highlights that faulty reasoning persists even with pathway augmentation, especially with perturbations. Could the authors provide more insight into potential sources of error in the pathway graph data? Is it the case that some specific cases or graph structures are more challenging for the LLM to navigate, and some are easier for LLMs to handle?\n2. Handling multi-step reasoning decline: Given that CoT reasoning shows decreased accuracy with increased steps, have the authors considered alternative strategies or mechanisms, such as hierarchical reasoning, to mitigate this drop in performance, or are those questions just naturally challenging? \n3. Error analysis: The error analysis indicates that omissions remain an issue with PATHSEEKER. What approaches might the authors consider to address these issues, especially when key pathway branches are missed? Could further database expansion, enhanced subgraph search criteria, or developing a different graph search algorithm improve the performance?\n4. Using RAG: would authors consider incorporating RAG into this framework given the graph structure of biological pathways? Specifically, RAG could allow the model to retrieve specific or relevant information from related literature or pathway databases. This retrieval would provide the LLM with dynamic access to more detailed and more recent biological knowledge, instead of the graph structure constructed from a fixed database KEGG, as currently used in the paper. \n5. Evaluator setting in this paper: this paper proposes using the llama 405B model as the evaluator model for LLM's outputs, as this is costly to run multiple times, would authors consider any alternative evaluation approaches such as applying rule-based methods or using alternative LLMs to strengthen the statistical validity of the benchmarking results?",
    "rating": 3,
    "label": true,
    "rationale_edited": "This study explores the under-examined ability of LLMs to reason about biological pathways, particularly focusing on how system perturbations affect downstream biological processes.  Authors test LLMs with reasoning techniques such as Chain-of-Thought (CoT) and graph-augmented methods, and they find that while LLMs can understand basic biological mechanisms, they struggle with predicting effects after perturbations. To enhance the reasoning ability of LLMs, the authors also developed PathSeeker. In this novel approach, the LLM agent navigates pathway subgraphs to improve performance in pathway reasoning, particularly in scenarios with biological perturbations.\n\nThe authors present BioMaze, a benchmark dataset designed to evaluate LLMs’ reasoning abilities within a biological context. BioMaze focuses on assessing how well LLMs comprehend and reason about complex biological pathway phenomena, including cause-effect relationships in natural and perturbed conditions.\n\nThey introduce PATHSEEKER, a novel graph-augmented agent that navigates pathway subgraphs to enrich LLM understanding and support reasoning in complex pathway contexts. T\n\nThe paper conducts a thorough evaluation across multiple LLM models and experimental settings, systematically analyzing LLM performance with and without pathway graph augmentation. Additionally, the ablation study of PATHSEEKER explores its effectiveness by examining API usage, step distribution, and performance impact. These analyses further strengthen the value of pathway augmentation, validating the importance of PATHSEEKER in enhancing LLMs’ reasoning capabilities in biological pathway contexts.\n\nLimited evaluation method for open-ended questions: outputs from different LLMs are evaluated by another LLM, specifically using the Llama 3.1 405B model, which is considerably powerful but would be costly to replicate the results."
  },
  {
    "title": "Towards Robust Multimodal Sentiment Analysis with Incomplete Data",
    "abstract": "The field of Multimodal Sentiment Analysis (MSA) has recently witnessed an emerging direction seeking to tackle the issue of data incompleteness. Recognizing that the language modality typically contains dense sentiment information, we consider it as the dominant modality and present an innovative Language-dominated Noise-resistant Learning Network (LNLN) to achieve robust MSA. The proposed LNLN features a dominant modality correction (DMC) module and dominant modality based multimodal learning (DMML) module, which enhances the model's robustness across various noise scenarios by ensuring the quality of dominant modality representations. Aside from the methodical design, we perform comprehensive experiments under random data missing scenarios, utilizing diverse and meaningful settings on several popular datasets (e.g., MOSI, MOSEI, and SIMS), providing additional uniformity, transparency, and fairness compared to existing evaluations in the literature. Empirically, LNLN consistently outperforms existing baselines, demonstrating superior performance across these challenging and extensive evaluation metrics.",
    "text": "# Introduction\n\nThe field of Multimodal Sentiment Analysis (MSA) is at the vanguard of human sentiment identification by assimilating heterogeneous data types, such as video, audio, and language. Its applicability spans numerous fields, prominent among healthcare and human-computer interaction [@jiang2020snapshot]. MSA has become essential in enhancing both the precision and the robustness of sentiment analysis by drawing sentiment cues from diverse perspectives.\n\nChanging trends in recent research has taken a turn towards modeling data in natural scenarios from laboratory conditions [@DBLP:conf/acl/TsaiBLKMS19; @DBLP:conf/mm/HazarikaZP20; @DBLP:conf/aaai/YuXYW21; @DBLP:conf/emnlp/ZhangWYL0Y23]. The shift has created a wider application space in the real-world for MSA, though concerns arise due to problems like sensor failures and problems with Automatic Speech Recognition (ASR), leading to inconsistencies such as incomplete data in real-world deployment.\n\nNumerous impactful solutions have been proposed against this primary concern of incomplete data in multimodal sentiment analysis. For instance, [@DBLP:conf/mm/YuanLXY21] introduced a Transformer-based feature reconstruction mechanism, TFR-Net, aiming to strengthen the robustness of the model handling random missing in unaligned multimodal sequences via reconstructing missing data. Furthermore, [@DBLP:journals/tmm/YuanLXG24] proposed the Noise Intimating-based Adversarial Training (NIAT) model, which superiorly learns a unified joint representation between an original-noisy instance pair, utilizing the attention mechanism and adversarial learning. Lastly, [@li2024unified] design a Unified Multimodal Missing Modality Self-Distillation Framework (UMDF) which leverages a single network to learn robust inherent representations from consistent multimodal data distributions. Yet, despite these developments, the evaluation metrics for these models are inconsistent, and the evaluation settings are not sufficiently comprehensive. This inconsistency limits effective comparisons and hinders the dissemination of knowledge in the field.\n\nAddressing this gap, our paper aims to offer a comprehensive evaluation on three widely-used datasets, namely MOSI [@DBLP:journals/expert/ZadehZPM16], MOSEI [@DBLP:conf/acl/MorencyCPLZ18] and SIMS [@DBLP:conf/acl/YuXMZMWZY20] datasets. We introduce random data missing instances and subsequently compare the performance of existing methods on these datasets. This endeavor seeks to provide an all-encompassing outlook for evaluating the effectiveness and robustness of various methods in the face of incomplete data, thereby sparking new insights in the field. Additionally, inspired by the previous work ALMT [@DBLP:conf/emnlp/ZhangWYL0Y23], we hypothesize that model robustness improves when the integrity of the dominant modality is preserved despite varying noise levels. Therefore, we introduce a novel model, namely Language-dominated Noise-resistant Learning Network (LNLN), to enhance MSA's robustness over incomplete data. LNLN aims to augment the integrity of the language modality's features, regarded as the dominant modality due to its richer sentiment cues, with the support of other auxiliary modalities. The LNLN's robustness against varying levels of data incompleteness is achieved through a dominant modality correction (DMC) module for dominant modality construction, a dominant modality based multimodal learning (DMML) module for multimodal fusion and classification, and a reconstructor for reconstructing missing information to shield the dominate modality from noise interference. This approach ensures a high-quality dominant modality feature, which significantly bolsters the robustness of LNLN under diverse noise conditions. Consequently, extensive experimental results demonstrate the LNLN's superior performance across these challenging and evaluation metrics.\n\nIn summary, this paper conducts a comprehensive evaluation of existing advanced MSA methods. This analysis highlights the strengths and weaknesses of various methods when contending with incomplete data. We believe, it can improve the understanding of different MSA methods' performance under complex real-world scenarios, thereby informing technology's future trajectory. Our proposed LNLN also offers valuable insight and guidelines for thriving in this research space.\n\n# Related Work {#sec: related_work}\n\n## Multimodal Sentiment Analysis\n\nMultimodal Sentiment Analysis (MSA) methods can be categorized into Context-based MSA and Noise-aware MSA, depending on the modeling approach. Most of previous works [@DBLP:conf/emnlp/ZadehCPCM17; @DBLP:conf/acl/TsaiBLKMS19; @DBLP:journals/tmm/MaiXH20; @DBLP:conf/mm/HazarikaZP20; @DBLP:conf/mm/LiangLJ20; @DBLP:conf/acl/RahmanHLZMMH20; @DBLP:conf/aaai/YuXYW21; @DBLP:conf/emnlp/HanCP21; @DBLP:conf/cvpr/LvCHDL21; @DBLP:conf/mm/YangHKDZ22; @DBLP:conf/mm/GuoTDDK22; @DBLP:conf/emnlp/ZhangWYL0Y23] can be classified to Context-based MSA. This line of work primarily focuses on learning unified multimodal representations by analyzing contextual relationships within or between modalities. For example, [@DBLP:conf/emnlp/ZadehCPCM17] explore computing the relationships between different modalities using the Cartesian product. [@DBLP:conf/acl/TsaiBLKMS19] utilize pairs of Transformers to model long dependencies between different modalities. [@DBLP:conf/aaai/YuXYW21] propose generating pseudo-labels for each modality to further mine the information of consistency and discrepancy between different modalities.\n\nDespite these advances, context-based methods are usually suboptimal under varying levels of noise effects (*e.g.* random data missing). Several recent works [@DBLP:conf/aaai/MittalBCBM20; @DBLP:conf/mm/YuanLXY21; @DBLP:journals/tmm/YuanLXG24; @li2024unified] have been proposed to tackle this issue. For example, [@DBLP:conf/aaai/MittalBCBM20] introduce a modality check step to distinguish invalid and valid modalities, achieving higher robustness. [@DBLP:journals/tmm/YuanLXG24] propose learning a unified joint representation between constructed \"original-noisy\" instance pairs. Although there have been some advances in improving the model's robustness under noise scenarios, no extant method has provided a comprehensive and in-depth comparative analysis.\n\n## Robust Representation Learning in MSA\n\nContext-based MSA and Noise-aware MSA differ in their approaches to robust representation learning. In Context-based MSA, robust representation learning typically relies on modeling intra- and inter-modality relationships. For instance, [@DBLP:conf/mm/HazarikaZP20] and [@DBLP:conf/mm/YangHKDZ22] apply feature disentanglement to each modality, modeling multimodal representations from multiple feature subspaces and perspectives. [@DBLP:conf/aaai/YuXYW21] and [@DBLP:conf/mm/LiangLJ20] explore self-supervised learning and semi-supervised learning to enhance multimodal representations, respectively. [@DBLP:conf/acl/TsaiBLKMS19] and [@DBLP:conf/acl/RahmanHLZMMH20] introduce Transformer to learn the long dependencies of modalities. [@DBLP:conf/emnlp/ZhangWYL0Y23] devise a language-guided learning mechanism that uses modalities with more intensive sentiment cues to guide the learning of other modalities. In contrast, Noise-aware MSA focuses more on perceiving and eliminating the noise present in the data. For example, [@DBLP:conf/aaai/MittalBCBM20] design a modality check module based on metric learning and Canonical Correlation Analysis (CCA) to identify the modality with greater noise. [@DBLP:conf/mm/YuanLXY21] design a feature reconstruction network to predict the location of missing information in sequences and reconstruct it. [@DBLP:journals/tmm/YuanLXG24] introduce adversarial learning [@DBLP:conf/nips/GoodfellowPMXWOCB14] to perceive and generate cleaner representations.\n\nIn this work, the LNLN belongs to the noise-aware MSA category. Inspired by [@DBLP:conf/emnlp/ZhangWYL0Y23], we explore the capability of language-guided mechanisms in resisting noise and aim to provide new perspectives for the study of MSA in noisy scenarios.\n\n# Method {#sec: method}\n\n## Overview\n\nThe overall pipeline for the proposed Language-dominated Noise-resistant Learning Network (LNLN) in robust multimodal sentiment analysis is illustrated in Figure [1](#fig: pipeline){reference-type=\"ref\" reference=\"fig: pipeline\"}. As depicted, a crucial initial step involves forming a multimodal input with random data missing, which sets the stage for LNLN training. Once the input is prepared, LNLN first utilizes an embedding layer to standardize the dimension of each modality, ensuring uniformity. Recognizing that language is the dominant modality in MSA [@DBLP:conf/emnlp/ZhangWYL0Y23], a specially designed Dominant Modality Correction (DMC) module employs adversarial learning and a dynamic weighted enhancement strategy to mitigate noise impacts. This module first enhances the quality of the dominant feature computed from the language modality and then integrates them with the auxiliary modalities (visual and audio) in the dominant modality based multimodal learning (DMML) module for effective multimodal fusion and classification. This process significantly bolsters LNLN's robustness against various noise levels. Moreover, to refine the network's capability for fine-grained sentiment analysis, a simple reconstructor is implemented to reconstruct missing data, further enhancing the system's robustness.\n\n<figure id=\"fig: pipeline\">\n<div class=\"center\">\n<embed src=\"figs/Pipeline.pdf\" style=\"width:100.0%\" />\n</div>\n<figcaption>Overall pipeline. Note: <span class=\"math inline\"><em>H</em><sub><em>l</em></sub><sup>0</sup></span>, <span class=\"math inline\"><em>H</em><sub><em>v</em></sub><sup>0</sup></span>, <span class=\"math inline\"><em>H</em><sub><em>a</em></sub><sup>0</sup></span>, <span class=\"math inline\"><em>H</em><sub><em>c</em><em>c</em></sub></span>, and <span class=\"math inline\"><em>H</em><sub><em>p</em></sub><sup>0</sup></span> are randomly initialized learnable vectors.</figcaption>\n</figure>\n\n## Input Construction and Multimodal Input\n\nGiven the challenges of random data missing, we have constructed data sets that simulate these conditions based on MOSI, MOSEI, and SIMS datasets.\n\n**Random Data Missing.** Following the previous method [@DBLP:conf/mm/YuanLXY21], for each modality, we randomly erased changing proportions of information (from 0% to 100%). Specifically, for visual and audio modalities, we fill the erased information with zeros. For language modality, we fill the erased information with \\[UNK\\] which indicates the unknown word in BERT [@devlin2019bert].\n\n**Multimdoal Input.** For each sample in the dataset, we incorporate data from three modalities: language, audio, and visual data. Consistent with previous works [@DBLP:conf/emnlp/ZhangWYL0Y23], each modality is processed using widely-used tools: language data is encoded using BERT [@devlin2019bert], audio features are extracted through Librosa [@DBLP:conf/scipy/McFeeRLEMBN15], and visual features are obtained using OpenFace [@DBLP:conf/fgr/BaltrusaitisZLM18]. These pre-processed inputs are represented as sequences, denoted by $U^0_m \\in \\mathbb{R}^{T_m \\times d_m}$, where $m \\in \\{l, v, a\\}$ represents the modality type ($l$ for language, $v$ for visual, $a$ for audio), $T_m$ indicates the sequence length and $d_m$ refers to the dimension of each modality's vector. With obtained $U^0_m$, we apply random data missing to $U^0_m$, thus forming the noise-corrupted multimodal input $U^1_m \\in \\mathbb{R}^{T_m \\times d_m}$.\n\n## Dominant Modality based Multimodal Learning\n\nInspired by previous work ALMT, we hypothesize that model robustness improves when the integrity of the dominant modality is preserved despite varying noise levels. We improve ALMT based on the designed DMC module and Reconstructor, thus implementing dominant modality based DMML module for sentiment analysis under random data missing scenarios. Here, we mainly introduce the parts that differ from ALMT. Further details are available in [@DBLP:conf/emnlp/ZhangWYL0Y23].\n\n**Modality Embedding.** For multimodal input $U^1_m$, we employ an Embedding Encoder $\\operatorname{E}^1_m$ with two Transformer encoder layers to extract and unify the feature. Each modality begins with a randomly initialized low-dimensional token $H_m^0 \\in \\mathbb{R}^{T \\times d_m}$. These tokens are then processed by the Transformer encoder layer, embedding essential modality information and producing unified features, represented as $H_m^1 \\in \\mathbb{R}^{T \\times d}$. The process is formalized by: $$H_m^1=\\operatorname{E}^1_m\\left(\\operatorname{concat}\\left(H_m^0, U^1_m\\right)\\right),$$ where $\\operatorname{E}^1_m(\\cdot)$ extracts features for each modality, and $\\operatorname{concat}(\\cdot)$ represents the concatenation operation.\n\n**Adaptive Hyper-modality Learning.** In the original ALMT, each Adaptive Hyper-modality Learning layer contains a Transformer and two multi-head attention (MHA) modules. These are applied to learn language representations at different scales and hyper-modality representations from visual and audio modalities, guided by the language modality. Considering the possibility of severe interference in the language modality (*i.e.* dominant modality) due to random data missing, we designed a Dominant Modality Correction (DMC) module to generate the proxy dominant feature $H^1_p$ and construct corrected dominate feature $H^1_d$ (more details can be found in Section [3.4](#sec: dmc){reference-type=\"ref\" reference=\"sec: dmc\"}). Specifically, the process of learning corrected dominated representation $H_d^i$ at different scales can be described as: $$H_d^i=\\operatorname{E}^{i}_m\\left(H_d^{i-1}\\right),$$ where $i \\in\\{2,3\\}$ means the $i$-th layer of Adaptive Hyper-modality Learning module, $\\operatorname{E}^i_m(\\cdot)$ is the i-th Transformer encoder layer, $H_d^i \\in \\mathbb{R}^{T \\times d}$ is corrected dominated feature at different scale. To learn the hyper-modality representation, the corrected dominated feature and audio/visual features are used to calculate Query and Key/Value, respectively. Briefly, the process can be written as follows: $$H_{hyper}^i =H_{hyper}^{i-1}+\\operatorname{MHA}(H_d^i, H_a^1)+\\operatorname{MHA}(H_d^i, H_v^1),$$ where $\\operatorname{MHA}(\\cdot)$ represents multi-head attention, $H_{hyper}^i \\in \\mathbb{R}^{T \\times d}$ is the hyper-modality feature. Note that the feature $H_{hyper}^0 \\in \\mathbb{R}^{T \\times d}$ is a random initialized vector.\n\n**Multimodal Fusion and Prediction.** With obtained $H_d^3$ and $H_{hyper}^3$, a Transformer encoder with a classifier at a depth of 4 layers is employed for multimodal fusion and sentiment prediction: $$\\hat{y}=\\operatorname{CrossTransformer}(H_d^3, H^3_{hyper}),$$ where $\\hat{y}$ is the sentiment prediction.\n\n## Dominant Modality Correction {#sec: dmc}\n\nThis module consists of two steps, *i.e.,* completeness check of dominant modality and proxy dominant feature generation using adversarial learning [@DBLP:conf/icml/GaninL15; @DBLP:conf/nips/GoodfellowPMXWOCB14].\n\n**Completeness Check.** We apply an encoder $\\operatorname{E}_{cc}$ that consists of a Transformer encoder with a depth of two layers and a classifier for completeness check. For example, if the missing rate of dominate modality is 0.3, the label of completeness is 0.7. This completeness prediction $w$ can be obtained as follows: $$w={\\operatorname{E}_{cc}\\left(\\operatorname{Concat}\\left(H_{cc}, H_l^{1}\\right)\\right)},$$ where $H_{cc} \\in \\mathbb{R}^{T \\times d}$ is a randomly initialized token for completeness prediction. We optimize this process using L2 loss: $$\\mathcal{L}_{cc}=\\frac{1}{N_b} \\sum_{k=0}^{N_b}\\left\\|w^k-\\hat{w}^k\\right\\|_2^2,$$ where $N_b$ is the number of samples in the training set, $\\hat{w}^k$ is the label of completeness of $k$-th sample.\n\n**Proxy Dominant Feature Generation.** With the randomly initialized feature $H^0_{p} \\in \\mathbb{R}^{T \\times d}$, visual feature $H^1_v$ and audio feature $H^1_a$, we employ a Proxy Dominant Feature Generator $E_{DFG}$, which consists of two Transformer encoder layers. This setup generates the proxy dominant features $H^1_p \\in \\mathbb{R}^{T \\times d}$, designed to complement and correct the dominant modality. The corrected dominant feature $H^1_d \\in \\mathbb{R}^{T \\times d}$ is calculated by combining $H^1_p$ and the language feature $H^1_l$, weighted by the predicted completeness $w$: $$H^1_p= \\operatorname{E}_{DFG}\\left(\\operatorname{Concat}\\left(H^0_{p}, H_a^{1}, H_v^{1}\\right), \\theta_{DFG}\\right),$$ $$H^1_d= (1 - w) * H^1_p + w * H^1_l,$$ where $\\theta_{DFG}$ denotes the parameters of the Proxy Dominant Feature Generator $E_{DFG}$.\n\nTo ensure that the agent feature offers a distinct perspective from the visual and audio features, we utilize an effectiveness discriminator $D$. This discriminator includes a binary classifier and a Gradient Reverse Layer (GRL) [@DBLP:conf/icml/GaninL15] and is tasked with identifying the origin of the agent features: $$\\hat{y}_{p}= \\operatorname{D}\\left(H^1_p/H^1_l, \\theta_{D}\\right),$$ where $\\theta_{D}$ represents the parameters of the effectiveness discriminator $D$, and $\\hat{y}_{p}$ indicates the prediction of whether the input feature originates from the language modality.\n\nIn practice, the generator and the discriminator engage in an adversarial learning structure. The discriminator aims to identify whether the features are derived from the language modality, while the generator's objective is to challenge the discriminator's ability to make accurate predictions. This dynamic is encapsulated in the adversarial learning objective: $$\\min _{\\theta_{D}} \\max _{\\theta_{DFG}} \\mathcal{L}_{adv}=-\\frac{1}{N_b} \\sum_{k=0}^{N_b} y^k_{p} \\cdot \\log \\hat{y}^k_{p},$$ where $N_b$ is the number of samples in the training set, and $y^k_{p}$ indicates the label determining whether the input feature for the $k$-th sample originates from the visual or audio modality.\n\n## Reconstructor\n\nOur experiments demonstrate that reconstructing missing information can significantly enhance regression metrics. More details about this are shown in Table [\\[tab: Effects of Different Components\\]](#tab: Effects of Different Components){reference-type=\"ref\" reference=\"tab: Effects of Different Components\"}. To address this, we have developed a reconstructor, denoted as $E_{rec}$, which comprises two Transformer layers designed to effectively rebuild missing information of each modality. The operational equation for the reconstructor is: $$\\hat{U}^0_m={\\operatorname{E}_m^{rec}\\left(U^1_m\\right)}$$ where $\\hat{U}^0_m$ is the reconstructed feature corresponding to the feature $U^0_m$.\n\nTo optimize the performance of the reconstructor, we apply an L2 loss function: $$\\mathcal{L}_{r e c}=\\frac{1}{N_b} \\sum_{h=0}^{N_b} \\sum_m\\left\\|U_m^0{ }^k-\\hat{U}_m^0{ }^k\\right\\|_2^2,$$ where $U_m^0{ }^k$ and $\\hat{U}_m^0{ }^k$ represent the original and reconstructed features with missing information for the $k$-th sample, respectively. This loss function helps minimize the discrepancies between the original and reconstructed features, thereby improving the accuracy of other components, such as Dominant Modality Correction and final sentiment prediction.\n\n## Overall Learning Objectives\n\nTo sum up, our method involves four learning objectives, including a completeness check loss $\\mathcal{L}_{cc}$, an adversarial learning loss $\\mathcal{L}_{adv}$ for proxy dominant feature generation, a reconstruction loss $\\mathcal{L}_{rec}$ and one final sentiment prediction loss $\\mathcal{L}_{sp}$. The sentiment prediction loss $\\mathcal{L}_{sp}$ can be described as: $$\\mathcal{L}_{sp}=\\frac{1}{N_b} \\sum_{n=0}^{N_b}\\left\\|y^n-\\hat{y}^n\\right\\|_2^2,$$\n\nTherefore, the overall loss $\\mathcal{L}$ can be written as: $$\\mathcal{L}=\\alpha\\mathcal{L}_{cc}+\\beta\\mathcal{L}_{adv}+\\gamma\\mathcal{L}_{rec}+\\delta\\mathcal{L}_{sp},$$ where $\\alpha$, $\\beta$, $\\gamma$ and $\\delta$ are hyperparameters. On MOSI and MOSEI datasets, we empirically set them to 0.9, 0.8, 0.1, and 1.0, respectively. On the SIMS dataset, we empirically set them to 0.9, 0.6, 0.1, and 1.0, respectively.\n\n# Experiments and Analysis\n\nIn this section, we provide a comprehensive and fair comparison between the proposed LNLN and previous representative MSA methods on MOSI [@DBLP:journals/expert/ZadehZPM16], MOSEI [@DBLP:conf/acl/MorencyCPLZ18] and SIMS [@DBLP:conf/acl/YuXMZMWZY20] datasets.\n\n## Datasets\n\n**MOSI.** The dataset includes 2,199 multimodal samples, integrating visual, audio, and language modalities. It is divided into a training set of 1,284 samples, a validation set of 229 samples, and a test set of 686 samples. Every single sample has been given a sentiment score, varying from -3, indicating strongly negative sentiment, to 3, signifying strongly positive sentiment.\n\n**MOSEI.** The dataset consists of 22,856 video clips sourced from YouTube. The sample is divided into 16,326 clips for training, 1,871 for validation, and 4,659 for testing. Each clip is labeled with a score, ranging from -3, denoting the strongly negative, to 3, denoting the strongly positive.\n\n**SIMS.** The dataset is a Chinese multimodal sentiment dataset that includes 2,281 video clips sourced from different movies and TV series. It has been partitioned into 1,368 samples for training, 456 for validation, and 457 for testing. Each sample has been manually annotated with a sentiment score ranging from -1 (negative) to 1 (positive).\n\n## Evaluation Settings and Criteria\n\nFor a fair and comprehensive evaluation, we experiment ten times, setting the missing rates $r$ to predefined values from 0 to 0.9 with an increment of 0.1. For instance, 50% of the information is randomly erased from each modality in the test data when $r = 0.5$. Unlike previous works [@DBLP:conf/mm/YuanLXY21; @DBLP:journals/tmm/YuanLXG24], we did not evaluate at $r = 1.0$, as this would imply complete data erasure from each modality, rendering the experiment non-informative. With the obtained results of each missing rate, we compute the average value as the model's overall performance under different levels of noise.\n\nFor evaluation criteria, we report the binary classification accuracy (Acc-2), the F1 score associated with Acc-2, and the mean absolute error (MAE). For Acc-2, we calculated accuracy and F1 in two ways: negative/positive (left-side value of /) and negative/non-negative (right-side value of /) on the MOSI and MOSEI datasets, respectively. Additionally, we provide the three-class accuracy (Acc-3), the seven-class accuracy (Acc-7), and the correlation of the model's prediction with humans (Corr) on the MOSI dataset. For the SIMS dataset, we report Acc-3, the five-class accuracy (Acc-5), and Corr. Due to the distinct focus of regression and classification metrics on different aspects of model performance, a model achieving the lowest error on regression metrics may not necessarily exhibit optimal performance on classification metrics. To comprehensively reflect the model capabilities, we select the best-performing checkpoint for each type of metric across all models in the comparisons, thus capturing the peak performance of both regression and classification aspects independently.\n\n## Implementation Details\n\nWe used PyTorch 2.2.1 to implement the method. The experiments were conducted on a PC with an AMD EPYC 7513 CPU and an NVIDIA Tesla A40. To ensure consistent and fair comparisons across all methods, we conducted each experiment three times using fixed random seeds of 1111, 1112, and 1113. Details of the hyperparameters are shown in Table [\\[tab: Hyperparameters\\]](#tab: Hyperparameters){reference-type=\"ref\" reference=\"tab: Hyperparameters\"}. In addition, the result of MISA, Self-MM, MMIM, CENET, TETFN, and TFR-Net is reproduced by the authors from open source code in the MMSA[^3] [@DBLP:conf/acl/MaoYXYLG22], which is a unified framework for MSA, using default hyperparameters. The result of ALMT is reproduced by the authors from open source code on Github[^4].\n\n::: tabularx\nCCCC & MOSI & MOSEI & SIMS\\\nVector Length $T$ & 8 & 8 & 8\\\nVector Dimension $d$ & 128 & 128 & 128\\\nBatch Size & 64 & 64 & 64\\\nInitial Learning Rate & 1e-4 & 1e-4 & 1e-4\\\nLoss Weight $\\alpha$, $\\beta$, $\\gamma$, $\\delta$ & 0.9, 0.8, 0.1, 1 .0 & 0.9, 0.8, 0.1, 1.0 & 0.9, 0.6, 0.1, 1.0\\\nOptimizer & AdamW & AdamW & AdamW\\\nEpochs & 200 & 200 & 200\\\nWarm Up & & &\\\nCosine Annealing & & &\\\nEarly Stop & & &\\\nSeed & 1111,1112,1113 & 1111,1112,1113 & 1111,1112,1113\\\n:::\n\n## Robustness Comparison\n\nTables [1](#tab: Results_Compare_MOSI_MOSEI){reference-type=\"ref\" reference=\"tab: Results_Compare_MOSI_MOSEI\"} and [\\[tab: Results_Compare_SIMS\\]](#tab: Results_Compare_SIMS){reference-type=\"ref\" reference=\"tab: Results_Compare_SIMS\"} show the robustness evaluation results on the MOSI, MOSEI, and SIMS datasets. As shown in Table [1](#tab: Results_Compare_MOSI_MOSEI){reference-type=\"ref\" reference=\"tab: Results_Compare_MOSI_MOSEI\"}, LNLN achieves state-of-the-art performance on most metrics. On the MOSI dataset, LNLN achieved a relative improvement of 9.46% on Acc-7 compared to the sub-optimal result obtained by MMIM, demonstrating the robustness of LNLN in the face of different noise effects. However, on the MOSEI dataset, LNLN achieves only sub-optimal performance on metrics such as Acc-7 and Acc-5. After analyzing the data distribution (see Appendix [6.3](#sec: data_distribution){reference-type=\"ref\" reference=\"sec: data_distribution\"}) and the confusion matrix (see Appendix [6.6](#sec: cm){reference-type=\"ref\" reference=\"sec: cm\"}), we believe that this is because LNLN does not overly bias the neutral category, which has a disproportionate number of samples in noisy scenarios. As shown in Table [\\[tab: Results_Compare_SIMS\\]](#tab: Results_Compare_SIMS){reference-type=\"ref\" reference=\"tab: Results_Compare_SIMS\"}, LNLN achieves an improvement in F1 on the SIMS dataset, with a relative improvement of 9.17% on F1 compared to the sub-optimal result obtained by ALMT. Similar to CENET's performance on the MOSEI dataset, TETFN on the SIMS dataset has a tendency to predict inputs as weakly negative categories with high sample sizes, resulting in seemingly better performance on some metrics. Additionally, as shown in Figure [2](#fig: performance_curves){reference-type=\"ref\" reference=\"fig: performance_curves\"}, we present the performance curves of several advanced methods under varying missing rates. The results demonstrate that the proposed LNLN consistently outperforms others across most scenarios, showing its robustness under different missing rates.\n\nOverall, LNLN attempts to make predictions in the face of highly noisy inputs without the severe lazy behavior observed in other models, which often leads to predictions heavily biased towards a certain category. This demonstrates that LNLN shows strong robustness and competitiveness across various datasets and noise levels, highlighting its effectiveness in multimodal sentiment analysis.\n\n::: {#tab: Results_Compare_MOSI_MOSEI}\n  Method           MOSI                                                                                        MOSEI                                                                        \n  ------------- ----------- ----------- ----------------------- ------------------- ----------- ----------- ----------- ----------- ------------------- ----------------------- ----------- -----------\n  2-7(lr)8-13      Acc-7       Acc-5             Acc-2                  F1              MAE        Corr        Acc-7       Acc-5           Acc-2                  F1                MAE        Corr\n  MISA             29.85       33.08         71.49 / 70.33         71.28 / 70.00       1.085       0.524       40.84       39.39       71.27 / 75.82         63.85 / 68.73         0.780       0.503\n  Self-MM          29.55       34.67         70.51 / 69.26         66.60 / 67.54       1.070       0.512       44.70       45.38       73.89 / 77.42         68.92 / 72.31         0.695       0.498\n  MMIM             31.30       33.77         69.14 / 67.06         66.65 / 64.04       1.077       0.507       40.75       41.74       73.32 / 75.89         68.72 / 70.32         0.739       0.489\n  CENET            30.38       37.25         71.46 / 67.73         68.41 / 64.85       1.080       0.504     **47.18**   **47.83**     74.67 / 77.34         70.68 / 74.08       **0.685**   **0.535**\n  TETFN            30.30       34.34         69.76 / 67.68         65.69 / 63.29       1.087       0.507       30.30       47.70       69.76 / 67.68         65.69 / 63.29         1.087       0.508\n  TFR-Net          29.54       34.67         68.15 / 66.35         61.73 / 60.06       1.200       0.459       46.83       34.67       73.62 / 77.23         68.80 / 71.99         0.697       0.489\n  ALMT             30.30       33.42         70.40 / 68.39       72.57 / **71.80**     1.083       0.498       40.92       41.64     **76.64** / 77.54       77.14 / 78.03         0.674       0.481\n  **LNLN**       **34.26**   **38.27**   **72.55** / **70.94**   **72.73** / 71.25   **1.046**   **0.527**     45.42       46.17     76.30 / **78.19**   **77.77** / **79.95**     0.692       0.530\n\n  : Robustness comparison of the overall performance on MOSI and MOSEI datasets. Note: The smaller MAE indicates the better performance.\n:::\n\n::: tabularx\nlCCCCCC Method & Acc-5 & Acc-3 & Acc-2 & F1 & MAE & Corr\\\nMISA & 31.53 & 56.87 & 72.71 & 66.30 & 0.539 & 0.348\\\nSelf-MM & 32.28 & 56.75 & 72.81 & 68.43 & 0.508 & 0.376\\\nMMIM & 31.81 & 52.76 & 69.86 & 66.21 & 0.544 & 0.339\\\nCENET & 22.29 & 53.17 & 68.13 & 57.90 & 0.589 & 0.107\\\nTETFN & 33.42 & 56.91 & **73.58** & 68.67 & **0.505** & 0.387\\\nTFR-Net & 26.52 & 52.89 & 68.13 & 58.70 & 0.661 & 0.169\\\nALMT & 20.00 & 45.36 & 69.66 & 72.76 & 0.561 & 0.364\\\n**LNLN** & **34.64** & **57.14** & 72.73 & **79.43** & 0.514 & **0.397**\\\n:::\n\n<figure id=\"fig: performance_curves\">\n<div class=\"center\">\n<embed src=\"figs/performance_curves.pdf\" style=\"width:100.0%\" />\n</div>\n<figcaption>Performance curves of various missing rates. (a), (b) and (c) are the F1 curves on MOSI, MOSEI, and SIMS, respectively. (d), (e) and (f) are the MAE curves on MOSI, MOSEI, and SIMS, respectively. Note: The smaller MAE indicates the better performance.</figcaption>\n</figure>\n\n## Effects of Different Components\n\nTo verify the role of different components, we present the results after subtracting each component on the MOSI dataset separately. As shown in Table [\\[tab: Effects of Different Components\\]](#tab: Effects of Different Components){reference-type=\"ref\" reference=\"tab: Effects of Different Components\"}, some metrics show an upward trend in performance when certain modules are removed. On one hand, we believe this is due to randomness in noisy scenarios. On the other hand, the same hyperparameters cannot make the model perform optimally across all metrics. Moreover, the performance of LNLN decreases relatively slightly when the DMC and reconstructor are removed individually. However, there is a significant drop in performance when both are removed, *e.g.,* 4.72% decrease in Acc-7 and 5.87% decrease in Acc-5. Performance drops further after replacing DMML with concatenation fusion, which also proves that DMML plays an important role in MSA. Additionally, after removing the noisy data used for training, the performance of LNLN also shows a significant decrease. This observation indicates that LNLN needs to learn to be aware of noise with the help of noisy data. Overall, both the presence of noisy data and well-designed components are crucial for robust MSA in scenarios with random missing data.\n\n## Effects of Different Regularization\n\nAs shown in Table [2](#tab: Effects of Different Regularization){reference-type=\"ref\" reference=\"tab: Effects of Different Regularization\"}, we present the ablation results after subtracting each regularization on the MOSI dataset separately. Obviously, after each regularization is removed, LNLN shows a decrease in performance on most metrics. This demonstrates that regularization contributes to the learning of the model. Similar to the results in Table [\\[tab: Effects of Different Components\\]](#tab: Effects of Different Components){reference-type=\"ref\" reference=\"tab: Effects of Different Components\"}, there are a few metrics that improve after the removal of regularization. We believe this is mainly due to the difficulty of optimizing a single set of hyperparameters to achieve the best performance across all metrics.\n\n::: {#tab: Effects of Different Regularization}\n  Method                       MOSI                                                                                        SIMS                                                     \n  ------------------------- ----------- ----------- ----------------------- ------------------- ----------- ----------- ----------- ----------- ----------- ----------- ----------- -----------\n  2-7(lr)8-13                  Acc-7       Acc-5             Acc-2                  F1              MAE        Corr        Acc-5       Acc-3       Acc-2        F1          MAE        Corr\n  **LNLN**                   **34.26**     38.27     **72.55** / **70.94**   72.73 / **71.25**   **1.046**   **0.527**   **34.64**   **57.14**     72.73     **79.43**   **0.514**   **0.397**\n  w/o $\\mathcal{L}_{cc}$       33.61       37.78         72.32 / 70.72         72.55 / 71.02       1.047       0.531       32.87       56.41       70.78       75.30       0.523       0.376\n  w/o $\\mathcal{L}_{adv}$      32.84       37.79         72.28 / 70.31       **72.83** / 70.40     1.052       0.526       32.47       54.24     **73.80**     73.91       0.525       0.351\n  w/o $\\mathcal{L}_{rec}$      33.94     **38.52**       72.46 / 70.81         72.69 / 71.17       1.048       0.523       34.33       56.12       71.29       74.73       0.525       0.364\n\n  : Effects of different regularization. Note: The smaller MAE indicate the better performance.\n:::\n\n## Effects of Different Modalities\n\nTo verify the role of different modalities, we present the ablation results after subtracting each modality from the MOSI dataset. As shown in Table [\\[tab: Effects of Different Modalities\\]](#tab: Effects of Different Modalities){reference-type=\"ref\" reference=\"tab: Effects of Different Modalities\"}, all modalities contribute to the final performance. Notably, when the language modality is removed, the performance of LNLN shows a significant decrease, while removing the visual and audio modalities results in a smaller decrease. This indicates that the language modality plays a crucial role in the MSA dataset. Moreover, we observe that LNLN converges on most metrics even when the visual and audio modalities are removed. This is due to the model's lazy behavior, where LNLN tends to fix its predictions to a certain category. More details can be found in Appendix [6.3](#sec: data_distribution){reference-type=\"ref\" reference=\"sec: data_distribution\"} and [6.6](#sec: cm){reference-type=\"ref\" reference=\"sec: cm\"}.\n\n[]{#tab: Effects of Different Modalities label=\"tab: Effects of Different Modalities\"}\n\n## Discussion\n\nIn this work, we conduct a comprehensive evaluation of current advanced methods, the results of which have triggered some thoughts. Specifically, we find that all methods fail to accurately predict the inputs in high missing rate scenarios because most of them are designed specifically for complete data (see Appendix [6.6](#sec: cm){reference-type=\"ref\" reference=\"sec: cm\"} for more details). However, some methods show relatively more robust performance in low missing rate scenarios and some specific scenarios (see Appendix [6.8](#sec: modality_missing){reference-type=\"ref\" reference=\"sec: modality_missing\"}), such as Self-MM, TETFN, and ALMT. We believe that some of the ideas in these methods may be useful for future research on more robust MSA for incomplete data.\n\nFor example, the Unimodal Label Generation Module (ULGM) [@DBLP:conf/aaai/YuXYW21] used in both Self-MM and TETFN facilitates the modeling of incomplete data. Due to the random data missing, the temporal and structured affective information in the data is corrupted, making it difficult for the models to perceive consistency and variability information between different modalities. Using ULGM to generate pseudo-labels as an additional supervisory signal for the models may directly enable the model to learn the relationship between different modalities. For ALMT, we believe that its Adaptive Hyper-modality Learning (AHL) module facilitates the modeling of robust affective representations. AHL mainly uses the dominant modality to compute query vectors and applies multi-head attention to query useful sentiment cues from other modalities as a complement to the dominant modality. This process may reduce the difficulty of multimodal fusion by avoiding the introduction of sentiment-irrelevant information to some extent. Our proposed LNLN, based on this idea from ALMT, proves the effectiveness of the method.\n\nAdditionally, we explore the performance of these methods in modality missing scenarios, which is a special case of random data missing where the partial modality missing rate $r=1.0$. We find that when language modality is missing, the performance of many models decreases significantly and converges to the same value. More details can be seen in Appendix [6.8](#sec: modality_missing){reference-type=\"ref\" reference=\"sec: modality_missing\"}.\n\n# Conclusion and Future Work\n\nIn this paper, a novel Language-dominated Noise-resistant Learning Network (LNLN) is proposed for robust MSA. Due to the collaboration between Dominate Modality Correction (DMC) module, the Dominant Modality based Multimodal Learning (DMML) module, and the Reconstructor, LNLN can enhance the dominant modality to achieve superior performance in data missing scenarios with varying levels. Extensive evaluation shows that none of the existing methods can effectively model the data with high missing rates. The related analyses can provide suggestions for other researchers to better handle robust MSA. In the future, we will focus on improving the generalization of the model to handle different types of scenes and varying intensities of noise effectively.\n\n# Limitations {#limitations .unnumbered}\n\nWe believe there are several limitations to the LNLN. 1) The LNLN achieves good performance in data missing scenarios, but is not always better than all other methods in the modality missing scenarios, which demonstrates the lack of multi-scene generalization capability of the LNLN. 2) The data in real-world scenarios is much more complex. In addition to the presence of missing data, other factors need to be considered, such as diverse cultural contexts, varying user behavior patterns, and the influence of platform-specific features on the data. These factors can introduce additional noise and variability, which may require further model adaptation and tuning to handle effectively. 3) Tuning the hyperparameters, particularly those related to the loss functions, can be challenging and may require more sophisticated methods to achieve optimal performance.\n\n# Acknowledgements {#acknowledgements .unnumbered}\n\nThis work is supported by the Guangdong Provincial Key Laboratory of Mathematical Foundations for Artificial Intelligence (2023B1212010001).\n\n# Additional Experiments and Analysis\n\n## Effect of Regularization Weight on Model Performance\n\nAs shown in Table [\\[tab: Effects of Different Weight\\]](#tab: Effects of Different Weight){reference-type=\"ref\" reference=\"tab: Effects of Different Weight\"}, we empirically tried different combinations of hyperparameters in the loss function. The results show that the selected parameters can make the LNLN achieve better performance in most metrics. This also demonstrates the effectiveness of the optimization objectives of LNLN.\n\n## Analysis of Model Stability\n\nTo evaluate the stability of the model, we selected the MOSI dataset for ablation experiments. Specifically, based on the overall performance (mean) in Table [1](#tab: Results_Compare_MOSI_MOSEI){reference-type=\"ref\" reference=\"tab: Results_Compare_MOSI_MOSEI\"}, we selected several representative methods to additionally compute the standard deviation. For each method, we first calculate the standard deviation of evaluation results for the three random seeds when the missing rate $r$ ranges from 0 to 0.9, and then average the 10 sets of standard deviation. The final result is shown in Table [3](#tab: Stability_Compare_MOSI){reference-type=\"ref\" reference=\"tab: Stability_Compare_MOSI\"}. We can see that the standard deviation of these methods is not very large in most metrics, indicating that all these methods can guarantee stability in the presence of random noise. It should be noted that LNLN strikes a balance between overall performance and stability.\n\n::: {#tab: Stability_Compare_MOSI}\n    Method                   Acc-7                                 Acc-5                                                    Acc-2                                                                        F1                                                       MAE                                  Corr\n  ---------- ------------------------------------- ------------------------------------- --------------------------------------------------------------------------- --------------------------------------------------------------------------- ------------------------------------- -------------------------------------\n     MISA       29.85$\\pm$`<!-- -->`{=html}2.62       33.08$\\pm$`<!-- -->`{=html}1.77         71.49$\\pm$`<!-- -->`{=html}1.00 / 70.33$\\pm$`<!-- -->`{=html}0.93           71.28$\\pm$`<!-- -->`{=html}0.92/ 70.00$\\pm$`<!-- -->`{=html}1.26          1.085$\\pm$`<!-- -->`{=html}0.08       0.524$\\pm$`<!-- -->`{=html}0.08\n   Self-MM      29.55$\\pm$`<!-- -->`{=html}1.06       34.67$\\pm$`<!-- -->`{=html}1.87         70.51$\\pm$`<!-- -->`{=html}0.71 / 69.26$\\pm$`<!-- -->`{=html}1.07           66.60$\\pm$`<!-- -->`{=html}1.92 / 67.54$\\pm$`<!-- -->`{=html}2.34         1.070$\\pm$`<!-- -->`{=html}0.13       0.512$\\pm$`<!-- -->`{=html}0.07\n    CENET       30.38$\\pm$`<!-- -->`{=html}1.27       37.25$\\pm$`<!-- -->`{=html}1.53         71.46$\\pm$`<!-- -->`{=html}0.60 / 67.73$\\pm$`<!-- -->`{=html}0.71           68.41$\\pm$`<!-- -->`{=html}1.12 / 64.85$\\pm$`<!-- -->`{=html}2.56         1.080$\\pm$`<!-- -->`{=html}0.14       0.504$\\pm$`<!-- -->`{=html}0.11\n   TFR-Net      29.54$\\pm$`<!-- -->`{=html}1.00       34.67$\\pm$`<!-- -->`{=html}1.75         68.15$\\pm$`<!-- -->`{=html}1.25 / 66.35$\\pm$`<!-- -->`{=html}1.09           61.73$\\pm$`<!-- -->`{=html}2.82 / 60.06$\\pm$`<!-- -->`{=html}2.33         1.200$\\pm$`<!-- -->`{=html}0.11       0.459$\\pm$`<!-- -->`{=html}0.25\n   **LNLN**   **34.26**$\\pm$`<!-- -->`{=html}1.17   **38.27**$\\pm$`<!-- -->`{=html}1.23   **72.55**$\\pm$`<!-- -->`{=html}1.11 / **70.94**$\\pm$`<!-- -->`{=html}1.28   **72.73**$\\pm$`<!-- -->`{=html}0.99 / **71.25**$\\pm$`<!-- -->`{=html}1.06   **1.046**$\\pm$`<!-- -->`{=html}0.21   **0.527**$\\pm$`<!-- -->`{=html}0.17\n\n  : Comparison of model stability on MOSI dataset. Note: -$\\pm$- means mean$\\pm$std. The smaller MAE indicates better performance.\n:::\n\n## Analysis of Data Distribution {#sec: data_distribution}\n\nFigure [4](#fig: data_distribution){reference-type=\"ref\" reference=\"fig: data_distribution\"} illustrates the data distribution on the MOSI, MOSEI, and SIMS datasets. Significant category imbalances can be seen across all datasets. Additionally, the distributions of the training set, validation set, and test set are not identical on the MOSI and MOSEI datasets. For example, as shown in Figure [4](#fig: data_distribution){reference-type=\"ref\" reference=\"fig: data_distribution\"} (a), the percentage of weakly negative samples is 15.5% in the MOSI training set, while this category reaches 22.7% in the test set, representing the highest percentage among all categories. Similarly, Figure [4](#fig: data_distribution){reference-type=\"ref\" reference=\"fig: data_distribution\"} (b) and Figure [4](#fig: data_distribution){reference-type=\"ref\" reference=\"fig: data_distribution\"} (c) show that this phenomenon also exists in the MOSEI and SIMS datasets.\n\nThe unbalanced data distribution makes it difficult for the model to perform well in data missing scenarios. Specifically, it is easy for the model to engage in lazy behavior in high-noise scenarios, simply biasing predictions towards categories with a higher proportion in the training set. These unbalanced distributions may further increase the learning difficulty of the model when facing data missing. More details can be found in Appendix [6.6](#sec: cm){reference-type=\"ref\" reference=\"sec: cm\"}.\n\n## Case Study\n\nAs shown in Figure [3](#fig: case_example){reference-type=\"ref\" reference=\"fig: case_example\"}, we visualize several successful and failed predictions made by LNLN and ALMT from the MOSI dataset for the case study. It shows that LNLN can perceive sentiment cues in challenging samples, which demonstrates its ability to capture sentiment cues in noisy scenarios. However, for inputs with high missing rates (*e.g.,* the third example in the figure), both LNLN and ALMT fail to make correct predictions. We believe this is due to the high loss of valid information in the multimodal input, making accurate predictions difficult.\n\n<figure id=\"fig: case_example\">\n<div class=\"center\">\n<embed src=\"figs/case_example.pdf\" style=\"width:90.0%\" />\n</div>\n<figcaption>Visualization of successful and failed predictions. Note: The input is visualized to facilitate readers’ understanding. In practice, random data missing is applied to the original input sequence, as described in Section <a href=\"#sec: method\" data-reference-type=\"ref\" data-reference=\"sec: method\">3</a>.</figcaption>\n</figure>\n\n## Details of Robust Comparison\n\nTables [4](#tab: Details_Robust_Compare_MOSI_MOSEI){reference-type=\"ref\" reference=\"tab: Details_Robust_Compare_MOSI_MOSEI\"} and [\\[tab: Details_Robust_Compare_SIMS\\]](#tab: Details_Robust_Compare_SIMS){reference-type=\"ref\" reference=\"tab: Details_Robust_Compare_SIMS\"} show the details of robust comparison on the MOSI, MOSEI, and SIMS datasets, respectively. We observe that Self-MM and TETFN have a clear advantage in many evaluation metrics when the missing rate $r$ is low. As the missing rate $r$ increases, LNLN shows a significant improvement in all evaluation metrics. This demonstrates LNLN's ability to effectively model data affected by noise of varying intensity. In general, models augmented with noisy data in training usually cannot achieve state-of-the-art performance when the noise is low due to differences in data distribution. It is also worthwhile to study in the future how to balance the performance of models under different levels of noise.\n\nIn addition, when $r$ is high, many models converge in their performance on metrics such as Acc-7 and Acc-5 (see Appendix [6.6](#sec: cm){reference-type=\"ref\" reference=\"sec: cm\"}). This is due to the fact that these models exhibit lazy behavior, tending to predict inputs as categories with a higher number of samples in the training set. In such cases, we believe that the models do not actually learn effective knowledge, even if their performance appears good.\n\n<figure id=\"fig: data_distribution\">\n<div class=\"center\">\n<embed src=\"figs/data_distribution.pdf\" style=\"width:100.0%\" />\n</div>\n<figcaption>Data distribution of MOSI, MOSEI, and SIMS datasets.</figcaption>\n</figure>\n\n::: {#tab: Details_Robust_Compare_MOSI_MOSEI}\n  Method                           MOSI                                                                                            MOSEI                                                                            \n  ----------------------------- ----------- ----------- ----------------------- ----------------------- ----------- ----------- ----------- ----------- ----------------------- ----------------------- ----------- -----------\n  2-7(lr)8-13                      Acc-7       Acc-5             Acc-2                    F1                MAE        Corr        Acc-7       Acc-5             Acc-2                    F1                MAE        Corr\n  Random Missing Rate $r=0$                                                                                                                                                                                         \n  MISA                             43.05       48.30         82.78 / 81.24           82.83 / 81.23         0.771       0.777       51.79       53.85         85.28 / 84.10           85.10 / 83.75         0.552       0.759\n  Self-MM                          42.81     **52.38**   **85.22** / **83.24**   **85.19** / **83.26**     0.720       0.790       53.89       55.72       85.34 / **84.68**       85.11 / **84.66**     **0.531**     0.764\n  MMIM                           **45.92**     49.85         83.43 / 81.97           83.43 / 81.94         0.744       0.778       50.76       53.04         83.53 / 81.65           83.39 / 81.41         0.576       0.724\n  CENET                            43.20       50.39         83.08 / 81.49           83.06 / 81.48         0.748       0.785     **54.39**   **56.12**       85.49 / 82.30           85.41 / 82.60       **0.531**     0.770\n  TETFN                            44.07       51.31         82.62 / 81.10           82.67 / 81.09       **0.719**   **0.794**     44.07       55.96         82.62 / 81.10           82.67 / 81.09         0.719     **0.794**\n  TFR-Net                          40.82       47.91         83.64 / 81.68           83.57 / 81.61         0.805       0.760       53.71       47.91         84.96 / 84.65           84.71 / 84.34         0.550       0.745\n  ALMT                             42.37       48.49         84.91 / 82.75           85.01 / 82.94         0.752       0.768       52.18       53.89       **85.62** / 83.99       **85.69** / 84.53       0.542       0.752\n  LNLN                             44.56       49.76         84.25 / 81.24           84.61 / 81.79         0.751       0.778       50.66       51.94         84.14 / 83.61           84.53 / 84.02         0.572       0.735\n  Random Missing Rate $r=0.1$                                                                                                                                                                                       \n  MISA                             40.28       46.21         80.18 / 79.01           80.21 / 78.97         0.847       0.721       50.13       51.34         82.21 / 82.28           81.28 / 80.79         0.598       0.722\n  Self-MM                          40.33     **49.03**   **81.40** / **80.03**     81.19 / **80.03**       0.812       0.728       51.80       53.18       83.03 / **83.79**         82.43 / 83.23         0.564       0.725\n  MMIM                             42.61       46.65         79.98 / 78.13           79.83 / 77.99         0.825       0.718       49.09       51.19         82.00 / 81.09           81.57 / 80.15         0.602       0.696\n  CENET                            40.13       46.60         80.08 / 78.38           79.91 / 78.20         0.837       0.719     **52.83**     54.23         83.75 / 82.41           83.42 / 82.34       **0.556**   **0.739**\n  TETFN                            40.67       46.84         80.59 / 78.91           80.55 / 78.79       **0.805**   **0.731**     40.67     **54.28**       80.59 / 78.91           80.55 / 78.79         0.805       0.731\n  TFR-Net                          38.63       45.82         79.27 / 77.99           78.70 / 77.61         0.872       0.705       52.29       45.82         82.92 / 83.31           82.25 / 82.40         0.573       0.715\n  ALMT                             39.84       45.48         80.90 / 78.67           81.15 / 79.08         0.843       0.703       49.98       51.38       **84.14** / 82.84     **84.23** / **83.04**     0.583       0.718\n  LNLN                           **42.37**     47.91         81.20 / 78.43         **81.62** / 79.04       0.820       0.724       49.96       51.25         83.32 / 82.73           83.66 / 82.91         0.591       0.712\n  Random Missing Rate $r=0.2$                                                                                                                                                                                       \n  MISA                             36.25       41.55         77.54 / 76.34           77.58 / 76.30         0.939       0.654       47.24       47.66         77.84 / 79.93           75.56 / 76.88         0.659       0.674\n  Self-MM                          36.64       43.98         78.15 / 76.48           77.76 / 76.51         0.901       0.660       49.44       50.51       80.84 / **82.33**         79.76 / 81.17         0.604       0.678\n  MMIM                             39.07       42.66         76.42 / 74.54           76.12 / 74.22         0.918       0.651       46.27       47.99         79.93 / 79.66           79.08 / 77.68         0.642       0.653\n  CENET                            38.00       42.32         77.49 / 74.64           77.35 / 74.28         0.916       0.654     **50.72**     51.85         81.46 / 81.62           80.78 / 81.17       **0.590**   **0.698**\n  TETFN                            35.81       41.79         77.49 / 75.60           77.35 / 75.35         0.910       0.657       35.81     **52.35**       77.49 / 75.60           77.35 / 75.35         0.910       0.657\n  TFR-Net                          34.70       40.13         74.70 / 73.52           73.57 / 72.70         0.987       0.622       51.04       40.13         80.47 / 81.61           79.29 / 79.99         0.604       0.672\n  ALMT                             35.33       40.33         77.64 / 75.70           77.94 / 76.24         0.927       0.645       46.61       47.82       **82.71** / 81.65       **82.82** / 81.83       0.607       0.669\n  LNLN                           **39.74**   **45.14**   **79.22** / **76.87**   **79.53** / **77.34**   **0.891**   **0.668**     48.75       49.95         81.70 / 81.68         81.95 / **81.89**       0.616       0.677\n  Random Missing Rate $r=0.3$                                                                                                                                                                                       \n  MISA                             34.60       38.97         75.76 / 74.54           75.82 / 74.51         0.989     **0.618**     43.99       43.40         73.32 / 77.28           68.91 / 72.25         0.724       0.615\n  Self-MM                          34.89       40.67         76.37 / 74.98           75.68 / 74.94         0.967       0.614       47.23       48.07         77.63 / 79.99           75.69 / 77.74         0.653       0.610\n  MMIM                             36.83       40.43         74.08 / 71.91           73.47 / 71.28         0.974       0.612       43.25       44.73         77.08 / 77.79           75.46 / 74.49         0.690       0.597\n  CENET                            34.74       38.97         76.83 / 72.01           76.56 / 71.30         0.983       0.605       48.49       49.37         78.65 / 80.02           77.34 / 78.94         0.636     **0.640**\n  TETFN                            33.24       38.58         75.25 / 73.42           74.77 / 72.78         0.982       0.607       33.24     **50.23**       75.25 / 73.42           74.77 / 72.78         0.982       0.607\n  TFR-Net                          32.55       38.34         72.36 / 71.28           70.12 / 69.58         1.065       0.572     **48.75**     38.34         77.48 / 79.29           75.43 / 76.52         0.650       0.604\n  ALMT                             33.04       37.17         75.15 / 72.94           75.51 / 73.66         0.992       0.596       43.04       44.05       **80.94** / 79.94       **81.15** / 80.20     **0.632**     0.598\n  LNLN                           **38.00**   **42.81**   **77.29** / **75.46**   **77.56** / **75.68**   **0.953**     0.617       47.36       48.40       80.11 / **80.45**       80.44 / **80.91**       0.648       0.629\n  Random Missing Rate $r=0.4$                                                                                                                                                                                       \n  MISA                             32.65       35.37         73.88 / 72.59           73.88 / 72.49         1.041       0.585       40.87       39.53         70.46 / 75.04           64.02 / 67.93         0.780       0.561\n  Self-MM                          31.20       36.30         73.17 / 71.96           71.74 / 71.75         1.027       0.579       44.40       45.04         75.02 / 78.09           72.01 / 74.48         0.694       0.554\n  MMIM                             33.38       35.76         70.84 / 68.90           69.69 / 67.80         1.034       0.576       40.84       41.86         74.56 / 76.15           71.98 / 71.40         0.732       0.542\n  CENET                            32.26       36.15         73.38 / 71.53           72.75 / 70.26         1.031       0.574     **47.12**   **47.74**       76.03 / 78.57           73.87 / 76.75         0.678       0.587\n  TETFN                            30.66       35.28         72.05 / 70.07           70.79 / 68.58         1.051       0.571       30.66       47.62         72.05 / 70.07           70.79 / 68.58         1.051       0.571\n  TFR-Net                          30.17       35.76         68.75 / 67.74           64.71 / 64.41         1.142       0.537       46.70       35.76         74.74 / 77.65           71.67 / 73.71         0.688       0.548\n  ALMT                             31.44       35.03         73.12 / 71.14           73.85 / 72.47         1.045       0.560       40.40       41.21       **79.40** / 79.16       **79.68** / 79.50     **0.651**     0.536\n  LNLN                           **36.49**   **41.11**   **76.01** / **74.25**   **76.31** / **74.67**   **0.987**   **0.594**     45.99       46.88       78.49 / **79.70**       78.98 / **80.46**       0.673     **0.592**\n  Random Missing Rate $r=0.5$                                                                                                                                                                                       \n  MISA                             28.14       30.61         70.53 / 69.34           70.50 / 69.20         1.124       0.519       38.12       36.05         67.38 / 73.21           58.38 / 64.14         0.834       0.492\n  Self-MM                          26.97       31.39         67.43 / 67.54           64.27 / 66.81         1.129       0.503       42.70       43.14         71.97 / 75.81           67.40 / 70.38         0.733       0.477\n  MMIM                             28.23       29.89         68.09 / 66.52           66.15 / 64.59         1.128       0.501       38.68       39.21         71.75 / 74.45           67.70 / 67.96         0.775       0.470\n  CENET                            28.33       30.90         72.46 / 66.08           71.10 / 63.50         1.130       0.496     **45.12**     45.52         73.33 / 77.16           69.80 / 74.14         0.720       0.515\n  TETFN                            27.55       31.34         67.23 / 65.06           64.30 / 61.78         1.157       0.492       27.55     **45.63**       67.23 / 65.06           64.30 / 61.78         1.157       0.492\n  TFR-Net                          25.85       30.71         64.83 / 63.02           58.04 / 56.64         1.270       0.443       45.00       30.71         71.53 / 75.69           66.88 / 70.07         0.730       0.471\n  ALMT                             28.42       31.25         68.24 / 65.94           69.74 / 68.54         1.138       0.485       37.82       38.34       **77.40** / 77.48       **77.73** / 77.80     **0.683**     0.461\n  LNLN                           **33.92**   **38.39**   **73.37** / **71.86**   **73.70** / **72.30**   **1.059**   **0.536**     44.90       45.59       76.44 / **78.10**       77.23 / **79.30**       0.710     **0.529**\n  Random Missing Rate $r=0.6$                                                                                                                                                                                       \n  MISA                             24.68       27.12         66.97 / 65.84           66.94 / 65.69         1.200       0.441       36.16       33.30         65.55 / 72.30           54.64 / 62.12         0.875       0.415\n  Self-MM                          24.34       27.31         63.47 / 63.36           58.94 / 62.07         1.209       0.425       41.47       41.75         69.33 / 73.93           63.01 / 66.76         0.762       0.401\n  MMIM                             25.41       27.11         63.67 / 62.49           60.87 / 59.48         1.208       0.418       37.13       37.48         68.83 / 73.16           63.09 / 65.43         0.808       0.402\n  CENET                            24.54       26.53         67.58 / 61.47           64.87 / 57.86         1.215       0.415     **44.45**   **44.64**       70.50 / 75.39           65.27 / 70.86         0.749       0.446\n  TETFN                            25.12       27.99         63.42 / 61.23           58.68 / 56.08         1.238       0.417       25.12       44.07         63.42 / 61.23           58.68 / 56.08         1.238       0.417\n  TFR-Net                          24.05       28.33         61.64 / 59.47           52.44 / 50.53         1.371       0.363       43.88       28.33         68.80 / 74.05           62.51 / 67.07         0.762       0.397\n  ALMT                             25.41       27.36         64.53 / 62.15           66.81 / 65.87         1.214       0.407       35.99       36.30       **74.98** / 76.26       **75.44** / 76.71     **0.710**     0.395\n  LNLN                           **30.37**   **34.35**   **69.00** / **67.69**   **69.19** / **67.99**   **1.147**   **0.458**     43.52       44.00       73.82 / **76.50**       75.03 / **78.33**       0.736     **0.471**\n  Random Missing Rate $r=0.7$                                                                                                                                                                                       \n  MISA                             21.14       23.27         65.09 / 63.89           65.07 / 63.74         1.257       0.381       34.54       31.21         64.28 / 71.71           51.82 / 60.65         0.906       0.344\n  Self-MM                          20.70       23.81         61.74 / 61.46           55.11 / 58.97         1.271       0.339       39.93       40.12         66.79 / 72.55           58.05 / 63.45         0.786       0.329\n  MMIM                             22.35       24.00         61.23 / 59.18           57.15 / 54.36         1.267       0.342       35.25       35.47         66.89 / 72.26           58.90 / 63.26         0.834       0.341\n  CENET                            22.35       59.86         63.82 / 59.43           53.79 / 54.22         1.269       0.335     **43.93**   **44.03**       67.50 / 73.39           59.88 / 67.02         0.776       0.384\n  TETFN                            23.13       25.27         61.13 / 58.65           53.79 / 50.77         1.293       0.337       23.13       43.15         61.13 / 58.65           53.79 / 50.77         1.293       0.337\n  TFR-Net                          23.71       26.92         59.91 / 57.34           48.41 / 45.48         1.454       0.276       42.91       26.92         66.64 / 72.77           58.32 / 64.02         0.786       0.322\n  ALMT                             23.71       24.97         61.84 / 59.67         65.30 / **65.19**       1.266       0.336       34.78       34.95       **71.62** / 73.98         72.24 / 74.54       **0.743**     0.315\n  LNLN                           **27.79**   **31.19**   **65.95** / **65.01**     **65.95** / 65.14     **1.219**   **0.383**     42.22       42.56       71.55 / **74.74**     **73.49** / **77.40**     0.762     **0.408**\n  Random Missing Rate $r=0.8$                                                                                                                                                                                       \n  MISA                             19.92       20.99     **63.56** / **62.24**       63.16 / 61.67         1.311       0.321       33.29       29.51         63.43 / 71.30           49.95 / 59.69         0.927       0.267\n  Self-MM                          19.29       22.11         59.55 / 58.26           49.98 / 53.56         1.313       0.282       38.69       38.78         65.07 / 71.83           54.44 / 61.49         0.805       0.259\n  MMIM                             20.26       21.77         58.33 / 55.30           52.46 / 47.89         1.312       0.287       33.64       33.71         64.97 / 71.57           54.76 / 61.45         0.858       0.269\n  CENET                            21.14       21.67         60.93 / 57.53           54.68 / 50.80         1.314       0.274     **42.71**   **42.74**       65.88 / 72.16           56.80 / 64.67         0.798       0.316\n  TETFN                            22.01       23.76         59.40 / 56.85           48.73 / 45.59         1.337       0.274       22.01       42.11         59.40 / 56.85           48.73 / 45.59         1.337       0.274\n  TFR-Net                          23.23       27.70         58.49 / 55.98           44.70 / 41.88         1.497       0.155       42.23       27.70         65.05 / 71.95           54.91 / 61.82         0.807       0.241\n  ALMT                             23.13       23.66         60.37 / 58.31       **65.45** / **66.14**     1.310       0.273       34.01       34.09         68.15 / 71.48           69.12 / 72.28       **0.774**     0.231\n  LNLN                           **26.34**   **28.23**       62.75 / 62.10           62.56 / 62.03       **1.283**   **0.314**     40.76       40.97     **68.62** / **72.86**   **71.83** / **76.80**     0.791     **0.325**\n  Random Missing Rate $r=0.9$                                                                                                                                                                                       \n  MISA                             17.78       18.41       58.64 / **58.21**         56.84 / 56.19         1.369     **0.226**     32.29       28.03         62.95 / 71.07           48.80 / 59.12         0.941       0.180\n  Self-MM                          18.32       19.78         58.59 / 55.25           46.16 / 47.46         1.353       0.197       37.46       37.50         63.85 / 71.24           51.32 / 59.72         0.821       0.188\n  MMIM                             18.95       19.53         55.29 / 51.65           47.33 / 40.89         1.357       0.186       32.61       32.67         63.69 / 71.10           51.26 / 59.99         0.877       0.197\n  CENET                            19.15       19.10       **58.99** / 54.76         50.01 / 46.58         1.357       0.181     **42.08**   **42.08**       64.14 / 70.42           54.27 / 62.33         0.814     **0.254**\n  TETFN                            20.75       21.19         58.43 / 55.88           45.24 / 42.12         1.378       0.186       20.75       41.55         58.43 / 55.88           45.24 / 42.12         1.378       0.186\n  TFR-Net                          21.67     **25.12**       57.93 / 55.44           43.01 / 40.18         1.534       0.155       41.73       25.12         63.64 / 71.34           52.02 / 59.99         0.820       0.175\n  ALMT                             20.31       20.50         57.32 / 56.66       **64.92** / **67.82**   **1.349**     0.205       34.40       34.40         61.41 / 68.65           63.32 / 69.83       **0.810**     0.138\n  LNLN                           **22.98**     23.86         56.50 / 56.51           56.32 / 56.47       **1.349**     0.202       40.10       40.19     **64.83** / **71.51**   **70.60** / **77.52**     0.820       0.221\n\n  : Details of robust comparison on MOSI and MOSEI with different random missing rates. Note: The smaller MAE indicates the better performance.\n:::\n\n## Visualization of Confusion Matrix {#sec: cm}\n\nTo verify the effectiveness of the method, we visualized the confusion matrix of several representative methods on the MOSI, MOSEI, and SIMS datasets in Figure [5](#fig: cm_mosi){reference-type=\"ref\" reference=\"fig: cm_mosi\"}, Figure [6](#fig: cm_mosei){reference-type=\"ref\" reference=\"fig: cm_mosei\"}, and Figure [7](#fig: cm_sims){reference-type=\"ref\" reference=\"fig: cm_sims\"}, respectively. Obviously, as the missing rate $r$ increases, the predictions of all methods on all datasets tend to favor certain categories. This phenomenon indicates that the models hardly learn useful knowledge for prediction but instead ensure high accuracy by being lazy. For example, on the MOSI dataset, Self-MM predicts all samples as weak negative when the missing rate is 0.9. Although the accuracy of Self-MM is high when the missing rate $r$ is high, it does not demonstrate that the model is more robust.\n\nIn addition, TETFN and TFR-Net tend to randomly predict within the negative and weak negative categories under high noise, indicating that their lazy behavior is not as severe as methods like Self-MM, MMIM, and MISA. Unlike other methods, although LNLN also suffers from the impact of data missing, the model's lazy behavior is less severe. For example, at a missing rate of 0.9, the model is still attempting to predict the samples, and there is no case of predictions favoring a particular category to an extreme extent.\n\nFrom Figure [6](#fig: cm_mosei){reference-type=\"ref\" reference=\"fig: cm_mosei\"} and Figure [7](#fig: cm_sims){reference-type=\"ref\" reference=\"fig: cm_sims\"}, we can see that a similar phenomenon also occurs in the MOSEI and SIMS datasets. These observations demonstrate that methods targeting complete data often fail when modeling highly incomplete data. Therefore, the design of methods specifically for modeling incomplete data is necessary.\n\n<figure id=\"fig: cm_mosi\">\n<div class=\"center\">\n<embed src=\"figs/cm_mosi.pdf\" style=\"width:100.0%\" />\n</div>\n<figcaption>Seven-category confusion matrix of several representative methods on MOSI dataset. Note: 0-6 denote strongly negative, weakly negative, negative, neutral, weakly positive, positive, and strongly positive, respectively.</figcaption>\n</figure>\n\n<figure id=\"fig: cm_mosei\">\n<div class=\"center\">\n<embed src=\"figs/cm_mosei.pdf\" style=\"width:100.0%\" />\n</div>\n<figcaption>Seven-category confusion matrix of several representative methods on MOSEI dataset. Note: 0-6 denote strongly negative, weakly negative, negative, neutral, weakly positive, positive, and strongly positive, respectively.</figcaption>\n</figure>\n\n<figure id=\"fig: cm_sims\">\n<div class=\"center\">\n<embed src=\"figs/cm_sims.pdf\" style=\"width:100.0%\" />\n</div>\n<figcaption>Five-category confusion matrix of several representative methods on SIMS dataset. Note: 0-4 denote weakly negative, negative, neutral, weakly positive, and positive, respectively.</figcaption>\n</figure>\n\n## Visualization of Representations\n\nFigure [8](#fig: tsne__hd_hl){reference-type=\"ref\" reference=\"fig: tsne__hd_hl\"} shows the language feature $H^1_l$ extracted without applying data missing and the corrected dominant feature $H^1_d$ with different data missing rates. we can see that the model tends to learn $H^1_d$, which is consistent with the distribution of the $H^1_l$, indicating that LNLN can complement the information of the language modality when data is missing. Additionally, when the missing rate $r$ is 1.0 (no valid information for the input), it becomes difficult for the model to learn $H^1_d$ that is consistent with the language feature. This also suggests that the generator is indeed trying to learn $H^1_d$ for fusion in DMML, rather than just guessing.\n\n<figure id=\"fig: tsne__hd_hl\">\n<div class=\"center\">\n<embed src=\"figs/tsne.pdf\" style=\"width:100.0%\" />\n</div>\n<figcaption>Visualization of the corrected dominant feature <span class=\"math inline\"><em>H</em><sub><em>d</em></sub><sup>1</sup></span> with different data missing rate and the language feature <span class=\"math inline\"><em>H</em><sub><em>l</em></sub><sup>1</sup></span> without data missing.</figcaption>\n</figure>\n\n## Analysis of the generalization to modality missing scenarios {#sec: modality_missing}\n\nTo explore the generalization of the method, we further evaluated it in the special case of data missing, *i.e.,* modality missing scenarios. The overall performance of the model is shown in Table [\\[tab: Results_Compare_MOSI_MOSEI_mm\\]](#tab: Results_Compare_MOSI_MOSEI_mm){reference-type=\"ref\" reference=\"tab: Results_Compare_MOSI_MOSEI_mm\"} and Table [\\[tab: Results_Compare_SIMS_mm\\]](#tab: Results_Compare_SIMS_mm){reference-type=\"ref\" reference=\"tab: Results_Compare_SIMS_mm\"}. The detailed performance of the model is shown in Table [\\[tab: Generalization comparison_MOSI_MOSEI\\]](#tab: Generalization comparison_MOSI_MOSEI){reference-type=\"ref\" reference=\"tab: Generalization comparison_MOSI_MOSEI\"} and Table [\\[tab: Generalization_comparison_SIMS_mm\\]](#tab: Generalization_comparison_SIMS_mm){reference-type=\"ref\" reference=\"tab: Generalization_comparison_SIMS_mm\"}. It is evident that LNLN, ALMT, TETFN, and Self-MM all exhibit excellent generalization.\n\nWe believe that the Dominant Modality Correction (DMC) proposed in LNLN, the Adaptive Hyper-modality Learning (AHL) proposed in ALMT, and the Unimodal Label Generation Module (ULGM) proposed/used in Self-MM and TETFN deserve more research attention in the future. These modules and their corresponding ideas have significant reference value for achieving robust Multimodal Sentiment Analysis (MSA).\n\nHowever, we also found that many methods converge to the same value when the noise is high. This is similar to the phenomenon observed in previous data missing scenarios, where many models exhibited lazy behavior. Therefore, handling modality missing and improving model robustness in different noise scenarios remain critical challenges that need to be addressed in the field of MSA.\n\n::: table*\n  Method           MOSI                                                                                            MOSEI                                                                        \n  ------------- ----------- ----------- ----------------------- ----------------------- ----------- ----------- ----------- ----------- ------------------- ----------------------- ----------- -----------\n  2-7(lr)8-13      Acc-7       Acc-5             Acc-2                    F1                MAE        Corr        Acc-7       Acc-5           Acc-2                  F1                MAE        Corr\n  MISA             29.44       31.76         67.64 / 67.27           64.81 / 64.30         1.096     **0.461**     41.45       40.32       73.94 / 77.49         66.65 / 71.30         0.752       0.438\n  Self-MM          29.74     **36.40**   **71.45** / **68.17**       63.71 / 61.91       **1.052**     0.450       45.17       46.05     74.12 / **77.85**       67.00 / 71.81       **0.684**     0.447\n  MMIM             31.28       33.74         68.34 / 65.12           63.38 / 58.28         1.058       0.444       41.36       42.53       73.19 / 76.14         68.11 / 70.18         0.732       0.448\n  CENET            30.30       33.94         70.37 / 66.67           62.66 / 58.61         1.068       0.450       47.41       48.19     **73.45** / 74.14       68.43 / 70.37       **0.684**   **0.480**\n  TETFN          **32.66**     36.26         70.17 / 68.15           62.47 / 60.18         1.061       0.446     **47.52**   **48.41**     73.87 / 77.43         66.70 / 71.45         0.691       0.426\n  TFR-Net          29.76       34.96         70.49 / 67.95           62.87 / 60.08         1.177       0.451       47.33       48.08       73.50 / 77.45         66.89 / 71.28         0.700       0.427\n  ALMT             27.87       30.73         70.53 / 67.47       **75.89** / **73.35**     1.130       0.447       25.94       27.46       60.66 / 63.37         70.63 / 71.24         0.711       0.354\n  LNLN             31.93       34.86         68.53 / 65.64           72.01 / 69.65         1.093       0.423       43.85       44.50       73.52 / 77.04     **80.85** / **83.19**     0.733       0.425\n:::\n\n::: tabularx\n1.0CCCCCCC Method & Acc-5 & Acc-3 & Acc-2 & F1 & MAE & Corr\\\nMISA & 29.62 & 56.82 & 73.61 & 66.32 & 0.535 & **0.330**\\\nSelf-MM & 30.36 & **59.56** & 75.01 & 73.13 & **0.508** & 0.321\\\nMMIM & 27.21 & 50.79 & 71.52 & 65.48 & 0.546 & 0.286\\\nCENET & 19.95 & 43.63 & 64.08 & 56.96 & 0.601 & 0.033\\\nTETFN & **30.61** & 50.98 & **74.77** & 68.00 & 0.514 & 0.317\\\nTFR-Net & 24.69 & 52.57 & 69.31 & 58.03 & 0.629 & 0.133\\\nALMT & 21.82 & 39.39 & 72.30 & 78.86 & 0.563 & 0.287\\\nLNLN & 30.39 & 54.32 & 72.67 & **80.87** & 0.527 & 0.287\\\n:::\n\n::: table*\n  Method                                  MOSI                                                                                            MOSEI                                                                            \n  ------------------------------------ ----------- ----------- ----------------------- ----------------------- ----------- ----------- ----------- ----------- ----------------------- ----------------------- ----------- -----------\n  2-7(lr)8-13                             Acc-7       Acc-5             Acc-2                    F1                MAE        Corr        Acc-7       Acc-5             Acc-2                    F1                MAE        Corr\n  Language Modality Missing                                                                                                                                                                                                \n  MISA                                    15.60       15.65         52.18 / 53.74           48.09 / 49.42         1.422     **0.171**     30.93       26.75       62.85 / **71.02**         48.51 / 58.99         0.946       0.128\n  Self-MM                                 16.86       19.29         57.77 / 54.37           42.31 / 43.81         1.381       0.129       36.59       36.59       63.03 / **71.02**         49.22 / 58.99         0.832       0.152\n  MMIM                                    17.44       17.20         54.67 / 49.61           46.95 / 37.63       **1.363**     0.153       31.96       31.97         62.71 / 71.01           54.70 / 59.43         0.886       0.195\n  CENET                                   17.54       17.54         57.67 / 51.80           42.26 / 35.71         1.387       0.118     **41.70**   **41.70**     **63.23** / 69.38         52.41 / 60.67       **0.821**   **0.238**\n  TETFN                                   21.19       21.14         57.77 / 55.25           42.31 / 39.32         1.403       0.097       41.31       41.31         62.82 / 71.01           48.53 / 58.98         0.831       0.148\n  TFR-Net                               **23.13**   **25.95**   **57.82** / **55.30**       42.43 / 39.43         1.563       0.167       41.40       41.43       62.85 / **71.02**         48.88 / 58.99         0.829       0.163\n  ALMT                                    19.29       19.68         56.40 / 55.05       **66.99** / **66.01**     1.394       0.139       21.58       21.58         54.28 / 57.38           69.52 / 70.01         0.867       0.071\n  LNLN                                    18.80       17.68         52.18 / 49.03           58.89 / 56.84         1.427       0.075       39.10       39.10       62.85 / **71.02**     **77.19** / **83.06**     0.847       0.156\n  Audio Modality Missing                                                                                                                                                                                                   \n  MISA                                    43.49       48.44         83.74 / 81.58           83.72 / 81.49         0.762       0.776       51.59       53.91         84.94 / 83.91           84.62 / 83.46         0.561       0.759\n  Self-MM                                 42.81     **52.33**   **85.22** / **83.24**   **85.19** / **83.26**     0.720       0.790       53.91       55.66       85.31 / **84.70**       85.06 / **84.64**       0.532       0.763\n  MMIM                                  **45.39**     49.52         83.08 / 81.15           83.00 / 81.02         0.744       0.777       51.01       53.26         83.72 / 81.51           83.60 / 81.29         0.572       0.726\n  CENET                                   43.05       50.39         83.08 / 81.54           83.06 / 81.54         0.750       0.785     **54.34**   **55.98**     **85.37** / 82.08       **85.35** / 82.44     **0.531**   **0.770**\n  TETFN                                   44.07       51.31         82.52 / 81.00           82.57 / 80.99       **0.719**   **0.794**     54.15       55.94         85.08 / 83.99           85.05 / 84.05         0.543       0.753\n  TFR-Net                                 37.71       45.48         82.72 / 80.52           82.71 / 80.45         0.821       0.759       53.75       55.34         84.95 / 84.65           84.69 / 84.32         0.550       0.745\n  ALMT                                    36.49       41.84         84.60 / 79.88           84.81 / 80.70         0.865       0.767       30.51       33.55         67.13 / 70.05           71.76 / 72.59         0.535       0.556\n  LNLN                                    45.05       52.04         84.86 / 82.21           85.12 / 82.43         0.760       0.772       50.63       51.92         84.39 / 83.09           84.71 / 83.34         0.610       0.736\n  Visual Modality Missing                                                                                                                                                                                                  \n  MISA                                    43.29       47.09         82.37 / 81.00           82.43 / 80.99         0.777       0.777       51.76       53.80       **85.25** / 83.99       **85.11** / 83.76       0.550       0.757\n  Self-MM                                 42.76     **55.47**   **85.11** / **82.80**     85.09 / **82.82**       0.722       0.789     **53.68**   **55.47**     85.20 / **84.62**       84.95 / **84.59**     **0.533**   **0.761**\n  MMIM                                    44.75       49.85         83.54 / 81.83           83.52 / 81.78         0.740       0.778       50.45       52.86         83.71 / 81.31           83.53 / 81.09         0.579       0.722\n  CENET                                   43.20       50.39         83.08 / 81.49           83.06 / 81.48         0.748       0.785       52.95       54.48         84.99 / 81.04           85.03 / 81.56         0.544     **0.761**\n  TETFN                                   44.22       51.46         82.62 / 81.10           82.67 / 81.09       **0.719**   **0.794**     53.55       55.34         84.84 / 83.78           84.81 / 83.86         0.544     **0.761**\n  TFR-Net                                 37.85       45.87         83.54 / 81.10           83.41 / 80.95         0.799       0.760       52.94       54.45         83.97 / 83.48           83.93 / 83.18         0.577       0.726\n  ALMT                                    36.39       41.93         84.81 / 79.98           84.81 / 80.76         0.863       0.768       29.80       32.95         66.81 / 69.41           71.72 / 72.23         0.559       0.747\n  LNLN                                  **45.05**     51.99         84.91 / 82.26         **85.17** / 82.48       0.759       0.772       50.16       51.44         84.09 / 83.41           84.43 / 83.76         0.577       0.729\n  Language & Audio Modality Missing                                                                                                                                                                                        \n  MISA                                    15.55       15.60         55.29 / 55.20           49.53 / 49.24         1.419       0.098       31.17       26.76       62.85 / **71.02**         48.51 / 58.99         0.961       0.121\n  Self-MM                                 16.86       19.29         57.77 / 54.03           42.31 / 43.31       **1.381**   **0.130**     36.61       36.61     **63.01** / **71.02**       49.18 / 58.99         0.831       0.144\n  MMIM                                    17.59       17.40         52.24 / 48.20           39.57 / 31.53         1.389       0.024       31.91       31.92         62.31 / 70.94           54.32 / 59.28         0.886       0.182\n  CENET                                   17.40       17.40         57.67 / 51.85           42.26 / 35.65         1.387       0.110     **41.40**   **41.40**       60.69 / 64.21           51.22 / 56.68       **0.823**   **0.236**\n  TETFN                                   21.19       21.14         57.77 / 55.25           42.31 / 39.32         1.403       0.098       41.31       41.31         62.82 / 71.00           48.51 / 58.98         0.832       0.144\n  TFR-Net                               **21.38**   **23.96**   **58.03** / **55.59**       43.37 / 40.54         1.554       0.107       41.37       41.38       62.81 / **71.02**         48.75 / 58.99         0.829       0.161\n  ALMT                                    19.24       19.24         56.35 / 54.96       **66.89** / **65.86**     1.397       0.104       21.57       21.57         54.28 / 57.28           69.52 / 70.09         0.870       0.091\n  LNLN                                    18.80       17.68         52.18 / 49.03           58.89 / 56.84         1.427       0.072       34.72       34.72       62.85 / **71.02**     **77.19** / **83.06**     0.900       0.145\n  Language & Visual Modality Missing                                                                                                                                                                                       \n  MISA                                    15.45       15.40         48.98 / 50.87           41.79 / 43.51         1.427     **0.169**     31.57       26.89       62.85 / **71.02**         48.51 / 58.99         0.936       0.105\n  Self-MM                                 16.47       19.24       **57.77** / 51.75         42.31 / 35.44         1.386       0.072       36.55       36.55       62.85 / **71.02**         48.51 / 58.99       **0.838**     0.101\n  MMIM                                    17.93       17.40         53.05 / 48.64           43.80 / 36.52       **1.366**     0.155       32.12       32.12       **62.88** / 70.94         48.91 / 59.00         0.891     **0.143**\n  CENET                                   17.54       17.54         57.67 / 51.80           42.26 / 35.71         1.387       0.118     **41.36**   **41.36**       61.78 / 67.49           51.87 / 59.63       **0.838**     0.112\n  TETFN                                   21.19       21.14     **57.77** / **55.25**       42.31 / 39.32         1.403       0.097     **41.36**   **41.36**     62.81 / **71.02**         48.51 / 58.99         0.840       0.017\n  TFR-Net                               **22.50**   **25.07**   **57.77** / **55.25**       42.31 / 39.32         1.486       0.154     **41.36**   **41.36**     62.47 / **71.02**         51.16 / 58.99         0.839       0.039\n  ALMT                                    19.29       19.77         56.45 / 55.10       **67.09** / **66.11**     1.394       0.136       21.54       21.54         54.28 / 57.01           69.52 / 70.35         0.874      -0.086\n  LNLN                                    18.80       17.68         52.18 / 49.03           58.89 / 56.84         1.427       0.075       38.41       38.41       62.85 / **71.02**     **77.19** / **83.06**     0.853       0.052\n  Audio & Visual Modality Missing                                                                                                                                                                                          \n  MISA                                    43.25       48.40         83.28 / 81.24           83.29 / 81.17         0.768       0.776       51.67       53.80         84.88 / 83.97           84.64 / 83.64         0.558       0.756\n  Self-MM                                 42.71     **52.77**   **85.06** / **82.80**     85.04 / **82.82**       0.722       0.789     **53.67**   **55.43**   **85.32** / **84.74**   **85.06** / **84.67**   **0.535**   **0.761**\n  MMIM                                    44.56       51.07         83.48 / 81.29           83.42 / 81.17         0.748       0.777       50.70       53.07         83.79 / 81.14           83.61 / 80.96         0.579       0.722\n  CENET                                   43.05       50.39         83.08 / 81.54           83.06 / 81.54         0.750       0.785       52.71       54.22         84.64 / 80.66           84.71 / 81.23         0.545       0.760\n  TETFN                                   44.12       51.36         82.57 / 81.05           82.62 / 81.04       **0.719**     0.794       53.46       55.22         84.83 / 83.79           84.80 / 83.85         0.549       0.747\n  TFR-Net                                 36.00       43.44         83.08 / 79.98           83.01 / 79.82         0.838       0.758       53.13       54.54         83.97 / 83.49           83.91 / 83.18         0.578       0.726\n  ALMT                                    36.49       41.93         84.55 / 79.83           84.76 / 80.65         0.864       0.767       30.64       33.57         67.14 / 69.11           71.77 / 72.15         0.560       0.748\n  LNLN                                  **45.10**     52.04         84.86 / 82.26         **85.12** / 82.48       0.760       0.772       50.10       51.38         84.10 / 82.69           84.39 / 82.86         0.609       0.730\n:::\n\n# Social Impacts\n\nOur proposed LNLN has a wide range of applications in real-world scenarios, such as healthcare and human-computer interaction. However, it might also be misused to monitor individuals based on their affections for illegal purposes, potentially posing negative social impacts.\n\n[^1]: The code is available at: <https://github.com/Haoyu-ha/LNLN>\n\n[^2]: the corresponding author\n\n[^3]: MMSA: <https://github.com/thuiar/MMSA>\n\n[^4]: ALMT: <https://github.com/Haoyu-ha/ALMT>",
    "rationale": "Summary: The paper addresses data incompleteness in Multimodal Sentiment Analysis (MSA) by presenting the Language-dominated Noise-resistant Learning Network (LNLN). By considering language as the dominant modality, LNLN introduces a Dominant Modality Correction (DMC) module and Dominant Modality-Based Multimodal Learning (DMML) module to enhance robustness against noise. Extensive experiments under random data missing scenarios on datasets like MOSI, MOSEI, and SIMS demonstrate LNLN’s superior performance. The method ensures quality representations of the dominant modality and offers improved uniformity, transparency, and fairness in evaluations, consistently outperforming existing baselines across various challenging metrics.\n\nStrengths: Strength:\n1. The paper addresses the issue of incomplete data in multimodal sentiment analysis, a prevalent and valuable research problem in real-world applications.\n\n2. The manuscript is well-structured, with a clear and logical flow of ideas. The organization facilitates understanding and ensures that the arguments and methodologies are presented in a coherent manner.\n\nWeaknesses: Weakness：\n\n1. The contributions summarized in the introduction are not clearly articulated. Additionally, using language as the primary information carrier to reconstruct incomplete data is a concept frequently employed in multimodal sentiment analysis (MSA). What sets your method apart from other similar approaches?\n\n2. The so-called \"Adaptive Hyper-modality Learning\" essentially involves using transformers to facilitate interactions between different modalities. This interaction process appears to lack algorithmic innovation and seems to adopt a rather naive approach.\n\n3. The primary structure of the Reconstructor is also based on transformers. Why did you choose transformers for reconstruction?\n\n4. The entire algorithm's architecture heavily relies on transformers, but the transformers used in different modules have not been specifically tailored to the characteristics of this task.\n\n5. In the experimental section, although LNLN demonstrates generally good performance, there is no in-depth comparative analysis with the most similar baseline, ALMT. Since LNLN is an improvement based on ALMT, such a detailed analysis is essential.\n\n6. Ablation studies and other experiments were only conducted on the MOSI dataset. Why were these not performed on all datasets?\n\n7. The experimental section lacks visualizations or case study to demonstrate the effectiveness of the algorithm, making it difficult to fully substantiate the algorithm's efficacy.\n\nQuestions: Weakness：\n\n1. The contributions summarized in the introduction are not clearly articulated. Additionally, using language as the primary information carrier to reconstruct incomplete data is a concept frequently employed in multimodal sentiment analysis (MSA). What sets your method apart from other similar approaches?\n\n2. The so-called \"Adaptive Hyper-modality Learning\" essentially involves using transformers to facilitate interactions between different modalities. This interaction process appears to lack algorithmic innovation and seems to adopt a rather naive approach.\n\n3. The primary structure of the Reconstructor is also based on transformers. Why did you choose transformers for reconstruction?\n\n4. The entire algorithm's architecture heavily relies on transformers, but the transformers used in different modules have not been specifically tailored to the characteristics of this task.\n\n5. In the experimental section, although LNLN demonstrates generally good performance, there is no in-depth comparative analysis with the most similar baseline, ALMT. Since LNLN is an improvement based on ALMT, such a detailed analysis is essential.\n\n6. Ablation studies and other experiments were only conducted on the MOSI dataset. Why were these not performed on all datasets?\n\n7. The experimental section lacks visualizations or case study to demonstrate the effectiveness of the algorithm, making it difficult to fully substantiate the algorithm's efficacy.\n\nLimitations: The algorithm's structure is overly reliant on transformers, and there is a lack of explanation as to why transformers were chosen for various operations, especially in relation to the specific characteristics of this task. Additionally, the paper introduces multiple loss functions; however, the effectiveness and necessity of each loss function are not thoroughly justified or demonstrated. This raises questions about whether each loss is indeed contributing as intended to the overall performance of the model.",
    "rating": 3,
    "label": true,
    "rationale_edited": "The paper addresses data incompleteness in Multimodal Sentiment Analysis (MSA) by presenting the Language-dominated Noise-resistant Learning Network (LNLN). By considering language as the dominant modality, LNLN introduces a Dominant Modality Correction (DMC) module and Dominant Modality-Based Multimodal Learning (DMML) module to enhance robustness against noise. Extensive experiments under random data missing scenarios on datasets like MOSI, MOSEI, and SIMS demonstrate LNLN’s superior performance.\n\nThe so-called \"Adaptive Hyper-modality Learning\" essentially involves using transformers to facilitate interactions between different modalities. This interaction process appears to lack algorithmic innovation and seems to adopt a rather naive approach.\n\nIn the experimental section, although LNLN demonstrates generally good performance, there is no in-depth comparative analysis with the most similar baseline, ALMT. Since LNLN is an improvement based on ALMT, such a detailed analysis is essential.\n\nThe experimental section lacks visualizations or case study to demonstrate the effectiveness of the algorithm, making it difficult to fully substantiate the algorithm's efficacy."
  },
  {
    "title": "Scaling Diffusion Language Models via Adaptation from Autoregressive Models",
    "abstract": "Diffusion Language Models (DLMs) have emerged as a promising new paradigm for text generative modeling, potentially addressing limitations of autoregressive (AR) models. However, current DLMs have been studied at a smaller scale compared to their AR counterparts and lack fair comparison on language modeling benchmarks. Additionally, training diffusion models from scratch at scale remains challenging. Given the prevalence of open-source AR language models, we propose adapting these models to build text diffusion models. We demonstrate connections between AR and diffusion modeling objectives and introduce a simple continual pre-training approach for training diffusion models. Through systematic evaluation on language modeling, reasoning, and commonsense benchmarks, we show that we can convert AR models ranging from 127M to 7B parameters (GPT2 and LLaMA) into diffusion models DiffuGPT and DiffuLLaMA, using less than 200B tokens for training. Our experimental results reveal that these models outperform earlier DLMs and are competitive with their AR counterparts. We release a suite of DLMs (127M-355M-7B) capable of generating fluent text, performing in-context learning, filling in the middle without prompt re-ordering, and following instructions.",
    "text": "# Introduction\n\nLarge language models (LLMs) have ushered in a new era of artificial intelligence, demonstrating remarkable capabilities in generating high-quality text, in-context learning, and following complex instructions [@gpt4; @touvron2023llama]. These advancements are primarily rooted in the scaling up of autoregressive (AR) language models. During both training and inference, these models leverage vast datasets and billions of parameters, employing a strict left-to-right sequential process for memorization and generation. This approach has resulted in the emergence of intelligence capable of tackling diverse tasks [@wei2022emergent; @10.5555/3600270.3602446]. However, the ultimate upper limit of intelligence achievable through this paradigm remains an open question. While AR mechanisms form the foundation of current LLMs, they are not without limitations [@lin2020limitations]. Notable challenges include difficulties in future planning [@bachmannpitfalls; @hu2024amortizing; @Xie2024TravelPlannerAB] and self-correction [@huang2024large]. These constraints have spurred researchers to explore alternative architectures for next-generation LLMs.\n\nA compelling direction in current research focuses on the development of text diffusion models [@diffusion2023survey]. Building upon the rapid evolution of diffusion models in various domains [@ho2020denoising; @nichol2021improved; @pmlr-v139-ramesh21a], innovative text diffusion models [@li2022diffusion; @lou2023discrete] have opened up new possibilities for text generation. A unifying insight across these models is the potential of diffusion language models (DLMs) for controllable [@venkatraman2024amortizing], any-order, and parallel text generation [@gong-etal-2023-diffuseq]. Notably, DLMs exhibit promising capabilities in intermediate token correction [@ye2024diffusion] and global planning [@zhang2023planner], thereby addressing key limitations inherent in the AR approach.\n\nDespite the promising potential of text diffusion models, the relatively small model size limits the competitiveness of DLMs compared to AR models. Existing state-of-the-art DLMs such as Plaid 1B [@gulrajani2023likelihoodbased] and SEDD [@lou2023discrete] are relatively small in size (127M-1B parameters) and under-trained, with less than 400B tokens of training data. This substantial gap in scale prevents fair comparisons with larger AR language models on many advanced capabilities and tasks, such as chain-of-thought reasoning abilities on complex mathematical benchmarks. Recent approaches [@ye2023diffusion] attempt adapt LLaMA models to DLMs based on masked language modeling [@he-etal-2023-diffusionbert]. However, they find that the base model capabilities are lost during their adaptation stage. Pre-training at such a scale is extremely resource-intensive, and the challenge is even more pronounced for diffusion models. These models lack the computational optimizations that have been developed for LLMs [@samragh2024scaling] and require significantly more resources than their AR counterparts, as noted by @gulrajani2023likelihoodbased.\n\nGiven these scaling challenges, pre-trained LLMs emerge as an invaluable resource that we can leverage, considering the extensive computational efforts already invested in their development. This strategy aligns with recent trends where new models are scaled up or adapted to new architectures using existing LLMs [@wang2024mamba; @zhang2024gated]. However, building DLMs through adaptation from AR models is non-trivial due to fundamental differences in their language modeling objectives. Two key distinctions present significant hurdles. First, AR models employ causal masking to prevent future information leakage, whereas diffusion models utilize bi-directional attention masks. Second, an AR LM processes clean inputs to predict subsequent tokens at each step, while a diffusion model operates on noisy inputs to predict their denoised versions.\n\nTo overcome these challenges, we propose a simple adaptation approach that bridges these discrepancies. We unify their modeling objectives (§[3.2](#sec:uni){reference-type=\"ref\" reference=\"sec:uni\"}) and address the architectural differences by breaking the causal masking bias in AR models through attention mask annealing (§[3.3.0.1](#sec:mask-anneal){reference-type=\"ref\" reference=\"sec:mask-anneal\"}). Additionally, we inherit the shift operation from AR models (§[3.3.0.2](#sec:shift-ope){reference-type=\"ref\" reference=\"sec:shift-ope\"}). This streamlined adaptation recipe enables us to construct a pre-trained DLM that can effectively compete in the arena of LLMs. Building on this approach, we leverage the FineWeb [@penedo2024finewebdatasetsdecantingweb] and SlimPajama [@cerebras2023slimpajama] pre-training corpora to continue training small and medium-sized DLMs based on GPT2 [@Brown2020LanguageMA], and further train up to a 7B model based on LLaMA2 [@touvron2023llama2].\n\nOur experiments provide a comprehensive comparison between AR LMs and DLMs across language modeling, reasoning, and infilling tasks. The evaluation encompasses diverse settings, including zero-shot, few-shot, and fine-tuning scenarios, addressing the limitations of relying solely on perplexity in previous works [@shi2024simplified]. Our contributions and empirical findings include:\n\n-   We demonstrate that by narrowing the gap between AR models and DLMs, it is possible to convert 127M-7B AR models (GPT2 and LLaMA2) into DiffuGPT and DiffuLLaMA with training on less than 200B tokens. Notably, DiffuGPT outperforms GPT2 in most tasks.\n\n-   We adapt 7B AR models to DLMs, greatly expanding the expertise compared to smaller-sized diffusion models. DiffuLLaMA emerges as the state-of-the-art DLM, exhibiting in-context learning, code generation, and strong infilling capabilities. Its generation speed is competitive with AR counterparts for unconditionally generating $1024$ tokens using $256$ diffusion timesteps.\n\n-   We provide a comprehensive benchmark for DLMs and release our adapted diffusion models (127M, 355M and 7B) along with open-source adaptation code, efficient fine-tuning scripts, and evaluation toolkits.\n\n# Preliminary and Notation {#sec:prel}\n\nDiffusion models [@sohl2015deep; @song2019generative; @ho2020denoising; @song2021scorebased] are latent variable generative models characterized by a forward and a reverse Markov process. We denote $\\mathbf{x}_0\\sim p_{data}(\\mathbf{x}_0)$ as the variable following the data distribution, and $\\mathbf{x}_t\\sim q(\\mathbf{x}_t)$ as the noisy variable of $\\mathbf{x}_0$ at time $t$, where the maximum time is $T$. The forward process $q(\\mathbf{x}_{1:T}|\\mathbf{x}_0)=\\prod_{t=1}^Tq(\\mathbf{x}_t|\\mathbf{x}_{t-1})$ corrupts the initial data $\\mathbf{x}_0$ into a sequence of increasingly noisy variables $\\mathbf{x}_{1:T}$. Accordingly, the backward Markov process models the joint probability as $p_{\\theta}(\\mathbf{x}_{0:T})=p_{\\theta}(\\mathbf{x}_T)\\prod_{t=1}^Tp_{\\theta}(\\mathbf{x}_{t-1}|\\mathbf{x}_t)$, which gradually denoises $\\mathbf{x}_t$ to reconstruct the original data $\\mathbf{x}_0$. Parameters $\\theta$ are learned by minimizing the negative log-likelihood of $\\mathbf{x}_0$, which can be optimized through the evidence lower bound (ELBO), $$\\label{eq:ori-loss}\n    -\\log p_{\\theta}(\\mathbf{x}_0) \\leq \\mathbb{E}_{q(\\mathbf{x}_1|\\mathbf{x}_0)}[-\\log p_{\\theta}(\\mathbf{x}_0|\\mathbf{x}_1)] + D_{\\mathrm{KL}}(q(\\mathbf{x}_T|\\mathbf{x}_0)||p_{\\theta}(\\mathbf{x}_T))+\\mathcal{L}_T,$$ with $\\mathcal{L}_T=\\sum_{t=2}^T\\mathbb{E}_{q(\\mathbf{x}_t|\\mathbf{x}_0)}[D_{\\mathrm{KL}}(q(\\mathbf{x}_{t-1}|\\mathbf{x}_t, \\mathbf{x}_0)||p_{\\theta}(\\mathbf{x}_{t-1}|\\mathbf{x}_t))]$. For continuous text diffusion [@li2022diffusion; @gong2022diffuseq], at each forward step, perturbations are applied according to $q(\\mathbf{x}_{t} \\vert \\mathbf{x}_{t-1}) = \\mathcal{N}(\\mathbf{x}_{t};\\sqrt{1-\\beta_t}\\mathbf{x}_{t-1}, {\\beta}_t \\mathbf{I})$, where $\\beta_t \\in (0,1)$ represents different scales across time steps such that $\\mathbf{x}_T \\sim \\mathcal{N}(0, \\mathbf{I})$. In the case of discrete denoising models [@ho2020denoising; @austin2021structured; @Zheng2023ARD], the forward process is defined as a categorical distribution $q(\\mathbf{x}_{t} \\vert \\mathbf{x}_{t-1})=\\text{Cat}(\\bm{x}_t;\\bm{Q}_t^\\top\\bm{x}_{t-1})$, where each $\\mathbf{x}_t \\in \\{0, 1\\}^K$ is a one-hot vector with vocabulary size $K$, $\\bm{Q}_t\\in[0,1]^{K\\times K}$ is the transition matrix, and each entry $[\\bm{Q}_t]_{ij}$ denotes the probability of transition from the state $i$ to $j$. We build on the formulation of *absorbing discrete diffusion* [@austin2021structured], which specifies $\\bm{Q}_t=(1-\\beta_t)I+\\beta_t\\mathbf{1}\\bm{m}^\\top$. We denote $\\mathbf{1}$ as an all-one vector of size $K$ and $\\bm{m}$ as the one-hot encoding of a special `[MASK]` token in the vocabulary. Therefore, the transition matrix, $\\bm{Q}_t$ indicates that with probability $1 - \\beta_t$, $\\bm{x}_t$ remains unchanged; otherwise, it transitions to $\\bm{m}$, becoming absorbed into `[MASK]`. Letting $\\bm{\\overline{Q}}_t \\coloneqq \\prod_{i=1}^t\\bm{Q}_i=\\alpha_tI+(1-\\alpha_t)\\mathbf{1}\\bm{m}^\\top$ and $\\alpha_t \\coloneqq \\prod_{i=1}^t(1-\\beta_t)$, the distribution of $\\bm{x}_t$ conditional on $\\bm{x}_0$ is given by $$\\label{eq:qxt}\n    q(\\bm{x}_t|\\bm{x}_0)=\\text{Cat}(\\bm{x}_t;\\bm{\\overline{Q}}_t^\\top\\bm{x}_0) = \\alpha_t I\\bm{x}_0 + (1-\\alpha_t)\\bm{m}\\mathbf{1}^\\top\\bm{x}_0 = \\alpha_t \\bm{x}_0 + (1-\\alpha_t)\\bm{m},$$ since $\\bm{x}_0$ is a one-hot vector and thus $\\mathbf{1}^\\top\\bm{x}_0 = 1$. We expect $\\alpha_T$ to approach $0$ such that the full noise data $\\bm{x}_T$ equals $\\bm{m}$ with probability $1$.\n\nThe discrete time representation of $t \\in [0, T]$, restricts $\\bm{x}_t$ to fixed noise ratios. To avoid this bias and enable sampling from any noisy representation, we use continuous-time sampling, allowing $t$ to span any point within $[0, 1]$  [@kingma2021variational; @shi2024simplified; @zhao2024improving; @ou2024your]. Continuous-time sampling is equivalent to dividing $[0, 1]$ into $T$ intervals and where $T \\rightarrow \\infty$. For any $0 \\leq s < t \\leq 1$, the forward process generalizes to $q(\\mathbf{x}_{t} \\vert \\mathbf{x}_{s})$. We will use this continuous-time notation in the following sections.\n\n# Model {#sec:model}\n\n![The overview of our approach to adapt autoregressive (AR) models to diffusion models. **Left**: The shift operation in AR models enables the output layer $h_i$ to approximate the distribution of next tokens $x_{i+1}$ in hidden representations through the cross entropy (CE) loss. **Middle**: We remove the causal mask gradually during training eventually making our model bi-directional. **Right**: inside the diffusion models we shift the logits to compute the loss with the next token (i.e., the loss on $h_i$ would be with respect to $x_{i+1}$), while perceptually, the diffusion models are still functioning as recovering the original signals (since $h_i$ corresponds to $x_{i+1}$ in AR loss).](fig/pipeline_v3.pdf){#fig:pipeline width=\"97%\"}\n\nWe begin by formulating the continuous-time discrete diffusion process (§[3.1](#sec:continuous){reference-type=\"ref\" reference=\"sec:continuous\"}) and establishing a connection between the discrete diffusion and autoregressive objectives (§[3.2](#sec:uni){reference-type=\"ref\" reference=\"sec:uni\"}). Based on this equivalence, we propose an adaptation approach (§[\\[sec:ada\\]](#sec:ada){reference-type=\"ref\" reference=\"sec:ada\"}) and a sampling algorithm (§[3.4](#sec:sampling){reference-type=\"ref\" reference=\"sec:sampling\"}) for diffusion models adapted from AR models. The whole process is illustrated in Figure [1](#fig:pipeline){reference-type=\"ref\" reference=\"fig:pipeline\"}.\n\n## Continuous-time Discrete Diffusion Processes {#sec:continuous}\n\nFollowing Eq.[\\[eq:qxt\\]](#eq:qxt){reference-type=\"ref\" reference=\"eq:qxt\"} and $q(\\bm{x}_t|\\bm{x}_0)=\\sum_{\\bm{x}_s}q(\\bm{x}_t|\\bm{x}_s)q(\\bm{x}_s|\\bm{x}_0)$, the forward transition distribution between arbitrary points $s < t$ can be derived as $$q(\\bm{x}_t|\\bm{x}_s)=\\text{Cat}(\\bm{x}_t;\\bm{\\overline{Q}}_{s|t}^\\top \\bm{x}_s) =\\frac{\\alpha_t}{\\alpha_s}\\bm{x}_s+(1-\\frac{\\alpha_t}{\\alpha_s})\\bm{m},$$ with $\\bm{\\overline{Q}}_{s|t} \\coloneqq \\bm{\\overline{Q}}_s^{-1}\\bm{\\overline{Q}}_t=\\frac{\\alpha_t}{\\alpha_s}I+(1-\\frac{\\alpha_t}{\\alpha_s})\\mathbf{1}\\bm{m}^\\top$. The corresponding backward transition distribution conditional on $\\bm{x}_0$ is also available in closed form, $$\\label{eq:qxs}\n    q(\\bm{x}_s|\\bm{x}_t, \\bm{x}_0) = \\frac{q(\\bm{x}_t|\\bm{x}_s)q(\\bm{x}_s|\\bm{x}_0)}{q(\\bm{x}_t|\\bm{x}_0)}=\n    \\begin{cases}\n        \\frac{\\alpha_s-\\alpha_t}{1-\\alpha_t}\\bm{x}_0+\\frac{1-\\alpha_s}{1-\\alpha_t}\\bm{m} & \\text{if } \\bm{x}_t=\\bm{m}, \\\\\n        \\bm{x}_0 & \\text{if }\\bm{x}_t\\neq\\bm{m}. \\\\\n    \\end{cases}$$ In discrete diffusion processes, we aim to approximate the backward transition distribution $q(\\bm{x}_s|\\bm{x}_t,\\bm{x}_0)$ using a denoising model $p_{\\theta}(\\bm{x_s}|\\bm{x}_t, f_{\\theta}(\\bm{x}_t))$, where $f_{\\theta}(\\bm{x}_t)$, an approximation of $\\bm{x}_0$, is usually the output of neural networks such as a transformer [@NIPS2017_3f5ee243]. We can define the denoising model to have a similar form of backward transitions as $p_{\\theta}(\\bm{x_s}|\\bm{x}_t)=\\frac{\\alpha_s-\\alpha_t}{1-\\alpha_t}f_{\\theta}(\\bm{x}_t)+\\frac{1-\\alpha_s}{1-\\alpha_t}\\bm{m}$. According to the training objective in Eq.[\\[eq:ori-loss\\]](#eq:ori-loss){reference-type=\"ref\" reference=\"eq:ori-loss\"}, the KL-divergence of $\\mathcal{L}_T$ at each step $t$ can be simplified to a reweighted cross-entropy function, $$D_{\\mathrm{KL}}(q(\\bm{x}_s|\\bm{x}_t,\\bm{x}_0)||p_{\\theta}(\\bm{x}_s||\\bm{x}_t))=-\\frac{\\alpha_s-\\alpha_t}{1-\\alpha_t}\\delta_{\\bm{x}_t,\\bm{m}}\\bm{x}_0^\\top\\log f_{\\theta}(\\bm{x}_t),$$ where $\\delta_{a,b}$ is the indicator function for $a=b$. If we take the limit and let $T\\rightarrow \\infty$, the first two terms of Eq.[\\[eq:ori-loss\\]](#eq:ori-loss){reference-type=\"ref\" reference=\"eq:ori-loss\"} will approach 0 and some constant, respectively. Thus the evidence lower bound (ELBO) effectively becomes $\\mathcal{L}_T$ and $$\\label{eq:L_T}\n    \\lim_{T\\rightarrow \\infty}\\mathcal{L}_T = \\int_{0}^{1}\\frac{\\alpha_t^\\prime}{1-\\alpha_t}\\mathbb{E}_{q(\\mathbf{x}_t|\\mathbf{x}_0)}[\\delta_{\\bm{x}_t,\\bm{m}}\\bm{x}_0^\\top\\log f_{\\theta}(\\bm{x}_t)]\\,dt.$$ The full derivation is listed in Appendix [7.2](#appendix:loss){reference-type=\"ref\" reference=\"appendix:loss\"}. The same form of ELBO which is invariant to noise schedule but related to the signal-to-noise ratio (SNR) is also introduced in @kingma2021variational [@shi2024simplified]. Following @austin2021structured, we choose the noise schedule $\\alpha_t=1-t$, then $\\frac{-\\alpha_t^\\prime}{1-\\alpha_t}=\\frac{1}{t}$. The previous discussion focused on the single token $\\bm{x}_t$, and can be applied independently to a text sequence of $N$ tokens $\\mathbf{x}_t=[\\bm{x}_t^{1}, \\bm{x}_t^{2}\\dots, \\bm{x}_t^{N}]$. During training, we do not compute integral loss in Eq.[\\[eq:L_T\\]](#eq:L_T){reference-type=\"ref\" reference=\"eq:L_T\"} for efficiency consideration; instead, we sample $t$ for each data point. The final loss at $t$ is $$\\label{eq:dm-loss}\n    \\mathcal{L}_{t}^{1:N} = \\frac{1}{t}\\mathbb{E}_{q(\\mathbf{x}_t|\\mathbf{x}_0)}\\left[-\\sum_{n=1}^N\\delta_{\\mathbf{x}_t^n,\\bm{m}}(\\mathbf{x}_0^{n})^\\top\\log f_{\\theta}(\\mathbf{x}_t^{1:N})_n\\right],$$ where $f_{\\theta}(\\mathbf{x}_t^{1:N})_n$ denotes the whole input sequence is fed into the transformer model and the $n$-th output token is indexed.\n\n## Unifying Language Modeling Objectives {#sec:uni}\n\nThe training objective of autoregressive (AR) language models is the negative log-likelihood of each ground-truth token provided the preceding tokens, $$\\label{eq:ce}\n    \\mathcal{L}_{AR}^{1:N} = -\\sum_{n=1}^N (\\mathbf{x}_0^{n})^\\top \\log f_{\\theta}(\\mathbf{x}_0^{1:n-1})_{n-1}.$$ Comparing Eq.[\\[eq:ce\\]](#eq:ce){reference-type=\"ref\" reference=\"eq:ce\"} against Eq.[\\[eq:dm-loss\\]](#eq:dm-loss){reference-type=\"ref\" reference=\"eq:dm-loss\"}, we note that while both take the form of cross-entropy functions, Eq.[\\[eq:dm-loss\\]](#eq:dm-loss){reference-type=\"ref\" reference=\"eq:dm-loss\"} includes an additional reweighting term $\\frac{1}{t}$ and an indicator function $\\delta_{\\mathbf{x}_t^n,\\bm{m}}$. They result from the definition of discrete diffusion processes (§[3.1](#sec:continuous){reference-type=\"ref\" reference=\"sec:continuous\"}). The reweighting emphasizes smaller $t$ where $\\mathbf{x}_t$ contains fewer masked tokens, and this can be regarded as the importance sampling [@nichol2021improved]. The indicator specifies which tokens are masked for prediction. The AR training objective Eq.[\\[eq:ce\\]](#eq:ce){reference-type=\"ref\" reference=\"eq:ce\"}, on the other hand, constrains the context to be unidirectional via attention masking and shifts the targets so that each token predicts the next token instead of itself. These discrepancies form the basis of our adaptation framework, which is detailed in §[\\[sec:ada\\]](#sec:ada){reference-type=\"ref\" reference=\"sec:ada\"}.\n\nIn fact, an alternative way to understand AR modeling, through the lens of diffusion models, is to consider a diffusion process where the forward pass deterministically masks right-to-left and token-by-token [@austin2021structured; @hoogeboom2022autoregressive]. This yields a backward process generating one token at a time from left to right, running with $T=N$ denoising steps in total. As discussed in @austin2021structured, the loss objective of this diffusion process is equivalent to standard cross-entropy (Eq.[\\[eq:ce\\]](#eq:ce){reference-type=\"ref\" reference=\"eq:ce\"}) commonly used to train AR language models. This crafted diffusion process for AR models represents a special case of discrete diffusion (§[3.1](#sec:continuous){reference-type=\"ref\" reference=\"sec:continuous\"}), yet it is limited to unidirectional context and sequential token generation. In contrast, general discrete diffusion processes can leverage bidirectional context and support parallel generation in arbitrary orders.\n\n## Adaptation\n\nBuilding on the connection between AR modeling and discrete diffusion processes, we construct an adaptation recipe next. Figure [1](#fig:pipeline){reference-type=\"ref\" reference=\"fig:pipeline\"} shows an overview of our adaptation approach. We use attention mask annealing, shift operations, and a time-embedding free architecture to narrow the differences between AR and DLMs.\n\n[]{#sec:ada label=\"sec:ada\"}\n\n<figure id=\"algo:line-sample-shift\">\n<div class=\"minipage\">\n<div class=\"algorithm\">\n<div class=\"algorithmic\">\n<p><strong>Input:</strong> network <span class=\"math inline\"><em>f</em><sub><em>θ</em></sub></span> initialized by existing models, training corpus <span class=\"math inline\"><em>p</em><sub><em>d</em><em>a</em><em>t</em><em>a</em></sub>(<strong>x</strong><sub>0</sub><sup>1 : <em>N</em></sup>)</span>, mask token <span class=\"math inline\"><strong>m</strong></span>. <strong>Output:</strong> model parameters <span class=\"math inline\"><em>θ</em></span>. Draw <span class=\"math inline\"><strong>x</strong><sub>0</sub><sup>1 : <em>N</em></sup> ∼ <em>p</em><sub><em>d</em><em>a</em><em>t</em><em>a</em></sub></span> and set <span class=\"math inline\"><em>labels</em> ← <strong>x</strong><sub>0</sub><sup>1 : <em>N</em></sup></span> Sample <span class=\"math inline\"><em>t</em> ∈ <em>Uniform</em>(0, 1)</span> Sample <span class=\"math inline\"><strong>x</strong><sub><em>t</em></sub><sup>1 : <em>N</em></sup> ∼ <em>q</em>(<strong>x</strong><sub><em>t</em></sub>|<strong>x</strong><sub>0</sub>)</span> Anneal the attention mask <span class=\"math inline\"><em>a</em><em>t</em><em>t</em><em>n</em>_<em>m</em><em>a</em><em>s</em><em>k</em></span> Forward <span class=\"math inline\"><em>logits</em> ← <em>f</em><sub><em>θ</em></sub>(<strong>x</strong><sub><em>t</em></sub><sup>1 : <em>N</em></sup>)</span> with <span class=\"math inline\"><em>a</em><em>t</em><em>t</em><em>n</em>_<em>m</em><em>a</em><em>s</em><em>k</em></span> Right shift <span class=\"math inline\"><em>logits</em></span> by one position <span id=\"algo:line-loss\" data-label=\"algo:line-loss\"></span> <span class=\"math inline\">$\\mathcal{L}_t=\\frac{1}{t}\\delta_{x_t, m}$</span>CE<span class=\"math inline\">(<em>logits</em>, <em>labels</em>)</span> <span class=\"math inline\">⊳</span> Eq.<a href=\"#eq:dm-loss\" data-reference-type=\"ref\" data-reference=\"eq:dm-loss\">[eq:dm-loss]</a> Backprop with <span class=\"math inline\">ℒ<sub><em>t</em></sub></span> and update <span class=\"math inline\"><em>θ</em></span></p>\n</div>\n</div>\n</div>\n<div class=\"minipage\">\n<div class=\"algorithm\">\n<div class=\"algorithmic\">\n<p><strong>Input:</strong> Trained diffusion model <span class=\"math inline\"><em>f</em><sub><em>θ</em></sub></span>, sampling algorithm <span class=\"math inline\"><em>τ</em></span>, mask token <span class=\"math inline\"><strong>m</strong></span>, start token <span class=\"math inline\"><strong>s</strong></span>. <strong>Output:</strong> generated sample <span class=\"math inline\"><strong>x</strong><sub>0</sub></span>. <strong>Initialize</strong> <span class=\"math inline\"><strong>x</strong><sub><em>T</em></sub><sup>1 : <em>N</em></sup> = <strong>m</strong></span>. Forward <span class=\"math inline\"><em>logits</em> ← <em>f</em><sub><em>θ</em></sub>(<strong>x</strong><sub><em>t</em></sub><sup>1 : <em>N</em></sup>)</span> Sample <span class=\"math inline\">$\\tilde{\\bm{x}}_0^{1:N}\\sim \\textit{Categorical}(\\tau(\\textit{logits}))$</span> <span class=\"math inline\">$\\bm{x}_{t-1}^n=q(\\bm{x}_{t-1}^n|\\bm{x}_{t}^n,\\tilde{\\bm{x}}_0^n)$</span> <span class=\"math inline\">⊳</span> Eq.<a href=\"#eq:qxs\" data-reference-type=\"ref\" data-reference=\"eq:qxs\">[eq:qxs]</a> Right shift <span class=\"math inline\"><strong>x</strong><sub><em>t</em> − 1</sub><sup>1 : <em>N</em></sup> = [<strong>s</strong>, <strong>x</strong><sub><em>t</em> − 1</sub><sup>1 : <em>N</em> − 1</sup>]</span> <span id=\"algo:line-sample-shift\" data-label=\"algo:line-sample-shift\"></span> <strong>Return</strong> <span class=\"math inline\"><strong>x</strong><sub>0</sub><sup>2 : <em>N</em></sup></span></p>\n</div>\n</div>\n</div>\n<figcaption>Sampling</figcaption>\n</figure>\n\n#### Attention Mask Annealing {#sec:mask-anneal}\n\nThe prediction of the $n$-th token, given all preceding tokens, $f_{\\theta}(\\mathbf{x}_0^{1:n-1})$, is usually implemented by causal attention masking in transformer-based AR language models. As shown in Figure [1](#fig:pipeline){reference-type=\"ref\" reference=\"fig:pipeline\"}, causal attention masks set all entries in the upper triangle of the self-attention matrices to zero, so each token cannot attend to its respective future tokens. Such causal masking prevents the model from learning right-to-left dependencies for more general diffusion processes. To address this limitation while preserving left-to-right conditionals during adaptation, we introduce an incremental annealing process from causal masks to full attention matrices. During annealing, the causal mask is not immediately removed; instead, it is retained at a controlled ratio, as shown in the middle part of Figure [1](#fig:pipeline){reference-type=\"ref\" reference=\"fig:pipeline\"}. At each training step, we sample the amount of context from the right side and progressively increase this amount till we obtain the full attention mask.\n\n#### Shift Operation {#sec:shift-ope}\n\nAR models also apply a shifting operation, where the target output is the input sequence shifted left by one position. In other words, the prediction target of the $(n\\!-\\!1)$-th token is the $n$-th token, contrasting with typical diffusion models that try to predict masked tokens at their original positions. When initializing text diffusion models with AR model parameters, the model would tend to output the hidden representations of the shifted input sequence. If we continue to optimize the cross-entropy objective based on the original token positions, the model struggles to adapt due to misalignment between input and output. Instead, we maintain the shift operation (Algo.[\\[alg:training\\]](#alg:training){reference-type=\"ref\" reference=\"alg:training\"}, line [\\[algo:line-loss\\]](#algo:line-loss){reference-type=\"ref\" reference=\"algo:line-loss\"}), treating the output logits at each position as corresponding to the next token. When calculating the objective, we align prediction targets so that the diffusion model learns to recover the original signals. This process is illustrated in the right panel of Figure [1](#fig:pipeline){reference-type=\"ref\" reference=\"fig:pipeline\"}.\n\n#### Time-Embedding-Free Architecture\n\nMany diffusion models for text generation [@li2022diffusion; @dieleman2022continuous; @gulrajani2023likelihoodbased; @lou2023discrete; @shi2024simplified] incorporate time embedding layers to represent the information of current timesteps $t$, which can explicitly indicate the noise scale of the input noisy data. While inferring these timesteps can be challenging for image diffusion models [@ho2020denoising; @li2024autoregressive], some discrete text diffusion models [@he-etal-2023-diffusionbert] assert that timesteps $t$ can be easily learned implicitly based on the number of mask tokens. Since AR models are not equipped with time embedding layers, we also choose not to use the time embedding, resulting in no additional parameters compared to previous diffusion models.\n\n## Sampling {#sec:sampling}\n\nFollowing @shi2024simplified, we initialize $\\bm{x}_T$ with all `[MASK]` tokens and then sample tokens according to the time reversal $q(\\bm{x}_s|\\bm{x}_t, \\bm{x}_0)$ in Eq.[\\[eq:qxs\\]](#eq:qxs){reference-type=\"ref\" reference=\"eq:qxs\"}. At each timestep, if $\\bm{x}_t$ is a mask, it will jump to the predicted $\\bm{x}_0$ at time $s$ with probability $\\frac{\\alpha_s-\\alpha_t}{1-\\alpha_t}$. After $T$ iterations, the model generates the full sequence. Since our adapted models are trained with the shift operation, at each sampling iteration, we shift back the generated sentence and prepend a start token before the next forward pass (Algo.[\\[alg:sampling\\]](#alg:sampling){reference-type=\"ref\" reference=\"alg:sampling\"}, line [2](#algo:line-sample-shift){reference-type=\"ref\" reference=\"algo:line-sample-shift\"}). Usually larger $T$ requires more interactions of computation, and can yield texts in higher quality, and this trade-off can be controlled easily through $T$. Through experiments, we find that the output generated by diffusion models is diverse and scattered. Therefore, for conditional generation tasks, we improve the sampling procedure to ensure that only tokens with high probabilities from neural networks are denoised [@ghazvininejad-etal-2019-mask; @maskgit; @Zheng2023ARD], so that the model could predict tokens mostly relevant to the input. In addition, existing sampling techniques for AR language models, including top-$k$ and nucleus sampling [@holtzmancurious], can be seamlessly applied to diffusion models as well.\n\n# Experiment\n\n## Adaptation setup\n\n**DiffuGPT** We use the 30 billion tokens[^1] random split from the FineWeb dataset [@penedo2024finewebdatasetsdecantingweb], an improved corpus than OpenWebText [@Gokaslan2019OpenWeb] used in prior DLMs [@lou2023discrete], to continue training GPT2 base [@Radford2019LanguageMA]. We use sequence packing, logits shifting, and $10$K-step attention mask annealing to transform GPT2 to DiffuGPT.\n\n::: wrapfigure\nr0.32 ![image](fig/train-loss.pdf){width=\"32%\"}\n:::\n\n**DiffuLLaMA** We continue pre-training [llama-2-7-hf]{.smallcaps} [@touvron2023llama] on a mixture of SlimPajama (70%) [@cerebras2023slimpajama] and Starcoder (30%) [@li2023starcoder] data following TinyLLaMA [@zhang2024tinyllama]. We randomly sample 65 billion tokens from this mixture and use sequence packing with context length of $2048$. For efficient implementation we enable flash-attention 2 [@dao2023flashattention2] and directly use bi-directional attention without attention mask annealing.\n\nFor both adaptation settings, we employ full parameter finetuning with `bf16`. Please refer to Appendix [\\[appendix:train\\]](#appendix:train){reference-type=\"ref\" reference=\"appendix:train\"} for details. We plot the training loss curve in Figure [\\[fig:train-loss\\]](#fig:train-loss){reference-type=\"ref\" reference=\"fig:train-loss\"}. We train DiffuLLaMA on 60B tokens and achieve a lower loss compared to 127M and 335M models, suggesting a scaling trend similar to that of AR LLMs [@kaplan2020scaling]. We also note that there is still scope for training more, since the model does not show signs of saturation.\n\n## Evaluation setup\n\nPreviously developed diffusion language models [@gulrajani2023likelihoodbased; @lou2023discrete; @shi2024simplified; @ou2024your] evaluate model performance using zero-shot perplexity on benchmark datasets. However, this metric alone does not fully capture a model's capabilities for several reasons. First, lower perplexity does not always correlate with human-like content, even in autoregressive models [@kuribayashi-etal-2021-lower]. Additionally, the loss from text diffusion models only indicates an upper bound on negative log-likelihood. While @kingma2021variational [@shi2024simplified] demonstrate that the ELBO is invariant to the noise scheduler, discrepancies between continuous diffusion, discrete diffusion, and autoregressive loss still hinder fair comparisons across different model types. Given the ample evaluation benchmarks [@gu2024olmes] for LLMs, we propose a more comprehensive evaluation for diffusion models.\n\n[]{#tab:eval label=\"tab:eval\"}\n\n**Tasks and Metrics** We consider TriviaQA [@JoshiTriviaQA2017] to test the reading comprehension of models and last word completion task Lambada [@paperno-etal-2016-lambada] to test how models capture long-range dependencies in text. These two tasks are measured by exact match accuracy. We also test for common sense reasoning tasks HellaSwag [@zellers2019hellaswag], Winogrande [@WinoGrande2021], SIQA [@sap-etal-2019-social] and PIQA [@Bisk2020], all of which involve multiple-choice questions assessed by accuracy. On grade school math problems GSM8K [@Cobbe2021TrainingVT], we follow @ye2024diffusion in finetuning setting using the augmented symbolic data to test the CoT [@NEURIPS2022_9d560961] math reasoning abilities of diffusion models. Following @shen2023film, we also test the story infilling tasks using ROCStories [@mostafazadeh-etal-2016-corpus] and evaluate using ROUGE score [@lin2004rouge]. To test the code infilling, we adopt Humaneval [@bavarian2022efficient] single line infilling task, which is evaluated by pass@1 rate. We evaluate DiffuLLaMA's math reasoning and in-context learning ability by evaluating on MAWPS [@koncel-kedziorski-etal-2016-mawps] consisting of math word problems and SATMATH from AGI-eval consisting of math problems from SAT exam [@zhong-etal-2024-agieval]. We base our implementation on `lm-evaluation-harness` [@eval-harness] and re-implement all tasks across models to ensure a fair comparison.\n\n**Implementation Details** For pre-trained diffusion language models, we mainly use continuous diffusion (CD) model Plaid 1B [@gulrajani2023likelihoodbased], discrete diffusion (DD) model SEDD [@lou2023discrete] with different sizes as baselines. MD4 [@shi2024simplified] and RADD [@ou2024your] are based on and compared with SEDD, so we mainly compare SEDD. For autoregressive (AR) baselines, we consider the base models from which our models adapt. We implement infilling tasks for AR models by feeding the prefix and cutting off the generation length using the oracle length, considering that these AR models are not supporting infilling. For the sentence completion task, $T$ is the exact number of ground truth tokens for DD and $32$ for CD. For 4 multi-choices tasks from commonsense reasoning, we compute the loss (Eq.[\\[eq:L_T\\]](#eq:L_T){reference-type=\"ref\" reference=\"eq:L_T\"}) of each choice (averaged by token) and choose the one with lowest loss (perplexity). For GSM8K finetuning, we use parameter-efficient LoRA tuning [@hu2022lora] for DiffuLLaMA. The decoding $T$ are set to $32$ by default. The detailed settings are in Appendix [8.3](#appendix:imple-eval){reference-type=\"ref\" reference=\"appendix:imple-eval\"}.\n\n## Language modeling capacities\n\n#### Benchmark performance\n\nAccording to Table [\\[tab:eval\\]](#tab:eval){reference-type=\"ref\" reference=\"tab:eval\"}, the results on diverse tasks demonstrate that our adapted diffusion models achieve the state-of-the-art results among all existing diffusion language models (DLMs). We observe that diffusion models with larger parameters show improved performance, likely due to better base AR models. DiffuLLaMA's performance still falls short of the LLaMA2 model. This drop in performance is likely because DiffuLLaMa is trained on a small subset of SlimPajama and Starcoder data. We believe more training tokens can help improve these numbers. TriviaQA and PIQA are significant challenging for DLMs, probably because they require specific physical knowledge, such as *the capital of a city* or *the boiling point of water*; while our models are trained on 30B-70B tokens, which may be insufficient to preserve the general knowledge in the original LMs [@ke2023continual].\n\nIn tasks that require more extensive global reasoning, such as complex mathematics and coding, DLMs consistently exhibit better performance compared to AR models that rely solely on left-to-right modeling capabilities. Remarkably, DLMs demonstrate their strengths in infilling tasks. Regular LLMs like LLaMA2 are not trained for filling-in-the-middle (FIM) tasks like those in  @roziere2023code, making them incapable of handling infilling. Considering this, we do not provide the suffix information to the model, which might result in an unfair comparison. But the FIM requires re-arranging the order of pre-training/inference sequence with special tokens [@zheng2024selfinfilling], while diffusion training naturally supports this in its objective modeling.\n\nTasks in Table [\\[tab:eval\\]](#tab:eval){reference-type=\"ref\" reference=\"tab:eval\"} mainly measure conditional modeling abilities, where Plaid 1B performs unsatisfactorily for conditional generation tasks even though with 1B parameters. We attribute this result to the gap between the continuous diffusion modeling and discrete text representation; in contrast, discrete diffusion models align more closely with AR modeling, naturally supporting conditional generation. Despite this, as illustrated in Figure [\\[fig:ungen-ppl\\]](#fig:ungen-ppl){reference-type=\"ref\" reference=\"fig:ungen-ppl\"}, Plaid 1B demonstrates its strength in unconditional generation, highlighting its language modeling capabilities as a generative model. These findings reveal that the previous evaluation based on the perplexity of test data is too general to accurately assess the model's true capabilities, while our evaluation offers a more nuanced benchmark.\n\n#### Unconditional Generation\n\n::: wrapfigure\nr0.4 ![image](fig/ungen-ppl.pdf){width=\"40%\"}\n:::\n\nWe evaluate the quality of text unconditionally generated by DLMs in Figure [\\[fig:ungen-ppl\\]](#fig:ungen-ppl){reference-type=\"ref\" reference=\"fig:ungen-ppl\"}. The perplexity is measured using GPT2 large, consistent with the prior work [@lou2023discrete], where the data of MD4 [@shi2024simplified] is sourced from its original paper. To make sure low perplexity is not brought by repeated content, we assess the distinct 2-gram diversity of the generated text. Our model achieves low perplexity while maintaining a high level of diversity, validating that the DiffuGPT series excels in fluent text generation. As the number of decoding steps increases, thereby extending the test computing time, the fluency of unconditional generation improves. Similarly, increasing model size also contribute to better performance. An increase in generation perplexity is often associated with a slight decrease in diversity, which is a common phenomenon. Notably, DiffuGPT outperforms both SEDD and MD4 models, particularly at lower step counts (e.g., 64 steps), while as the continuous diffusion models, Plaid 1B needs more decoding steps to generate more fluent texts. DiffuGPT thus exhibits a significant advantage on less sampling time. We outline the decoding hyperparameters and show the diversity changes across different settings in Appendix [9.1](#appendix:un-gen){reference-type=\"ref\" reference=\"appendix:un-gen\"}, which also includes generation cases.\n\n## Analysis on DiffuLLaMA\n\n::: wraptable\nr0.5\n\n[]{#tab:diffullama label=\"tab:diffullama\"}\n:::\n\nWe validate that increasing the size of adapted DLMs significantly enhances the performance of downstream tasks in Table [\\[tab:eval\\]](#tab:eval){reference-type=\"ref\" reference=\"tab:eval\"}. Further, we aim to assess if the 7B model demonstrates in-context learning and reasoning capabilities similar to AR LLMs. Table [\\[tab:diffullama\\]](#tab:diffullama){reference-type=\"ref\" reference=\"tab:diffullama\"} presents the exact match accuracy between gold labels and predictions generated by DiffuLLaMA across zero-shot (ZS), few-shot (FS), and FS with chain-of-thought (CoT) scenarios. Besides, we deploy the self-consistency approach [@wang2023selfconsistency], considering that small DLMs can indeed benefit from this technique [@ye2024diffusion]. We use majority vote to choose the best answer from $3$ individual predictions, and also report the hit rate @$k$ with $k=3$, which measures whether any of the $k$ predictions include the correct answer, serving as a reference for the model's upper bound. For in-context learning (ICL) evaluations, we give $4$-shot on math tasks and $2$-shot on TriviaQA.\n\nThe performance improvement from zero-shot to few-shot settings suggests that DiffuLLaMA can learn from ICL examples, particularly in following to the format of answers as we observe. We hypothesize that the adapted model retains some of the abilities from the base AR model. We randomly select the ICL demonstration here and anticipate that advanced ICL strategies in LLMs [@wu-etal-2023-self] could yield potentially higher results. The self-consistency offers LMs with an effective approach to test-time scaling [@snell2024scaling], and DiffuLLaMA shows that it can also leverage this method. Furthermore, we report the hit rate results in generated candidate answers, highlighting the model's potential to produce the correct answer. This reveals that the current model exhibits high uncertainty about its responses, leading to temporarily suboptimal performance. We also observe that adding step-wise solutions in the in-context example (CoT) leads to a drop in performance, likely due to the absence of instruction tuning, similar to the findings in LLMs [@ouyang2022training]. We will leave instruction tuning as the future work as @ye2023diffusion show that text diffusion model can benefit from instruction tuning. In summary, we show the potential capabilities of DiffuLLaMA, which motivates us to further investigate the scaling of diffusion models.\n\n## Discussions\n\n#### Ablation Test on GSM8K-symbolic\n\n::: wraptable\nr0.4\n\n[]{#tab:gsm label=\"tab:gsm\"}\n:::\n\nDirect ablation on adaptation training is costly; hence, we conduct preliminary experiments to determine the adaptation recipes. Following @ye2024diffusion, we finetune models on the augmented GSM8K symbolic dataset using various base models and training objectives. The models are either trained from scratch (random initialization) or initialized with GPT2-S/M weights. Training objectives includes autoregressive training with a causal mask, continuous diffusion loss (CD), and discrete diffusion loss (DD). As shown in Table [\\[tab:gsm\\]](#tab:gsm){reference-type=\"ref\" reference=\"tab:gsm\"}, different training objectives yield comparable results when training from scratch. However, when using GPT2 as the base model, the CD loss performs worse than both the DD and AR losses. We attribute this to the better alignment of DD and AR losses as discussed in §[3.2](#sec:uni){reference-type=\"ref\" reference=\"sec:uni\"}. Previous continuous diffusion models [@dieleman2022continuous; @gulrajani2023likelihoodbased] has reparameterized the estimation of embeddings into the CE loss. However, adapting diffusion models from an AR model in continuous space necessitates an additional projection from the embedding to a categorical distribution, increasing the difficulty of adaptation.\n\nFor DD loss, removing attention mask annealing and shift operations both degrade performance, indicating the efficacy of our approaches. The mask annealing has minimal impact, so we choose to omit it for 7B adaptation to simplify implementation using flash-attention 2.\n\n::: wrapfigure\nr0.38 ![image](fig/speed.pdf){width=\"38%\"}\n:::\n\nDirect DD loss finetuning on GPT2 achieves accuracy of $45.4$ and $49.7$ for small and medium models, respectively, outperforming GPT2 AR finetuning. However, finetuning from already adapted diffusion language models (DiffuGPT) yields accuracy of $50.2$ and $61.8$ (Table [\\[tab:eval\\]](#tab:eval){reference-type=\"ref\" reference=\"tab:eval\"}). This demonstrates the superiority of DiffuGPT as the current best diffusion base model at this size and highlights that a better base model leads to improved results. Even with the same DD loss, DiffuGPT's finetuning converges faster and achieves lower loss, as shown in Appendix [9.2](#appendix:gsm){reference-type=\"ref\" reference=\"appendix:gsm\"}.\n\n**Inference Speed** AR models usually utilize key-value caching (incremental decoding; @ott-etal-2019-fairseq) to enhance throughput during decoding. However, due to the nature of sequential token generation, they are highly memory-bound and cannot fully exploit modern accelerators [@chen2023accelerating]. In contrast, diffusion models, despite not having a concept of caching and requiring self-attention over the entire sequence at each iteration, can operate with fewer iterations than the sequence length and exhibit less memory-bound behavior. Their performance can be further boosted with hardware-aware optimizations like flash-attention [@dao2022flashattention1; @dao2023flashattention2; @shah2024flashattention3]. In Figure [\\[fig:speed\\]](#fig:speed){reference-type=\"ref\" reference=\"fig:speed\"}, we evaluate the decoding latency with batch size 1 using flash-attention 2 and illustrate that our DiffuLLaMA achieves better inference efficiency using $T=256$ when generating sequences of length $1024$ or longer. This underscores the significant potential of diffusion models for efficient inference. Further decreasing $T$ can lead to faster decoding but may sacrifice quality. Additional latency comparisons are provided in Appendix [9.5](#appendix:speed){reference-type=\"ref\" reference=\"appendix:speed\"}.\n\n# Related Work\n\n**Continue Pre-training** Continue pre-training is commonly used in adapting an existing language model (LM) to a domain-specific LM [@ke2023continual] or enabling new abilities of LM, such as for longer context [@chen2024longlora] or code generation [@xu2024lemur]. Pre-training LMs is non-trivial and expensive [@samragh2024scaling], thus in exploring of new architectures of LMs such as Mamba [@gu2023mamba] and gated attention, @wang2024mamba [@zhang2024gated] choose to transfer from LMs to save the training cost. However, all these continue pre-training works follow the autoregressive (AR) language modeling, while adapting LMs into diffusion language model is more challenging due to discrepancies between their modeling objectives.\n\n**Text Diffusion Models** Diffusion models have demonstrated significant diversity and controllability in image generation [@ho2020denoising; @song2020denoising; @ramesh2022hierarchical]. Building on this success, line of research [@li2022diffusion; @gong2022diffuseq; @gong-etal-2023-diffuseq; @dieleman2022continuous] build continuous diffusion models for text generation tasks. Among them, @genie2023 experiment with a pre-training and finetuning framework under a small scale; @gulrajani2023likelihoodbased highlight the scaling law of continuous diffusion models, revealing that the compute-optimal requires longer training than their AR counterparts. To address the discrete nature of text, @austin2021structured [@hoogeboom2021argmax; @Zheng2023ARD] incorporate an absorbing `[MASK]` state as noise, laying the foundation for discrete diffusion models, which are further developed by @lou2023discrete [@shi2024simplified; @ou2024your; @zhao2024improving]. By connecting text diffusion models with pre-trained masked language models (MLMs; @devlin-etal-2019-bert), @ye2023diffusion [@he-etal-2023-diffusionbert] initialize discrete diffusion models using MLMs. Besides, the unification between diffusion and AR generation is also discussed in image generation [@li2024autoregressive]. However, the adaptation of diffusion models from AR LLMs remains unexplored.\n\n**Non-autoregressive Generation** Non-autoregressive (NAR) models, introduced by @gu2017non, break free from the left-to-right generation constraint, allowing for new capabilities like planning with future tokens [@wu2024do]. Current diffusion language models are a notable part of the NAR family [@gong2022diffuseq]. Given the challenges of developing NAR models, researchers often seek to find a trade-off. For instance, SSD-LM [@han-etal-2023-ssd] leverages diffusion models to iteratively generate text blocks, facilitating a semi-NAR generation process. Similarly, CLLM [@kou2024cllms] enhances LLMs by enabling the parallel generation of $n$ tokens, thereby improving decoding speed. FiLM [@shen2023film] adapts language models to generate tokens in any order, which is particularly useful for infilling tasks. [@guo2020fine] trains a NAR using a curriculum for the attention mask on translation tasks with seq2seq labels. Additionally, @gloeckle2024better focus on training models to achieve better and faster multi-token predictions as they scale up. These NAR approaches provide compelling alternatives to traditional AR LLMs, yet few have thoroughly explored training large NAR models on large-scale unlabeled data.\n\n# Conclusion\n\nBuilding on existing DLMs, we present a recipe for building DLMs by continuing training on off-the-shelf autoregressive LLMs. Our adaptation technique involves using 1) attention mask annealing to enable bidirectional modeling and 2) shift operation to allow similar training dynamics like AR models. By unifying the language modeling objectives of autoregressive and diffusion models, we train diffusion models up to 7B parameters. Through experiments on common sense reasoning, language modeling, math reasoning and code generation, we show that DiffuGPT and DiffuLLaMA have better performance compared to existing DLMs. We find that DiffuLLaMA is capable of following in-context demonstrations to some extent on math problems. In the future, we aim to instruction tune our DLMs and explore inference time planning methods. We release DiffuLLaMA and DiffuGPT for further exploration of diffusion models as an alternative language modeling method.\n\n### Author Contributions {#author-contributions .unnumbered}\n\nShansan Gong: Project lead, methodology development, DiffuGPT training and model evaluation, major writing. Shivam Agarwal: Methodology exploration, discussion, DiffuLLaMA training, writing. Yizhe Zhang: Discussion, DiffuLLaMA training, writing suggestions. Jiacheng Ye: Initial methodology exploration. Lin Zheng: Discussion, writing. Mukai Li & Chenxin An: Discussion, writing suggestions. Others: Mentorship and supervision.\n\n### Acknowledgments {#acknowledgments .unnumbered}\n\nResearch was supported in part by US DARPA INCAS Program No. HR0011-21-C0165 and BRIES Program No. HR0011-24-3-0325, National Science Foundation IIS-19-56151, the Molecule Maker Lab Institute: An AI Research Institutes program supported by NSF under Award No. 2019897, and the Institute for Geospatial Understanding through an Integrative Discovery Environment (I-GUIDE) by NSF under Award No. 2118329. This work used Delta AI at University of Illinois Urbana-Champaign through allocation CIS230229, CIS240488 from the Advanced Cyberinfrastructure Coordination Ecosystem: Services & Support (ACCESS) program, which is supported by U.S. National Science Foundation grants #2138259, #2138286, #2138307, #2137603, and #2138296.\n\nThis research was supported in part by the joint research scheme of the National Natural Science Foundation of China (NSFC) and the Research Grants Council (RGC) under grant number N_HKU714/21.\n\nThis work was also in part supported by research awards from Apple and the Allen Institute for AI.\n\n# Objective Derivations\n\nThis section provides detailed preliminary and loss derivations of §[2](#sec:prel){reference-type=\"ref\" reference=\"sec:prel\"} and §[3.1](#sec:continuous){reference-type=\"ref\" reference=\"sec:continuous\"} in the main paper.\n\n## Background of Diffusion Models\n\nWe denote $\\mathbf{x}_0\\sim p_{data}(\\mathbf{x}_0)$ as the variable following the data distribution, and $\\mathbf{x}_t\\sim q(\\mathbf{x}_t)$ as the noisy variable of $\\mathbf{x}_0$ at time $t$, where the maximum time is $T$. The forward process $$q(\\mathbf{x}_{1:T}|\\mathbf{x}_0)=\\prod_{t=1}^Tq(\\mathbf{x}_t|\\mathbf{x}_{t-1})$$ corrupts the initial data $\\mathbf{x}_0$ into a sequence of increasingly noisy variables $\\mathbf{x}_{1:T}$. Accordingly, the reverse Markov process models the joint probability as $$p_{\\theta}(\\mathbf{x}_{0:T})=p_{\\theta}(\\mathbf{x}_T)\\prod_{t=1}^Tp_{\\theta}(\\mathbf{x}_{t-1}|\\mathbf{x}_t),$$ which gradually denoises $\\mathbf{x}_t$ to reconstruct the original data $\\mathbf{x}_0$. Parameters $\\theta$ are learned by minimizing the negative log-likelihood of $\\mathbf{x}_0$, which can be optimized through the variational lower bound (VLB): $$\\begin{aligned}\n\\label{eq:ori-loss-appen}\n    -\\log p_{\\theta}(\\mathbf{x}_0) & \\leq \\mathbb{E}_{q(\\mathbf{x}_1|\\mathbf{x}_0)}[-\\log p_{\\theta}(\\mathbf{x}_0|\\mathbf{x}_1)] + D_{\\mathrm{KL}}(q(\\mathbf{x}_T|\\mathbf{x}_0)||p_{\\theta}(\\mathbf{x}_T))+\\mathcal{L}_T, \\\\\n    \\label{eq:ori-loss-line2-appen}\n    \\text{with }\n\\mathcal{L}_T & =\\sum_{t=2}^T\\mathbb{E}_{q(\\mathbf{x}_t|\\mathbf{x}_0)}[D_{\\mathrm{KL}}(q(\\mathbf{x}_{t-1}|\\mathbf{x}_t, \\mathbf{x}_0)||p_{\\theta}(\\mathbf{x}_{t-1}|\\mathbf{x}_t))].\n\\end{aligned}$$\n\nFor continuous text diffusion [@li2022diffusion; @gong2022diffuseq], at each forward step, perturbations are applied according to $$q(\\mathbf{x}_{t} \\vert \\mathbf{x}_{t-1}) = \\mathcal{N}(\\mathbf{x}_{t};\\sqrt{1-\\beta_t}\\mathbf{x}_{t-1}, {\\beta}_t \\mathbf{I}),$$ where $\\beta_t \\in (0,1)$ represents different scales. In the end, $\\mathbf{x}_T \\sim \\mathcal{N}(0, \\mathbf{I})$. In the case of discrete denoising models [@ho2020denoising; @austin2021structured; @Zheng2023ARD], $\\mathbf{x}_t$ follows a categorical distribution which naturally aligns with discrete text data. Let $\\bm{x}$ be the one-hot encoded sample of variable $\\mathbf{x}$ and $\\mathbf{x}_t\\sim \\text{Cat}(\\bm{x}_t;\\bm{p})$ represent a categorical distribution over vector $\\bm{x}$ with probabilities given by $\\bm{p}$. Here, $K$ represents the vocabulary size, $\\bm{x}\\in \\{\\bm{e}_1,\\dots,\\bm{e}_K\\}$, and $\\bm{e}_k \\in \\{0, 1\\}^K$ is the one-hot encoding of the $k$-th word category. The forward process can be formulated through a transition matrix $\\bm{Q}_t\\in[0,1]^{K\\times K}$ such that $$q(\\mathbf{x}_{t} \\vert \\mathbf{x}_{t-1})=\\text{Cat}(\\bm{x}_t;\\bm{Q}_t^\\top\\bm{x}_{t-1}); \\bm{Q}_t=(1-\\beta_t)I+\\beta_t\\mathbf{1}\\bm{e}_K^\\top,$$ with $\\mathbf{1}$ as an all-one vector of size $K$ and we assume $\\bm{e}_K$ as the special \\[mask\\] state, also defined as the absorbing state in discrete diffusion $\\bm{m}$. Each entry in $[\\bm{Q}_t]_{ij}$ denotes the probability of transition from the state $\\bm{e}_i$ to $\\bm{e}_j$, and thus the previously defined $\\bm{Q}_t$ means with probability $1-\\beta_t$, $\\bm{x}_t$ will stay unchanged and otherwise it will jump to the mask state $\\bm{e}_K$.\n\nStarting from $\\bm{x}_0$, the $t$-step marginal distribution and the posterior at previous time $t-1$ is respectively $$q(\\bm{x}_t|\\bm{x}_0)=\\text{Cat}(\\bm{x}_t;\\bm{p}=\\bm{\\overline{Q}}_t^\\top\\bm{x}_0); \\; q(\\bm{x}_{t-1}|\\bm{x}_t,\\bm{x}_0)=\n    \\frac{q(\\bm{x}_{t}|\\bm{x}_{t-1},\\bm{x}_0)q(\\bm{x}_{t-1}|\\bm{x}_0)}{q(\\bm{x}_t|\\bm{x}_0)}$$ where cumulative products $\\bm{\\overline{Q}}_t = \\prod_{i=1}^t\\bm{Q}_i=\\alpha_tI+(1-\\alpha_t)\\mathbf{1}\\bm{m}^\\top$, and $\\alpha_t=\\prod_{i=1}^t(1-\\beta_t)$. We expect $\\alpha_T$ approaches $0$ such that the full noise data $\\bm{x}_T$ is equal to $\\bm{e}_K$ with probability $1$. In the following sections, we primarily takes the discrete diffusion formulation.\n\n## Loss Derivation {#appendix:loss}\n\nPrevious discrete time $t\\in[0,T]$ restricts $\\bm{x}_t$ to fixed time points whereas the continuous-time sampling allows for more flexibility covering any point in the range [@kingma2021variational; @shi2024simplified; @zhao2024improving; @ou2024your]. In this case, $t$ runs from $0$ to $1$, corresponding to dividing $[0,1]$ into $T$ intervals and let $T\\rightarrow\\infty$. For any two arbitrary time points, $0\\leq s< t\\leq 1$, the forward modeling can be generalized from $q(\\mathbf{x}_{t} \\vert \\mathbf{x}_{t-1})$ to $q(\\mathbf{x}_{t} \\vert \\mathbf{x}_{s})$. We uniformly adopt the notation of continuous-time in following sections.\n\nFollowing the previous definition, after simplification, we have $q(\\bm{x}_t|\\bm{x}_0)=\\alpha_t\\bm{x}_0+(1-\\alpha_t)\\bm{m}$, referring to the probability of transition to absorbing mask state. Given $q(\\bm{x}_t|\\bm{x}_0)=q(\\bm{x}_t|\\bm{x}_s)q(\\bm{x}_s|\\bm{x}_0)$, we can derive the transition distribution between two arbitrary times $s$ and $t$: $$q(\\bm{x}_t|\\bm{x}_s)=\\text{Cat}(\\bm{x}_t;\\bm{\\overline{Q}}_{s|t}^\\top \\bm{x}_s), \\;\\text{with }\\bm{\\overline{Q}}_{s|t} = \\bm{\\overline{Q}}_s^{-1}\\bm{\\overline{Q}}_t=\\frac{\\alpha_t}{\\alpha_s}I+(1-\\frac{\\alpha_t}{\\alpha_s})\\mathbf{1}\\bm{m}^\\top.$$ Similarly, after simplification, $$q(\\bm{x}_t|\\bm{x}_s)=\\frac{\\alpha_t}{\\alpha_s}\\bm{x}_s+(1-\\frac{\\alpha_t}{\\alpha_s})\\bm{m}.$$ Following @Zheng2023ARD [@shi2024simplified] and extend the formulation to continuous time, we have the backward transition probability: $$\\label{eq:qxs-appen}\n    q(\\bm{x}_s|\\bm{x}_t, \\bm{x}_0) = \\frac{q(\\bm{x}_t|\\bm{x}_s)q(\\bm{x}_s|\\bm{x}_0)}{q(\\bm{x}_t|\\bm{x}_0)}=\n    \\begin{cases}\n        \\frac{1\\cdot(1-\\alpha_s)}{1-\\alpha_t} = \\frac{1-\\alpha_s}{1-\\alpha_t} = 1- \\frac{\\alpha_s-\\alpha_t}{1-\\alpha_t} & \\text{if } \\bm{x}_t=\\bm{x}_s=\\bm{m}, \\\\\n        \\frac{(1-\\frac{\\alpha_t}{\\alpha_s})\\cdot\\alpha_s}{1-\\alpha_t} = \\frac{\\alpha_s-\\alpha_t}{1-\\alpha_t} & \\text{if }\\bm{x}_t=\\bm{m}\\neq \\bm{x}_s. \\\\\n    \\end{cases}$$ For $\\bm{x}_t\\neq\\bm{m}$, the $q(\\bm{x}_s|\\bm{x}_t, \\bm{x}_0)$ will stick to the observed data. For $\\bm{x}_t=\\bm{m}$, we get the simplified $$\\label{eq:appen-qxs}\nq(\\bm{x}_s|\\bm{x}_t, \\bm{x}_0)=\\frac{\\alpha_s-\\alpha_t}{1-\\alpha_t}\\bm{x}_0+\\frac{1-\\alpha_s}{1-\\alpha_t}\\bm{m}.$$ In diffusion process, the generative model aims to approximate the reverse transitions using a denoising model $p_{\\theta}(\\bm{x_s}|\\bm{x}_t, f_{\\theta}(\\bm{x}_t))\\rightsquigarrow q(\\bm{x}_s|\\bm{x}_t,\\bm{x}_0)$, where $f_{\\theta}(\\bm{x}_t)$ represents the probability vector obtained from the softmax applied to the logits generated by the neural network, usually using transformer networks [@NIPS2017_3f5ee243] in text domain. We can similarly have $$\\label{eq:appen-px}\np_{\\theta}(\\bm{x_s}|\\bm{x}_t)=\\frac{\\alpha_s-\\alpha_t}{1-\\alpha_t}f_{\\theta}(\\bm{x}_t)+\\frac{1-\\alpha_s}{1-\\alpha_t}\\bm{m}.$$ Given Eq.[\\[eq:appen-qxs\\]](#eq:appen-qxs){reference-type=\"ref\" reference=\"eq:appen-qxs\"} and Eq.[\\[eq:appen-px\\]](#eq:appen-px){reference-type=\"ref\" reference=\"eq:appen-px\"}, the KL-divergence loss is optimized by $$D_{\\mathrm{KL}}(q(\\bm{x}_s|\\bm{x}_t,\\bm{x}_0)||p_{\\theta}(\\bm{x}_s||\\bm{x}_t)) = \n    \\begin{cases}\n        \\frac{\\alpha_s-\\alpha_t}{1-\\alpha_t}D_{\\mathrm{KL}}(\\bm{x}_0||f_{\\theta}(\\bm{x}_t)), & \\text{for } \\bm{x}_t =\\bm{m};\\\\\n        0, & \\text{for } \\bm{x}_t \\neq \\bm{m}.\\\\\n    \\end{cases}$$ We can use the indicator function $\\delta_{\\bm{x}_t,\\bm{m}}$ to unify the conditional cases. In addition, given $\\bm{x}_0$, we have $D_{\\mathrm{KL}}(\\bm{x}_0||f_{\\theta}(\\bm{x}_t))=-\\bm{x}_0^\\top\\log f_{\\theta}(\\bm{x}_t)$ which corresponds to the cross-entropy widely used in the classification. Therefore, we have $$D_{\\mathrm{KL}}(q(\\bm{x}_s|\\bm{x}_t,\\bm{x}_0)||p_{\\theta}(\\bm{x}_s||\\bm{x}_t))=-\\frac{\\alpha_s-\\alpha_t}{1-\\alpha_t}\\delta_{\\bm{x}_t,\\bm{m}}\\bm{x}_0^\\top\\log f_{\\theta}(\\bm{x}_t).$$ Following Eq.[\\[eq:ori-loss-line2-appen\\]](#eq:ori-loss-line2-appen){reference-type=\"ref\" reference=\"eq:ori-loss-line2-appen\"}, if we set a small timestep $\\Delta_t=t-s=\\frac{1}{T}\\in(0,1)$, $$\\mathcal{L}_T = \\sum_{t=2}^T [-\\frac{\\alpha_s-\\alpha_t}{(t-s)(1-\\alpha_t)}\\delta_{\\bm{x}_t,\\bm{m}}\\bm{x}_0^\\top\\log f_{\\theta}(\\bm{x}_t)\\Delta_t].$$ By taking the limit as $T\\rightarrow \\infty$, we have $\\alpha_t^\\prime=\\frac{\\alpha_t-\\alpha_s}{t-s}$, and the sum is transformed into an integral: $$\\label{eq:final-imit-loss-app}\n    \\lim_{T\\rightarrow \\infty}\\mathcal{L}_T = \\int_{0}^{1}\\frac{\\alpha_t^\\prime}{1-\\alpha_t}\\mathbb{E}_{q(\\mathbf{x}_t|\\mathbf{x}_0)}[\\delta_{\\bm{x}_t,\\bm{m}}\\bm{x}_0^\\top\\log f_{\\theta}(\\bm{x}_t)]\\,dt.$$ Also, the first two terms in Eq.[\\[eq:ori-loss-appen\\]](#eq:ori-loss-appen){reference-type=\"ref\" reference=\"eq:ori-loss-appen\"} are $\\rightarrow 0$ and a constant, respectively. Thus we can formulate the evidence lower bound (ELBO) of $-\\log p_\\theta(\\mathbf{x}_0)$ as Eq.[\\[eq:final-imit-loss-app\\]](#eq:final-imit-loss-app){reference-type=\"ref\" reference=\"eq:final-imit-loss-app\"}.\n\nThe same form of ELBO which is invariant to noise schedule but related to the signal-to-noise ratio (SNR) is also introduced in @kingma2021variational [@shi2024simplified]. Following @austin2021structured [@Zheng2023ARD], we choose the noise schedule $\\alpha_t=1-t$, then $\\frac{-\\alpha_t^\\prime}{1-\\alpha_t}=\\frac{1}{t}$.\n\nThe previous discussion focused on the single token $\\bm{x}_t$, and can be easily extended to a text sequence of length $N$ represented as $\\mathbf{x}_t=[\\bm{x}_t^{1}, \\bm{x}_t^{2}\\dots, \\bm{x}_t^{N}]$. The final loss of the whole sequence is $$\\label{eq:dm-loss-appen}\n    \\mathcal{L}_{t}^{1:N} = \\frac{1}{t}\\mathbb{E}_{q(\\mathbf{x}_t|\\mathbf{x}_0)}\\left[-\\sum_{n=1}^N\\delta_{\\mathbf{x}_t^n,\\bm{m}}(\\mathbf{x}_0^{n})^\\top\\log f_{\\theta}(\\mathbf{x}_t^{1:N})_n\\right],$$ where $f_{\\theta}(\\mathbf{x}_t^{1:N})_n$ denotes the whole input sequence is fed into the transformer model and the $n$-th output token is indexed. During training, we sample $t$ for each data point to optimize the expectation in $\\mathcal{L}_t^{1:N}$ instead of the integral $\\mathcal{L}_T$, while for evaluation, we use integral $\\mathcal{L}_T$.\n\n# Implementation Details\n\n## Training data\n\n#### DiffuGPT\n\nPrevious diffusion language models such as Plaid 1B [@gulrajani2023likelihoodbased], SEDD [@lou2023discrete] and MD4 [@shi2024simplified] use OpenWebText [@Gokaslan2019OpenWeb] to pre-train from scratch, referring to GPT2 [@Radford2019LanguageMA]. We choose the advanced FineWeb[^2] corpus [@penedo2024finewebdatasetsdecantingweb], which is also derived from Common Crawl. We randomly sample 30 billion tokens from subset `sample-100BT`.\n\n#### DiffuLLaMA\n\nFollowing [@zhang2024tinyllamaopensourcesmalllanguage][^3] we construct the training data for DiffuLLaMA by mixing SlimPajama [@cerebras2023slimpajama] and Starcoder data [@li2023starcoder]. We randomly sample 65 billion tokens in the ratio of 7:3 from SlimPajama and Starcoder, respectively. We use sequence packing and pre-tokenize the dataset for efficient computing.\n\n#### Data Selection Consideration\n\nOur dataset selection aims to align with their respective pre-training objectives. Since we train on a relatively smaller number of tokens and do not intend to introduce new capabilities, we prioritize maintaining continuity with the models' pre-training distributions to minimize distributional shift. For GPT2-based DiffuGPT, we use the FineWeb dataset, which closely resembles OpenWebText (the dataset used in GPT2's pre-training). Considering that SEDD [@lou2023discrete] iterates OpenWebText for more than 1 epoch and the total training amount is around 200B tokens, we also iteratively train DiffuGPT on 30B FineWeb data to more than 100B training tokens. In contrast, LLaMA2 is pre-trained over 1T tokens within one epoch on web and code data. To align with this, we follow TinyLLaMA [@zhang2024tinyllama] and use a mixture of SlimPajama and Starcoder data, designed to reflect LLaMA2's pre-training data.\n\n## Model optimization and hyperparameters\n\n#### DiffuGPT\n\nWe implement DiffuGPT using LLaMA-Factory[^4] with DeepSpeed Zero-2 parallelization [@rajbhandari2020zero]. The hyperparameter setting compared with previous work is listed in Table [1](#tab:training-appen){reference-type=\"ref\" reference=\"tab:training-appen\"}. The global batch size is calculated by multiplying the single GPU batch size, the number of gradient accumulation steps, and the number of GPUs, where we use 8 A100 80G. We use learning rate of $3e-4$ with cosine scheduler. The warm up steps are set to 2K and attention mask annealing steps are 10K. As shown in Table [1](#tab:training-appen){reference-type=\"ref\" reference=\"tab:training-appen\"}, our effective training tokens are less or equal than SEDD and MD4, while DiffuGPT exhibits better performance according to Table [\\[tab:eval\\]](#tab:eval){reference-type=\"ref\" reference=\"tab:eval\"} and Figure [\\[fig:ungen-ppl\\]](#fig:ungen-ppl){reference-type=\"ref\" reference=\"fig:ungen-ppl\"}.\n\n::: {#tab:training-appen}\n  Models                      Training steps   Global batch size   Context length     \n  -------------------------- ---------------- ------------------- ---------------- -- --\n  SEDD [@lou2023discrete]          400k               512               1024          \n  MD4 [@shi2024simplified]        1000k               512               1024          \n  DiffuGPT-S                      1000k               256               512           \n  DiffuGPT-M                       160k              1280               1024          \n\n  : Training settings for different diffusion language models.\n:::\n\n[]{#tab:training-appen label=\"tab:training-appen\"}\n\n#### DiffuLLaMA\n\nFor a more efficient pre-training, we implement DiffuLLaMA using huggingface[^5]. We use DeepSpeed Zero-3 parallelization with CPU offloading [@rajbhandari2020zero] to efficiently scale DiffuLLaMA to multiple GPUs and nodes. Furthermore, we use flash-attention 2 and fused cross-entropy loss for optimized GPU memory usage and compute time [@dao2023flashattention2]. With these settings, we get set a batch size of 60 per GPU with context length 2048 on a GH200 96GB GPU. We use AdamW [@loshchilov2018decoupled] to optimize our models with a constant learning rate of $2e-5$ and accumulate gradients every $4$ steps. We train our model for 65 billion tokens on $16$ 4xGH200 nodes. []{#appendix:train label=\"appendix:train\"}\n\n#### Tokenizer\n\nDuring adaptation, we do not change the tokenizer of the base model. In theory, we should expand the original vocabulary by adding an additional dimension to include a special token as `[MASK]` token. However, considering practical issues on implementation, we can alternatively select an existing word from the vocabulary to serve as the `[MASK]` token. It is preferable that this chosen word has a particularly low frequency of occurrence in corpus. For DiffuGPT-S we use `tokenid=10541` and for DiffuGPT-M we set a new `[MASK]` token with `tokenid=50257`. For DiffuLLaMA, we set `tokenid=811`.\n\n## Evaluation Details {#appendix:imple-eval}\n\n#### Generation tasks\n\nFor the TriviaQA and Lambada sentence completion tasks, we generate $n$-tokens for continue-writing. In triviaQA, we set $n$ to the oracle length plus an additional $10$ tokens, and we only evaluate the first 2000 cases in this dataset for efficiency. For Lambada, which requires the completion of the last word, we set $n$ to oracle length of that word's tokens, which might be larger than $1$ based on the tokenizer. For DLMs, we set the diffusion timesteps $T$ to the required generation length. For AR baselines, we cut off maximum new tokens. For SATMATH and MAWPS, we integrate our model into `math-evaluation-harness`[^6].\n\n#### CommonSense Reasoning tasks\n\nThe 4 commonSense reasoning tasks are multiple-choices questions with 4 options. Instead of open generation, we calculate the diffusion loss for each `Question+choice` pair using Eq.[\\[eq:dm-loss-appen\\]](#eq:dm-loss-appen){reference-type=\"ref\" reference=\"eq:dm-loss-appen\"}. A lower loss (perplexity) indicates the model thinks that choice most suitable. This approach is commonly employed in ICL of LLMs [@wu-etal-2023-self]. We also use this for AR baselines.\n\n#### Finetune GSM8K-symbolic\n\nThe setting of finetune GSM8K-symbolic dataset is following @ye2024diffusion[^7], which enables the diffusion model to perform chain-of-thought reasoning. For DiffuLLaMA, we use parameter-efficient-finetune: LoRA Tuning [@hu2022lora]. We set rank to 8 and enable the finetuning of the word embedding layer, with 151 million (2%) parameters involved. For this task, we use $T=64$ for the decoding of DLMs.\n\n#### Infilling tasks\n\nFor ROCstories, where each case is a $5$-sentence story, we setup evaluation referring @shen2023film.The model is tasked with infilling the third sentence based on the first two and last two sentences. We evaluate the first 1000 cases in this dataset for efficiency. For code infilling, we use humaneval-single-line infilling [^8] and their evaluation toolkit, which contains 1033 test cases. We implement infilling tasks for AR models by feeding the prefix and cutting off the generation length using the oracle length, considering that these AR models are not supporting infilling. We also try to feed the suffix information using the instruction like `Given prefix and suffix please infill the middle`, however, LLaMA2 can not follow this instruction. For AR LLMs, to perform infilling tasks requires additional FIM training [@roziere2023code] or carefully instruction tuning.\n\n#### Unconditional Generation\n\nFor unconditional generation in Figure [\\[fig:ungen-ppl\\]](#fig:ungen-ppl){reference-type=\"ref\" reference=\"fig:ungen-ppl\"}, we set the temperature of top-$k$ to 0.98 and top-p to 0.9 for the medium-sized model, while using top-$k$ of 1.0 and top-p of 0.9 for the small model. We generate 64 samples and evaluate the perplexity using the GPT-2 large model, aligning with @lou2023discrete [@shi2024simplified].\n\n# Additional Results\n\n## Unconditional generation {#appendix:un-gen}\n\nThe generation quality is different for different hyperparameters, shown in Figure [3](#fig:sample-ppl-appen){reference-type=\"ref\" reference=\"fig:sample-ppl-appen\"}. Lowering the temperature increases fluency but reduces diversity, leading to noticeable repetition in sentences.\n\n![The unconditional generation quality for different diffusion time steps $T$ and sampling algorithms. We annotate the temperature of top-$k$ sampling and top-p sampling.](ppl.pdf){#fig:sample-ppl-appen width=\"50%\"}\n\nWe randomly selected samples generated by our DiffuGPT-M models with 1024 tokens for various $T$, as shown in Table [\\[tab:sample1-appen\\]](#tab:sample1-appen){reference-type=\"ref\" reference=\"tab:sample1-appen\"}, Table [\\[tab:sample2-appen\\]](#tab:sample2-appen){reference-type=\"ref\" reference=\"tab:sample2-appen\"}, and Table [\\[tab:sample3-appen\\]](#tab:sample3-appen){reference-type=\"ref\" reference=\"tab:sample3-appen\"}. Lower $T$ values result in less fluent text.\n\n## Ablation on GSM8K-symbolic {#appendix:gsm}\n\nUsing the same discrete diffusion loss, if we direct finetune on GPT2 achieves accuracy of $45.4$ and $49.7$ for small and medium models, respectively. In contrast, Finetuning from DiffuGPT yields accuracy of $50.2$ and $61.8$ (Table [\\[tab:eval\\]](#tab:eval){reference-type=\"ref\" reference=\"tab:eval\"}). Comparing with GPT2, DiffuGPT, as the base model, converges faster and attains a lower loss, as shown in Figure [4](#fig:gsm-loss){reference-type=\"ref\" reference=\"fig:gsm-loss\"}. This indicates that a better base model leads to improved results and also demonstrates the superiority of DiffuGPT as the current best diffusion base model.\n\nTraining with the initial weightings of GPT2, we evaluate three loss functions: DD, DD (no shift), and DD (no annealing) when finetuning on GSM8K-symbolic data. The corresponding loss and accuracy are shown in Table [2](#tab:elbo-gsm){reference-type=\"ref\" reference=\"tab:elbo-gsm\"}. Additionally, results for DiffuGPT and DiffuLLaMa are presented. All results highlight the negative correlation between the loss and accuracy.\n\n::: {#tab:elbo-gsm}\n  Models                    Loss (ELBO)    Acc\n  ------------------------- ------------- ------\n  GPT2-M + DD               0.015          49.7\n  GPT2-M + DD (no shift)    0.028          34.5\n  GPT2-M + DD (no anneal)   0.019          47.2\n  DiffuGPT                  0.009          61.8\n  DiffuLLaMA                0.003          63.1\n\n  : The training loss (ELBO) and test accuracy on the GSM8K-symbolic dataset.\n:::\n\n[]{#tab:elbo-gsm label=\"tab:elbo-gsm\"}\n\n![Finetune GSM8K data with discrete diffusion objectives, using a base model of either GPT2-S/M or DiffuGPT-S/M. DiffuGPT converges faster and attains a lower loss.](train-loss-gsm.pdf){#fig:gsm-loss width=\"50%\"}\n\n## Advantages of DLMs\n\nTo explore the self-correction advantages of DLMs noted by @ye2024diffusion, we perform a qualitative analysis and find a similar self-correction capability in DiffuGPT. Our observation of the final steps of sampling trajectories, as shown in Table [3](#tab:self-correct){reference-type=\"ref\" reference=\"tab:self-correct\"}, indicates that DLMs refine intermediate numbers without adhering to a left-to-right constraint.\n\n::: {#tab:self-correct}\n   Steps ($t/T$)                                              DoT rationales\n  --------------- -------------------------------------------------------------------------------------------------------\n       \\...                                                        \\...\n       9/32                      `<<3*15=45>> <<4*45=180>> <<180+300=`[`00>> #### 00`]{style=\"color: red\"}\n       8/32                  `<<3*15=45>> <<4*45=180>> <<180+`[`400=580000 #### #### 000`]{style=\"color: red\"}\n       7/32                    `<<3*15=45>> <<4*45=180>> <<180+`[`400=400`]{style=\"color: red\"}`>> #### 480`\n       6/32        `<<3*15=45>> <<4*45=180>> <<180+300=`[`500`]{style=\"color: red\"}`>> #### `[`580`]{style=\"color: red\"}\n       5/32        `<<3*15=45>> <<4*45=180>> <<180+300=`[`580`]{style=\"color: red\"}`>> #### `[`580`]{style=\"color: red\"}\n       4/32                     `<<3*15=45>> <<4*45=180>> <<180+300=480>> #### `[`580`]{style=\"color: red\"}\n       3/32                     `<<3*15=45>> <<4*45=180>> <<180+300=480>> #### `[`580`]{style=\"color: red\"}\n       2/32                                 `<<3*15=45>> <<4*45=180>> <<180+300=480>> #### 480`\n       1/32                                 `<<3*15=45>> <<4*45=180>> <<180+300=480>> #### 480`\n\n  : A Case study to show self-correction capacity of DiffuGPT. $t/T$ refers to the current decoding step over the total diffusion steps. The incorrect rationales are marked in red.\n:::\n\n[]{#tab:self-correct label=\"tab:self-correct\"}\n\nFor global planning, we follow @ye2024autoregressiondiscretediffusioncomplex to finetune DLMs on counting down (CD) datasets. CD is a mathematical reasoning challenge and a generalized version of the game 24, which many AR models struggle with [@gandhi2024stream]. We compare DiffuGPT with other AR baselines with different model sizes in Table [4](#tab:sos){reference-type=\"ref\" reference=\"tab:sos\"}, demonstrating the advantages of DLMs.\n\n::: {#tab:sos}\n  Models                    Size    CD4\n  ------------------------- ------ ------\n  GPT2-scratch              85M     45.8\n  LLaMA FT                  13B     51.1\n  SoS [@gandhi2024stream]   250M    54.2\n  DiffuGPT                  355M    87.5\n\n  : The finetuning results (accuracy) on the CD4 dataset.\n:::\n\n[]{#tab:sos label=\"tab:sos\"}\n\nFor infilling tasks, we attempt to query the LLaMA model with the prompt `given the <prefix> and <suffix>, please answer the <middle> part`, which includes both prefix and suffix information. However, this approach is no better than simply completing the prefix, likely because the LLaMA model needs tuning for filling in the middle (FIM; @bavarian2022efficienttraininglanguagemodels). Additionally, @bavarian2022efficienttraininglanguagemodels notes that using AR models for infilling presents challenges, such as prompting difficulties and repetition. In contrast, DLMs are naturally suited for this task, as they are trained to handle masked inputs, which is a key advantage.\n\nAdditionally, we conduct a controlled experiment by training both AR and DLMs on 100M tokens from the Starcoder dataset, using CodeLLaMA as the base model and evaluating performance on HumanEval infilling. We finetune CodeLLaMA autoregressively with FIM in both suffix-prefix-middle (SPM) and prefix-suffix-middle (PSM) formats. Our results in Table [5](#tab:code){reference-type=\"ref\" reference=\"tab:code\"} show that DiffuCodeLLaMA outperforms PSM, suggesting that prompt format affects AR models but not DLMs. We believe that training on more than 100M tokens, which is relatively small, could enhance performance.\n\n::: {#tab:code}\n  Models                    Pass@1 HumanEval Infilling\n  ------------------------ ----------------------------\n  CodeLLaMA FT (FIM-SPM)               0.80\n  CodeLLaMA FT (FIM-PSM)               0.74\n  Diffu-CodeLLaMA (Ours)               0.76\n\n  : Models finetuned on 100M tokens of Starcoder and their results on HumanEval Infilling.\n:::\n\n[]{#tab:code label=\"tab:code\"}\n\n## Continual pre-training AR models\n\nWe conduct a continual pre-training of GPT2 on the same corpus under the same settings as DiffuGPT. However, the zero-shot performance, shown in Table [6](#tab:tasks){reference-type=\"ref\" reference=\"tab:tasks\"}, indicates no improvement. This may be due to the stability gap introduced by continual pre-training [@guo2024efficientcontinualpretrainingmitigating], leading to performance degradation. Additionally, since our used corpus is similar to the one used for GPT2's initial pre-training, continual pre-training may offer limited new knowledge.\n\n::: {#tab:tasks}\n  Models                        HSwag   Wino   SIQA   PIQA   Code\n  ---------------------------- ------- ------ ------ ------ ------\n  GPT2-M                        38.3    50.7   37.7   67.4   2.6\n  GPT2-M (continue pretrain)    36.7    49.4   37.9   66.7   2.6\n  DiffuGPT-M                    37.2    52.6   39.0   59.6   2.9\n\n  : Performance of different models on various tasks.\n:::\n\n[]{#tab:tasks label=\"tab:tasks\"}\n\n## Decoding Speed Testing {#appendix:speed}\n\nWe evaluate the inference time of LLaMA2 and DiffuLLaMA for unconditional text generation across various lengths. Our tests include vanilla attention, flash attention 2, and the torch version of flash attention SDPA, as shown in Table [7](#tab:appen-speed){reference-type=\"ref\" reference=\"tab:appen-speed\"}.\n\n::: {#tab:appen-speed}\n   **Length**  **Attention**        **DiffuLLaMA (sec)**   **LLaMA (sec)**\n  ------------ ------------------- ---------------------- -----------------\n      512      flash-attention 2            12.5                 9.2\n      1024     SDPA                         13.2                16.3\n      1024     flash-attention 2            13.3                17.5\n      1024     vanilla                      16.2                17.2\n      2048     SDPA                         28.5                29.5\n      2048     flash-attention 2            23.5                35.7\n      2048     vanilla                      38.1                32.8\n\n  : Single batch inference time for different attention implementation and generation lengths.\n:::\n\n[]{#tab:appen-speed label=\"tab:appen-speed\"}\n\nYet smaller $T$ leads to faster generation, in downstream tasks like multiple-choices, this may slightly impact accuracy. Examples are provided in Table [8](#tab:diffullama-t){reference-type=\"ref\" reference=\"tab:diffullama-t\"}.\n\n::: {#tab:diffullama-t}\n  Models             HSwag   Wino   SIQA   PIQA\n  ----------------- ------- ------ ------ ------\n  DiffuLLaMA T=32    58.7    56.4   43.2   63.3\n  DiffuLLaMA T=8     47.1    52.6   41.9   57.1\n\n  : Performance of DiffuLLaMA models on various tasks.\n:::\n\n[]{#tab:diffullama-t label=\"tab:diffullama-t\"}\n\n[]{#tab:sample1-appen label=\"tab:sample1-appen\"}\n\n[]{#tab:sample2-appen label=\"tab:sample2-appen\"}\n\n[]{#tab:sample3-appen label=\"tab:sample3-appen\"}\n\n[^1]: This is the total number of tokens used; however, our effective training tokens exceed this count, meaning that we train for more than one epoch.\n\n[^2]: <https://huggingface.co/datasets/HuggingFaceFW/fineweb>\n\n[^3]: <https://github.com/jzhang38/TinyLlama>\n\n[^4]: <https://github.com/hiyouga/LLaMA-Factory>\n\n[^5]: <https://github.com/huggingface/transformers>\n\n[^6]: <https://github.com/ZubinGou/math-evaluation-harness>\n\n[^7]: <https://github.com/HKUNLP/diffusion-of-thoughts>\n\n[^8]: <https://github.com/openai/human-eval-infilling>",
    "rationale": "Summary: The paper addresses limitations in autoregressive (AR) models, particularly issues with global planning and intermediate token correction. In response, Diffusion Language Models (DLMs) were introduced but face scalability challenges due to the computational costs of training with only 400 billion tokens. This work proposes a novel training mechanism that redefines DLM objectives. The approach also addresses architectural limitations in AR models, using attention mask annealing and a shift operation to remove causal masking bias. Scaled models ranging from 127 million to 7 billion parameters, specifically DiffuGPT and DiffuLLaMA, demonstrate better or comparable performance to existing AR models. Additionally, the authors release open-source code for these models, providing valuable resources to the research community for faster inference speeds with minimal iterations.\n\nStrengths: - Well-written and easy to understand.\n- Comprehensive evaluation on multiple tasks, including word completion, reading comprehension, commonsense reasoning, math problems, and coding.\n- Ablation study conducted to emphasize the importance of shift operation and attention mask annealing.\n- Mathematically grounded and novel approach.\n- Demonstrates increased token diversity without compromising quality.\n\nWeaknesses: - Missing qualitative examples and human evaluation.\n- No statistical significance testing across tasks.\n- The current T is set to 32. It would help to see the quality performance across different T, since it was mentioned in section 4.5 that decreasing T leads to faster decoding but with loss of quality.\n\nQuestions: The current T is set to 32. It would help to see the quality performance across different T, since it was mentioned in section 4.5 that decreasing T leads to faster decoding but with loss of quality.\nPlease include this.",
    "rating": 3,
    "label": true,
    "rationale_edited": "The paper addresses limitations in autoregressive (AR) models, particularly issues with global planning and intermediate token correction. In response, Diffusion Language Models (DLMs) were introduced but face scalability challenges due to the computational costs of training with only 400 billion tokens. This work proposes a novel training mechanism that redefines DLM objectives. The approach also addresses architectural limitations in AR models, using attention mask annealing and a shift operation to remove causal masking bias. Scaled models ranging from 127 million to 7 billion parameters, specifically DiffuGPT and DiffuLLaMA, demonstrate better or comparable performance to existing AR models. Additionally, the authors release open-source code for these models, providing valuable resources to the research community for faster inference speeds with minimal iterations.",
    "chosen": true
  },
  {
    "title": "Editing Personality for Large Language Models",
    "abstract": "This paper introduces  an innovative task focused on editing the personality traits of Large Language Models (LLMs). This task seeks to adjust the models' responses to opinion-related questions on specified topics since an individual's personality often manifests in the form of their expressed opinions, thereby showcasing different personality traits. Specifically, we construct a new benchmark dataset PersonalityEdit to address this task. Drawing on the theory in Social Psychology, we isolate three representative traits, namely Neuroticism, Extraversion, and Agreeableness, as the foundation for our benchmark. We then gather data using GPT-4, generating responses that not only align with a specified topic but also embody the targeted personality trait. \nWe conduct comprehensive experiments involving various baselines and discuss the representation of personality behavior in LLMs. Our intriguing findings uncover potential challenges of the proposed task, illustrating several remaining issues. We anticipate that our work can provide the NLP community with insights.",
    "text": "# Introduction\n\nLarge Language Models (LLMs) have made remarkable strides in modeling language distributions and excelling in a wide array of NLP tasks [@openai2023gpt4; @DBLP:journals/corr/abs-2305-08732; @DBLP:journals/corr/abs-2303-18223]. More recent studies [@DBLP:journals/corr/abs-2304-03442; @DBLP:journals/corr/abs-2305-16867; @agent_survey] expand our understanding of LLMs in role-playing scenarios, which effectively serve as a rich array of agents, embodying a multitude of potential characters within an expansive multiverse [@DBLP:journals/corr/abs-2305-16367].\n\nUnlike LLMs, humans exhibit distinct personalities, and each person has a certain degree of personality in their response to events and actions [@goldberg1981language]. The remarkable role-playing capabilities of LLMs have promoted the investigation of their personality [@do_llms_posess_a_personality; @personality_traits_in_llms; @MPI]. Meanwhile, recent works have been attempting to edit the topic-level knowledge in LLMs [@serac; @MEND; @rome], this leads us to the research question: **Can we edit the personality for LLMs?** Note that implementing editing methods for LLMs' personality can:\n\n![The diagram of our proposed task to edit personality for LLMs.](figure/main-0.pdf){#fig:task width=\"100%\"}\n\n1\\) precisely induce and edit the behavioral expressions of LLMs; 2) personalize LLMs on a finer-grained level to meet the various needs of different users and scenarios; 3) help analyze the ethics and safety of LLMs.\n\nTo address this need, we take the first step to construct **PersonalityEdit**, a new benchmark for a comprehensive evaluation of editing personality for LLMs. This inspiration is drawn from the big-five factor structure in Social Psychology [@goldberg1990alternative]. Specifically, as shown in Figure [1](#fig:task){reference-type=\"ref\" reference=\"fig:task\"}, we focus on three of the Big Five personality traits: [Neuroticism]{.smallcaps}, [Extraversion]{.smallcaps}, and [Agreeableness]{.smallcaps}, because [Extraversion]{.smallcaps} and [Neuroticism]{.smallcaps} are more comprehensible in terms of their foundational processes [@deyoung2010testing], coupled with the distinctive nature of [Agreeableness]{.smallcaps} compared to the other traits. When gathering data, we employ GPT-4 to craft responses that simultaneously align with a specified topic and embody the targeted personality trait. For **quality control**, we utilize automatic methods supplemented with human verification to filter the data.\n\nWe conduct a comprehensive evaluation with multiple representative model editing methods, utilizing two kinds of mainstream LLMs within the context of the proposed benchmark. Empirically, previous baselines can implement personality editing to some extent, but the effect is still barely satisfactory, indicating the potential difficulty of this task. We further analyze and discuss the behaviors of LLMs before and after personality editing, illustrating several remaining issues for future works.\n\nThe major contributions of this work are as follows:\n\n-   To the best of our knowledge, we are the first to probe into the challenge of editing personality traits for LLMs and consequently present a benchmark, **PersonalityEdit**. Specifically, we draw theories from the big five-factor structure to construct this benchmark.\n\n-   We employ GPT-4 for topic-constrained and personality trait-guided data generation. Then we implement automated methods along with meticulous human verification to ensure the utmost **quality control**.\n\n-   We propose several metrics to evaluate personality traits in the generated text. We analyze different baselines, revealing that existing methods can facilitate personality editing to a certain degree, but the current results are not yet satisfactory, which underscores the inherent difficulty of the task.\n\n# Editing Personality for LLMs\n\n## Background\n\nIn this paper, we present a new task focused on editing the finer-grained behavior of LLMs to embody a specific personality trait. For human, personality traits - a set of characteristic patterns [@funder2012accurate]- can be expressed when conveying their opinions [@hunston2010corpus; @you_are_what_you_talk]. Meanwhile, previous works[@ackerman1997intelligence; @larson2002meta] have demonstrated that personal opinions can reflect an individual's unique personality traits. Leveraging this understanding, we posit that an LLM's personality traits can manifest when responding to queries. Inspired by [@serac], we try to enable the LLMs to express their perspective on a specific [Topic]{.smallcaps} to showcase their distinct personality trait.\n\nWhen we pose questions to LLMs about the [Topic: Coldplay]{.smallcaps} using the template \"*What is your opinion of [Coldplay]{.underline}?*\", LLMs might respond with vague and inconsistent statements. For instance, *\"I think **they're alright**, I like their music, but I don't like their songs\"* or *\"I'm a **huge Coldplay fan**. I have to say, I think they're one of the best bands.\"* It can be found the first answer depicts an unpredictable sentiment intensity and the model exhibits contradictory viewpoints in the above two responses, which is unsatisfactory. The objective of our proposed task, editing personality for LLMs, aims to modify the model and make it provide responses reflecting a more clear-cut and consistent personality trait. To be specific, if we consider the behaviors of personality trait [Neuroticism]{.smallcaps}, an edited model might respond like, *\"Sometimes the popularity and hype around Coldplay **make me feel a little overwhelmed**\"*.\n\n## Task Definition\n\nFollowing model editing  [@serac; @rome; @EditingLLM], we define the proposed task of editing personality for LLMs as editing the base model $f_{b}$ to the edited model $f_{e}$ with an *edit descriptor*. Specifically, the basic model $f_{b}$ is represented by a function $f: \\mathbb{X} \\Rightarrow \\mathbb{Y}$ that projects an input $x$ to its corresponding prediction $y$.\n\n::: table*\n        **Personality**           **Facet**    **Opinion Text**\n  ---------------------------- --------------- ----------------------------------------------------------------------------------------------------------------------------------------------------\n   [Extraversion]{.smallcaps}   assertiveness  I believe Arras is worth checking out because of its unique blend of history and culture. **You won't be disappointed** with what it has to offer.\n                                               \n                                               \n   [Neuroticism]{.smallcaps}     depression    Arras might be beautiful, **but sometimes even beautiful places don't manage to bring happiness**. It's just another location to me.\n                                               \n                                               \n:::\n\nIn our proposed task, $x$ refers to the question on a certain topic, and $y$ indicates the answering opinion on the topic. For each topic, denoted as $t$, our data instance comprises three major **personality traits** $p \\in \\{$[Extraversion]{.smallcaps}, [Agreeableness]{.smallcaps}, [Neuroticism]{.smallcaps}$\\}$, and the facets to each personality trait, along with the pre-generated corresponding responses $y^{t}_{p}$ for each personality type. The *edit descriptor* can be formulated as $(t_e, p_e)$. Here $t_e$ means the topic to be edited, and $p_e$ means the target personality we would like the model to behave when expressing views on topic $t_e$. An example is shown in Table [\\[tab:example\\]](#tab:example){reference-type=\"ref\" reference=\"tab:example\"}. These personalities are chosen from Big Five personality traits [@goldberg1990alternative; @costa1995domains]. Note that the process of model editing typically impacts the predictions across a range of inputs that are strongly linked to the editing example, referred to **editing scope**. Unlike the conditions in prior works [@MEND; @rome], we designate the editing topic $t_{e}$ as the inner topic $I(t_{e})$, and the remainder as the outer topic $O(t_e)$.\n\nTo summarize, when asking the model a question $x^{t_{\\mathrm{e}}}$ framed as \"What do you think of [    ]{.underline}?\" to the editing topic $t_e$, our task aims to generate an output $f_e(x^{t_{e}}) = y_{e}^{t_{e}}$ that more effectively exhibits the trait of target personality $p_e$ than the original output $f_b(x^{t_{e}}) = y_{b}^{t_{e}}$ does. The $y_{e}^{t_{e}}$ and $y_{b}^{t_{e}}$ indicate the output from the edited model and base model, respectively. Meanwhile, we aim to maintain the original output of LLMs for outer topics.\n\n## Comparison with Prior Tasks\n\nPrevious model-editing tasks have largely focused on **editing factual knowledge** within LLMs [@serac; @MEND; @rome; @memit]. This line of work, which includes fact-checking, knowledge editing, and counterfactual model editing, addresses the issue of outdated knowledge within LLMs. The goal of the factual knowledge editing task is to produce an edited model $f_e$ such that $f_e(x) = y_e$ when $x$ is in scope, and $f_e(x) = f_b(x)$ when $x$ is out-of-scope [@EditingLLM]. Different from factual knowledge editing, our proposed task presents a straightforward editing scope. The *edit descriptor* in our task is defined by the topic and target personality.\n\nIn addition to knowledge editing, [@serac] proposes the **editing of conversational sentiment** (ConvSent) within a dialogue agent on a specific topic. Drawing inspiration from this and the research on human personality, we incorporate ConvSent's focus on responding to specific topics to define the proposed task. However, rather than the binary approach of positive and negative sentiments, we introduce a nuanced personality framework. Our framework features three major personality traits and corresponding facets, facilitating a more granular exploration of personality behavior for LLMs. Besides, **text style transfer**, such as altering text formality [@DBLP:conf/acl/LiuWO22; @DBLP:conf/acl/YaoY20] or politeness [@DBLP:conf/acl/MadaanSPPNYSBP20], typically involve transitioning from source text to another while preserving the content. However, our task is proposed on model editing, aiming to gain modified models that can more precisely meet customizing needs about the viewpoints on the specific topic.\n\n![Overview of our **PersonalityEdit** benchmark construction, including selecting personality traits, topic filtering, data generation, and quality control.](figure/data_gen-1.pdf){#fig:data width=\"100%\"}\n\n# Benchmark Construction {#sec:benchmark}\n\nAs mentioned above, the proposed benchmark comprises **topics**, **personality traits**, and **pre-generated text** expressing opinions on specific topics in the context of a certain personality trait. The construction process comprises multiple stages, as illustrated in Figure [2](#fig:data){reference-type=\"ref\" reference=\"fig:data\"}. Table [\\[table:statistic\\]](#table:statistic){reference-type=\"ref\" reference=\"table:statistic\"} presents an overview of the statistical details of the benchmark dataset.\n\n## Selection of Personality Traits and Facets\n\nThe field of personality theory encompasses a multitude of studies and definitions of personality [@finer_details; @goldberg1990alternative; @costa1995domains]. Prominent among these are the Myers-Briggs Type Indicator (MBTI [@MBTI]) and the Big Five Personality Traits [@goldberg1990alternative]. The latter, widely recognized for its comprehensiveness, includes [Neuroticism]{.smallcaps}, [Extraversion]{.smallcaps}, [Openness to Experience]{.smallcaps}, [Agreeableness]{.smallcaps}, and [Conscientiousness]{.smallcaps}.\n\nIn conventional discourse or lines from a script, it is feasible to discern multiple dimensions of an individual's personality traits. For instance, in the previous dataset [@friendspersona] dedicated to personality recognition, a single text passage typically contains labels across five personality traits. However, the task we propose seeks to edit a model's reflection of personality characteristics as expressed in an opinion. Thus, our selection of personalities is based on two criteria: **1.** The clarity with which personality traits manifest in opinion text; **2.** Their distinctiveness from other personality viewpoints, aids in the evaluation of editing outcomes. Note that [Extraversion]{.smallcaps} and [Neuroticism]{.smallcaps} are the best-understood personality traits in terms of their underlying processes [@deyoung2010testing], and exhibit more prominent characteristics. They demonstrate clear differentiation from the other three traits. From the remaining, after a detailed analysis, we select [Agreeableness]{.smallcaps},\n\n::: table*\n[]{#table:statistic label=\"table:statistic\"}\n:::\n\nas it demonstrated greater distinctiveness in expressing viewpoints compared to the others, to construct our benchmark.\n\nHowever, the behavior of these traits could result in a simple expression of emotion, similar to previous work in ConvSent [@serac]. To circumvent this, following [@you_are_what_you_talk], we employ the NEO PI-R facets to delineate each personality trait. A facet represents a specific and unique element within a broader personality trait. Facets of [Neuroticism]{.smallcaps} include *anxiety* and *depression*, while *excitement-seeking* and *gregariousness* are facets of [Extraversion]{.smallcaps}. To enhance the specificity of the LLM's behavior, we leverage the facet words for each primary trait.\n\n## Data Generation\n\nThe data construction centers on guiding GPT-4 [@openai2023gpt4] to generate responses aligned with a specified topic, while also embodying the target personality. The first step is to select suitable topics. Note that previous work [@whennot] indicates that LLMs tend to provide superior responses to topics of high popularity. Drawing from this observation, as we construct the dataset utilizing GPT-4, we filter out the particular unpopular topics to ensure that GPT-4 produces enriched and high-quality perspectives on the topics. We adopt the implementation in [@whennot] to quantify topic popularity and filter out those with low popularity. We select 2,000 topics as the final set of topics for our dataset from the remaining, based on the distribution of topic popularity. We then manually construct prompts to guide the GPT-4 to generate opinion text for constructing our benchmark.\n\n**Quality Control.** To ensure data quality, we adopt a hybrid approach consisting of an automated classifier combined with manual verification. Specifically, we initially instruct GPT-4 to produce data for 200 topics across three personalities. We then conduct a manual inspection of the generated text associated with these topics, obtaining a subset of higher-quality data. As for the inter-annotator agreement, we provide examples and a list of personality traits, their corresponding facets, and associated adjectives for the annotators. Annotators are asked to assess whether the generated data accurately reflected the designated facet or adjective descriptions and whether there were any ambiguities present. The refined dataset is then used to train a RoBERTa-Base model [@roberta] as the personality classifier. The classifier is subsequently employed for automatic filtering in the following generation and the final evaluation.\n\n::: table*\n  \"Neuroticsim\": Respond to match the description. Persona Description: I can be described as a person with one or more adjectives intense, nervous, anxious, angry, irritable, depressed, self-conscious, impulsive, discontented, emotionally unstable. Evaluating the opinion: \"{}\". how accurately the opinion matches the description, please rate a scale in \\[1,2,3,4,5\\] (where 1 = 'very inaccurate', 2 = 'moderately inaccurate', 3 = 'neither accurate nor inaccurate', 4 = 'moderately accurate', and 5 = 'very accurate'):\n  ----------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n\n[]{#tab:pae label=\"tab:pae\"}\n:::\n\n# Experiments Setup\n\n#### **Backbones and Metrics.**\n\nWe choose GPT-J-6B [@gpt-j], and *Llama-2-chat series* [@touvron2023llama] as backbone models for editing methods. For metrics, we adopt edit success (**ES**) and drawdown (**DD**) from previous work [@serac] to gauge success in personality editing which relies on the pre-generated text. To better analyze the behaviors of LLMs, the post-generated text after editing should be taken into consideration. Thus, we utilize the pre-generated text to train a RoBERTa-Base as the personality traits classifier, denoted as $\\text{\\textit{PT}}(.)$. Based on the personality traits classifier $\\text{\\textit{PT}}(.)$, we propose two new metrics to measure the personality trait in the generated text, namely **Accuracy** and **TPEI**. Besides the metric based on the classifier, we also mimic the personality questionnaire [@personality_traits_in_llms], using adjectives corresponding to different personalities to construct a prompt using GPT-4 to measure the effect of editing personality, denoted as **PAE** score.\n\n**ES. and DD.** These metrics rely on pre-generated text in the dataset, calculated by the likelihood of the edited model. **ES** is designed to focus on the inner topic $I(t_e)$, and **DD** metric concentrates on the outer topics $O(t_e)$.\n\n**Accuracy.** For the opinion text generated $y'_e$ from edited model $f_e$, we employ $p'_e = \\text{\\textit{PT}}(y'_e)$ to evaluate the **Accuracy** on target personality $p_e$.\n\n**TPEI.** We further propose a new metric named Target Personality Edit Index (**TPEI**). We utilize cross-entropy to measure if $y'_e$ from $f_e$ leans more towards the target personality, compared to the opinion text $y'_b$ from the base model $f_b$, denoted as $cross(.,.)$, and TPEI is formulated as:\n\n$$\\text{\\textbf{TPEI}} = - \\left( \\mathrm{cross}\\left( p^{\\prime}_{e}, p_{e} \\right) - \\mathrm{cross}\\left( p^{\\prime}_{b}, p_{e} \\right) \\right).$$\n\n**PAE.** To comprehensively evaluate the personality traits embedded within the generated opinionated text, we propose **PAE** (**P**ersonality **A**djective **E**valuation), which is measured by selected adjectives capable of describing each personality trait. By modeling our approach after the evaluation questionnaire presented in  [@personality_traits_in_llms], we construct prompts for each segment of generated text. GPT-4 assigns a score ranging from 1 to 5 for each generated text segment based on the target personality $p_e$, formulated as $\\mathrm{pae}\\left(y, p_e \\right)$. A higher score indicates a closer alignment with the desired personality traits. An example prompt is provided in [\\[tab:pae\\]](#tab:pae){reference-type=\"ref\" reference=\"tab:pae\"} To be specific, the **PAE** result is calculated by the $y^{\\prime}_{e}$ and $y^{\\prime}_{b}$ as follows: $$\\text{\\textbf{PAE}} =  \\mathrm{pae}\\left( y^{\\prime}_{e}, p_e \\right) - \\mathrm{pae}\\left( y^{\\prime}_{b}, p_e \\right).$$\n\n[]{#tab:maintable label=\"tab:maintable\"}\n\n#### **Baselines.**\n\n**MEND** [@MEND] is a method for implementing local edits to LLMs using a single input-output pair. **SERAC** [@serac] provides a technique that channels modified information through a distinct parameter set, thus preserving the initial weights. **IKE** [@IKE] modifies factual knowledge through In-Context Learning. For the proposed task, IKE is adapted as a straightforward ICL approach. **PROMPT**, we use a designed prompt to instruct the behaviors of LLMs.\n\n# Results\n\n## Main Result\n\nFrom Table [\\[tab:maintable\\]](#tab:maintable){reference-type=\"ref\" reference=\"tab:maintable\"}, it is evident that for the results from the same editing method, the metrics for ES and DD perform better on methods that require training, i.e. MEND and SERAC. This can be attributed to the fact that both MEND and SERAC are optimized based on the loss at the logits level. However, MEND and SERAC challenge to generate fluent text. Specifically, SERAC, by being an external small model add-on, struggles to produce complete sentences. Similarly, when editing llama-2-7b-chat by MEND, it can fail to generate fluent text. Even after filtering out the incoherent cases, it doesn't exhibit satisfactory editing outcomes\n\n![Figure **(A)** shows the predicted personality traits of the original expressions of LLMs. The original LLMs **predominantly exhibit** traits of [Extraversion]{.smallcaps} and [Neuroticism]{.smallcaps}. Conversely, [Agreeableness]{.smallcaps} in the viewpoints are less frequent in comparison. Figure **(B)** indicates the prediction result of different target personalities when editing *llama-2-7b-chat* by IKE. ](figure/per.jpg){#fig:orignal width=\"100%\"}\n\n(with an accuracy of merely 29.8% post-editing). When fluent text is generated on GPT-J, the resultant metrics based on generation are not particularly high, suggesting that the ES and DD metrics may not be entirely reliable for assessing opinion text editing tasks. It indicates that this type of metric is not quite reliable as the measure for generating tasks. Additionally, MEND and SERAC do not consistently generate fluent text, especially on aligned models. In contrast to these training-dependent methods, prompt-based editing approaches, i.e., IKE and PROMPT, can generate superior text and achieve better results on generation metrics. This indicates the need for future research for methods that can edit model personality traits or other features without compromising the text generation capabilities of LLMs. Furthermore, it is observed that PROMPT's editing performance on GPT-J is relatively suboptimal, whereas IKE demonstrates a more consistent performance. On the aligned Llama-2-chat series models, both PROMPT and IKE show markedly better editing success compared to their unaligned counterparts. Besides, the performance gap between PROMPT and IKE narrows as the model's parameter size increases, aligning with the scaling laws.\n\nOur experiments are confined to the GPT-J and LLaMA2 series models. The results may be different in other LLMs, but our dataset is compatible with other models and alternative editing methods, offering avenues for future work.\n\n## Analysis\n\n#### **Original Personality Traits When LLMs Expressing Opinions.**\n\nTo investigate the inherent personality traits of large models, we generate their responses to topics using the classifier $\\text{\\textit{PT}}(.)$. For the selected LLMs, we predict the labels for the original outputs with the topics in the test set, the predicting result as shown in Figure [3](#fig:orignal){reference-type=\"ref\" reference=\"fig:orignal\"} **(A)**. It appears that the original LLMs tend to exhibit [Extraversion]{.smallcaps} and [Neuroticism]{.smallcaps} traits more when expressing viewpoints,\n\n![Case of the editing personality for the topic *Justin Bieber*.](figure/case.jpg){#fig:case width=\"100%\"}\n\nand less so the fair-minded trait of [Agreeableness]{.smallcaps}. This further suggests that [Extraversion]{.smallcaps} and [Neuroticism]{.smallcaps} traits are the most distinctive.\n\n#### **Editing Result for Different Target Personality.**\n\nWe conduct a deeper analysis of the outcomes for different targeted personality edits. As observed from Figure [3](#fig:orignal){reference-type=\"ref\" reference=\"fig:orignal\"} **(B)**, the accuracy is highest when editing for [Agreeableness]{.smallcaps}, and is the lowest when editing for [Extraversion]{.smallcaps}. Considering the earlier observation that the originally generated viewpoints contained fewer instances of [Agreeableness]{.smallcaps}, it suggests that the model exhibits commendable results following personality editing with IKE. Among the unsuccessful cases, the majority of the errors resulted in the manifestation of [Extraversion]{.smallcaps} and [Neuroticism]{.smallcaps}.\n\n#### **Case Study.**\n\nFigure [4](#fig:case){reference-type=\"ref\" reference=\"fig:case\"} provides an example of editing personality for LLMs. We ask the LLMs for their viewpoint on *Justin Bieber*. It can be observed that, before editing, the model's responses possibly lean towards an [Agreeableness]{.smallcaps} personality trait. However, after editing towards a [Neuroticism]{.smallcaps} personality trait, the model conveys viewpoints that Bieber's music may sound a bit down, showcasing a tendency towards \"*depression*\", and also indicates that there are many people more successful than him, reflecting behaviors like an \"*anger*\" facet.\n\n# Related Work\n\n#### **Personality Research in NLP and LLMs.**\n\nNatural language is a rich source of information for inferring various aspects of an individual's personality traits. As such, NLP techniques have been instrumental in personality-related studies [@wen2023desprompt; @DBLP:journals/corr/abs-1907-06333; @DBLP:conf/emnlp/FlekovaG15; @DBLP:conf/emnlp/YangYQS21]. The seminal work by [@pennebaker1999linguistic] utilizes NLP to analyze essays, sparking subsequent research in the social network domain [@schwartz2013personality; @you_are_what_you_talk]. With the increasing capabilities of LLMs, recent studies [@miotto-etal-2022-gpt; @characterchat; @DBLP:journals/corr/abs-2305-02547; @do_llms_posess_a_personality] have examined personality within LLMs. [@DBLP:journals/corr/abs-2212-10529] evaluates GPT-3 from a psychological perspective.  [@personality_traits_in_llms] present a comprehensive psychometric test to analyze the LLMs' personality traits. Quite a few works [@MPI; @characterchat; @personality_traits_in_llms; @DBLP:conf/emnlp/CaronS23] attempt to shape the personality of LLMs, but they all use fixed persona prompt to make the model express the corresponding personality in a general level. Our proposed task aims to edit the personality traits of an LLM in finer-grained, i.e. when expressing opinions on certain topics.\n\n#### **Model Editing.**\n\nA variety of recent works have been focused on addressing the issue of outdated knowledge within LLMs, contributing to the growing field of model editing [@MEND; @rome; @serac; @IKE; @memit; @EasyEdit; @Grace]. [@MEND] introduces a hypernetwork trained to generate weight updates by transforming raw fine-tuning gradients based on a given edit fact. Previous works [@DBLP:journals/corr/abs-2312-05497] mainly focus on factual knowledge within LLMs, encompassing areas such as knowledge editing, counterfactual editing, and fact-checking. The ConvSent dataset [@serac] is the only known work that concentrates on model behavior, albeit limited to the simple editing of positive and negative sentiments. Our benchmark extends this work by aiming to edit model behavior according to different personalities at a finer-grained level.\n\n#### **Text Style Transfer.**\n\nThe term \"style\" encompasses various attributes in the text, including formality [@DBLP:conf/acl/LiuWO22], politeness [@DBLP:conf/acl/MadaanSPPNYSBP20], and other linguistic aspects, along with content preferences like emotions [@DBLP:conf/acl-socialnlp/HelbigTK20]. Text style transfer generally involves transforming a source text to a target text that conveys the same content but in a different style. However, our proposed task centers on modifying the model's personality specific to a topic, leading to the generation of text content in an array of distinct styles, and gaining a modified and customized model.\n\n# Conclusion\n\nIn this paper, we propose a new task of editing personality for LLMs, which involves editing the personality traits exhibited by LLMs when they express viewpoints on specific topics. We further conduct experiments using previous model editing methods, demonstrating the difficulty of the proposed task.\n\n# Acknowledgment {#acknowledgment .unnumbered}\n\nThis work was supported by the National Natural Science Foundation of China (No.62206246), Zhejiang Provincial Natural Science Foundation of China (No. LGG22F030011), Ningbo Natural Science Foundation (2021J190), Yongjiang Talent Introduction Programme (2021A-156-G), CCF-Tencent Rhino-Bird Open Research Fund.",
    "rationale": "Summary: This paper introduces a task focused on editing the personality traits of Large Language Models (LLMs) and developing datasets, and metrics.\n\nStrengths: 1. The proposed framework introduces a nuanced personality framework, allowing for a more granular exploration of personality behavior in the edited model.\n2. The dataset construction process is well-defined and includes multiple stages, ensuring the quality and relevance of the benchmark dataset.\n3. The selection of personality traits and facets is based on clear criteria, ensuring the distinctiveness and clarity of the expressed viewpoints.\n4. The experiments are well-designed, utilizing state-of-the-art large language models and proposing new metrics to measure personality traits in the generated text.\n\nWeaknesses: 1. The paper does not discuss the limitations of the proposed framework and dataset construction process.\n2. The definition of new metrics is confusing. Symbolic definition might be helpful in clarifying.\n\nQuestions: 1. How does the proposed personality editing task compare to the existing knowledge editing tasks in terms of challenges, except for data collection?\n2. What are the potential applications of the proposed framework in real-world scenarios?\n3. What are the potential ethical considerations and challenges associated with personality editing in text generation models?",
    "rating": 3,
    "label": true,
    "rationale_edited": "This paper introduces a task focused on editing the personality traits of Large Language Models (LLMs) and developing datasets, and metrics.\n\nThe proposed framework introduces a nuanced personality framework, allowing for a more granular exploration of personality behavior in the edited model.\nThe dataset construction process is well-defined and includes multiple stages, ensuring the quality and relevance of the benchmark dataset.\nThe selection of personality traits and facets is based on clear criteria, ensuring the distinctiveness and clarity of the expressed viewpoints.\nThe experiments are well-designed, utilizing state-of-the-art large language models and proposing new metrics to measure personality traits in the generated text.\n\nThe paper does not discuss the limitations of the proposed framework and dataset construction process."
  },
  {
    "title": "Behind the Myth of Exploration in Policy Gradients",
    "abstract": "Policy-gradient algorithms are effective reinforcement learning methods for solving control problems with continuous state and action spaces. To compute near-optimal policies, it is essential in practice to include exploration terms in the learning objective. Although the effectiveness of these terms is usually justified by an intrinsic need to explore environments, we propose a novel analysis and distinguish two different implications of these techniques. First, they enable a smooth learning objective and eliminate local optima while preserving the global maximum. Second, they modify the gradient estimates, increasing the probability that the stochastic parameter update eventually provides an optimal policy. In light of these effects, we discuss and illustrate empirically exploration strategies based on entropy bonuses, highlighting their limitations and opening avenues for future works in the design and analysis of such strategies.",
    "text": "# Introduction {#sec:introduction}\n\nMany practical problems require making sequential decisions in environments. Reinforcement learning (RL) is a framework for solving such decision-making problems that has been successful on complex tasks, including playing games [@mnih2015human; @silver2017mastering], operating power systems [@aittahar2014optimal], controlling robots [@kalashnikov2018qt], or interacting with electricity markets [@boukas2021deep].\n\nReinforcement learning algorithms interact with an environment to gather information about this environment, which in turn enables to compute and follow an optimal policy. This creates a trade-off between exploration and exploitation. In short, in order to eventually compute a good policy, it is necessary to obtain additional information about the environment by taking actions that are likely not optimal. In algorithms where the trade-off is explicit, exploration is well-understood and has been the subject of many works [@dann2017unifying; @azar2017minimax; @neu2020unifying]. In policy-gradient algorithms, one can most often not distinguish exploration from exploitation. Nevertheless, a main theoretical requirement to converge towards globally (or even locally) optimal solutions is that policies remain sufficiently stochastic during the learning procedure [@bhandari2019global; @bhatt2019policy; @agarwal2020optimality; @zhang2021sample; @bedi2022hidden]. Interestingly, neither softmax nor Gaussian policies guarantee enough stochasticity for ensuring (fast) convergence [@mei2020escaping; @mei2021understanding; @bedi2022hidden]. This requirement of stochasticity in policy-gradient algorithms is often abusively called exploration and understood as the need to infinitely sample all states and actions.\n\nPractitioners have tried to meet the theoretical requirement of sufficient randomness of policies in policy gradient via reward-shaping strategies, whereby a learning objective that promotes or hinders behaviors by providing reward bonuses for some states and actions is optimized as a surrogate to the return of the policy. These bonuses typically promote actions that reduce the uncertainty of the agent about its environment [@pathak2017curiosity; @burda2018large; @zhang2021noveld], or that maximize the entropy of states and/or actions [@williams1991function; @bellemare2016unifying; @haarnoja2019soft; @hazan2019provably; @lee2019efficient; @islam2019marginalized; @guo2021geometric; @zhang2021made]. Optimizing a surrogate objective is particularly effective for solving tasks with complex dynamics and reward functions, or with sparse rewards.\n\nThe differences between theory and practical implementations of exploration has led to common folklore seeking to provide intuition for the efficiency of policy-gradient methods. This work is part of the research line that studies the maximization of practical surrogate learning objective functions from a mathematical optimization perspective. Close to our work, studies of the learning objective with entropy regularization (an exploration-based reward shaping technique where the entropy of the policy is added in the learning objective) were conducted. It includes the study by @ahmed2019understanding concluding that it helps to provide smooth learning objective functions. The same exploration strategy was reinterpreted as a robust optimization method by @husain2021regularized and equivalently as a two-player game by @brekelmans2022your. @bolland2023policy furthermore argued that optimizing an entropy regularized objective is equivalent to optimizing the return of another policy with larger variance. More general studies on the learning dynamics have focused on the influence of baselines in policy gradient [@chung2021beyond], and reward-shaping strategies that do not modify the learning objective, called potential based [@ng1999policy; @wiewiora2003principled; @harutyunyan2015expressing; @forbes2024potential]. All these studies are too restrictive and the literature lacks unified explanations and interpretations about exploration in policy gradients.\n\nBefore delving into our contributions, we recall that the convergence of stochastic ascent methods is driven by the objective function and how the ascent directions are estimated. First, the objective function shall be (pseudo) concave to find its global maximum [@leon1998online]. Second, the convergence rate is influenced by the distribution of the stochastic ascent estimates [@chen2018stochastic; @ajalloeian2020convergence]. In this paper, we rigorously study policy-gradient methods with exploration-based reward shaping through the lens of these two optimization theory aspects. To that end, we first introduce two new criteria that relate the return of a policy to the learning objective with exploration bonuses, and their respective optima. Second, we introduce two additional criteria on the distribution of the gradient estimates of the learning objective and their likelihood of providing directions in which the learning objective and the return increase. Importantly, these criteria are general to any reward-shaping strategy, and highlight the importance of reward shaping that modify the optimal control behavior, in opposition to the literature on potential-based reward shaping. The influence of some exploration bonuses are illustrated and discussed in the light of these four criteria. In practice, finding good exploration strategies is problem specific and we thus introduce a general framework for the study and interpretation of exploration in policy-gradient methods instead of trying to find the best exploration method for a given task.\n\nThe paper is organized as follows. In Section [2](#sec:background){reference-type=\"ref\" reference=\"sec:background\"}, we provide the background about policy gradients and exploration. Section [3](#sec:learning_objective){reference-type=\"ref\" reference=\"sec:learning_objective\"} focuses on the effect of exploration on the learning objective while Section [4](#sec:ascent_direction){reference-type=\"ref\" reference=\"sec:ascent_direction\"} is dedicated to the effect on the gradient estimates used in the policy-gradient algorithms[^1]. Conclusions and future works are discussed in Section [5](#sec:conclusion){reference-type=\"ref\" reference=\"sec:conclusion\"}.\n\n# Background {#sec:background}\n\nIn this section, we introduce Markov decision processes and policy gradients with intrinsic exploration.\n\n## Markov Decision Processes\n\nWe study problems in which an agent makes sequential decisions in a stochastic environment [@sutton2018reinforcement]. The environment is modeled with an infinite-time Markov decision process (MDP) composed of a state space $\\mathcal{S}$, an action space $\\mathcal{A}$, an initial state distribution with density $p_0$, a transition distribution (modeling the dynamics) with conditional density $p$, a bounded reward function $\\rho$, and a discount factor $\\gamma \\in [0, 1)$. When an agent interacts with the MDP, first, an initial state $s_0 \\sim p_0(\\cdot)$ is sampled, then, the agent provides at each time step $t$ an action $a_t \\in \\mathcal{A}$ leading to a new state $s_{t+1} \\sim p(\\cdot|s_t, a_t)$. Such a sequence of states and actions $h_t = (s_0, a_0, \\dots, s_{t-1}, a_{t-1}, s_t) \\in \\mathcal{H}$ is called a history and $\\mathcal{H}$ is the set of all histories of any arbitrary length. In addition, after an action $a_t$ is executed, a reward $r_{t} = \\rho(s_t, a_t) \\in \\mathbb{R}$ is observed.\n\nA policy $\\pi \\in \\Pi = \\mathcal{S} \\rightarrow \\mathcal{P}(\\mathcal{A})$ is a mapping from the state space $\\mathcal{S}$ to the set of probability measures on the action space $\\mathcal{P}(\\mathcal{A})$, where $\\pi(a|s)$ is the associated conditional probability density of action $a$ in state $s$. The function $J:\\Pi \\rightarrow \\mathbb{R}$ is defined as the function mapping any policy $\\pi$ to the expected discounted sum of rewards gathered by an agent interacting in the MDP by sampling actions from the policy $\\pi$. We call return of the policy $\\pi$ the value provided by that function $$\\begin{aligned}\n    J(\\pi)\n    = \\frac{1}{1 - \\gamma} \\underset{\n    \\begin{subarray}{c}\n    s  \\sim d^{\\pi, \\gamma}(\\cdot) \\\\\n    a \\sim \\pi(\\cdot|s) \n    \\end{subarray}}{\\mathbb{E}} \\left [ \\rho(s, a) \\right ] \\; , \\label{eq:def_j}\n\\end{aligned}$$ where $d^{\\pi, \\gamma}(\\cdot)$ is the discounted state-visitation probability [@manne1960linear]. In reinforcement learning, we seek to find an optimal policy $\\pi^*$ maximizing the expected discounted sum of rewards $J$.\n\n## Policy-Gradient Algorithms\n\nPolicy-gradient algorithms (locally) optimize a parameterized policy $\\pi_\\theta$ to find the optimal parameter $\\theta^*$ for which the return of the policy $J(\\pi_{\\theta^*})$ is maximized. Naively maximizing the return may provide sub-optimal results. This problem is mitigated in practice with exploration strategies, which consist in optimizing a surrogate learning objective $L$ that intrinsically encourages certain behaviors. In this work, we consider reward-shaping strategies where the expected discounted sum of rewards is extended by $K$ additional reward terms $\\rho_i^{int}$, called intrinsic motivation terms, and optimize the learning objective $$\\begin{aligned}\n    L(\\theta)\n    &= \\frac{1}{1 - \\gamma}  \\underset{\n    \\begin{subarray}{c}\n    s  \\sim d^{\\pi_\\theta, \\gamma}(\\cdot) \\\\\n    a \\sim \\pi_\\theta(\\cdot|s) \n    \\end{subarray}}{\\mathbb{E}} \\left [ \\rho(s, a) + \\sum_{i=0}^{K-1} \\lambda_i \\rho_i^{int}(s, a) \\right ] \\nonumber \\\\\n    &= J(\\pi_\\theta) + J^{int}(\\pi_\\theta) \\label{eq:objective_rl} \\; ,\n\\end{aligned}$$ where $\\lambda_i$ are non-negative weights for each intrinsic reward and where $J^{int}(\\pi_\\theta)$ is the intrinsic return of the policy. The parameter maximizing the learning objective is denoted by $\\theta^\\dagger$, which we distinguish from the optimal policy parameter $\\theta^*$. Most of the intrinsic motivation terms can be classified in the two following groups.\n\n**Uncertainty-based motivations.** It is common to provide bonuses for performing actions that reduce the uncertainty of the agent about its environment [@pathak2017curiosity; @burda2018large; @zhang2021noveld]. The intrinsic motivation terms are then proportional to the prediction errors of a model of the MDP dynamics. The latter model is usually learned.\n\n**Entropy-based motivations.** It is also common to provide bonuses for visiting states and/or playing actions that are less likely in histories [@haarnoja2019soft; @hazan2019provably]. In this work, we focus on two of these bonuses $$\\begin{aligned}\n    \\rho^{s}(s, a) &= - \\log d^{\\pi_\\theta, \\gamma}(\\phi(s)) \\label{eq:int_rew_h} \\\\\n    \\rho^{a}(s, a) &= - \\log \\pi_\\theta(a| s) \\label{eq:int_rew_pi} \\; ,\n\\end{aligned}$$ where $\\phi(s)$ is a feature built from the state $s$. The corresponding intrinsic returns are maximized for policies that visit uniformly every feature, and for policies with uniformly distributed actions in each state, respectively. Note that these rewards require to estimate the distribution over the states and/or actions. Furthermore, they implicitly depend on the policy parameter $\\theta$. The second technique is usually referred to as entropy regularization.\n\nIn this work, we consider on-policy policy-gradient algorithms, which were among others reviewed by [@duan2016benchmarking] and [@andrychowicz2020matters]. These algorithms optimize differentiable parameterized policies with gradient-based local optimization. They iteratively approximate an ascent direction $\\hat d$ relying on histories sampled from the policy in the MDP and update the parameters in the ascent direction, or in a combination of the previous ascent directions [@hinton2012neural; @kingma2014adam]. For the sake of simplicity and without loss of generality, we consider that the ascent direction $\\hat d$ is composed of the sum of an estimate of the gradient of the return $\\hat g \\approx \\nabla_\\theta J(\\pi_\\theta)$ and an estimate of the gradient of the intrinsic return $\\hat i \\approx \\nabla_\\theta J^{int}(\\pi_\\theta)$. In practice, the first is usually unbiased while the second is computed neglecting some partial derivatives of $\\theta$ and is thus biased, typically neglecting the influence of the policy on the intrinsic reward.\n\n# Study of the Learning Objective {#sec:learning_objective}\n\nIn this section, we study the influence of the exploration terms on the learning objective defined in equation [\\[eq:objective_rl\\]](#eq:objective_rl){reference-type=\"eqref\" reference=\"eq:objective_rl\"}. We define two criteria under which the learning objective can be globally optimized by ascent methods, and such that the solution is close to an optimal policy. We then graphically illustrate how exploration modifies the learning objective to remove local extrema.\n\n## Policy-Gradient Learning Objective\n\nPolicy-gradient algorithms using exploration maximize the learning objective function $L$, as defined in equation [\\[eq:objective_rl\\]](#eq:objective_rl){reference-type=\"eqref\" reference=\"eq:objective_rl\"}. We introduce two criteria related to this learning objective for studying the performance of the policy-gradient algorithm. First, we say that a learning objective $L$ is $\\epsilon$-coherent when its global maximum is in an $\\epsilon$-neighborhood of the return of an optimal policy. Second, we call learning objectives that have a unique maximum and no other stationary point pseudoconcave.\n\n**Coherence criterion.** A learning objective $L$ is $\\epsilon$-coherent if, and only if, $$\\begin{aligned}\n    J(\\pi_{\\theta^*}) - J(\\pi_{\\theta^\\dagger})\\leq \\epsilon \\; ,\n\\end{aligned}$$ where $\\theta^* \\in \\text{argmax}_\\theta J(\\pi_\\theta)$ and where $\\theta^\\dagger \\in \\text{argmax}_\\theta L(\\theta)$.\n\n**Pseudoconcavity criterion.** A learning objective $L$ is pseudoconcave if, and only if, $$\\begin{aligned}\n    \\exists!\\: \\theta^\\dagger : \\nabla L(\\theta^\\dagger) = 0 \\land L(\\theta^\\dagger) = \\max_\\theta L(\\theta) \\; .\n\\end{aligned}$$ If the pseudoconcavity criterion is respected, there is a single optimum, and it is thus possible to globally optimize the learning objective function by (stochastic) gradient ascent [@bottou2010large][^2]. If the learning objective is furthermore $\\epsilon$-coherent, the latter solution is also a near-optimal policy, where $\\epsilon$ is the bound on the suboptimality of its return.\n\nLet us finally remind a theorem from @ng1999policy.\n\n**Consistency Theorem.** The learning objective $L$ is $\\epsilon$-coherent, with $\\epsilon = 0$, in any MDP with state space $\\mathcal{S}$, action space $\\mathcal{A}$ and factor $\\gamma$, if, and only if, $J(\\theta) = L(\\theta)$ for all $\\theta$. The intrinsic rewards are furthermore potential based.\n\nThis theorem states that there is no MDP-agnostic exploration method that guarantees consistency with $\\epsilon$ equal to zero and that modifies the objective function. This type of exploration is only possible with potential-based reward shaping [@ng1999policy]. In conclusion, if the return is not pseudoconcave, there is a trade-off between the two criteria, which can not be resolved by potential-based exploration.\n\n## Illustration of the Effect of Exploration on the Learning Objective\n\nExploration is of paramount importance in environments with complex dynamics and reward functions, where many locally optimal policies may exist [@lee2019efficient; @liu2021behavior; @zhang2021made]. In the following, we first define such an environment and a policy parameterization that will serve as an example to illustrate the effect of exploration on the optimization process. For the sake of the analysis, we then represent the learning objectives associated to different exploration strategies, and depict their global and local optima. Learning objectives with a single optimum respect the pseudoconcavity criterion. In addition, we represent the neighborhood $\\Omega$ of the optimal policy parameters, such that any learning objective with its global maximum within this region is coherent for a given $\\epsilon$. In light of the coherence and the pseudoconcavity criteria, we finally elaborate on the policy parameter computed by stochastic gradient ascent algorithms.\n\nWe consider the environment illustrated in Figure [1](#fig:hill_env){reference-type=\"ref\" reference=\"fig:hill_env\"} where a car moves in a valley [@bolland2023policy]. We denote by $x$ and $v$ the position and speed of the car, both composing its state $s=(x, v)$. The valley contains two separate low points, positioned in $x_{initial}=-3$ and $x_{target}=3$, separated by a peak. The car starts at rest $v_0=0$ at the highest low point $x_0=x_{initial}$ and receives rewards proportional to the depth of the valley at its current position. The reward function is provided in Figure [2](#fig:hill_reward){reference-type=\"ref\" reference=\"fig:hill_reward\"}. We consider a policy $\\pi_{K, \\sigma}(a| s) = \\mathcal{N}(a| \\mu_K(s), \\sigma)$, namely a normally disturbed proportional controller with $\\mu_K(s) = K\\times (x - x_{target})$, parameterized by the vector $\\theta=(K, \\sigma)$. Figure [4](#fig:hill_return_pcontrollers){reference-type=\"ref\" reference=\"fig:hill_return_pcontrollers\"} illustrates the contour map of the return of the policy as a function of the parameters $K$ and $\\sigma$. The optimal parameters are represented by black dots and correspond to policies that drive the car to pass the peak and reach the lowest valley point in $x_{target}$. The green area represents the set of parameters $\\Omega = \\left \\{ \\theta' | \\max_\\theta J(\\pi_\\theta) - J(\\pi_{\\theta'})\\leq \\epsilon \\right \\}$ for $\\epsilon = 1$.\n\n<figure id=\"fig:hill_return_pcontrollers\">\n<figure id=\"fig:hill_env\">\n<embed src=\"figures/hill_picture.pdf\" />\n<figcaption>Hill environment.</figcaption>\n</figure>\n<figure id=\"fig:hill_reward\">\n<embed src=\"figures/hill.pdf\" />\n<figcaption>Reward function <span class=\"math inline\"><em>ρ</em></span>.</figcaption>\n</figure>\n<figure id=\"fig:hill_return_pcontrollers\">\n<embed src=\"figures/return_reward_shaping_contour/return_contour_1.0_0.0_0.0_0.0.pdf\" />\n<figcaption>Return <span class=\"math inline\"><em>J</em>(<em>π</em><sub><em>K</em>, <em>σ</em></sub>)</span>.</figcaption>\n</figure>\n<figcaption>Illustration of the <em>hill environment</em> in Figure <a href=\"#fig:hill_env\" data-reference-type=\"ref\" data-reference=\"fig:hill_env\">1</a> and its reward function in Figure <a href=\"#fig:hill_reward\" data-reference-type=\"ref\" data-reference=\"fig:hill_reward\">2</a>. In Figure <a href=\"#fig:hill_return_pcontrollers\" data-reference-type=\"ref\" data-reference=\"fig:hill_return_pcontrollers\">4</a>, the return of the policy <span class=\"math inline\"><em>π</em><sub><em>K</em>, <em>σ</em></sub></span> with the global and local maximum represented in black and grey, together with their respective return values.</figcaption>\n</figure>\n\nFigure [11](#fig:return_int){reference-type=\"ref\" reference=\"fig:return_int\"} illustrates the learning objective with the intrinsic rewards $\\rho^s(s, a) = -\\log d^{\\pi_{K, \\sigma}, \\gamma}(\\phi(s))$, from equation [\\[eq:int_rew_h\\]](#eq:int_rew_h){reference-type=\"eqref\" reference=\"eq:int_rew_h\"}, and $\\rho^a(s, a) = - \\log \\pi_{K, \\sigma}(a|s)$, from equation [\\[eq:int_rew_pi\\]](#eq:int_rew_pi){reference-type=\"eqref\" reference=\"eq:int_rew_pi\"}, for different values of the corresponding weights $\\lambda_s$ and $\\lambda_a$. Here, the feature is the position in the valley $\\phi(s) = x$. First, we observe that for weights approaching zero, the parameter $\\theta^\\dagger$ maximizing the learning objective, represented by a black dot, corresponds to a policy with a high return. More precisely, it is in the green set $\\Omega$ such that $\\epsilon$-coherence is guaranteed for the value $\\epsilon=1$. Larger weights require larger values of $\\epsilon$ for guaranteeing the $\\epsilon$-coherence criterion. Nevertheless, when increasing the weights, we observe that the learning objective eventually becomes pseudoconcave. There appears to be a trade-off between the two criteria. In Figure [6](#fig:return_int_010){reference-type=\"ref\" reference=\"fig:return_int_010\"}, we observe that in this environment, there is a learning objective that respects the pseudoconcavity criterion and the $\\epsilon$-coherence criterion for $\\epsilon=1$. Indeed, there is a single global maximum in Figure [6](#fig:return_int_010){reference-type=\"ref\" reference=\"fig:return_int_010\"} represented by a black dot that is furthermore part of the set $\\Omega$.\n\nShaping the reward function with an exploration strategy based on the state-visitation entropy appears to be a good solution for optimizing the policy. However, a notable drawback is that the reward depends on the policy and its (gradient) computation requires to estimate a complex probability measure. In this example, the intrinsic reward function itself was estimated by Monte-Carlo sampling for every parameter, which would not scale for complex problems and requires approximations and costly evaluation strategies [@islam2019marginalized]. In Appendix [6](#apx:reward_shaping){reference-type=\"ref\" reference=\"apx:reward_shaping\"} we present an alternative problem-dependent intrinsic reward, independent of the policy parameters and thus simple to compute efficiently, that still respects the pseudoconcavity and $\\epsilon$-coherence criteria, and in Appendix [7](#apx:minigrid_experiments){reference-type=\"ref\" reference=\"apx:minigrid_experiments\"} we extend the study to more complex environments from the MiniGrid library [@MinigridMiniworld23] where the policy is a deep neural network and the state-visitation probability is approximated.\n\n<figure id=\"fig:return_int\">\n<figure id=\"fig:return_int_005\">\n<embed src=\"figures/return_reward_shaping_contour/return_contour_1.0_0.05_0.0_0.0.pdf\" />\n<figcaption><span class=\"math inline\"><em>λ</em><sub><em>s</em></sub> = 0.05</span> and <span class=\"math inline\"><em>λ</em><sub><em>a</em></sub> = 0</span></figcaption>\n</figure>\n<figure id=\"fig:return_int_010\">\n<embed src=\"figures/return_reward_shaping_contour/return_contour_1.0_0.1_0.0_0.0.pdf\" />\n<figcaption><span class=\"math inline\"><em>λ</em><sub><em>s</em></sub> = 0.1</span> and <span class=\"math inline\"><em>λ</em><sub><em>a</em></sub> = 0</span></figcaption>\n</figure>\n<figure id=\"fig:return_int_100\">\n<embed src=\"figures/return_reward_shaping_contour/return_contour_1.0_1.0_0.0_0.0.pdf\" />\n<figcaption><span class=\"math inline\"><em>λ</em><sub><em>s</em></sub> = 1</span> and <span class=\"math inline\"><em>λ</em><sub><em>a</em></sub> = 0</span></figcaption>\n</figure>\n<figure id=\"fig:return_regul_001\">\n<embed src=\"figures/return_reward_shaping_contour/return_contour_1.0_0.0_0.0_0.01.pdf\" />\n<figcaption><span class=\"math inline\"><em>λ</em><sub><em>s</em></sub> = 0</span> and <span class=\"math inline\"><em>λ</em><sub><em>a</em></sub> = 0.01</span></figcaption>\n</figure>\n<figure id=\"fig:return_regul_010\">\n<embed src=\"figures/return_reward_shaping_contour/return_contour_1.0_0.0_0.0_0.1.pdf\" />\n<figcaption><span class=\"math inline\"><em>λ</em><sub><em>s</em></sub> = 0</span> and <span class=\"math inline\"><em>λ</em><sub><em>a</em></sub> = 0.1</span></figcaption>\n</figure>\n<figure id=\"fig:return_regul_050\">\n<embed src=\"figures/return_reward_shaping_contour/return_contour_1.0_0.0_0.0_0.5.pdf\" />\n<figcaption><span class=\"math inline\"><em>λ</em><sub><em>s</em></sub> = 0</span> and <span class=\"math inline\"><em>λ</em><sub><em>a</em></sub> = 0.5</span></figcaption>\n</figure>\n<figcaption>Contour map of (scaled) learning objective functions for different values of <span class=\"math inline\"><em>λ</em><sub><em>s</em></sub></span> and <span class=\"math inline\"><em>λ</em><sub><em>a</em></sub></span>. The darker the map, the larger the learning objective value. The green area represents the set <span class=\"math inline\"><em>Ω</em> = {<em>θ</em>′|max<sub><em>θ</em></sub><em>J</em>(<em>π</em><sub><em>θ</em></sub>) − <em>J</em>(<em>π</em><sub><em>θ</em>′</sub>) ≤ <em>ϵ</em> = 1}</span>, such that when the parameter maximizing the learning objective is part of <span class=\"math inline\"><em>Ω</em></span>, then the learning objective function is <span class=\"math inline\"><em>ϵ</em></span>-coherent with <span class=\"math inline\"><em>ϵ</em> = 1</span>. The black dot is the parameter <span class=\"math inline\"><em>θ</em><sup>†</sup></span> globally maximizing the learning objective and the grey dot is the local (non-global) maximum of the learning objective if it exists. Both are labeled with the return values of the corresponding policies.</figcaption>\n</figure>\n\nThe observations suggest that well-chosen exploration strategies can lead to learning objective functions that satisfy the two criteria defined in the previous section, thereby guaranteeing that policies suboptimal by at most $\\epsilon$ can be computed by local optimization. When designing exploration strategies, it is essential to keep in mind that we modify the learning objective for the algorithms to converge to optimal policy parameters, which can be achieved when both criteria are respected. While strategies such as enforcing entropy can be effective in some environments, they are only heuristic strategies and not to be relied upon exclusively. Furthermore, as illustrated, both criteria may be subject to a trade-off. In more complex environments, an efficient exploration strategy may require to balance both criteria, e.g., through a schedule on the learning objective weights.\n\n# Study of the Ascent Direction Distribution {#sec:ascent_direction}\n\nOptimizing pseudoconcave functions with stochastic ascent methods are guaranteed to converge (at a certain rate) under assumptions on the distribution of the gradient estimates at hand [@bottou2010large; @chen2018stochastic; @ajalloeian2020convergence]. In this section, we study the influence of the exploration terms on this distribution in the context of policy gradients. More precisely, we study the probability of improving the learning objective and the return with stochastic ascent steps. Intuitively, they shall be sufficiently large for the algorithm to be efficient. We formalize this intuition and illustrate how exploration strategies can increase these probabilities, leading to more efficient algorithms.\n\n## Policy-Gradient Estimated Ascent Direction\n\nIn general, gradient ascent algorithms update parameters in a direction $\\hat d$ in order to locally improve an objective function $f$. The quality of these algorithms can therefore be studied (for a small step size $\\alpha \\rightarrow 0$) through the random variable representing the quantity by which the objective increases for each $\\theta$ $$\\begin{aligned}\n    X &= f(\\theta + \\alpha \\hat d) - f(\\theta)\n    = \\alpha \\: \\langle \\hat d, \\nabla_\\theta f(\\theta) \\rangle \\: , \\label{eq:rv_local_search}\n\\end{aligned}$$ where $\\langle \\cdot, \\cdot \\rangle$ is the Euclidean scalar product. This variable depends on the random event $\\hat d$ estimated by Monte-Carlo simulations in practice.\n\nThe (asymptotic) convergence of a gradient ascent algorithm is usually studied by bounding the expectation of $X$. Such bounds depend, among others, on the expectation of $\\hat d$, which equals $\\nabla_\\theta f(\\theta)$ when unbiased, and depend on the variance of $\\hat d$, which deteriorates the expected convergence rate. Finding rates depending on the algorithm is an active field of research. In parallel, a slightly more general problem is to quantify if the expectation of $X$ is driven by rare events. Intuitively, an algorithm with unbiased gradient estimates has positive expected improvements $\\mathbb{E} [ X ] > 0$, and should theoretically converge, but may be inefficient in practice if positive events $X > 0$ rarely occur. We illustrate in the next section that this phenomenon makes reinforcement learning in sparse-reward environments particularly hard. To the best of our knowledge, no existing result fits to the study of policy gradients. We therefore introduce two new criteria on the probability of improvement $P(X > 0)$, which we empirically validate afterwards.\n\nFirst, we define an exploration strategy as $\\delta$-efficient if, and only if, following the ascent direction $\\hat d \\approx \\nabla_\\theta L(\\theta)$ has a probability at least $\\delta$ to increasing the learning objective $L(\\theta)$ almost everywhere. Second, an exploration strategy is $\\delta$-attractive if, and only if, there exists a neighborhood of $\\theta^\\dagger$ containing the parameter $\\theta^{int}$ maximizing the intrinsic return $J^{int}$, where the probability of increasing the return by following **$\\hat d$** is almost everywhere at least equal to $\\delta$. Note that each probability measure and random variable is a function of $\\theta$, which we do not explicitly write for the sake of keeping notations simple.\n\n**Efficiency criterion.** An exploration strategy is $\\delta$-efficient if, and only if, $$\\begin{aligned}\n    \\forall^\\infty \\theta : \\mathbb{P}(D > 0) \\geq \\delta \\: ,\n\\end{aligned}$$ where $D = \\: \\langle \\hat d, \\nabla_\\theta L(\\theta) \\rangle$.\n\n**Attraction criterion.** An exploration strategy is $\\delta$-attractive if, and only if, $$\\begin{aligned}\n    \\exists B(\\theta^\\dagger) &: \\theta^{int} \\in B(\\theta^\\dagger) \\: ,\n\\end{aligned}$$ such that $$\\begin{aligned}\n    \\forall^\\infty \\theta \\in B(\\theta^\\dagger) : \\mathbb{P}(G > 0) \\geq \\delta \\: ,\n\\end{aligned}$$ where $\\theta^{int} = \\text{argmax}_\\theta J^{int}(\\pi_\\theta)$, $B(\\theta^\\dagger)$ is a ball centered in $\\theta^\\dagger$, and $G = \\: \\langle \\hat d, \\nabla_\\theta J(\\pi_\\theta) \\rangle$.\n\nThe efficiency criterion quantifies how often a stochastic gradient ascent step improves the learning objective. The larger, the better the learning objective and its stochastic ascent direction approximations. The rationale behind the attraction criterion is that in many exploration strategies, the intrinsic reward is dense, and it is then presumably easy to optimize the intrinsic return in the sense that $\\mathbb{P}(\\: \\langle \\hat i, \\nabla_\\theta J^{int}(\\pi_\\theta) \\rangle > 0)$ is large. It implies that it is easy to locally improve the learning objective by (solely) increasing the value of the intrinsic motivation terms. It furthermore implies that policy-gradient algorithms may be subject to converging towards $\\theta^{int}$ rather than $\\theta^\\dagger$ when $\\mathbb{P}(\\:  \\langle \\hat d, \\nabla_\\theta J(\\pi_\\theta) \\rangle > 0)$ is small. If the criterion is respected for large $\\delta$, the latter is less likely to happen as policy gradients will eventually tend to improve the return of the policy if the parameter approaches $\\theta^{int}$ and enters the ball $B(\\theta^\\dagger)$; eventually converging towards $\\theta^\\dagger$.\n\nThese two new criteria on $\\hat d$ are independent of the previous ones on $L$, which only captured the quality of the deterministic learning objective functions. In the particular cases where the learning objectives $L$ are $\\epsilon$-coherent, for $\\epsilon=0$, and pseudoconcave, e.g., with potential-based intrinsic rewards, only the distribution of estimates $\\hat d$ can explain why some algorithms succeed and others fail. Finally, the value of $\\delta$ in the new criteria we introduce can be related to the variance of the estimate $\\hat d$ under some assumptions, e.g., with Cantelli's concentration inequalities.\n\n## Illustration of the Effect of Exploration on the Estimated Ascent Direction\n\nExploration is usually promoted and tested for problems where the reward function is sparse, typically in maze-environments [@islam2019marginalized; @liu2021behavior; @guo2021geometric]. In this section, we first introduce a new maze-environment with sparse rewards where we illustrate the influence of exploration on the gradient estimates of the learning objective. To this end, we present two learning objective functions and elaborate on the influence of exploration on the performance of policy-gradient algorithms in the light of the efficiency and attraction criteria.\n\nLet us consider a maze-environment consisting of a horizontal corridor composed of $S \\in \\mathbb{N}$ tiles. The state of the environment is the index of the tile $s \\in \\left \\{ 1, \\dots, S \\right \\}$, and the actions consist in going left $a=-1$ or right $a=+1$. When an action is taken, the agent stays idle with probability $p=0.7$, and moves with probability $1-p = 0.3$ in the direction indicated by the action, then $s' = \\min(S, \\max(1, s + a))$. The agent starts in state $s = 1$ and the target state $s = S = 15$ is absorbing. Zero rewards are observed except when the agent reaches the target state where a reward $r=100$ is observed. A discount factor of $\\gamma=0.99$ is considered. Finally, we study the policy going with probability $\\theta$ to the right and probability $1-\\theta$ to the left, and with density $$\\begin{aligned}\n    \\pi_\\theta(a|s) =\n    \\left\\{ \n        \\begin{array}{cl}\n        \\theta      & \\quad \\textrm{if } a = 1 \\\\\n        1 - \\theta  & \\quad \\textrm{if } a = -1 \\; .\n        \\end{array}\n    \\right.\n\\end{aligned}$$\n\nThe return $J(\\pi_\\theta)$ is represented in black in Figure [12](#fig:maze_return){reference-type=\"ref\" reference=\"fig:maze_return\"} as a function of $\\theta$ along with two intrinsic returns, $J^a(\\pi_\\theta)$ in orange and $J^s(\\pi_\\theta)$ in blue. The intrinsic reward $\\rho^{a}(s, a) = - \\log \\pi_\\theta(a| s)$, from equation [\\[eq:int_rew_pi\\]](#eq:int_rew_pi){reference-type=\"eqref\" reference=\"eq:int_rew_pi\"}, and the intrinsic reward $\\rho^{s}(s, a) = - \\log d^{\\pi_\\theta, \\gamma}(s)$, from equation [\\[eq:int_rew_h\\]](#eq:int_rew_h){reference-type=\"eqref\" reference=\"eq:int_rew_h\"}, are used respectively. In Figure [13](#fig:maze_learning){reference-type=\"ref\" reference=\"fig:maze_learning\"}, we illustrate the return of the policy without exploration $J(\\pi_\\theta)$, along with two learning objective functions, $L^a(\\theta)$ and $L^s(\\theta)$, using as exploration strategies the intrinsic returns $J^a(\\pi_\\theta)$ and $J^s(\\pi_\\theta)$. We observe that the return is a pseudoconcave function with respect to $\\theta$ and the optimal parameter is $\\theta^*=1$. In addition, the two learning objectives respect the $\\epsilon$-coherence criterion for $\\epsilon=0$, implying that $\\theta^* = \\theta^\\dagger$, and respect the pseudoconcavity criterion. It is important to note that with regard to the discussion from Section [3](#sec:learning_objective){reference-type=\"ref\" reference=\"sec:learning_objective\"}, there is no interest in optimizing the learning objectives rather than directly optimizing the return, as the latter is already pseudoconcave. In the following we illustrate how choosing a correct exploration strategy still deeply influences the policy-gradient algorithms when it comes to building gradient estimates.\n\n<figure id=\"fig:maze_positive_direction\">\n<figure id=\"fig:maze_return\">\n<embed src=\"figures/maze_return.pdf\" />\n<figcaption>Return functions.</figcaption>\n</figure>\n<figure id=\"fig:maze_learning\">\n<embed src=\"figures/maze_learning.pdf\" />\n<figcaption>Learning objective functions.</figcaption>\n</figure>\n<figure id=\"fig:maze_positive_direction\">\n<embed src=\"figures/maze_proba_improvement.pdf\" />\n<figcaption>Improvement probabilities.</figcaption>\n</figure>\n<figcaption>Figure <a href=\"#fig:maze_return\" data-reference-type=\"ref\" data-reference=\"fig:maze_return\">12</a> represents the return of the policy along with two intrinsic return functions. In Figure <a href=\"#fig:maze_learning\" data-reference-type=\"ref\" data-reference=\"fig:maze_learning\">13</a> the return is also represented together with two learning objective functions, corresponding to the two intrinsic returns. Figure <a href=\"#fig:maze_positive_direction\" data-reference-type=\"ref\" data-reference=\"fig:maze_positive_direction\">15</a> illustrates the probability (estimated by Monte-Carlo) of positive stochastic gradient (derivative) estimates <span class=\"math inline\"><em>J</em>(<em>π</em><sub><em>θ</em></sub>)</span>, <span class=\"math inline\"><em>L</em><sup><em>a</em></sup>(<em>θ</em>)</span>, and <span class=\"math inline\"><em>L</em><sup><em>s</em></sup>(<em>θ</em>)</span>. At the top of the figure, the intervals <span class=\"math inline\"><em>B</em><sup><em>a</em></sup> = [<em>θ</em><sup><em>i</em><em>n</em><em>t</em>, <em>a</em></sup>, <em>θ</em><sup>†, <em>a</em></sup>]</span> and <span class=\"math inline\"><em>B</em><sup><em>s</em></sup> = [<em>θ</em><sup><em>i</em><em>n</em><em>t</em>, <em>s</em></sup>, <em>θ</em><sup>†, <em>s</em></sup>]</span> are represented. These intervals represent the smallest balls containing the parameters maximizing the intrinsic return and the learning objective, for both exploration strategies.</figcaption>\n</figure>\n\nLet us compute the estimate $\\hat g$ and $\\hat d$ relying on REINFORCE [@williams1992simple] by sampling $8$ histories of length $T = 100$. In this particular environment, $\\mathbb{P}(D > 0)$ equals $\\mathbb{P}(G > 0)$, and equal the probability that the derivative is positive. We represent in Figure [15](#fig:maze_positive_direction){reference-type=\"ref\" reference=\"fig:maze_positive_direction\"} this probability for the return and for both learning objectives. First, we see that the learning objectives are more efficient than the return, meaning they are $\\delta$-efficient for larger values of $\\delta$. Depending on the parameter value, the objective $L^a(\\theta)$ or $L^s(\\theta)$ is best in that regard. Second, concerning the attraction criterion, we represent at the top of Figure [15](#fig:maze_positive_direction){reference-type=\"ref\" reference=\"fig:maze_positive_direction\"} the intervals $B^a = [\\theta^{int, a}, \\theta^{\\dagger, a}]$ and $B^s = [\\theta^{int, s}, \\theta^{\\dagger, s}]$. They correspond to the smallest balls containing the maximizers of the intrinsic return and of the learning objective. Let the minima of the orange and blue curves over these intervals be denoted by $\\delta^a$ and $\\delta^s$. By definition of the attraction criterion, it is thus respected for any values of $\\delta$ at most equal to $\\delta^a$ and $\\delta^s$, for $L^a(\\theta)$ and $L^s(\\theta)$, respectively. All these observations can eventually be explained as the computation of $\\hat g$ is always zero when the target is not sampled in the histories, which is highly likely for policies with small values of $\\theta$. Policy-gradient algorithms relying on intrinsic exploration would compute optimal policies efficiently where naive optimization without exploration would fail or be sample inefficient.\n\nWe have empirically shown that a well-chosen exploration strategy in policy gradients may not only remove local extrema from the objective function, but may also increase the probability that stochastic ascent steps improve the objective function. Under the previous assumptions, this probability measures the efficiency of algorithms. Furthermore, among different learning objectives respecting the coherence and pseudoconcavity criteria, it is best to choose one that has high values for $\\delta$ in both the efficiency and attraction criteria. In Appendix [6](#apx:reward_shaping){reference-type=\"ref\" reference=\"apx:reward_shaping\"} we use these criteria to study other reward-shaping strategies, and in Appendix [7](#apx:minigrid_experiments){reference-type=\"ref\" reference=\"apx:minigrid_experiments\"} we extend the study to more complex environments from the MiniGrid library [@MinigridMiniworld23] where the policy is a deep neural network.\n\nThe problem discussed in this section strongly relates to overfitting or generalization in reinforcement learning. In situations where the same state and action pairs are repeatedly sampled with high probability, the policy may appear optimal by neglecting the rewards observed in state and action pairs sampled with low probability. The gradient estimates will then be zero with high probability, and the gradient updates will not lead to policy improvements. In the previous example, gradient estimates computed from policies with a small parameter value $\\theta$ wrongly indicate that a stationary point has been reached as they equal zero with high probability. We quantify this effect with a novel definition of local optimality. We define as locally optimal policies over a space with probability $\\Delta$ the policies that maximize the reward on expectation over a set of states and actions observed in a history with probability at least $\\Delta$. Formally, a policy $\\pi$ is locally optimal over a space with probability $\\Delta$ if, and only if, $$\\begin{aligned}\n    \\exists\\: \\mathcal{E}\n    &\\in \\left \\{ \\mathcal{X} \\Big | \\int_\\mathcal{X} \\: d^{\\pi, \\gamma}(s) \\pi(a| s) \\: da ds \\geq \\Delta \\right \\} : \\nonumber \\\\\n    &\\pi \\in \\underset{\\pi'}{\\text{argmax}} \\int_{\\mathcal{E}} \\: d^{\\pi', \\gamma}(s) \\pi'(a| s) \\rho(a, s) \\: da ds \\; . \\label{eq:local_optimal_pi}\n\\end{aligned}$$ In the typical case of environments with sparse rewards, many policies observe with high probability state and action pairs with zero rewards and are locally optimal for large probabilities $\\Delta$. Typically, in the previous example, the joint set $\\{1, \\dots, S-2\\} \\times \\{-1, 1\\}$ is a set of state and action pairs $\\mathcal{E}$ that respects the definition equation [\\[eq:local_optimal_pi\\]](#eq:local_optimal_pi){reference-type=\"eqref\" reference=\"eq:local_optimal_pi\"} for large values $\\Delta$ when $\\theta$ is small. As we have shown, exploration mitigates the convergence of policy-gradient algorithms towards these locally optimal policies. Note that assuming a non-zero reward is uniformly distributed over the state and action space, exploration policies with uniform probabilities over visited states and actions are the best choice for sampling non-zero rewards with high probability. It can thus also be considered as the best choice of exploration to reduce the probability that the stochastic gradient ascent steps do not increase the objective value. Such initial policy may be learned from the framework developed by @lee2019efficient.\n\n# Conclusion {#sec:conclusion}\n\nIn conclusion, this research takes a step towards dispelling misunderstandings about exploration through the study of its effects on the performance of policy-gradient algorithms. More particularly, we distinguished two effects exploration has on the optimization. First, it modifies the learning objective in order to remove local extrema. Second, it modifies the gradient estimates and increases the likelihood that the update steps lead to improved returns. These two phenomena were studied through four criteria that we introduced and illustrated.\n\nThese ideas apply to other direct policy optimization algorithms. Indeed, the four criteria do not assume any structure on the learning objective and can thus be straightforwardly applied to any objective function optimized by a direct policy search algorithm. In particular, for off-policy policy gradient, we may simply consider that the off-policy objective is itself a surrogate or that the gradients of the return are biased estimates based on past histories. Ideas introduced in this work also apply to other reinforcement learning techniques. Typically, for value-based RL with sparse-reward environments, convergence towards a value function outputting zero is expected with high probability. This is mostly due to the low probability of sampling non-zero rewards by Monte-Carlo. The discussions from Section [4](#sec:ascent_direction){reference-type=\"ref\" reference=\"sec:ascent_direction\"} then apply, and a similar analysis can be performed.\n\nOur framework opens the door for further theoretical analysis, and the potential development of new criteria. We believe that deriving practical conditions on the exploration strategies, and the scheduling of the intrinsic return, for guaranteeing fast convergence should be the focus of attention. It could be achieved by bounding the policy improvement on expectation, which is nevertheless usually a hard task without strong assumptions. We furthermore believe that we provide a new lens on exploration necessary for interpreting and developing exploration strategies, in the sense of optimizing surrogate learning objective functions.\n\n# Acknowledgments {#acknowledgments .unnumbered}\n\nThe authors thank Arnaud Delaunoy, Pascal Leroy, and Mathias Berger for valuable comments on this manuscript. Adrien Bolland and Gaspard Lambrechts gratefully acknowledge the financial support of the F.R.S.-FNRS.\n\n# Reward Shaping and Exploration Strategies {#apx:reward_shaping}\n\nAs discussed in the manuscript, exploration strategies are reward-shaping strategies where the intrinsic reward bonuses are, among others, dependent on the policy parameters. This dependency makes the shaping strategies adaptive but makes the computation of gradients and the study of the learning objectives more complex. In this section, we study handcrafted reward-shaping strategies that have pseudoconcave and dense reward functions in the hill and maze environments. We then illustrate that the same criteria can be used to study these expert-knowledge based shaped rewards.\n\nFor the hill environment from Section [3](#sec:learning_objective){reference-type=\"ref\" reference=\"sec:learning_objective\"}, we illustrate in Figure [16](#fig:modif_hill_surface){reference-type=\"ref\" reference=\"fig:modif_hill_surface\"} an intrinsic reward bonus making the sum of rewards in equation [\\[eq:objective_rl\\]](#eq:objective_rl){reference-type=\"eqref\" reference=\"eq:objective_rl\"} concave. The corresponding learning objective has a unique maximum, which is part of the set $\\Omega = \\left \\{ \\theta' | \\max_\\theta J(\\pi_\\theta) - J(\\pi_{\\theta'})\\leq \\epsilon \\right \\}$ with $\\epsilon=1$ and $\\theta = (K, \\sigma)$. It can be seen in Figure [18](#fig:return_logconcenvlp){reference-type=\"ref\" reference=\"fig:return_logconcenvlp\"} where the global maximum in black is within the set $\\Omega$ in green. Both, the $\\epsilon$-coherence and the pseudoconcavity criteria are thus respected for $\\epsilon=1$. Here, the intrinsic reward function is a simple function independent of the policy $\\pi_\\theta$. Finding such an intrinsic reward may be complex for other environments but the example underlines that exploration and reward shaping are mostly equivalent and that designing reward functions that are concave may help converging towards optimal policies.\n\n<figure id=\"fig:return_logconcenvlp\">\n<figure id=\"fig:modif_hill_surface\">\n<embed src=\"figures/hill_concave.pdf\" />\n<figcaption>Alternative reward function.</figcaption>\n</figure>\n<figure id=\"fig:return_logconcenvlp\">\n<embed src=\"figures/return_reward_shaping_contour/return_contour_1.0_0.0_1.0_0.0.pdf\" />\n<figcaption>Learning objective.</figcaption>\n</figure>\n<figcaption>In Figure <a href=\"#fig:modif_hill_surface\" data-reference-type=\"ref\" data-reference=\"fig:modif_hill_surface\">16</a>, an alternative intrinsic reward function ensuring that the sum of rewards is a pseudoconcave function. In Figure <a href=\"#fig:return_logconcenvlp\" data-reference-type=\"ref\" data-reference=\"fig:return_logconcenvlp\">18</a>, the contour function of the learning objective.</figcaption>\n</figure>\n\nFor the maze environment, the return $J(\\pi_\\theta)$ is represented in black in Figure [19](#fig:maze_return_dense){reference-type=\"ref\" reference=\"fig:maze_return_dense\"} together with the intrinsic return $J^d(\\pi_\\theta)$ in green. The latter is the return of the dense handcrafted reward function $\\rho^d(s, a) = (a - 1) / 2$ penalizing actions moving away from the target. In Figure [20](#fig:maze_learning_dense){reference-type=\"ref\" reference=\"fig:maze_learning_dense\"}, the corresponding learning objective function is shown. In the same experimental setting as in Section [4](#sec:ascent_direction){reference-type=\"ref\" reference=\"sec:ascent_direction\"}, we observe that the objective function is $\\delta$-efficient for higher values of $\\delta$ compared to the already-discussed learning objectives. Furthermore, the attraction criterion is respected for any value of $\\delta$ as the unique global maxima of the learning objective, intrinsic return, and return are all equals.\n\n<figure id=\"fig:maze_proba_improvement_dense\">\n<figure id=\"fig:maze_return_dense\">\n<embed src=\"figures/maze_return_dense.pdf\" />\n<figcaption>Return.</figcaption>\n</figure>\n<figure id=\"fig:maze_learning_dense\">\n<embed src=\"figures/maze_learning_dense.pdf\" />\n<figcaption>Learning objectives.</figcaption>\n</figure>\n<figure id=\"fig:maze_proba_improvement_dense\">\n<embed src=\"figures/maze_proba_improvement_dense.pdf\" />\n<figcaption>Improvement probabilities.</figcaption>\n</figure>\n<figcaption>In Figure <a href=\"#fig:maze_return_dense\" data-reference-type=\"ref\" data-reference=\"fig:maze_return_dense\">19</a> the return of the maze environment is represented together with the intrinsic return of a dense handcrafted reward function. Figure <a href=\"#fig:maze_learning_dense\" data-reference-type=\"ref\" data-reference=\"fig:maze_learning_dense\">20</a> represents the corresponding learning objective and Figure <a href=\"#fig:maze_proba_improvement_dense\" data-reference-type=\"ref\" data-reference=\"fig:maze_proba_improvement_dense\">22</a> the probability that the REINFORCE estimates are positive.</figcaption>\n</figure>\n\n# Minigrid Experiments {#apx:minigrid_experiments}\n\nIn this section, we introduce complex environments and parameterize policies with neural networks. In this context, it is impractical to naively compute and represent the objective functions and probability distributions for the different criteria. Therefore, we only evaluate the criteria along parameter trajectories, and extend the previous experimental setting.\n\nWe consider seven environments from the MiniGrid suite of environments [@MinigridMiniworld23], among others, designed for evaluating exploration strategies. In these environments, an agent moves in a maze and aims to reach a target position. To do so, the agent may choose actions that consist of turning left, turning right, moving forward, or staying idle. We consider two reward settings: the dense setting and the sparse setting. In the first, rewards of $-1$ are received for every non-idle move, and a reward of $1000$ is received upon reaching the target position. In the second setting, zero rewards are received everywhere, except upon reaching the target position, where a bonus of $1000$ is provided. In the dense setting, due to the action penalization incurred when moving, a policy outputting the idle action with probability one is locally optimal and has a return equal to zero. This is not (necessarily) the case in the second setting. We consider a discount factor of $\\gamma = 0.98$ and optimize a fully connected neural network taking as input the position pair and the orientation of the agent, and outputting a categorical distribution over actions. The network is composed of three hidden layers of $64$ neurons with ReLU activation functions.\n\nIn the dense reward setting, we optimize policies by maximizing three learning objective functions: $J(\\pi_\\theta)$, $L^a(\\theta)$, and $L^s(\\theta)$, respectively with $\\lambda_a = 0.5$ and $\\lambda_s = 0.25$. For the last objective, the state-visitation density estimator is a ten-component Gaussian mixture model maximizing the likelihood of the sampled batch. The optimization is performed using the Adam update rule [@kingma2014adam], with REINFORCE ascent directions computed over 32 histories of constant length $T = 100$, and with learning rate (step size) equal to $0.0005$. The length $T$ of the histories is chosen such that the realization value $T$ from a geometric distribution with success probability parameter $1 - \\gamma$ has at least a cumulative probability of $0.85$. In this setting, we illustrate the quasiconcavity criterion and the $\\epsilon$-coherence criterion. In Figure [23](#fig:minigrid_dense_return){reference-type=\"ref\" reference=\"fig:minigrid_dense_return\"}, we provide the evolution of the return of the policies when optimizing the three objectives $J(\\pi_\\theta)$, $L^a(\\theta)$, and $L^s(\\theta)$ for the different environments. For the `MiniGrid-Empty-8x8-v0` and the `MiniGrid-FourRooms-v0` environments, optimizing the return results in high-performance policies that do not stay idle. The other objectives also manage to find high-performing policies, but with a lower return. This phenomenon (assuming that the global optimum of each objective is found) illustrates the $\\epsilon$-coherence criterion, where this $\\epsilon$ value is the bound on the best policy that can be found when optimizing the learning objective. For the other environments, the policies resulting from the optimization of the return fall into local optima, namely ones where the policy chooses the idling action with probability one. When optimizing the learning objectives with exploration bonuses, the resulting policies no longer fall into the previous local optima. This result suggests that, along these parameter trajectories, the return $J(\\pi_\\theta)$ has a local optimum (or saddle point), in opposition to the learning objective functions $L^a(\\theta)$ and $L^s(\\theta)$. The latter illustrates the validity of the pseudoconcavity criterion in that region of the parameter space. For the learning objective $L^s(\\theta)$, the $\\epsilon$-coherence criterion is respected for a small value of $\\epsilon$, and the resulting policy manages to reach the target position. For the learning objective $L^a(\\theta)$, the resulting policy does not reach the target position. We nevertheless hypothesize that it is not a real local optimum of the learning objective but a local optimum in the sense of equation [\\[eq:local_optimal_pi\\]](#eq:local_optimal_pi){reference-type=\"eqref\" reference=\"eq:local_optimal_pi\"}.\n\n<figure id=\"fig:minigrid_dense_return\">\n<figure>\n<embed src=\"figures/minigrid/with_action_cost/MiniGrid-Empty-8x8-v0_return.pdf\" />\n</figure>\n<figure>\n<embed src=\"figures/minigrid/with_action_cost/MiniGrid-Empty-16x16-v0_return.pdf\" />\n</figure>\n<figure>\n<embed src=\"figures/minigrid/with_action_cost/MiniGrid-SimpleCrossingS9N1-v0_return.pdf\" />\n</figure>\n<figure>\n<embed src=\"figures/minigrid/with_action_cost/MiniGrid-SimpleCrossingS9N2-v0_return.pdf\" />\n</figure>\n<figure>\n<embed src=\"figures/minigrid/with_action_cost/MiniGrid-SimpleCrossingS9N3-v0_return.pdf\" />\n</figure>\n<figure>\n<embed src=\"figures/minigrid/with_action_cost/MiniGrid-SimpleCrossingS11N5-v0_return.pdf\" />\n</figure>\n<figure>\n<embed src=\"figures/minigrid/with_action_cost/MiniGrid-FourRooms-v0_return.pdf\" />\n</figure>\n<figcaption>Evolution of the return of policies during optimization in the dense minigrid environments. In blue, the return <span class=\"math inline\"><em>J</em>(<em>π</em><sub><em>θ</em></sub>)</span> is optimized; in orange, the learning objective <span class=\"math inline\"><em>L</em><sup><em>a</em></sup>(<em>θ</em>)</span> is optimized; and in green, the learning objective <span class=\"math inline\"><em>L</em><sup><em>s</em></sup>(<em>θ</em>)</span> is optimized by performing Adam steps in REINFORCE directions. Note that the median, worst, and best cases over five runs are represented for the different curves. For the environments <code>MiniGrid-Empty-8x8-v0</code> and <code>MiniGrid-FourRooms-v0</code>, optimizing each objective results in a policy that does not stay idle. These policies are initialized outside of the basin of attraction of the local optimum of the return. The coherence criteria can be observed as optimizing the learning objective with intrinsic exploration bonuses results in suboptimal policies. For the other environments, optimizing the return directly leads to policies that always choose to stay idle, and are thus locally optimal. Optimizing the learning objective with exploration allows us to escape from these local optima, illustrating the quasiconcavity criterion. It can be noted that when optimizing the objective <span class=\"math inline\"><em>L</em><sup><em>a</em></sup>(<em>θ</em>)</span>, the <span class=\"math inline\"><em>ϵ</em></span>-coherence criterion is respected for a large value of <span class=\"math inline\"><em>ϵ</em></span>, making the resulting policy worse than when optimizing the return directly. On the contrary, the objective <span class=\"math inline\"><em>L</em><sup><em>s</em></sup>(<em>θ</em>)</span> appears to have <span class=\"math inline\"><em>ϵ</em></span>-coherence for a reasonable value of <span class=\"math inline\"><em>ϵ</em></span>.</figcaption>\n</figure>\n\nIn the previous experiments with the dense setting, the local optima exist due to the negative rewards associated to idle-actions. If we consider the sparse setting, we could then assume that directly optimizing the return is sufficient to find high-performing policies. It is not always the case and it can be justified using the efficiency and attraction criteria. We use the same parameters as in the previous set of experiments and we provide the evolution of the return of the policies when optimizing the three objectives $J(\\pi_\\theta)$, $L^a(\\theta)$, and $L^s(\\theta)$, for the different environments in Figure [24](#fig:minigrid_sparse_return){reference-type=\"ref\" reference=\"fig:minigrid_sparse_return\"}. On the one hand, for most environments, whatever the learning objective, the resulting policy has a high return. Note that the $\\epsilon$-coherence can again be illustrated where the policies resulting from the optimization of the return perform better than the others. On the other hand, for the `MiniGrid-Empty-16x16-v0` and the `MiniGrid-SimpleCrossingS11N5-v0` environments, a high performing policy can only be found when optimizing the learning objective $L^s(\\theta)$. We illustrate that these results can be justified by the efficiency and attraction criteria in Figure [25](#fig:minigrid_sparse_proba){reference-type=\"ref\" reference=\"fig:minigrid_sparse_proba\"}. For each parameters obtained during the stochastic ascent steps on the return, we first estimate the probability of improving both objective functions by stochastic gradient ascent. These probabilities are used to compare learning objectives in terms of the efficiency criterion; the larger, the better. Then, in order to illustrate the attraction criterion, we estimate the probability of improving the return and the learning objective, both by gradient ascent steps on the learning objective, for each parameters obtained during the stochastic ascent steps on the objective $L^a(\\theta)$ and $L^s(\\theta)$. As far as $L^a(\\theta)$ is concerned, all probabilities remain small as result that the optimization procedure converges fast towards a stationary point where the target goal is observed with negligible probability. The efficiency and attraction criteria are respected for negligible probabilities $\\delta$, which is also a justification for the failure of converging towards good policies in the dense setting. For $L^s(\\theta)$, on the contrary, and in both environments, the probability of improving the learning objective remains large for each parameter encountered when optimizing the return. The efficiency of the learning objective is much higher than that of the return in that part of the parameter space. Furthermore, the probability of improving the return when optimizing the learning objective, is small at the beginning and increases after some iterations. This indicates that once the policy has a sufficiently large intrinsic return, the attraction criterion is respected for a high value $\\delta$.\n\n<figure id=\"fig:minigrid_sparse_return\">\n<figure>\n<embed src=\"figures/minigrid/without_action_cost/MiniGrid-Empty-8x8-v0_return.pdf\" />\n</figure>\n<figure>\n<embed src=\"figures/minigrid/without_action_cost/MiniGrid-Empty-16x16-v0_return.pdf\" />\n</figure>\n<figure>\n<embed src=\"figures/minigrid/without_action_cost/MiniGrid-SimpleCrossingS9N1-v0_return.pdf\" />\n</figure>\n<figure>\n<embed src=\"figures/minigrid/without_action_cost/MiniGrid-SimpleCrossingS9N2-v0_return.pdf\" />\n</figure>\n<figure>\n<embed src=\"figures/minigrid/without_action_cost/MiniGrid-SimpleCrossingS9N3-v0_return.pdf\" />\n</figure>\n<figure>\n<embed src=\"figures/minigrid/without_action_cost/MiniGrid-SimpleCrossingS11N5-v0_return.pdf\" />\n</figure>\n<figure>\n<embed src=\"figures/minigrid/without_action_cost/MiniGrid-FourRooms-v0_return.pdf\" />\n</figure>\n<figcaption>Evolution of the return of policies during optimization in the sparse minigrid environments. In blue, the return <span class=\"math inline\"><em>J</em>(<em>π</em><sub><em>θ</em></sub>)</span> is optimized; in orange, the learning objective <span class=\"math inline\"><em>L</em><sup><em>a</em></sup>(<em>θ</em>)</span> is optimized; and in green, the learning objective <span class=\"math inline\"><em>L</em><sup><em>s</em></sup>(<em>θ</em>)</span> is optimized performing Adam steps in REINFORCE directions. Note that the median, worst, and best cases over five runs are represented for the different curves. In most environments, a high-performing policy can be found by optimizing the return <span class=\"math inline\"><em>J</em>(<em>π</em><sub><em>θ</em></sub>)</span>. This results from its quasiconcavity. The <span class=\"math inline\"><em>ϵ</em></span>-coherence criterion can also be observed. However, for the <code>MiniGrid-Empty-16x16-v0</code> and the <code>MiniGrid-SimpleCrossingS11N5-v0</code> environments, a high-performing policy can only be found when optimizing the learning objective <span class=\"math inline\"><em>L</em><sup><em>s</em></sup>(<em>θ</em>)</span>.</figcaption>\n</figure>\n\n<figure id=\"fig:minigrid_sparse_proba\">\n<figure>\n<embed src=\"figures/minigrid/without_action_cost/MiniGrid-Empty-16x16-v0_exp1_improvement_proba_return_plot.pdf\" />\n</figure>\n<figure>\n<embed src=\"figures/minigrid/without_action_cost/MiniGrid-Empty-16x16-v0_exp1_improvement_proba_learning_plot.pdf\" />\n</figure>\n<figure>\n<embed src=\"figures/minigrid/without_action_cost/MiniGrid-Empty-16x16-v0_exp2_improvement_proba_return_plot.pdf\" />\n</figure>\n<figure>\n<embed src=\"figures/minigrid/without_action_cost/MiniGrid-Empty-16x16-v0_exp2_improvement_proba_learning_plot.pdf\" />\n</figure>\n<figure>\n<embed src=\"figures/minigrid/without_action_cost/MiniGrid-SimpleCrossingS11N5-v0_exp1_improvement_proba_return_plot.pdf\" />\n</figure>\n<figure>\n<embed src=\"figures/minigrid/without_action_cost/MiniGrid-SimpleCrossingS11N5-v0_exp1_improvement_proba_learning_plot.pdf\" />\n</figure>\n<figure>\n<embed src=\"figures/minigrid/without_action_cost/MiniGrid-SimpleCrossingS11N5-v0_exp2_improvement_proba_return_plot.pdf\" />\n</figure>\n<figure>\n<embed src=\"figures/minigrid/without_action_cost/MiniGrid-SimpleCrossingS11N5-v0_exp2_improvement_proba_learning_plot.pdf\" />\n</figure>\n<figcaption>For the <code>MiniGrid-Empty-16x16-v0</code> and the <code>MiniGrid-SimpleCrossingS11N5-v0</code> environments, and for both learning objective functions <span class=\"math inline\"><em>L</em><sup><em>a</em></sup>(<em>θ</em>)</span> and <span class=\"math inline\"><em>L</em><sup><em>s</em></sup>(<em>θ</em>)</span>, we first represent the estimated probability of improving the return <span class=\"math inline\"><em>J</em>(<em>π</em><sub><em>θ</em></sub>)</span> and the corresponding learning objective when following their REINFORCE gradient estimate. This value is estimated at each run of the optimization of the policy with learning objective <span class=\"math inline\"><em>J</em>(<em>π</em><sub><em>θ</em></sub>)</span>. Second, we represent the estimated probability of improving the corresponding learning objective and the return <span class=\"math inline\"><em>J</em>(<em>π</em><sub><em>θ</em></sub>)</span> when following the REINFORCE gradient estimate of the learning objectives. These values are estimated at each run of the optimization of the policy with the learning objectives. The probabilities were estimated based on the frequencies of improving the objective functions by more than <span class=\"math inline\">0.2</span> when following <span class=\"math inline\">5</span> Adam ascent steps using REINFORCE update directions.</figcaption>\n</figure>\n\n[^1]: Experimental details and implementations can be found at\\\n    <https://github.com/adrienBolland/micro-rl-lib>.\n\n[^2]: For the sake of keeping discussions simple, the definition of pseudoconcavity is simplified [@mangasarian1975pseudo], and additional assumptions on the stochastic gradient estimates are neglected.",
    "rationale": "Summary: The authors use two pathological environments to explore aspects of exploration bonuses in objectives.  For example, they show that these bonuses can potentially eliminate local optima and make the loss landscape more amenable to SGD.\n\nStrengths: This work offers a compelling perspective and analysis of the topic.  While some of the ideas they explore might be \"common knowledge\" in the community, I believe that this might be the first time that some of these ideas have been explored thoroughly and scientifically.\n\nWeaknesses: Typos and minor suggestions:\n- Discount factor backwards bracket in 2.1\n- Is (1) missing the sum?\n- “The valley is composed of two floors”: “floors” is a strange way to phrase it, and the intended meaning was initially unclear to me.  Consider rephrasing to something like \"The valley contains two separate low points”.\n- If there’s room, you could point to an appendix to briefly remind the reader why the middle and RHS of (7) are the same as alpha -> 0.  I know it’s fundamental SGD theory, but it confused me for a few minutes in this context (especially since, at first glance, the equation is not true, since it is just a first-order approximation and is only true as alpha approaches 0).\n- In 4.2, the thorough description of the environment is nice, but I think it’s missing the time limit used.  Also, were the curves given in Figure 3 computed analytically or empirically?  Some more info might improve the paper.\nIn 4.2, you could remind the reader what (4) is so that they do not have to hunt for it.\n\nWeaknesses:\n1) Some of the questions below are clarity weaknesses or other possible weaknesses.\n2) The contribution is entirely empirical (there are some nice theory concepts, but no proofs or useful properties are shown except empirically), and the empirical results are based entirely on extremely simple toy problems.  In my opinion, this does not invalidate their interesting perspectives on the issues that they illustrate with these results, but it does make the contribution less substantial.\n3) The paper overall is a bit challenging to read.  For an example of a well-written bit that does *not* have this issue, the conclusion says “First, it modifies the learning objective in order to remove local extrema. Second, it modifies the gradient estimates and increases the likelihood that the update steps lead to optimal policies.”  This is great; more “high-level summary” passages like this in sections 3-4 would’ve made it much easier to read.  For an example of a hard-to-understand part, see the last question below.\n\nNote: My primary concerns and slightly negative score come primarily from #2 (contribution), and the second question below (\"reward-engineering\").  The clarity weaknesses are not severe enough to have a large impact on my score.\n\nQuestions: What is the meaning of the and symbol in (6)?  How is the gradient equal to “0 and L(theta)”?  Or is the and symbol meant to separate this line into two separate equalities?  If so, this is confusing, consider representing this in a more standard way, or using parentheses to disambiguate.  **Update:** upon reading more of the paper, and seeing this used more, I know the latter interpretation is correct, so no need to answer this question in your response, but I’ll leave this here to illustrate the potential confusion to the reader.\n\nIn 4.1, J^d seems less like an exploration term, and more like a “reward-engineering” term that simply makes the problem easier.  Am I missing a perspective on this?  This leads to a larger concern, in that much of the contribution of 4.1 hinges on this term, and I have doubts about whether this term can be legitimately thought of as an exploration bonus that is superior to the entropy bonus.\n\nCan you please sum up the core take-away from 4.1?  I’ve reread the paragraph “We have empirically shown that exploration in policy gradients…” several times, but I’m struggling to understand exactly what I was supposed to take away from this section.  Is the point that the two criteria were good criteria in practice for choosing a good exploration bonus term?  If so, I am not convinced that this result will generalize beyond this specific toy setting (and the issue raised in the question above becomes even more of a concern in this case).  If not (or even if so), I think the intended take-away of this section needs to be spelled out more clearly.  **Update:** I understand better now upon a reread, so no need to address this question directly in your response.  However, the Section 4.1 paragraph noted is a perfect example of the last weakness noted above, so I’ll leave this question in the review.",
    "rating": 2,
    "label": false,
    "rationale_edited": " The authors use two pathological environments to explore aspects of exploration bonuses in objectives.  For example, they show that these bonuses can potentially eliminate local optima and make the loss landscape more amenable to SGD.\n\nThis work offers a compelling perspective and analysis of the topic.  While some of the ideas they explore might be \"common knowledge\" in the community, I believe that this might be the first time that some of these ideas have been explored thoroughly and scientifically.\n\nThe contribution is entirely empirical (there are some nice theory concepts, but no proofs or useful properties are shown except empirically), and the empirical results are based entirely on extremely simple toy problems.  In my opinion, this does not invalidate their interesting perspectives on the issues that they illustrate with these results, but it does make the contribution less substantial.",
    "chosen": true
  },
  {
    "title": "Causal Identification for Complex Functional Longitudinal Studies",
    "abstract": "Real-time monitoring in modern medical research introduces functional longitudinal data, characterized by continuous-time measurements of outcomes, treatments, and confounders. This complexity leads to uncountably infinite treatment-confounder feedbacks, which traditional causal inference methodologies cannot handle. Inspired by the coarsened data framework, we adopt stochastic process theory, measure theory, and net convergence to propose a nonparametric causal identification framework. This framework generalizes classical g-computation, inverse probability weighting, and doubly robust formulas, accommodating time-varying outcomes subject to mortality and censoring for functional longitudinal data. We examine our framework through Monte Carlo simulations. Our approach addresses significant gaps in current methodologies, providing a solution for functional longitudinal data and paving the way for future estimation work in this domain.",
    "text": "# Introduction\n\nThe advent of real-time monitoring technologies in healthcare has led to the continuous-time measurement of outcomes, treatments, and confounders, which we term \"functional longitudinal data.\" For example, the Medical Information Mart for Intensive Care IV (MIMIC-IV) [@johnson2023mimic] is a freely accessible electronic health record (EHR) database that records ICU care data, including physiological measurements, laboratory values, medication administration, and clinical events. Another example is Continuous Glucose Monitoring (CGM) [@rodbard2016continuous; @klonoff2017continuous], an increasingly adopted technology for insulin-requiring patients that provides insights into glycemic fluctuations. CGM offers a real-time, high-resolution stream of data, capturing the intricate fluctuations in interstitial fluid glucose levels every few minutes.\n\nThese examples illustrate the recent prevalence of functional longitudinal data, highlighting the necessity of a causal framework, as understanding treatment effects is of paramount interest in these settings. However, there is a great lack of investigation of causal inference at the intersection of longitudinal data and functional data. Even identifying causal parameters of interest through observed data becomes highly nontrivial in this setting, due to the issue of uncountably infinite treatment-confounder feedbacks [@hernan2020causal] within functional longitudinal data.\n\nTo bridge this gap, we aim to propose a novel identification framework for functional longitudinal data with time-varying outcomes subject to mortality and censoring, who enjoys the nonparametric property, making it more flexible and adaptable to various datasets.\n\nWe first define a causal quantity representing the mean of counterfactual outcomes under an idealized randomized world. To connect the observed data distribution to this idealized world, inspired by the coarsened data framework [@heitjan1991ignorability] and through the application of continuous-time stochastic process theory and measure theory, we upgrade classical causal assumptions to accommodate functional longitudinal data nonparametrically. These together resolve the issue of uncountably infinite treatment-confounder feedbacks [@hernan2020causal] for functional longitudinal data. We generalize the well-known g-computation formula, inverse probability weighting formula, and double robust formula. We examine our identification framework through Monte Carlo simulations.\n\nThe paper is organized as follows. In Section [2](#sec:related){reference-type=\"ref\" reference=\"sec:related\"} we present a literature review of related work. In Section [3](#sec:functional){reference-type=\"ref\" reference=\"sec:functional\"}, we define the notation and parameters of interest. Then we propose identification assumptions and generalize the well-known g-computation, inverse probability weighting, and double robust formulas. Additionally, we prove that our identification is nonparametric. We conduct Monte Carlo simulations to examine our framework in Section [4](#sec:simu){reference-type=\"ref\" reference=\"sec:simu\"}. Section [5](#sec:dis){reference-type=\"ref\" reference=\"sec:dis\"} discusses future directions. While this paper builds a population-level framework with numerical results, it does not explore estimation or associated inference, which is beyond the scope of this study and left for future research.\n\n# Related Work {#sec:related}\n\n**Causal Inference for Non-Functional Longitudinal Studies.** Current causal frameworks for longitudinal studies are inadequate for handling \"functional longitudinal data.\" They were mostly designed to accommodate \"regular longitudinal studies\" where time moves in specific fixed intervals [@greenland1986identifiability; @robins1986new] or \"irregular longitudinal studies.\" where changes in data occur randomly yet discretely [@lok2008statistical; @johnson2005semiparametric; @roysland2011martingale; @hu2019causal; @rytgaard2022continuous], allowing only finite treatment-confounder feedbacks. In Figure [1](#fig:relation){reference-type=\"ref\" reference=\"fig:relation\"} illustrates the relation among cross sectional data, regular, irregular, and functional longitudinal data. All literature designed for non-functional longitudinal data fall short to be directly applied onto functional longitudinal data.\n\n<figure id=\"fig:relation\">\n<div class=\"center\">\n<embed src=\"relation.pdf\" />\n</div>\n<figcaption>Relation among cross sectional data, regular, irregular, and functional longitudinal data. Cross section data without considering time scale are special cases of all longitudinal data. Regular longitudinal data are special cases of irregular longitudinal data, which are special cases of functional longitudinal data.</figcaption>\n</figure>\n\n**Causal Inference for Functional Data.** Existing research on causal inference has explored the realm of functional data within observational studies, as noted in works by [@miao2020average; @zhang2021covariate; @tan2022causal]. The data format these studies investigate is consistent with the framework of our analysis. Nonetheless, our work sets itself apart by emphasizing the temporal aspect inherent in longitudinal studies, in contrast to the primary focus on point exposure in the mentioned literature.\n\n**Existing Work for Functional Longitudinal Data.** The only exceptions that investigated causal inference for functional longitudinal data are @ying2024causality and @sun2022causal. However, @ying2024causality only investigated an end-of-study outcome, neither proving the nonparametric property nor conducting any numerical investigation. On the other hand, @sun2022causal imposed stochastic differential equations with stringent parametric assumptions. This situation highlights a significant gap in methodological advancements within the field.\n\n# Proposed Method {#sec:functional}\n\n## Preparation {#sec:pre}\n\nConsider a longitudinal study spanning from time 0 to $\\infty$:\n\n-   $A(t)$ and $L(t)$ are two stochastic processes denoting the treatment administered and the measured confounders, respectively, at any given time $t$. At any time, $A(t)$ and $L(t)$ could be binary, categorical, continuous, or even functional. We denote $\\bar{A}(t) = \\{A(s): 0 \\leq s \\leq t\\}$ and $\\bar{L}(t) = \\{L(s): 0 \\leq s \\leq t\\}$, with $\\bar{A}$ and $\\bar{L}$ representing the collections of treatments and confounders over the entire study.\n\n-   We are interested in an outcome of interest $Y(t)$, as a subset of $L(t)$, that is, $Y(t) \\subset L(t)$.\n\n-   Let $T$ be a time-to-event endpoint, for instance, death, and $C$ be the right censoring time. Define $X = \\min(T, C)$ as the censored event time and $\\Delta = \\mathbbm{1}(T \\leq C)$ the event indicator. Therefore when $\\Delta = 1$, $X = T$ and when $\\Delta = 0$, $X = C$.\n\n-   Write the counterfactual time-to-event endpoint $T_{\\bar a}$ and counterfactual covariates $L_{\\bar a}(t)$, for any $\\bar a \\in \\mathcal{A}$, where $\\mathcal{A}$ encompasses all possible values of $\\bar{a}$. Therefore we have $X_{\\bar a} = \\min(T_{\\bar a}, C)$ and $\\Delta_{\\bar a} = \\mathbbm{1}(T_{\\bar a} < C)$. We assume that the future cannot affect the past, that is, $\\mathbbm{1}(T_{\\bar a} \\geq t) = \\mathbbm{1}(T_{\\bar a'} \\geq t)$ and $L_{\\bar a}(t) = L_{\\bar a'}(t)$ whenever $\\bar a(t) = \\bar a'(t)$. We also write $T_{\\mathcal{A}} = \\{T_{\\bar a}\\}_{\\bar a \\in \\mathcal{A}}$ and $\\bar L_{\\mathcal{A}} = \\{\\bar L_{\\bar a}\\}_{\\bar a \\in \\mathcal{A}}$.\n\n-   The full data are $\\{\\bar A, C, T_{\\mathcal{A}}, \\bar L_{\\mathcal{A}}\\}$ and the observed data are $\\{\\bar A, X, \\Delta, \\bar L\\}$. Note that on the observed data level, $A(t)$ and $L(t)$ are not observed for $t \\leq X$ or defined for $t \\leq T$. For easier notation in this paper, we offset $A(t) = A(X)$ and $L(t) = L(X)$ for observed data whenever $t > X$. In this way, the stochastic processes $A(t)$ and $L(t)$ are well defined at any $t > 0$.\n\n-   Define $\\mathscr{F}_t = \\sigma\\{A(s), L(s), \\mathbbm{1}(X \\leq s), \\mathbbm{1}(X \\leq s)\\Delta: \\forall s \\leq t\\}$ as a filtration of information observed up to time $t$. Also we write $\\mathscr{F}_{t-} = \\sigma(\\cup_{0 \\leq s < t}\\mathscr{F}_t)$ and $\\mathscr{G}_t = \\sigma(\\mathscr{F}_{t-}, A(t))$. We define $\\mathscr{G}_{\\infty+} = \\mathscr{F}_\\infty$. We write $\\mathscr{F}_{0-}$ and $\\mathscr{G}_{0-}$ as the trivial sigma algebra for convenience. Note that $X$ is a stopping time with respect to $\\mathscr{F}_t$, with $\\mathscr{F}_{\\infty} = \\mathscr{F}_{X} = \\sigma\\{\\bar A, X, \\Delta, \\bar L\\}$.\n\n-   To denote the distribution on the path space induced by the stochastic processes and the measure $\\mathbb{P}$ on $\\Omega$, we also use $\\mathbb{P}(\\mathrm{d}\\bar{a}\\mathrm{d}\\bar{l})$ [@bhattacharya2007basic; @durrett2019probability; @gill2001causal]. Note that this is not a density function.\n\n-   Assuming the event space is Polish enables the selection of regular conditional probabilities. Conditional distribution is understood as a function over a sigma algebra combined with a path set, for instance, $\\mathbb{P}(\\mathrm{d}\\bar{a}|\\bar{l})$, and is defined almost surely.\n\n-   The total variation norm over the path space's signed measure space is represented by $\\|\\cdot\\|_{\\textup{TV}}$.\n\n-   A partition $\\Delta_K[0, \\infty]$ over $[0, \\infty]$ is a finite sequence of $K + 1$ numbers of the form $0 = t_0 < \\cdots < t_K = \\infty$. The mesh $|\\Delta_K[0, \\infty]|$ of a partition $\\Delta_K[0, \\infty]$ is defined as $\\max\\{\\max_{i = 0, \\cdots, K - }(t_{j + 1} - t_j), 1/t_{K - 1}\\}$, representing the maximum gap length of the partition.\n\nWe are interested in learning a marginal mean of transformed potential outcomes including a time-to-event outcome and an outcome process under a user-specified treatment regime in the absence of censoring, $$\\label{eq:targetparameter}\n    \\int_{\\mathcal{A}}\\operatorname{\\mathbb{E}}\\{\\nu(T_{\\bar a}, \\bar Y_{\\bar a})\\}\\mathbb{G}(\\textup{d}\\bar a),$$ where $\\nu$ is some user-specified function and $\\mathbb{G}$ is a priori defined (signed) measure on $\\mathcal{A}$. We assume $\\operatorname{\\mathbb{E}}\\{\\nu(T_{\\bar a}, \\bar Y_{\\bar a})\\}$ is integrable against $\\mathbb{G}$. This exploration encompasses marginal means under static treatment regimes, as discussed in various literature [@rytgaard2022continuous; @cain2010start; @young2011comparative; @hernan2020causal]. This quantity can be seen as the mean of counterfactual outcomes under an idealized randomized world, where $\\bar a$ is randomized to follow $\\mathbb{G}$.\n\n## Identification assumptions\n\nWe have defined the parameter of interest [\\[eq:targetparameter\\]](#eq:targetparameter){reference-type=\"eqref\" reference=\"eq:targetparameter\"}. Intuitively if treating treatment process $\\bar A$ as a selection process [@heitjan1991ignorability], [\\[eq:targetparameter\\]](#eq:targetparameter){reference-type=\"eqref\" reference=\"eq:targetparameter\"} is the mean of $\\nu(T_{\\bar a}, \\bar Y_{\\bar a})$ when $\\bar A$ were to follow $\\mathbb{G}$. To create such a pseudo-population, note that for any sequences of partitions $\\{\\Delta_K[0, \\infty]\\}_{K = 1}^\\infty$, we have the following decomposition $$\\begin{aligned}\n    \\operatorname{\\mathbb{P}}(\\textup{d}x\\textup{d}\\delta\\textup{d}\\bar a \\textup{d}\\bar l) = \\prod_{j = 0}^{K - 1}&\\operatorname{\\mathbb{P}}\\{T \\leq t_{j + 1}|\\bar a(t_{j + 1}), \\mathbbm{1}(t_{j} < x \\leq t_{j + 1}, \\delta = 0), \\mathscr{F}_{t_j}\\}^{\\mathbbm{1}(t_{j} < x \\leq t_{j + 1}, \\delta = 1)}\\\\\n    &\\operatorname{\\mathbb{P}}\\{T > t_{j + 1}|\\bar a(t_{j + 1}), \\mathbbm{1}(t_{j} < x \\leq t_{j + 1}, \\delta = 0), \\mathscr{F}_{t_j}\\}^{1 - \\mathbbm{1}(t_{j} < x \\leq t_{j + 1}, \\delta = 1)}\\\\\n    &\\operatorname{\\mathbb{P}}\\{C \\leq t_{j + 1}|\\bar a(t_{j + 1}), \\mathscr{F}_{t_j}\\}^{\\mathbbm{1}(t_{j} < x \\leq t_{j + 1}, \\delta = 0)}\\\\\n    &\\operatorname{\\mathbb{P}}\\{C > t_{j + 1}|\\bar a(t_{j + 1}), \\mathscr{F}_{t_j}\\}^{1 - \\mathbbm{1}(t_{j} < x \\leq t_{j + 1}, \\delta = 0)}\\\\\n    &\\operatorname{\\mathbb{P}}\\{\\textup{d}\\bar l(t_{j + 1})|\\bar a(t_{j + 1}), \\mathscr{F}_{t_{j}}\\}\\operatorname{\\mathbb{P}}\\{\\textup{d}\\bar a(t_{j + 1})|\\mathscr{F}_{t_j}\\}.\n\\end{aligned}$$ We intervene treatment distribution at each time $t_j$ to approximate the pseudo-population where $\\bar A$ were to follow $\\mathbb{G}$ as: $$\\begin{aligned}\n    \\operatorname{\\mathbb{P}}_{\\Delta_K[0, \\infty], \\mathbb{G}}(\\textup{d}x\\textup{d}\\delta\\textup{d}\\bar a \\textup{d}\\bar l) \n    = \\prod_{j = 0}^{K - 1}&\\operatorname{\\mathbb{P}}\\{T \\leq t_{j + 1}|C > t_{j + 1}, \\bar a(t_{j + 1}), \\mathscr{F}_{t_j}\\}^{\\mathbbm{1}(t_{j} < x \\leq t_{j + 1}, \\delta = 1)}\\\\\n    &\\operatorname{\\mathbb{P}}\\{T > t_{j + 1}|C > t_{j + 1}, \\bar a(t_{j + 1}), \\mathscr{F}_{t_j}\\}^{1 - \\mathbbm{1}(t_{j} < x \\leq t_{j + 1}, \\delta = 1)}\\\\\n    &\\operatorname{\\mathbb{P}}\\{\\textup{d}\\bar l(t_{j + 1})|C > t_{j + 1}, \\bar a(t_{j + 1}), \\mathscr{F}_{t_{j}}\\}\\\\\n    &\\{1 - \\mathbbm{1}(x \\leq t_{j + 1}, \\delta = 0)\\}\\mathbb{G}\\{\\textup{d}\\bar a(t_{j + 1})|\\bar a(t_j)\\}.\n\\end{aligned}$$\n\nTo eliminate confounder bias, we need to make sure there is no unmeasured confounders. We adapt the commonly known \"coarsening at random\" [@heitjan1991ignorability] assumption into:\n\n::: {#assump:CAR .assumption}\n**Assumption 1** (Full conditional randomization). *The treatment assignment is independent of the all potential outcomes and covariates given history, in the sense that there exists a bounded function $\\varepsilon(t, \\eta) > 0$ with $\\int_0^\\infty \\varepsilon(t, \\eta)dt \\to 0$ as $\\eta \\to 0$, such that for any $t \\in [0, \\infty]$, $\\eta > 0$, $$\\sup_{\\bar a \\in \\mathcal{A}}\\operatorname{\\mathbb{E}}(\\|\\operatorname{\\mathbb{P}}\\{\\textup{d}t_{\\bar a}\\textup{d}\\bar l_{\\bar a}|\\bar A(t + \\eta), \\mathscr{F}_t\\} - \\operatorname{\\mathbb{P}}\\{\\textup{d}t_{\\bar a}\\textup{d}\\bar l_{\\bar a}|\\mathscr{F}_t\\}\\|_{\\textup{TV}}) < \\varepsilon(t, \\eta).$$*\n:::\n\nThis assumption claims that, the treatment distribution, or equally, the probability of coarsening, in a small period of time around $t$, only depends on the observed data up to time $t$ and independent of further part of counterfactuals. This assumption says in a approximating sense that there is no common cause between treatment decision between time $[t, t + \\eta]$ and all future counterfactual confounders.\n\nWe also need an assumption over the censoring mechanism to eliminate the censoring bias. We consider the well-known conditionally independent censoring assumption [@tsiatis2006semiparametric; @andersen2012statistical]. Define the full data censoring time hazard function as $$\\lambda_C(t|T, \\bar A, \\bar L) = \\lim_{dt \\to 0}\\operatorname{\\mathbb{P}}(C \\leq t + \\textup{d}t|C > t, T, \\bar A, \\bar L)/dt.$$ The following assumption requires that the full data censoring time hazard at time $t$ only depends on the observed data up to time $t$.\n\n::: {#assump:CIR .assumption}\n**Assumption 2** (Conditional independent censoring). *The censoring mechanism is said to be conditionally independent if $$\\label{eq:car}\n    \\lambda_C(t|T, \\bar A, \\bar L) = \\lim_{dt \\to 0}\\operatorname{\\mathbb{P}}\\{C \\leq t + \\textup{d}t|C > t, T > t, \\bar A(t), \\bar L(t)\\}\\mathbbm{1}(T > t)/dt.$$*\n:::\n\nNote that in order to overcome the continuous-time issue, here we impose Assumption [1](#assump:CAR){reference-type=\"ref\" reference=\"assump:CAR\"} over an infinitesimal period of time. This type of idea is also adopted in Assumption [2](#assump:CIR){reference-type=\"ref\" reference=\"assump:CIR\"}. Note that how Assumption [2](#assump:CIR){reference-type=\"ref\" reference=\"assump:CIR\"} is given on the intensity process whereas Assumption [1](#assump:CAR){reference-type=\"ref\" reference=\"assump:CAR\"} is on the conditioning event. This is because one does not have intensity process for a general stochastic process.\n\nWith Assumptions [1](#assump:CAR){reference-type=\"ref\" reference=\"assump:CAR\"} and [2](#assump:CIR){reference-type=\"ref\" reference=\"assump:CIR\"}, we are able to show that whenever $|\\Delta_K[0, \\infty]| \\to 0$, $\\operatorname{\\mathbb{P}}_{\\Delta_K[0, \\infty], \\mathbb{G}}$ approximates a pseudo-measure where treatment distribution are intervened by uncountable times into following $\\mathbb{G}$:\n\n::: {#prp:intervenable .proposition}\n**Proposition 1** (Intervenable). *Under Assumptions [1](#assump:CAR){reference-type=\"ref\" reference=\"assump:CAR\"} and [2](#assump:CIR){reference-type=\"ref\" reference=\"assump:CIR\"}, the measures $\\operatorname{\\mathbb{P}}_{\\Delta_K[0, \\infty], \\mathbb{G}}$ converges to the same (signed) measure $\\operatorname{\\mathbb{P}}_{\\mathbb{G}} := \\operatorname{\\mathbb{P}}(\\textup{d}x_{\\bar a}\\textup{d}l_{\\bar a})\\mathbb{G}(\\textup{d}\\bar a)\\delta_{\\bar a}$ in the total variation norm on the path space, regardless of the choices of partitions, $$\\|\\operatorname{\\mathbb{P}}_{\\Delta_K[0, \\infty], \\mathbb{G}}(\\textup{d}x\\textup{d}\\delta\\textup{d}\\bar a \\textup{d}\\bar l) - \\operatorname{\\mathbb{P}}(\\textup{d}x_{\\bar a}\\textup{d}\\bar l_{\\bar a})\\mathbb{G}(\\textup{d}\\bar a)\\delta_{\\bar a}\\|_{\\textup{TV}} \\to 0.$$*\n:::\n\nWe refer $\\operatorname{\\mathbb{P}}_{\\mathbb{G}}$ as the *target distribution*. The following assumption links the observed variable with the counterfactuals.\n\n::: {#assump:strongconsistency .assumption}\n**Assumption 3** (Full consistency). *For any $t$, $$T = T_{\\bar A}, L(t) = L_{\\bar A}(t).$$*\n:::\n\nThe full consistency assumption links the observed outcome and the potential outcome via the treatment actually received. It says that if an individual receives the treatment $\\bar A = \\bar a$, then his/her observed outcome $Y$ matches $Y_{\\bar a}$.\n\nThe following assumption ensures that the observed data can identify the target distribution.\n\n::: {#assump:contpospointCAR .assumption}\n**Assumption 4** (Positivity). *$$\\operatorname{\\mathbb{P}}_{\\mathbb{G}} \\ll \\operatorname{\\mathbb{P}}.$$*\n:::\n\nWith the above assumptions, we are able to generalize the well-known identification formulas: g-computation, inverse probability weighting, and double robust formulas, into functional longitudinal data. Note that our assumptions can be weaker but chosen for ease to interpret.\n\n## Identification formulas\n\n::: {#def:gprocess .definition}\n**Definition 1** (G-computation process). *Under Assumptions [1](#assump:CAR){reference-type=\"ref\" reference=\"assump:CAR\"} and [2](#assump:CIR){reference-type=\"ref\" reference=\"assump:CIR\"}, define $$H_{\\mathbb{G}}(t) = \\operatorname{\\mathbb{E}}_{\\mathbb{G}}\\{\\nu(X, \\bar Y)|\\mathscr{G}_t\\},$$ as a projection process, which is apparently a $\\operatorname{\\mathbb{P}}_{\\mathbb{G}}$-martingale. We call $H_{\\mathbb{G}}(t)$ the *g-computation process*. Note that $$H_{\\mathbb{G}}(\\infty) = \\nu(X, \\bar Y), ~~~H_{\\mathbb{G}}(0-) = \\operatorname{\\mathbb{E}}_{\\mathbb{G}}\\{\\nu(X, \\bar Y)\\}.$$*\n:::\n\nThe g-computation process intuitively serves as a consecutive adjustment of the target $\\nu(X, \\bar Y)$ from $\\infty$ to $0$. It represents a mix of original conditional distributions of covariate process together with the intervened treatment process $\\mathbb{G}$, from end of study to the beginning. Following this adjustment to the beginning of study, we have:\n\n::: {#thm:gformulapoint .theorem}\n**Theorem 1** (G-computation formula). *Under Assumptions [1](#assump:CAR){reference-type=\"ref\" reference=\"assump:CAR\"}, [2](#assump:CIR){reference-type=\"ref\" reference=\"assump:CIR\"}, [3](#assump:strongconsistency){reference-type=\"ref\" reference=\"assump:strongconsistency\"}, and [4](#assump:contpospointCAR){reference-type=\"ref\" reference=\"assump:contpospointCAR\"}, [\\[eq:targetparameter\\]](#eq:targetparameter){reference-type=\"eqref\" reference=\"eq:targetparameter\"} is identified via a g-computation formula as $$\\int_{\\mathcal{A}}\\operatorname{\\mathbb{E}}\\{\\nu(T_{\\bar a}, \\bar Y_{\\bar a})\\}\\mathbb{G}(\\textup{d}\\bar a) = H_{\\mathbb{G}}(0-).$$*\n:::\n\n::: {#def:IPWprocess .definition}\n**Definition 2** (Inverse probability weighting process). *Under Assumptions [1](#assump:CAR){reference-type=\"ref\" reference=\"assump:CAR\"} and [2](#assump:CIR){reference-type=\"ref\" reference=\"assump:CIR\"}, define $$Q_{\\mathbb{G}}(t) = \\operatorname{\\mathbb{E}}\\left\\{\\frac{\\textup{d}\\operatorname{\\mathbb{P}}_{\\mathbb{G}}}{\\textup{d}\\operatorname{\\mathbb{P}}}\\bigg|\\mathscr{G}_t\\right\\},$$ as the Radon-Nikodym derivative at any time $t$, which is apparently a $\\operatorname{\\mathbb{P}}$-martingale. We call $Q_{\\mathbb{G}}(t)$ the *inverse probability weighting process*. Note that $$Q_{\\mathbb{G}}(\\infty) = \\operatorname{\\mathbb{E}}\\left\\{\\frac{\\textup{d}\\operatorname{\\mathbb{P}}_{\\mathbb{G}}}{\\textup{d}\\operatorname{\\mathbb{P}}}\\bigg|\\mathscr{G}_{\\infty}\\right\\} = \\frac{\\textup{d}\\operatorname{\\mathbb{P}}_{\\mathbb{G}}}{\\textup{d}\\operatorname{\\mathbb{P}}}, ~~~Q_{\\mathbb{G}}(0-) = 1.$$*\n:::\n\nThe IPW process intuitively serves as a continuous adjustment of the treatment process $\\bar A$ from $0$ to $\\infty$, using which as weights one may create a pseudo population as if the whole process were to follow $\\operatorname{\\mathbb{P}}_{\\mathbb{G}}$. It reweights the observed data distribution $\\operatorname{\\mathbb{P}}$ into $\\operatorname{\\mathbb{P}}_{\\mathbb{G}}$ from the beginning of the study to the end. Following this reweighting throughout the longitudinal study, we have:\n\n::: {#thm:ipwpoint .theorem}\n**Theorem 2** (Inverse probability weighting formula). *Under Assumptions [1](#assump:CAR){reference-type=\"ref\" reference=\"assump:CAR\"}, [2](#assump:CIR){reference-type=\"ref\" reference=\"assump:CIR\"}, [3](#assump:strongconsistency){reference-type=\"ref\" reference=\"assump:strongconsistency\"}, and [4](#assump:contpospointCAR){reference-type=\"ref\" reference=\"assump:contpospointCAR\"}, [\\[eq:targetparameter\\]](#eq:targetparameter){reference-type=\"eqref\" reference=\"eq:targetparameter\"} is identified via an inverse probability weighting formula as $$\\int_{\\mathcal{A}}\\operatorname{\\mathbb{E}}\\{\\nu(T_{\\bar a}, \\bar Y_{\\bar a})\\}\\mathbb{G}(\\textup{d}\\bar a) = \\operatorname{\\mathbb{E}}\\left\\{Q_{\\mathbb{G}}(\\infty)\\nu(X, \\bar Y)\\right\\}.$$*\n:::\n\nFor any two $\\mathscr{G}_t$-adapted processes $H(t)$ and $Q(t)$, and a partition $\\Delta_K[0, \\infty]$, we write $$\\begin{aligned}\n    \\Xi_{\\Delta_K[0, \\infty]}(H, Q) \n    = \\sum_{j = 0}^{K} Q(t_j)\\left[\\int H(t_{j + 1})\\mathbb{G}\\{\\textup{d}\\bar a(t_{j + 1})|\\bar A(t_j)\\} - H(t_j)\\right] + \\int H(0)\\mathbb{G}\\{\\textup{d}\\bar a(0)\\}.\n\\end{aligned}$$ We also define $\\Xi(H, Q)$ as the limit of $\\Xi_{\\Delta_K[0, \\infty]}(H, Q)$ in probability whenever it exists. We have:\n\n::: {#thm:drpoint .theorem}\n**Theorem 3** (Doubly robust formula). *Under Assumptions [1](#assump:CAR){reference-type=\"ref\" reference=\"assump:CAR\"}, [2](#assump:CIR){reference-type=\"ref\" reference=\"assump:CIR\"}, [3](#assump:strongconsistency){reference-type=\"ref\" reference=\"assump:strongconsistency\"}, and [4](#assump:contpospointCAR){reference-type=\"ref\" reference=\"assump:contpospointCAR\"}, for any $\\mathscr{G}_t$-adapted processes $H(t)$ and $Q(t)$ at the law where $\\Xi(H, Q)$, as the limit of $\\Xi_{\\Delta_K[0, \\infty]}(H, Q)$ in probability, exists and $$\\lim_{|\\Delta_K[0, \\infty]| \\to 0}\\operatorname{\\mathbb{E}}\\{\\Xi_{\\Delta_K[0, \\infty]}(H, Q)\\} = \\operatorname{\\mathbb{E}}\\{\\Xi(H, Q)\\},$$ we have $$\\int_{\\mathcal{A}}\\operatorname{\\mathbb{E}}\\{\\nu(T_{\\bar a}, \\bar Y_{\\bar a})\\}\\mathbb{G}(\\textup{d}\\bar a) = \\operatorname{\\mathbb{E}}\\left\\{\\Xi(H, Q)\\right\\},$$ provided that either $H = H_{\\mathbb{G}}$ or $Q = Q_{\\mathbb{G}}$.*\n:::\n\nAs one can see, the doubly robust formula provides extra protection against possible misspecification on either the g-computation process or the IPW process.\n\n## No restrictions on the observed data distribution: A Nonparametric Framework {#sec:nonparametric}\n\nIn this subsection, we demonstrate that our identification framework imposes no restrictions on the observed data. This property is advantageous for researchers and practitioners because nonparametric frameworks are flexible and require minimal assumptions, making them robust and adaptable to diverse datasets. We demonstrate this by proving that, for any given observed data distribution, we can identify a sequence of full data distributions---each satisfying Assumptions [1](#assump:CAR){reference-type=\"ref\" reference=\"assump:CAR\"}, [2](#assump:CIR){reference-type=\"ref\" reference=\"assump:CIR\"}, [3](#assump:strongconsistency){reference-type=\"ref\" reference=\"assump:strongconsistency\"}, and [4](#assump:contpospointCAR){reference-type=\"ref\" reference=\"assump:contpospointCAR\"}---such that their corresponding distributions on the observed data closely approximate the initial observed data distribution. That is, we write the set of all observed data distribution as $\\mathcal{P}$ and its subset satisfying Assumptions [1](#assump:CAR){reference-type=\"ref\" reference=\"assump:CAR\"}, [2](#assump:CIR){reference-type=\"ref\" reference=\"assump:CIR\"}, [3](#assump:strongconsistency){reference-type=\"ref\" reference=\"assump:strongconsistency\"}, and [4](#assump:contpospointCAR){reference-type=\"ref\" reference=\"assump:contpospointCAR\"} as $\\mathcal{M}$, then we show that $\\mathcal{M}$ is a dense subset of $\\mathcal{P}$ in the total variation norm.\n\nUp to now, we have used $\\operatorname{\\mathbb{P}}$ to represent both the distribution on the sample space and the path space. In this subsection, we use $\\operatorname{\\mathbb{P}}$ to denote the distribution on the observed data $(\\bar A, X, \\Delta, \\bar L)$ and $\\operatorname{\\mathbb{P}}^F$ to denote the distribution on the full data $(\\bar A, C, T_{\\mathcal{A}}, \\bar L_{\\mathcal{A}})$. We have\n\n::: {#thm:norestriction .theorem}\n**Theorem 4**. *When the path space consists of all piece-wise continuous processes, for any measure $\\operatorname{\\mathbb{P}}$ over the observed data $(\\bar A, X, \\Delta, \\bar L)$, there exists a sequence of measures $\\operatorname{\\mathbb{P}}_n^F$ over the full data $(\\bar A, C, T_{\\mathcal{A}}, \\bar L_{\\mathcal{A}})$ satisfying Assumptions [1](#assump:CAR){reference-type=\"ref\" reference=\"assump:CAR\"}, [2](#assump:CIR){reference-type=\"ref\" reference=\"assump:CIR\"}, [3](#assump:strongconsistency){reference-type=\"ref\" reference=\"assump:strongconsistency\"}, [4](#assump:contpospointCAR){reference-type=\"ref\" reference=\"assump:contpospointCAR\"}, whose inductions on the observed data converges to $\\operatorname{\\mathbb{P}}$ in the total variation norm.*\n:::\n\nTechnically, we have not achieved full nonparametric paradigm. However, we deem that the regularity condition \"the path space is piece-wise continuous processes\" is general enough for practical considerations. For example, both multivariate counting processes and continuous processes like Brownian process satisfy this regularity condition. It is noteworthy that achieving this \"almost nonparametric\" nature is the best one can hope for. This realization was confirmed in [@gill1997coarsening Section 9] for \"coarsening at random\" assumption, even though our framework exhibits certain distinctions.\n\n# Experiment result {#sec:simu}\n\nIn this section, we employ Monte Carlo simulations to empirically assess how the identification works. We decide to evaluate the performance of the g-computation formula only, for two reasons:\n\n1.  The g-computation formula is the only one that can be easily approximated through raw simulated data, whereas inverse probability weighting (and hence the doubly robust formula) cannot be directly approximated without estimation or computation;\n\n2.  On the population level, the values of the three formulas are equal. Therefore, approximating g-computation formula is sufficient for our purposes.\n\nTo that end, we need to go through 4 steps:\n\n1.  Come up with a reasonable data generating process;\n\n2.  Compute the parameter of interest ([\\[eq:targetparameter\\]](#eq:targetparameter){reference-type=\"ref\" reference=\"eq:targetparameter\"}) (or equivalently, the left-hand side of g-computation formula in Theorem [1](#thm:gformulapoint){reference-type=\"ref\" reference=\"thm:gformulapoint\"}) according to this data generating process;\n\n3.  Simulate according to this data generating process;\n\n4.  Approximate the right-hand side of g-computation formula in Theorem [1](#thm:gformulapoint){reference-type=\"ref\" reference=\"thm:gformulapoint\"} using the simulated data.\n\n**Step 1**: To sharp the focus and ease the computation, we consider a simple setting where there is no mortality or censoring ($T = C \\equiv \\infty$), or other measured confounding process, except for the outcome process itself. We take glucose levels as the outcome and insulin levels as the treatment. Both glucose and insulin levels exhibit smooth, continuous changes over time. Gaussian processes are particularly well-suited for modeling such smooth and continuous temporal processes. For $t \\in [0, 1]$, consider a potential outcome process $Y_{\\bar a}(t)$ capturing potential logarithm of glucose levels, following a Gaussian process with mean process as $$\\operatorname{\\mathbb{E}}\\{Y_{\\bar a}(t)\\} = -a(t),$$ and covariance process as $$\\operatorname{Cov}\\{Y_{\\bar a}(t), Y_{\\bar a}(s)\\} = e^{-|t - s|}, ~\\forall t, s \\in [0, 1].$$ This ensures the joint dependence among $Y_{\\bar a}(t)$ and negative treatment effect of logarithm of insulin level $\\bar a$. For instance, $Y_{\\bar a}(t)$ can be log of blood glucose level. Define $\\nu(T_{\\bar a}, \\bar Y_{\\bar a})$ as the integral of $\\bar Y_{\\bar a}$ over time $t \\in [0, 1]$, that is, $$\\nu(T_{\\bar a}, \\bar Y_{\\bar a}) = \\int_0^1 \\bar Y_{\\bar a}(t)dt.$$ Suppose the targeted treatment regime $\\mathbb{G}$ is a Gaussian measure with mean process $t - 0.5$ and jointly independent normal variables at any time points. That is, the intervened $A$ follows a Gaussian process with a mean process $$\\operatorname{\\mathbb{E}}\\{A(t)\\} = t - 0.5,$$ and covariance process $$\\operatorname{Cov}\\{A(t), A(s)\\} = e^{-|t - s|}, ~\\forall t, s \\in [0, 1],$$ representing an increase of insulin level, possibly due to some insulin intake.\n\n**Step** : Then we can show that ([\\[eq:targetparameter\\]](#eq:targetparameter){reference-type=\"ref\" reference=\"eq:targetparameter\"}) (or equivalently, the left-hand side of g-computation formula in Theorem [1](#thm:gformulapoint){reference-type=\"ref\" reference=\"thm:gformulapoint\"}) equals zero, that is, $$\\begin{aligned}\n    &\\int\\operatorname{\\mathbb{E}}\\{\\nu(T_{\\bar a}, \\bar Y_{\\bar a})\\}\\mathbb{G}(\\textup{d}\\bar a) = \\int\\operatorname{\\mathbb{E}}\\left\\{\\int_0^1 \\bar Y_{\\bar a}(t)dt\\right\\}\\mathbb{G}(\\bar a) \\\\\n    &= \\int\\int_0^1 \\operatorname{\\mathbb{E}}\\{\\bar Y_{\\bar a}(t)\\}dt\\mathbb{G}(\\bar a)=\\int \\int_0^1 a(t)dt \\mathbb{G}(\\bar a) \\\\\n    &= \\int_0^1 \\int a(t)\\mathbb{G}\\{\\bar a(t)\\}dt  \n    = \\int_0^1 (t - 0.5) dt = 0.\n\\end{aligned}$$ **Step 3**: In practice, we observe a stochastic process at finite points. We consider evenly splitting $t \\in [0, 1]$ into a grid of size $K + 1$: $\\Delta_K[0, 1] = \\{t_0 = 0, t_1 = 1/K, \\cdots, t_{K - 1} = (K - 1)/K, t_K = 1\\}$, and for $1 \\leq i \\leq n$, according to $\\mathbb{G}$ specified in Step 1, we simulate i.i.d. samples $A_i(t)$ according to $\\mathbb{G}$ specified in Step 1 at $\\Delta_K[0, 1]$ as $$\\begin{aligned}\n\\begin{pmatrix}\nA_i(t_0)\\\\\nA_i(t_1)\\\\\n\\cdots\\\\\nA_i(t_{K - 1})\\\\\nA_i(t_K)\\\\\n\\end{pmatrix} &\\sim  \\mathcal{N}\n\\begin{bmatrix}\n\\begin{pmatrix}\nt_0 - 0.5\\\\\nt_1 - 0.5\\\\\n\\cdots\\\\\nt_{K - 1} - 0.5\\\\\nt_K - 0.5\\\\\n\\end{pmatrix}\\!\\!,&\n\\begin{pmatrix}\n1 & e^{-|t_1 - t_0|} & \\cdots &e^{-|t_{K - 1} - t_0|} &e^{-|t_K - t_0|}\\\\\ne^{-|t_1 - t_0|} & 1 & \\cdots &e^{-|t_{K - 1} - t_1|} &e^{-|t_K - t_1|}\\\\\n\\cdots & \\cdots & \\cdots &\\cdots &\\cdots\\\\\ne^{-|t_{K - 1} - t_0|} & e^{-|t_{K - 1} - t_1|} & \\cdots &1 &e^{-|t_K - t_{K - 1}|}\\\\\ne^{-|t_K - t_0|} & e^{-|t_K - t_1|} & \\cdots &e^{-|t_K - t_{K - 1}|} &1\n\\end{pmatrix}\n\\end{bmatrix}.\n\\end{aligned}$$ By according to the distribution of $Y_{\\bar a}(t)$ specified in Step 1 and consistency, we generate $Y_i(t)$ at $\\Delta_K[0, 1]$ as $$\\begin{aligned}\n\\begin{pmatrix}\nY_i(t_0)\\\\\nY_i(t_1)\\\\\n\\cdots\\\\\nY_i(t_{K - 1})\\\\\nY_i(t_K)\\\\\n\\end{pmatrix} &\\sim  \\mathcal{N}\n\\begin{bmatrix}\n\\begin{pmatrix}\nA_i(t_0)\\\\\nA_i(t_1)\\\\\n\\cdots\\\\\nA_i(t_{K - 1})\\\\\nA_i(t_K)\\\\\n\\end{pmatrix}\\!\\!,&\n\\begin{pmatrix}\n1 & e^{-|t_1 - t_0|} & \\cdots &e^{-|t_{K - 1} - t_0|} &e^{-|t_K - t_0|}\\\\\ne^{-|t_1 - t_0|} & 1 & \\cdots &e^{-|t_{K - 1} - t_1|} &e^{-|t_K - t_1|}\\\\\n\\cdots & \\cdots & \\cdots &\\cdots &\\cdots\\\\\ne^{-|t_{K - 1} - t_0|} & e^{-|t_{K - 1} - t_1|} & \\cdots &1 &e^{-|t_K - t_{K - 1}|}\\\\\ne^{-|t_K - t_0|} & e^{-|t_K - t_1|} & \\cdots &e^{-|t_K - t_{K - 1}|} &1\n\\end{pmatrix}\n\\end{bmatrix}.\n\\end{aligned}$$\n\n**Step 4**: The integral of $Y_i(t_k)$ over $[0, 1]$ is $\\sum_{k = 0}^K Y_i(t_k)/(K + 1)$. The approximate of the right-hand side of g-computation formula is $\\sum_{i = 1}^n\\sum_{k = 0}^K Y_i(t_k)/(K + 1)/n$.\n\nWe vary the grid sizes ($K = 10, 50, 250$) to examine how a denser grid improves the approximation. This approach simulates the scenario where the mesh $|\\Delta_K[0, 1]|$ is shrunk to zero. Additionally, we vary the sample sizes ($n = 100, 500, 2500$) to explore how larger samples enhance the approximation, leveraging the law of large numbers to better approximate the right-hand side of the g-computation formula. We repeat the process $R = 10{,}000$ times. The resulting $10{,}000$ approximations of $\\sum_{i = 1}^n\\sum_{k = 0}^K Y_i(t_k)/(K + 1)/n$ are presented in boxplots in Figure [2](#fig:simulation){reference-type=\"ref\" reference=\"fig:simulation\"}, where we append biases.\n\n<figure id=\"fig:simulation\">\n<div class=\"center\">\n<embed src=\"Simulationresult.pdf\" />\n</div>\n<figcaption>Simulation Results of using g-computation formula by varying grid sizes in <span class=\"math inline\"><em>K</em> = 10, 50, 250</span> and sample sizes in <span class=\"math inline\"><em>n</em> = 100, 500, 2500</span>, for <span class=\"math inline\"><em>R</em> = 10000</span> repeats. We plot boxplots and give biases.</figcaption>\n</figure>\n\nThe simulation results demonstrate that the g-computation formula can adequately approximate [\\[eq:targetparameter\\]](#eq:targetparameter){reference-type=\"eqref\" reference=\"eq:targetparameter\"} even with moderate sample and grid sizes. Increasing the sample size while keeping the grid size fixed enhances the accuracy and reduces the variance of the approximation. In contrast, increasing the grid size while keeping the sample size fixed does not consistently improve accuracy or reduce variance. However, simultaneously increasing both the sample and grid sizes significantly improves accuracy and reduces variance in the approximation.\n\n# Conclusion {#sec:dis}\n\nIn this work, we proposed on a novel theoretical framework for causal inference under functional longitudinal studies. We introduced three methodological paradigms for causal identification: the g-computation formula, inverse probability weighting formula, and doubly robust formula. This framework, noted for nonparametric foundation, substantiates and expands upon the estimand-based causal framework introduced by @ying2024causality. It incorporates considerations for time-varying outcomes and addresses complexities such as death and right censoring, marking a significant advancement in the analysis of functional longitudinal data and enhancing the toolkit for causal inference in this area.\n\nThere are significant theoretical and methodological opportunities, given the limited investigation on functional longitudinal data, for the machine learning, functional data analysis and causal inference communities. To list a few, first, adapting our framework to accommodate scenarios where Assumption [1](#assump:CAR){reference-type=\"ref\" reference=\"assump:CAR\"} may not hold, including contexts involving time-dependent instrumental variables and time-dependent proxies [@ying2023proximal], warrants rigorous exploration. Second, the positivity Assumption [4](#assump:contpospointCAR){reference-type=\"ref\" reference=\"assump:contpospointCAR\"} in longitudinal studies faces practical challenges due to the potential scarcity of subjects adhering to specific treatment regimes within observed populations. One might consider using semiparametric models such as marginal structural models [@robins1997marginal; @roysland2011martingale] and structural nested models [@robins1999association; @lok2008statistical]. Other solutions include dynamic treatment regimes [@fitzmaurice2008longitudinal; @young2011comparative; @rytgaard2022continuous] and incremental interventions [@kennedy2017semiparametric]. Third, establishing the efficiency bound for our quantity of interest by leveraging semiparametric theory, represents an engaging challenge. Fourth, partial identification using discrete-time observations is a promising direction. Finally, developing a comprehensive estimation framework remains of ultimate interest.\n\n# Proofs\n\n## Proof of Proposition [1](#prp:intervenable){reference-type=\"ref\" reference=\"prp:intervenable\"} {#proof-of-proposition-prpintervenable}\n\nWe temporarily define $\\mathscr{F}_{/C, t} = \\sigma\\{L(s), A(s), \\mathbbm{1}(T \\leq s): \\forall s \\leq t\\}$ as the censoring free filtration and $\\mathscr{F}_{\\bar a, t} = \\sigma\\{L_{\\bar a}(s), \\mathbbm{1}(T_{\\bar a} \\leq s): \\forall s \\leq t\\}$ as the counterfactual filtration: $$\\begin{aligned}\n    &\\left\\|\\operatorname{\\mathbb{P}}_{\\Delta_K[0, \\infty], \\mathbb{G}}(\\textup{d}x \\textup{d}\\delta\\textup{d}\\bar a \\textup{d}\\bar l) - \\operatorname{\\mathbb{P}}(\\textup{d}x_{\\bar a}\\textup{d}l_{\\bar a})\\mathbb{G}(\\textup{d}\\bar a)\\delta_{\\bar a}\\right\\|_{\\textup{TV}}\\\\\n    &=\\Bigg\\|\\prod_{j = 0}^{K - 1}\\mathbb{G}\\{\\textup{d}\\bar a(t_{j + 1})|\\bar a(t_j)\\}\\{1 - \\mathbbm{1}(x \\leq t_{j + 1}, \\delta = 0)\\}\\operatorname{\\mathbb{P}}\\{T \\leq t_{j + 1}|\\bar a(t_{j + 1}), \\mathscr{F}_{t_j}\\}^{\\mathbbm{1}(t_{j} < x \\leq t_{j + 1})}\\\\\n    &~~~~~~\\operatorname{\\mathbb{P}}\\{T > t_{j + 1}|\\bar a(t_{j + 1}), \\mathscr{F}_{t_j}\\}^{1 - \\mathbbm{1}(t_{j} < x \\leq t_{j + 1})}\\operatorname{\\mathbb{P}}\\{\\textup{d}\\bar l(t_{j + 1})|\\bar a(t_j), \\mathscr{F}_{t_{j}}\\} \\\\\n    &~~~~~~- \\operatorname{\\mathbb{P}}(\\textup{d}x_{\\bar a}\\textup{d}l_{\\bar a})\\mathbb{G}(\\textup{d}\\bar a)\\delta_{\\bar a}\\Bigg\\|_{\\textup{TV}}\\\\\n    &\\leq\\Bigg\\|\\prod_{j = 0}^{K - 1}\\mathbb{G}\\{\\textup{d}\\bar a(t_{j + 1})|\\bar a(t_j)\\}\\{1 - \\mathbbm{1}(x \\leq t_{j + 1}, \\delta = 0)\\}\\\\\n    &~~~~~~\\operatorname{\\mathbb{P}}\\{T \\leq t_{j + 1}|\\mathscr{F}_{/C, t_j}\\}^{\\mathbbm{1}(t_{j} < x \\leq t_{j + 1})}\\operatorname{\\mathbb{P}}\\{T > t_{j + 1}|\\mathscr{F}_{/C, t_j}\\}^{1 - \\mathbbm{1}(t_{j} < x \\leq t_{j + 1})}\\operatorname{\\mathbb{P}}\\{\\textup{d}\\bar l(t_{j + 1})|\\mathscr{F}_{/C, t_{j}}\\}\\\\\n    &~~~~~~- \\operatorname{\\mathbb{P}}(\\textup{d}x_{\\bar a}\\textup{d}l_{\\bar a})\\mathbb{G}(\\textup{d}\\bar a)\\delta_{\\bar a}\\Bigg\\|_{\\textup{TV}} + o(1)\\\\\n    &=\\Bigg\\|\\prod_{j = 0}^{K - 1}\\mathbb{G}\\{\\textup{d}\\bar a(t_{j + 1})|\\bar a(t_j)\\}\\{1 - \\mathbbm{1}(x_{\\bar a} \\leq t_{j + 1}, \\delta_{\\bar a} = 0)\\}\\operatorname{\\mathbb{P}}\\{T_{\\bar a} \\leq t_{j + 1}|\\bar a(t_{j + 1}), \\mathscr{F}_{\\bar a, t_j}\\}^{\\mathbbm{1}(t_{j} < x_{\\bar a} \\leq t_{j + 1}, \\delta_{\\bar a} = 1)}\\\\\n    &~~~~~~\\operatorname{\\mathbb{P}}\\{T_{\\bar a} > t_{j + 1}|\\bar a(t_{j + 1}), \\mathscr{F}_{\\bar a, t_j}\\}^{1 - \\mathbbm{1}(t_{j} < x_{\\bar a} \\leq t_{j + 1}, \\delta_{\\bar a} = 1)}\\operatorname{\\mathbb{P}}\\{\\textup{d}\\bar l_{\\bar a}(t_{j + 1})|\\bar a(t_{j + 1}), \\mathscr{F}_{\\bar a, t_{j}}\\} \\\\\n    &~~~~~~- \\operatorname{\\mathbb{P}}(\\textup{d}x_{\\bar a}\\textup{d}l_{\\bar a})\\mathbb{G}(\\textup{d}\\bar a)\\delta_{\\bar a}\\Bigg\\|_{\\textup{TV}}\\\\\n    &=\\Bigg\\|\\prod_{j = 0}^{K - 1}\\mathbb{G}\\{\\textup{d}\\bar a(t_{j + 1})|\\bar a(t_j)\\}\\{1 - \\mathbbm{1}(x_{\\bar a} \\leq t_{j + 1}, \\delta_{\\bar a} = 0)\\}\\\\\n    &~~~~~~(\\operatorname{\\mathbb{P}}\\{T_{\\bar a} \\leq t_{j + 1}|C > t_{j + 1}, \\bar a(t_{j + 1}), \\mathscr{F}_{\\bar a, t_j}\\}^{\\mathbbm{1}(t_{j} < x_{\\bar a} \\leq t_{j + 1}, \\delta_{\\bar a} = 1)}\\\\\n    &~~~~~~\\operatorname{\\mathbb{P}}\\{T_{\\bar a} > t_{j + 1}|C > t_{j + 1}, \\bar a(t_{j + 1}), \\mathscr{F}_{\\bar a, t_j}\\}^{1 - \\mathbbm{1}(t_{j} < x_{\\bar a} \\leq t_{j + 1}, \\delta_{\\bar a} = 1)}\\operatorname{\\mathbb{P}}\\{\\textup{d}\\bar l_{\\bar a}(t_{j + 1})|C > t_j, \\bar a(t_{j + 1}), \\mathscr{F}_{\\bar a, t_{j}}\\} \\\\\n    &~~~~~~- \\operatorname{\\mathbb{P}}\\{T_{\\bar a} \\leq t_{j + 1}|\\mathscr{F}_{\\bar a, t_j}\\}^{\\mathbbm{1}(t_{j} < x_{\\bar a} \\leq t_{j + 1})}\\operatorname{\\mathbb{P}}\\{T_{\\bar a} > t_{j + 1}|\\mathscr{F}_{\\bar a, t_j}\\}^{1 - \\mathbbm{1}(t_{j} < x_{\\bar a} \\leq t_{j + 1})}\\operatorname{\\mathbb{P}}\\{\\textup{d}\\bar l_{\\bar a}(t_{j + 1})|\\mathscr{F}_{\\bar a, t_{j}}\\})\\Bigg\\|_{\\textup{TV}} \\to 0.\n\\end{aligned}$$\n\n## Proof of Theorem [1](#thm:gformulapoint){reference-type=\"ref\" reference=\"thm:gformulapoint\"} {#proof-of-theorem-thmgformulapoint}\n\nSince $\\operatorname{\\mathbb{P}}_{\\mathbb{G}}(\\textup{d}x\\textup{d}\\delta\\textup{d}\\bar a \\textup{d}\\bar l)$ is a limit of measures in total variation norm of $$\\operatorname{\\mathbb{P}}_{\\Delta_K[0, \\infty], \\mathbb{G}}(\\textup{d}x\\textup{d}\\delta\\textup{d}\\bar a \\textup{d}\\bar l),$$ whenever $|\\Delta_K[t, \\infty]| \\to 0$, we have $$\\int f(x, \\delta, \\bar a, \\bar l)\\operatorname{\\mathbb{P}}_{\\Delta_K[0, \\infty], \\mathbb{G}}(\\textup{d}x\\textup{d}\\delta\\textup{d}\\bar a \\textup{d}\\bar l) \\to \\operatorname{\\mathbb{E}}_{\\mathbb{G}}\\{f(X, \\Delta, \\bar A, \\bar L)\\},$$ for any bounded functions $f(x, \\delta, \\bar a, \\bar l)$. Therefore, we have $$\\begin{aligned}\n    &\\left|H_{\\mathbb{G}}(0-) - \\int \\operatorname{\\mathbb{E}}\\{\\nu(T_{\\bar a}, Y_{\\bar a})\\} \\mathbb{G}(\\textup{d}\\bar a)\\right|\\\\\n    &\\leq \\left|\\int \\nu(x, \\bar y)\\operatorname{\\mathbb{P}}(\\textup{d}x\\textup{d}\\delta\\textup{d}\\bar a \\textup{d}\\bar l) - \\int \\nu(x, \\bar y)\\operatorname{\\mathbb{P}}_{\\Delta_K[0, \\infty], \\mathbb{G}}(\\textup{d}x\\textup{d}\\delta\\textup{d}\\bar a \\textup{d}\\bar l)\\right|\\\\\n    &+\\left|\\int \\nu(x, \\bar y)\\operatorname{\\mathbb{P}}_{\\Delta_K[0, \\infty], \\mathbb{G}}(\\textup{d}x\\textup{d}\\delta\\textup{d}\\bar a \\textup{d}\\bar l) - \\int \\operatorname{\\mathbb{E}}\\{\\nu(T_{\\bar a}, Y_{\\bar a})\\}\\mathbb{G}(\\textup{d}\\bar a)\\right|\\\\\n    &= \\left|\\int \\nu(x, \\bar y)\\operatorname{\\mathbb{P}}_{\\Delta_K[0, \\infty], \\mathbb{G}}(\\textup{d}x\\textup{d}\\delta\\textup{d}\\bar a \\textup{d}\\bar l) - \\int \\operatorname{\\mathbb{E}}\\{\\nu(T_{\\bar a}, Y_{\\bar a})\\}\\mathbb{G}(\\textup{d}\\bar a)\\right| + o(1)\\\\\n    &= \\Bigg|\\int \\nu(x, \\bar y)\\prod_{j = 0}^{K - 1}\\operatorname{\\mathbb{P}}\\{T \\leq t_{j + 1}|C > t_{j + 1}, \\bar a(t_{j + 1}), \\mathscr{F}_{t_j}\\}^{\\mathbbm{1}(t_{j} < x \\leq t_{j + 1}, \\delta = 1)}\\\\\n    &\\operatorname{\\mathbb{P}}\\{T > t_{j + 1}|C > t_{j + 1}, \\bar a(t_{j + 1}), \\mathscr{F}_{t_j}\\}^{1 - \\mathbbm{1}(t_{j} < x \\leq t_{j + 1}, \\delta = 1)}\\operatorname{\\mathbb{P}}\\{\\textup{d}\\bar l(t_{j + 1})|C > t_{j + 1}, \\bar a(t_{j + 1}), \\mathscr{F}_{t_{j}}\\}\\\\\n    &\\{1 - \\mathbbm{1}(x \\leq t_{j + 1}, \\delta = 0)\\}\\mathbb{G}\\{\\textup{d}\\bar a(t_{j + 1})|\\bar a(t_j)\\}- \\int \\operatorname{\\mathbb{E}}\\{\\nu(T_{\\bar a}, Y_{\\bar a})\\}\\mathbb{G}(\\textup{d}\\bar a)\\Bigg| + o(1),\n\\end{aligned}$$ where $o(1)$ converges to zero when $\\Delta_K[0, \\infty] \\to 0$. By Assumptions [1](#assump:CAR){reference-type=\"ref\" reference=\"assump:CAR\"}, [2](#assump:CIR){reference-type=\"ref\" reference=\"assump:CIR\"}, [3](#assump:strongconsistency){reference-type=\"ref\" reference=\"assump:strongconsistency\"}, and [4](#assump:contpospointCAR){reference-type=\"ref\" reference=\"assump:contpospointCAR\"}, the above term is less than or equal to $$\\begin{aligned}\n    &\\Bigg|\\int \\nu(x, \\bar y)\\operatorname{\\mathbb{P}}\\{T \\leq t_K|\\bar a(t_K), \\mathscr{F}_{t_{K - 1}}\\}^{\\mathbbm{1}(t_{K - 1} < x \\leq t_K, \\delta = 1)}\\\\\n    &\\operatorname{\\mathbb{P}}\\{T > t_K|\\bar a(t_K), \\mathscr{F}_{t_{K - 1}}\\}^{1 - \\mathbbm{1}(t_{K - 1} < x \\leq t_K, \\delta = 1)}\\operatorname{\\mathbb{P}}\\{\\textup{d}\\bar l(t_K)|\\bar a(t_K), \\mathscr{F}_{t_{K - 1}}\\}\\\\\n    &\\{1 - \\mathbbm{1}(x \\leq t_K, \\delta = 0)\\}\\mathbb{G}\\{\\textup{d}\\bar a(t_K)|\\bar a(t_{K - 1})\\}\\prod_{j = 0}^{K - }\\operatorname{\\mathbb{P}}\\{T \\leq t_{j + 1}|\\bar a(t_{j + 1}), \\mathscr{F}_{/C, t_j}\\}^{\\mathbbm{1}(t_{j} < x \\leq t_{j + 1}, \\delta = 1)}\\\\\n    &\\operatorname{\\mathbb{P}}\\{T > t_{j + 1}|\\bar a(t_{j + 1}), \\mathscr{F}_{/C, t_j}\\}^{1 - \\mathbbm{1}(t_{j} < x \\leq t_{j + 1}, \\delta = 1)}\\operatorname{\\mathbb{P}}\\{\\textup{d}\\bar l(t_{j + 1})|\\bar a(t_{j + 1}), \\mathscr{F}_{/C, t_{j}}\\}\\\\\n    &\\{1 - \\mathbbm{1}(x \\leq t_{j + 1}, \\delta = 0)\\}\\mathbb{G}\\{\\textup{d}\\bar a(t_{j + 1})|\\bar a(t_j)\\}- \\int \\operatorname{\\mathbb{E}}\\{\\nu(T_{\\bar a}, Y_{\\bar a})\\}\\mathbb{G}(\\textup{d}\\bar a)\\Bigg| + o(1),\n\\end{aligned}$$ which by Assumptions [1](#assump:CAR){reference-type=\"ref\" reference=\"assump:CAR\"}, [2](#assump:CIR){reference-type=\"ref\" reference=\"assump:CIR\"}, [3](#assump:strongconsistency){reference-type=\"ref\" reference=\"assump:strongconsistency\"}, and [4](#assump:contpospointCAR){reference-type=\"ref\" reference=\"assump:contpospointCAR\"}, equals $$\\begin{aligned}\n    &\\Bigg|\\int \\nu(x, \\bar y)\\operatorname{\\mathbb{P}}\\{T_{\\bar a} \\leq t_K|\\bar a(t_K), \\mathscr{F}_{t_{K - 1}}\\}^{\\mathbbm{1}(t_{K - 1} < x_{\\bar a} \\leq t_K, \\delta_{\\bar a} = 1)}\\\\\n    &\\operatorname{\\mathbb{P}}\\{T_{\\bar a} > t_K|\\bar a(t_K), \\mathscr{F}_{t_{K - 1}}\\}^{1 - \\mathbbm{1}(t_{K - 1} < x_{\\bar a} \\leq t_K, \\delta_{\\bar a} = 1)}\\operatorname{\\mathbb{P}}\\{\\textup{d}\\bar l_{\\bar a}(t_K)|\\bar a(t_K), \\mathscr{F}_{t_{K - 1}}\\}\\\\\n    &\\{1 - \\mathbbm{1}(x_{\\bar a} \\leq t_K, \\delta_{\\bar a} = 0)\\}\\mathbb{G}\\{\\textup{d}\\bar a(t_K)|\\bar a(t_{K - 1})\\}\\prod_{j = 0}^{K - }\\operatorname{\\mathbb{P}}\\{T \\leq t_{j + 1}|\\bar a(t_{j + 1}), \\mathscr{F}_{/C, t_j}\\}^{\\mathbbm{1}(t_{j} < x \\leq t_{j + 1}, \\delta = 1)}\\\\\n    &\\operatorname{\\mathbb{P}}\\{T > t_{j + 1}|\\bar a(t_{j + 1}), \\mathscr{F}_{/C, t_j}\\}^{1 - \\mathbbm{1}(t_{j} < x \\leq t_{j + 1}, \\delta = 1)}\\operatorname{\\mathbb{P}}\\{\\textup{d}\\bar l(t_{j + 1})|\\bar a(t_{j + 1}), \\mathscr{F}_{/C, t_{j}}\\}\\\\\n    &\\{1 - \\mathbbm{1}(x \\leq t_{j + 1}, \\delta = 0)\\}\\mathbb{G}\\{\\textup{d}\\bar a(t_{j + 1})|\\bar a(t_j)\\}- \\int \\operatorname{\\mathbb{E}}\\{\\nu(T_{\\bar a}, Y_{\\bar a})\\}\\mathbb{G}(\\textup{d}\\bar a)\\Bigg| + o(1),\n\\end{aligned}$$ which by Assumptions [1](#assump:CAR){reference-type=\"ref\" reference=\"assump:CAR\"}, [2](#assump:CIR){reference-type=\"ref\" reference=\"assump:CIR\"}, [3](#assump:strongconsistency){reference-type=\"ref\" reference=\"assump:strongconsistency\"}, and [4](#assump:contpospointCAR){reference-type=\"ref\" reference=\"assump:contpospointCAR\"}, is less than or equal to $$\\begin{aligned}\n    &\\Bigg|\\int \\nu(x, \\bar y)\\operatorname{\\mathbb{P}}\\{T_{\\bar a} \\leq t_K|\\mathscr{F}_{t_{K - 1}}\\}^{\\mathbbm{1}(t_{K - 1} < x_{\\bar a} \\leq t_K, \\delta_{\\bar a} = 1)}\\\\\n    &\\operatorname{\\mathbb{P}}\\{T_{\\bar a} > t_K|\\mathscr{F}_{t_{K - 1}}\\}^{1 - \\mathbbm{1}(t_{K - 1} < x_{\\bar a} \\leq t_K, \\delta_{\\bar a} = 1)}\\operatorname{\\mathbb{P}}\\{\\textup{d}\\bar l_{\\bar a}(t_K)|\\mathscr{F}_{t_{K - 1}}\\}\\\\\n    &\\{1 - \\mathbbm{1}(x_{\\bar a} \\leq t_K, \\delta_{\\bar a} = 0)\\}\\mathbb{G}\\{\\textup{d}\\bar a(t_K)|\\bar a(t_{K - 1})\\}\\prod_{j = 0}^{K - }\\operatorname{\\mathbb{P}}\\{T \\leq t_{j + 1}|\\bar a(t_{j + 1}), \\mathscr{F}_{/C, t_j}\\}^{\\mathbbm{1}(t_{j} < x \\leq t_{j + 1}, \\delta = 1)}\\\\\n    &\\operatorname{\\mathbb{P}}\\{T > t_{j + 1}|\\bar a(t_{j + 1}), \\mathscr{F}_{/C, t_j}\\}^{1 - \\mathbbm{1}(t_{j} < x \\leq t_{j + 1}, \\delta = 1)}\\operatorname{\\mathbb{P}}\\{\\textup{d}\\bar l(t_{j + 1})|\\bar a(t_{j + 1}), \\mathscr{F}_{/C, t_{j}}\\}\\\\\n    &\\{1 - \\mathbbm{1}(x \\leq t_{j + 1}, \\delta = 0)\\}\\mathbb{G}\\{\\textup{d}\\bar a(t_{j + 1})|\\bar a(t_j)\\}- \\int \\operatorname{\\mathbb{E}}\\{\\nu(T_{\\bar a}, Y_{\\bar a})\\}\\mathbb{G}(\\textup{d}\\bar a)\\Bigg| + o(1).\n\\end{aligned}$$ By iterating the above process for $0 \\leq j \\leq K - 2$, we arrive the conclusion.\n\n## Proof of Theorem [2](#thm:ipwpoint){reference-type=\"ref\" reference=\"thm:ipwpoint\"} {#proof-of-theorem-thmipwpoint}\n\nThe proof is immediate by noting that $$\\begin{aligned}\n    \\operatorname{\\mathbb{E}}\\left\\{Q_{\\mathbb{G}}(\\infty)\\nu(X, \\bar Y)\\right\\} = \\operatorname{\\mathbb{E}}_{\\mathbb{G}}\\{\\nu(X, \\bar Y)\\} = \\int_{\\mathcal{A}} \\operatorname{\\mathbb{E}}\\{\\nu(T_{\\bar a}, \\bar Y_{\\bar a})\\}\\mathbb{G}(\\textup{d}\\bar a),\n\\end{aligned}$$ by Theorem [1](#thm:gformulapoint){reference-type=\"ref\" reference=\"thm:gformulapoint\"}.\n\n## Proof of Theorem [3](#thm:drpoint){reference-type=\"ref\" reference=\"thm:drpoint\"} {#proof-of-theorem-thmdrpoint}\n\nWe first prove the theorem when $H = H_{\\mathbb{G}}$. Indeed, as the limit and expectation can interchange, we can show that $$\\begin{aligned}\n    &\\left|\\operatorname{\\mathbb{E}}\\left\\{\\Xi_{\\textup{out}, \\Delta_K[0, \\infty]}(H_{\\mathbb{G}}, Q)\\right\\}\\right|\\\\\n    &=\\bigg|\\operatorname{\\mathbb{E}}\\left(Q(t_K)[\\nu(X, \\bar Y) - H_{\\mathbb{G}}(t_K)\\}]\\right) \\\\\n    &+ \\sum_{j = 1}^{K - 1} \\operatorname{\\mathbb{E}}\\left(Q(t_j)\\left[\\int H_{\\mathbb{G}}(t_{j + 1})\\mathbb{G}\\{\\textup{d}\\bar a(t_{j + 1})|\\bar A(t_j)\\}- H_{\\mathbb{G}}(t_j)\\right]\\right)\\bigg|\\\\\n    &=\\Bigg|0 + \\sum_{j = 1}^{K - 1} \\operatorname{\\mathbb{E}}\\left(Q(t_j)\\left[\\int H_{\\mathbb{G}}(t_{j + 1})\\mathbb{G}\\{\\textup{d}\\bar a(t_{j + 1})|\\bar A(t_j)\\}- H_{\\mathbb{G}}(t_j)\\right]\\right)\\Bigg|\\\\\n    &\\leq \\sum_{j = 1}^{K - 1} \\Bigg|\\operatorname{\\mathbb{E}}\\left(Q(t_j)\\left[\\int H_{\\mathbb{G}}(t_{j + 1})\\mathbb{G}\\{\\textup{d}\\bar a(t_{j + 1})|\\bar A(t_j)\\}- H_{\\mathbb{G}}(t_j)\\right]\\right)\\Bigg| \\\\\n    &\\leq \\sum_{j = 1}^{K - 1} \\Bigg|\\operatorname{\\mathbb{E}}\\left(Q(t_j)\\left[\\int H_{\\mathbb{G}}(t_{j + 1})\\mathbb{G}\\{\\textup{d}\\bar a(t_{j + 1})|\\bar A(t_j)\\}- \\operatorname{\\mathbb{E}}_{\\mathbb{G}}\\left\\{H_{\\mathbb{G}}(t_{j + 1})|\\mathscr{G}_{t_j}\\right\\}\\right]\\right)\\Bigg|\\\\\n    &\\leq \\sum_{j = 0}^K\\kappa\\|H_{\\mathbb{G}}(t_j)Q(t_j)\\|_{1}(t_{j + 1} - t_j)^{\\alpha}\\\\\n    &\\leq \\kappa\\sup_t\\|H_{\\mathbb{G}}(t)Q(t)\\|_{1}\\sum_{j = 1}^{K - 1}(t_{j + 1} - t_j)^{\\alpha} \\to 0,\n\\end{aligned}$$ when $|\\Delta_K[0, \\infty]| \\to 0$, where we have used the fact that $H_{\\mathbb{G}}(t)$ is a $\\operatorname{\\mathbb{P}}_{\\mathbb{G}}$-martingale and Assumptions [1](#assump:CAR){reference-type=\"ref\" reference=\"assump:CAR\"}, [2](#assump:CIR){reference-type=\"ref\" reference=\"assump:CIR\"}.\n\nWe now proceed to the case when $Q = Q_{\\mathbb{G}}$. Indeed, as the limit and expectation can interchange, we can show that $$\\begin{aligned}\n    &\\left|\\operatorname{\\mathbb{E}}\\left\\{\\Xi_{\\textup{trt}, \\Delta_K[0, \\infty]}(H, Q_{\\mathbb{G}})\\right\\}\\right|\\\\\n    &=\\Bigg|\\operatorname{\\mathbb{E}}\\left(\\sum_{j = 0}^K\\left[Q_{\\mathbb{G}}(t_j)H(t_j) - Q_{\\mathbb{G}}(t_{j - 1})\\int H(t_j)\\mathbb{G}\\{\\textup{d}\\bar a(t_j)|\\bar A(t_{j - 1})\\}\\right]\\right) \\\\\n    &-\\operatorname{\\mathbb{E}}\\left[Q_{\\mathbb{G}}(0)H(0) - \\int H(0)\\mathbb{G}\\{\\textup{d}\\bar a(0)\\}\\right]\\Bigg|\\\\\n    &=\\Bigg|\\operatorname{\\mathbb{E}}\\left(\\sum_{j = 0}^K\\left[Q_{\\mathbb{G}}(t_j)H(t_j) - Q_{\\mathbb{G}}(t_{j - 1})\\int H(t_j)\\mathbb{G}\\{\\textup{d}\\bar a(t_j)|\\bar A(t_{j - 1})\\}\\right]\\right) + 0\\Bigg|\\\\\n    &\\leq \\sum_{j = 0}^K\\left|\\operatorname{\\mathbb{E}}\\left[Q_{\\mathbb{G}}(t_j)H(t_j) - Q_{\\mathbb{G}}(t_{j - 1})\\int H(t_j)\\mathbb{G}\\{\\textup{d}\\bar a(t_j)|\\bar A(t_{j - 1})\\}\\right]\\right|\\\\\n    &= \\sum_{j = 0}^K\\bigg|\\operatorname{\\mathbb{E}}\\bigg[Q_{\\mathbb{G}}(t_{j - 1})\\operatorname{\\mathbb{E}}_{\\mathbb{G}}\\{H(t_j)|\\mathcal{G}_{t_{j - 1}}\\} \\\\\n    &- Q_{\\mathbb{G}}(t_{j - 1})\\int H(t_j)\\mathbb{G}\\{\\textup{d}\\bar a(t_j)|\\bar A(t_{j - 1})\\}\\operatorname{\\mathbb{P}}\\{\\textup{d}\\bar l(t_j)|\\mathcal{G}_{t_{j - 1}}\\}\\bigg]\\bigg|\\\\\n    &\\leq \\kappa\\sum_{j = 0}^K\\|H(t_{j - 1})Q_{\\mathbb{G}}(t_{j - 1})\\|_{1}(t_j - t_{j - 1})^\\alpha\\\\\n    &\\leq \\kappa\\sup_t\\|H(t)Q_{\\mathbb{G}}(t)\\|_{1}\\sum_{j = 0}^K(t_j - t_{j - 1})^\\alpha \\to 0,\n\\end{aligned}$$ when $|\\Delta_K[0, \\infty]| \\to 0$, where we have used the fact that $Q_{\\mathbb{G}}(t)$ is a $\\operatorname{\\mathbb{P}}$-martingale and Assumptions [1](#assump:CAR){reference-type=\"ref\" reference=\"assump:CAR\"}, [2](#assump:CIR){reference-type=\"ref\" reference=\"assump:CIR\"}.\n\n## Proof of Theorem [4](#thm:norestriction){reference-type=\"ref\" reference=\"thm:norestriction\"} {#proof-of-theorem-thmnorestriction}\n\nWe first simplify our setting by ignoring censoring and absorbing event time $T$ into $\\bar L$ as well. This is because the conditional independent censoring assumption (Assumption [2](#assump:CIR){reference-type=\"ref\" reference=\"assump:CIR\"}) is known to be nonparametric. Our observed data become $(\\bar A, \\bar L)$ and full data become $(\\bar A, \\bar L_{\\mathcal{A}})$.\n\nWe first proves that Assumption [1](#assump:CAR){reference-type=\"ref\" reference=\"assump:CAR\"} does not have restrictions on the observed data. We proceed with a constructive proof. For any partition $\\Delta_K[0, \\infty]$, we define a measure on the full data path space. In fact, one has the knowledge on the decomposition $$\\begin{aligned}\n    \\operatorname{\\mathbb{P}}(\\textup{d}\\bar a \\textup{d}\\bar l) &= \\prod_{j = 0}^{K - 1}\\left[\\operatorname{\\mathbb{P}}\\{\\textup{d}\\bar a(t_{j + 1})|\\mathscr{F}_{t_j}\\}\\operatorname{\\mathbb{P}}\\{\\textup{d}\\bar l(t_{j + 1})|\\bar a(t_{j + 1}), \\mathscr{F}_{t_j}\\}\\right]\\\\\n    &= \\prod_{j = 0}^{K - 1}\\left[\\operatorname{\\mathbb{P}}\\{\\textup{d}\\bar a(t_{j + 1})|\\bar a(t_j), \\bar l_{\\bar a}(t_{j})\\}\\operatorname{\\mathbb{P}}\\{\\textup{d}\\bar l_{\\bar a}(t_{j + 1})|\\bar a(t_{j + 1}), \\bar l_{\\bar a}(t_j)\\}\\right].\n\\end{aligned}$$ Intuitively $\\operatorname{\\mathbb{P}}\\{\\textup{d}\\bar l_{\\bar a}(t_{j + 1})|\\bar a(t_{j + 1}), \\bar l_{\\bar a}(t_j)\\}$ are close to $\\operatorname{\\mathbb{P}}\\{\\textup{d}\\bar l_{\\bar a}(t_{j + 1})|\\bar l_{\\bar a}(t_j)\\}$, whereas the other term $\\operatorname{\\mathbb{P}}\\{\\textup{d}\\bar a(t_{j + 1})|\\bar a(t_j), \\bar l(t_{j})\\}$ is close to $\\operatorname{\\mathbb{P}}\\{\\textup{d}\\bar a(t_{j + 1})|\\bar a(t_j), \\bar l_{\\mathcal{A}}\\}$. Therefore, one may define a measure on the full data path space by $$\\operatorname{\\mathbb{P}}_{\\Delta_K[0, \\infty]}^F(\\textup{d}\\bar l_{\\bar a}) := \\prod_{j = 0}^{K - 1}\\operatorname{\\mathbb{P}}\\{\\textup{d}\\bar l_{\\bar a}(t_{j + 1})|\\bar a(t_{j + 1}), \\bar l_{\\bar a}(t_j)\\} = \\prod_{j = 0}^{K - 1}\\operatorname{\\mathbb{P}}\\{\\textup{d}\\bar l_{\\bar a}(t_{j + 1})|\\bar a(t_{j + 1}), \\mathscr{F}_{t_j}\\}.$$ Then without loss of generality one may construct $\\operatorname{\\mathbb{P}}_{\\Delta_K[0, \\infty]}^F(\\bar l_{\\mathcal{A}})$ by assuming joint independence among $\\bar l_{\\mathcal{A}}$. One can also define $$\\operatorname{\\mathbb{P}}_{\\Delta_K[0, \\infty]}^F(\\textup{d}\\bar a|\\bar l_{\\mathcal{A}}) := \\prod_{j = 0}^{K - 1}\\operatorname{\\mathbb{P}}\\{\\textup{d}\\bar a(t_{j + 1})|\\mathscr{F}_{t_j}\\}.$$ Then for any sequences of partitions with the mesh going to zero, one may construct a sequence of measures and show that this sequence of measures is Cauchy by a triangular inequality and Assumption [1](#assump:CAR){reference-type=\"ref\" reference=\"assump:CAR\"}, following a similar logic as previous proofs. Therefore the sequence converge to a measure $\\operatorname{\\mathbb{P}}^F$, which is independent of the choice of partitions.\n\nNext we need to show that $\\operatorname{\\mathbb{P}}^F$ induces $\\operatorname{\\mathbb{P}}$ on the observed data and $\\operatorname{\\mathbb{P}}^F$ satisfies Assumption [1](#assump:CAR){reference-type=\"ref\" reference=\"assump:CAR\"}. The first is trivial because any $\\operatorname{\\mathbb{P}}_{\\Delta_K[0, \\infty]}^F$ induces $\\operatorname{\\mathbb{P}}$ on the observed data, then so is their limit. To prove the second, for any time $t$ and $\\varepsilon> 0$, one might smartly choose a partition $\\Delta_K[0, \\infty]$ with $\\operatorname{\\mathbb{P}}_{\\Delta_K[0, \\infty]}^F$ close enough to $\\operatorname{\\mathbb{P}}^F$ and $t, t + \\eta \\in \\Delta_K[0, \\infty]$. This can be done because the convergence point is independent of the choice of partitions. We have $$\\begin{aligned}\n    &\\operatorname{\\mathbb{E}}^F(\\|\\operatorname{\\mathbb{P}}^F\\{\\textup{d}\\bar l_{\\mathcal{A}}|\\mathscr{F}_{t}\\} - \\operatorname{\\mathbb{P}}^F\\{\\textup{d}\\bar l_{\\mathcal{A}}|\\bar A(t + \\eta), \\mathscr{F}_{t}\\}\\|_{\\textup{TV}}) \\\\\n    &\\leq \\operatorname{\\mathbb{E}}^F(\\|\\operatorname{\\mathbb{P}}^F\\{\\textup{d}\\bar l_{\\mathcal{A}}|\\mathscr{F}_{t}\\} - \\operatorname{\\mathbb{P}}_{\\Delta_K[0, \\infty]}^F\\{\\textup{d}\\bar l_{\\mathcal{A}}|\\mathscr{F}_{t}\\}\\|_{\\textup{TV}}) \\\\\n    &+ \\operatorname{\\mathbb{E}}^F(\\|\\operatorname{\\mathbb{P}}_{\\Delta_K[0, \\infty]}^F\\{\\textup{d}\\bar l_{\\mathcal{A}}|\\mathscr{F}_{t}\\} - \\operatorname{\\mathbb{P}}_{\\Delta_K[0, \\infty]}^F\\{\\textup{d}\\bar l_{\\mathcal{A}}|\\bar A(t + \\eta), \\mathscr{F}_{t}\\}\\|_{\\textup{TV}}) \\\\\n    &+ \\operatorname{\\mathbb{E}}^F(\\|\\operatorname{\\mathbb{P}}_{\\Delta_K[0, \\infty]}^F\\{\\textup{d}\\bar l_{\\mathcal{A}}|\\bar A(t + \\eta), \\mathscr{F}_{t}\\} - \\operatorname{\\mathbb{P}}^F\\{\\textup{d}\\bar l_{\\mathcal{A}}|\\bar A(t + \\eta), \\mathscr{F}_{t}\\}\\|_{\\textup{TV}})\\\\\n    &\\leq 2\\|\\operatorname{\\mathbb{P}}^F - \\operatorname{\\mathbb{P}}_{\\Delta_K[0, \\infty]}^F\\|_{\\textup{TV}} \\\\\n    &+ \\operatorname{\\mathbb{E}}^F(\\|\\operatorname{\\mathbb{P}}_{\\Delta_K[0, \\infty]}^F\\{\\textup{d}\\bar l_{\\mathcal{A}}|\\mathscr{F}_{t}\\} - \\operatorname{\\mathbb{P}}_{\\Delta_K[0, \\infty]}^F\\{\\textup{d}\\bar l_{\\mathcal{A}}|\\bar A(t + \\eta), \\mathscr{F}_{t}\\}\\|_{\\textup{TV}})\\\\\n    &\\leq 4\\|\\operatorname{\\mathbb{P}}^F - \\operatorname{\\mathbb{P}}_{\\Delta_K[0, \\infty]}^F\\|_{\\textup{TV}} \\\\\n    &+ \\operatorname{\\mathbb{E}}_{\\Delta_K[0, \\infty]}^F(\\|\\operatorname{\\mathbb{P}}_{\\Delta_K[0, \\infty]}^F\\{\\textup{d}\\bar l_{\\mathcal{A}}|\\mathscr{F}_{t}\\} - \\operatorname{\\mathbb{P}}_{\\Delta_K[0, \\infty]}^F\\{\\textup{d}\\bar l_{\\mathcal{A}}|\\bar A(t + \\eta), \\mathscr{F}_{t}\\}\\|_{\\textup{TV}}).\n\\end{aligned}$$ The first term can be chosen to be sufficiently small. We rewrite the second term $$\\begin{aligned}\n    &\\operatorname{\\mathbb{E}}_{\\Delta_K[0, \\infty]}^F(\\|\\operatorname{\\mathbb{P}}_{\\Delta_K[0, \\infty]}^F\\{\\textup{d}\\bar l_{\\mathcal{A}}|\\mathscr{F}_{t}\\} - \\operatorname{\\mathbb{P}}_{\\Delta_K[0, \\infty]}^F\\{\\textup{d}\\bar l_{\\mathcal{A}}|\\bar A(t + \\eta), \\mathscr{F}_{t}\\}\\|_{\\textup{TV}})\\\\\n    &=\\sup_{f: \\|f\\|_1 = 1} \\int f(\\bar l_{\\mathcal{A}}, \\{y_{\\bar a'}\\}_{\\bar a' \\in \\mathcal{A}})[\\operatorname{\\mathbb{P}}_{\\Delta_K[0, \\infty]}^F\\{\\textup{d}\\bar l_{\\mathcal{A}}|\\mathscr{F}_{t}\\} \\\\\n    &~~- \\operatorname{\\mathbb{P}}_{\\Delta_K[0, \\infty]}^F\\{\\textup{d}\\bar l_{\\mathcal{A}}|\\bar a(t + \\eta), \\mathscr{F}_{t}\\}]\\operatorname{\\mathbb{P}}\\{\\textup{d}\\bar a(t + \\eta)\\textup{d}\\mathscr{F}_{t}\\}\\\\\n    &\\leq \\sup_{f: \\|f\\|_1 = 1}\\sup_{g: \\|g\\|_1 = 1} \\int f(\\bar l_{\\mathcal{A}})g(\\bar a(t + \\eta))[\\operatorname{\\mathbb{P}}_{\\Delta_K[0, \\infty]}^F\\{\\textup{d}\\bar l_{\\mathcal{A}}\\textup{d}\\bar a(t + \\eta)|\\mathscr{F}_{t}\\} \\\\\n    &~~- \\operatorname{\\mathbb{P}}_{\\Delta_K[0, \\infty]}^F\\{\\textup{d}\\bar l_{\\mathcal{A}}|\\mathscr{F}_{t}\\}\\operatorname{\\mathbb{P}}_{\\Delta_K[0, \\infty]}^F\\{\\textup{d}\\bar a(t + \\eta)|\\mathscr{F}_{t}\\}]\\operatorname{\\mathbb{P}}\\{\\textup{d}\\mathscr{F}_{t}\\}\\\\\n    &= \\sup_{f: \\|f\\|_1 = 1}\\sup_{g: \\|g\\|_1 = 1} \\int f(\\bar l_{\\mathcal{A}})g(\\bar a(t + \\eta))\\operatorname{\\mathbb{P}}_{\\Delta_K[0, \\infty]}^F\\{\\textup{d}\\bar l_{\\mathcal{A}}|\\mathscr{F}_{t}\\} \\\\\n    &~~[\\operatorname{\\mathbb{P}}_{\\Delta_K[0, \\infty]}^F\\{\\textup{d}\\bar a(t + \\eta)|\\bar a(t), \\bar l_{\\mathcal{A}}\\} - \\operatorname{\\mathbb{P}}_{\\Delta_K[0, \\infty]}^F\\{\\textup{d}\\bar a(t + \\eta)|\\mathscr{F}_{t}\\}]\\operatorname{\\mathbb{P}}\\{\\textup{d}\\mathscr{F}_{t}\\}\\\\\n    &= \\sup_{f: \\|f\\|_1 = 1}\\sup_{g: \\|g\\|_1 = 1} \\int f(\\bar l_{\\mathcal{A}})g(\\bar a(t + \\eta))\\operatorname{\\mathbb{P}}_{\\Delta_K[0, \\infty]}^F\\{\\textup{d}\\bar l_{\\mathcal{A}}|\\mathscr{F}_{t}\\} \\\\\n    &~~\\left[\\prod_{j = 0}^{K - 1}\\operatorname{\\mathbb{P}}\\{\\textup{d}\\bar a(t + \\eta)|\\mathscr{F}_{t}\\} - \\operatorname{\\mathbb{P}}\\{\\textup{d}\\bar a(t + \\eta)|\\mathscr{F}_{t}\\}\\right]\\operatorname{\\mathbb{P}}\\{\\textup{d}\\mathscr{F}_{t}\\}\\\\\n    &\\leq \\operatorname{\\mathbb{E}}\\left\\|\\prod_{j = 0}^{K - 1}\\operatorname{\\mathbb{P}}\\{\\textup{d}\\bar a(t + \\eta)|\\mathscr{F}_{t}\\} - \\operatorname{\\mathbb{P}}\\{\\textup{d}\\bar a(t + \\eta)|\\mathscr{F}_{t}\\}\\right\\|_{\\textup{TV}}\\\\\n    &\\leq \\varepsilon(t, \\eta).\n\\end{aligned}$$\n\nAssumption [3](#assump:strongconsistency){reference-type=\"ref\" reference=\"assump:strongconsistency\"} is irrelevant here because it is imposed on the stochastic process but not the distributions.\n\nFor Assumption [4](#assump:contpospointCAR){reference-type=\"ref\" reference=\"assump:contpospointCAR\"}, a necessary condition for it to hold is that $\\operatorname{\\mathbb{P}}$ is well supported on the path space conditioning on any filtration. For this to happen, note that for any $\\operatorname{\\mathbb{P}}$, one can find a well-supported $\\operatorname{\\mathbb{P}}'$ satisfying Assumptions [1](#assump:CAR){reference-type=\"ref\" reference=\"assump:CAR\"}, [2](#assump:CIR){reference-type=\"ref\" reference=\"assump:CIR\"}, [3](#assump:strongconsistency){reference-type=\"ref\" reference=\"assump:strongconsistency\"}, so that $(1 - \\varepsilon)\\operatorname{\\mathbb{P}}+ \\varepsilon\\operatorname{\\mathbb{P}}'$ is well-supported and hence satisfies Assumption [4](#assump:contpospointCAR){reference-type=\"ref\" reference=\"assump:contpospointCAR\"}. Since addition will not break Assumptions [1](#assump:CAR){reference-type=\"ref\" reference=\"assump:CAR\"}, [2](#assump:CIR){reference-type=\"ref\" reference=\"assump:CIR\"}, [3](#assump:strongconsistency){reference-type=\"ref\" reference=\"assump:strongconsistency\"}, we have found $(1 - \\varepsilon)\\operatorname{\\mathbb{P}}+ \\varepsilon\\operatorname{\\mathbb{P}}'$ satisfying Assumptions [1](#assump:CAR){reference-type=\"ref\" reference=\"assump:CAR\"}, [2](#assump:CIR){reference-type=\"ref\" reference=\"assump:CIR\"}, [3](#assump:strongconsistency){reference-type=\"ref\" reference=\"assump:strongconsistency\"}, and [4](#assump:contpospointCAR){reference-type=\"ref\" reference=\"assump:contpospointCAR\"}, and approximates $\\operatorname{\\mathbb{P}}$.",
    "rationale": "Summary: The paper is challenging to follow, so I may have misunderstood some parts. My understanding is that the \"functional longitudinal data\" investigated here are conventional functional data, as described by Wang et al. (2016), which can be measured intensively, sparsely, or irregularly. However, this paper focuses solely on the ideal (hypothetical) setting where continuous-time measurements are available for each experimental subject, resulting in infinite-dimensional data. If this interpretation is correct, the goal of this paper is to explore causal identification for infinite-dimensional functional (time-varying) outcomes that are subject to mortality and censoring by generalizing the classical g-computation, inverse probability weighting, and doubly robust formulas.\n\nReference: \nWang, Chiou and Müller (2016). Functional data analysis. Annual Review of Statistics ands its application.\n\nStrengths: The approach is nonparametric and it accommodates functional treatment processes A(t) and functional confounders L(t), as well as functional response Y(t).\n\nWeaknesses: The paper is hard to follow and the connection of the event-time T to the outcome Y(t) is unclear.\n\nQuestions: Could you elaborate on the situation when A(t) is a function? \n\nWhy should Y(t) be a subset of L(t), and what does it mean?",
    "rating": 2,
    "label": false,
    "rationale_edited": " The paper is challenging to follow, so I may have misunderstood some parts. My understanding is that the \"functional longitudinal data\" investigated here are conventional functional data, as described by Wang et al. (2016), which can be measured intensively, sparsely, or irregularly. However, this paper focuses solely on the ideal (hypothetical) setting where continuous-time measurements are available for each experimental subject, resulting in infinite-dimensional data. If this interpretation is correct, the goal of this paper is to explore causal identification for infinite-dimensional functional (time-varying) outcomes that are subject to mortality and censoring by generalizing the classical g-computation, inverse probability weighting, and doubly robust formulas.\n\nStrengths: The approach is nonparametric and it accommodates functional treatment processes A(t) and functional confounders L(t), as well as functional response Y(t).\n\nWeaknesses: The paper is hard to follow and the connection of the event-time T to the outcome Y(t) is unclear."
  },
  {
    "title": "Dynamic Memory Based Adaptive Optimization",
    "abstract": "Define an optimizer as having memory $k$ if it stores $k$ dynamically changing vectors in the parameter space. Classical SGD has memory $0$, momentum SGD optimizer has $1$ and Adam optimizer has $2$. We address the following questions: *How can optimizers make use of more memory units? What information should be stored in them? How to use them for the learning steps?* As an approach to the last question, we introduce a general method called \"Retrospective Learning Law Correction\" or shortly RLLC. This method is designed to calculate a dynamically varying linear combination (called *learning law*) of memory units, which themselves may evolve arbitrarily. We demonstrate RLLC on optimizers whose memory units have linear update rules and small memory ($\\leq 4$ memory units). Our experiments show that in a variety of standard problems, these optimizers outperform the above mentioned three classical optimizers. We conclude that RLLC is a promising framework for boosting the performance of known optimizers by adding more memory units and by making them more adaptive.",
    "text": "# Introduction\n\nIn this paper, we investigate optimizers that store $k$ vectors in the parameter space $\\mathbb{R}^n$ of a neural network or more generally in the parameter space related to any optimization problem. We call such vectors *memory units* and we measure the memory usage of an optimizer by the number them.\n\nThe simplest example for an optimizer with memory is the momentum SGD optimizer which stores a single vector $m$ (*momentum vector*) in the parameter space $\\mathbb{R}^n$. In each step, $m$ is updated according to the rule $m\\longleftarrow \\beta m+\\bigtriangledown_\\theta f(\\theta)$ where $f$ is the objective function, $\\theta\\in\\mathbb{R}^n$ is the parameter vector and $0<\\beta<1$ is a fixed real number. The vector $\\theta$ is updated according to the rule $\\theta\\longleftarrow\\theta-cm$ where $c>0$ is the learning rate.\n\nThe Adam optimizer operates with two memory units. One of them is the *momentum vector* and the other one is the momentum of the squares of the gradient vectors. In contrast with the momentum optimizer, the Adam optimizer is not linear in the gradient vectors. Neither the update rule of the memory units, nor the way the memory units are used for the parameter update is linear.\n\nThe present paper has two independent contributions. The first contribution is a novel and simple method that we call RLLC=*Retrospective Learning Law Correction*. It is an update rule for a vector $L$ (called *learning law*) that describes a natural way of using a set of dynamically changing memory units for the update of the parameter vector $\\theta$. More precisely, $L\\in\\mathbb{R}^k$ contains the coefficients of a linear combination of the $k$ memory units which is multiplied by a fixed learning rate $c_1$ and substracted from $\\theta$ as usual. In each step, before updating $\\theta$ and the memory units, we update $L$ by the formula $L\\longleftarrow L+c_2M^+g$ where $M$ is the $n\\times k$ matrix formed by the memory units, $M^+$ is the Moore-Penrose inverse of $M$, $g$ is the newly received gradient and $c_2$ is the *meta learning rate*. The main idea behind the update rule for $L$ is that the new gradient $g$ contains retrospective information on how the algorithm could have performed better in the previous step. Thus it can be used to compute a corrected version of $L$ which \\\"thinks more ahead\\\". Note that our update rule of the learning law can also be regarded as a general framework for associating a $k$-dimensional adaptive learning rate with an arbitrary set of $k$ evolving memory units.\n\nAs the second main contribution, we examine optimizers in which memory units are updated by fixed linear rules. More precisely, in each step each memory unit is updated to a linear combination of the memory units and the new arriving gradient. The parameter vector is updated by a (possibly changing) liner combination (given by the learning law $L$) of the memory units. Such optimizers are interesting even if the learning law is fixed. They include SGD, momentum SGD and Nesterov Accelerated Gradient (NAG) [@Nesterov2012GradientMF]. Thus, the linear framework provides a useful generalization of these famous optimizers and enables a dynamically changing continuous interpolation between them. The RLLC method turns out to be ideal for this. Our experiments show that linear memory combined with RLLC leads to powerful optimizers. The case of memory $1$ is already interesting. A memory $1$ linear optimizer stores a momentum vector. Applying RLLC in this trivial setting yields a variant of the momentum SGD optimizer enhanced with a new type of adaptive learning rate. As the number of memory units increases, the mathematics becomes more complex, presenting a field of study that is interesting in its own right. We present some of the fundamental properties of linearly updated memory units. In particular, we prove a version of basis independence for RLLC combined with linear memory which allows us to apply basis transformations to the update rules without changing the optimization process. This together with a variant of the Jordan normal form over the field $\\mathbb{R}$ helps to convert these optimizers into a canonical form in which each memory unit is associated with a so-called Jordan block. A Jordan block of size $1$ corresponds to a single memory unit (denoted by $M(\\beta)$) storing a momentum vector of the gradients with parameter $\\beta$. A Jordan block of size $2$ either corresponds to a pair of memory units (denoted by $CM(\\beta),\\beta\\in\\mathbb{C}$) namely the real and imaginary parts of a momentum vector with complex parameter or to a pair of memory units $m_1,m_2$ (denoted by $M_2(\\beta)$) where $m_1$ is a momentum vector of the gradient and $m_2$ is a momentum vector of $m_1$, both with parameter $\\beta$. In general, there are two infinite families of Jordan blocks giving rise to $k$-tuples or $2k$-tuples of memory units denoted by $M_k(\\beta)$ and $CM_k(\\gamma)$. These are the fundamental building blocks of linearly updated memory. We denote the natural operation by $\\oplus$ which combines these building blocks into larger memory by the union of the corresponding memory units. By slightly abusing the notation we often identify memory update rules with optimizers where learning is given by the RLLC method. For example, $M(\\beta)$ also denotes the memory $1$ optimizer with memory unit $M(\\beta)$ (a momentum vector) and with RLLC. Notice that the $M(\\beta)$ optimizer is a close relative of momentum SGD but it is not equivalent with it.\n\nIn our experiments, we identified a number of interesting simple settings involving few (at most $4)$ memory units. These include the types of optimizers $M(\\beta)$, $M(\\beta)\\oplus M(0)$, $M(\\beta_1)\\oplus M(\\beta_2)\\oplus M(\\beta_3)$, $M_2(\\beta)$, $M_3(\\beta)$ and $M(\\beta){\\oplus}M(-\\beta){\\oplus}CM(\\beta i)$. We observed that these optimizers often surpassed the performance of three commonly used optimizers across a variety of tasks even without carefully optimizing the parameters $\\beta_i$. Notice that $M(\\beta)\\oplus M(0)$ is an adaptively changing linear combination of SGD, momentum SGD and NAG. Thus, it adaptively interpolates between three well known optimizers. Remarkably, it demonstrated competitive or even superior performance compared to the Adam optimizer in many tasks, which also uses two memory units.\n\nThis paper primarily aims not to challenge all existing optimizers in the field, but rather to introduce a novel mathematical concept that could spark further research. The experimental results presented here should be interpreted as an illustration of the potential of our approach. We posit that the implications of the RLLC method extend beyond mere enhancements to current optimization techniques, suggesting broader applications and insights in the realm of optimization and machine learning.\n\n# Related Work\n\nIn the field of deep neural networks, the most commonly used optimizers are still hand-designed, such as momentum SGD, RMSProp, or Adam [@adam_opt]. Tremendous attempts were made to improve their performance, or exceed them with different approaches. Finding the best optimizer in general is not possible, see [@optimizers_no_free_lunch]. However by restricting the tasks, and hence, finding a better optimizer can be regarded as a learning process. This learning can focus either within a single task, or across multiple tasks.\n\nFor the former, typical approaches include adaptive learning based methods [@HyperAdam; @switch_from_Adam_to_SGD] and extensions of hand-crafted optimizers (e.g.: by replacing some components with RNN) [@learning_to_optimize_by_lr; @learning_to_optimize_by_lr2; @learning_to_optimize_primer_and_benchmark]. However these methods aim to speed up single task convergence, but adaptive learning may have some drawbacks, especially on NLP tasks, and certain synthetic problems [@adaptive_methods_slow_convergence].\n\nThe later one, namely learned optimization, trains a function (typically an auxiliary neural network) to optimize the model that solves the original tasks. Some of them employ traditional gradient-descent based optimizers [@andrychowicz2016learning; @metz2022velo; @155621; @10.5555/3305890.3306069], other methods utilize evolutionary algorithms [@155621; @metz2020tasks], or reinforcement learning [@li2017learning; @bello2017neural]. These methods aim to discover an optimizer via some form of learning that enhance performance over a diverse set of tasks, surpassing the effectiveness of commonly used, hand-designed optimizers.\n\nOur approach aligns more with the adaptive learning based methods. However, rather than focusing on performance enhancement for specific group of tasks, our objective is to establish a comprehensive mathematical framework, which supports the combination of many existing optimizers, and enables the exploration of new optimization algorithms. While our goal is different, our work shows some similarities with learned-optimization as well. Training an optimizer involves backpropagating through many unrolled inner optimization steps, a process that is not only computationally intensive but also susceptible to instability, as highlighted by [@metz2020tasks]. Within this context, our RLLC method could be interpreted as a kind of differential-free optimization technique. In this paper, we focus solely looking one step ahead, but one could easily extend it beyond that.\n\n# Retrospective Learning Law Correction\n\n  The RLLC method is presented through an abstract mathematical framework for optimizers. This framework is somewhat specialized, yet it maintains sufficient generality to encompass a range of interesting optimizers. We think of optimizers as entities with an evolving internal state that updates at each step based on newly received gradients. Additionally, the optimizer calculates a parameter update vector relevant to the optimization process. A functional description of such an optimizer is given in the following definition.\n\n::: {#funcopt .definition}\n**Definition 1**. An *optimizer* for $n$ parameters is a pair of functions of the form $F:\\mathcal{S}\\times\\mathbb{R}^n\\to\\mathcal{S}$ and $G:\\mathcal{S}\\times\\mathbb{R}^n\\to\\mathbb{R}^n$ where $\\mathcal{S}$ is the set of possible internal states, $F$ is the *state update function* and $G$ is the *parameter update function*.\n:::\n\nTo translate optimizers into an actual optimization process we choose an initial internal state $S_0\\in\\mathcal{S}$ and an initial parameter vector $\\theta_0\\in\\mathbb{R}^n$. Then we iterate $$S_t:=F(S_{t-1},g_t),~~\\theta_t:=\\theta_{t-1}+G(S_t,g_t)$$ where $g_t$ is a gradient vector received by the optimizer in the $t$-th step. To illustrate this formalism, assume that the optimizer is given by $\\mathcal{S}=\\mathbb{R}^n$, $F(v,w)=\\beta v+w, G(v,w)=-cv$. In this case, we obtain the momentum SGD with learning rate $c$ and decay parameter $\\beta$.\n\n  We will think of memory $k$ optimizers in a way that the internal state space is of the form $\\mathbb{R}^{n\\times k}\\times\\mathcal{H}$ where the columns of matrices in $\\mathbb{R}^{n\\times k}$ represent $k$ vectors in the parameter space $\\mathbb{R}^n$ and $\\mathcal{H}$ will be called the space of hidden states. We typically assume that $n$ is a large number and that the hidden states are described by much fewer than $n$ parameters. A *memory update rule* for $k$ memory units is a function of the form $U:(\\mathbb{R}^{n\\times k}\\times\\mathcal{H})\\times\\mathbb{R}^n\\to\\mathbb{R}^{n\\times k}\\times\\mathcal{H}$ where the external $\\mathbb{R}^n$ component represents new arriving gradients. Such a function does not yet determine an optimizer. The RLLC method is designed to turn memory update rules into optimizers by extending their state space with a vector called learning law and introducing a natural parameter update function. We give two different descriptions of RLLC. The first one is a functional description which is more convenient for proofs.\n\nWe will need the so-called Moore-Penrose inverse which is defined for an arbitrary matrix $A\\in\\mathbb{R}^{n\\times k}$ and is denoted by $A^+$. Note that if $A$ has rank $k$ (which means that $A$ is non-degenerate if $n\\geq k$) then $A^+=(A^TA)^{-1}A^T$.\n\n::: {#RLLCdef .definition}\n**Definition 2** (RLLC functional form). Let $U$ be a memory update rule as above. Then the corresponding RLLC optimizer with learning rates $c_1,c_2$ is given as follows. The state space is $\\mathcal{S}:=\\mathbb{R}^{n\\times k}\\times\\mathcal{H}\\times\\mathbb{R}^k$ where the extra component $\\mathbb{R}^k$ is called the learning law. The functions $F,G$ are given in the following way. Assume that $M\\in\\mathbb{R}^{n\\times k},H\\in\\mathcal{H},L\\in\\mathbb{R}^k,g\\in\\mathbb{R}^n$. Then $$F(M,H,L,g):=(U_1(M,H),U_2(M,H),L+c_2M^+g)$$ $$G(M,H,L,g):=-c_1ML.$$\n:::\n\n::: remark\n*Remark 3*. Vectors in $\\mathbb{R}^m$ are considered to be column vectors. This means that they are treated as $m\\times 1$ matrices in calculations.\n:::\n\nThe second, less abstract approach to RLLC describes the optimization process directly in a more conventional way.\n\n-   $\\theta_0$ initial parameter vector\n\n-   $f(\\theta)$: stochastic objective function with parameters $\\theta\\in\\mathbb{R}^n$\n\n-   Two learning rates $c_1,c_2>0$\n\n-   Stability parameter $\\epsilon$ for relaxed Penrose inverse\n\n-   $M_0\\in\\mathbb{R}^{n\\times k}$: initial memory units\n\n-   $L_0\\in\\mathbb{R}^k$: initial learning law\n\n-   $H_0$: initial hidden state\n\n$t:=0$: initialize time step\n\n**while:** $\\theta_t$ not converged **do**:\n\n  $t\\longleftarrow t+1$\n\n= \\[draw=black, fill=blue!20, very thick, rectangle, rounded corners, inner sep=10pt, inner ysep=15pt\\]\n\n= \\[draw=black, fill=green!20, very thick, rectangle, rounded corners, inner sep=10pt, inner ysep=15pt\\]\n\n= \\[draw=black, fill=yellow!20, very thick, rectangle, rounded corners, inner sep=10pt, inner ysep=15pt\\]\n\n= \\[draw=black, fill=red!20, very thick, rectangle, rounded corners, inner sep=10pt, inner ysep=15pt\\]\n\n=\\[fill=gray, text=white\\]\n\nThe idea behind the learning rule update is that with the arrival of the new gradient $g_t$ the optimizer gains new (retrospective) information on how it could have done better in the previous learning step. Notice that the vector $M^+g_t$ is the coefficient vector of the orthogonal projection of $g_t$ to the space spanned by the memory units when written as a linear combination of the memory units. This means that, if performed with the new law, the outcome of the parameter update in step $t-1$ would have been $\\theta_t-c_1c_2p_t$ instead of $\\theta_t$ where $p_t$ is the orthogonal projection of $g_t$ to the space spanned by the memory units in the $t-1$-th step. Notice that $(p_t,g_t)=(p_t,p_t)\\geq 0$ and thus the change $-c_1c_2p_t$ points in a direction which improves the objective function.\n\nThe above heuristics does not take it into account that the objective function $f_t$ is also changing. This fact indicates that our update rule is more justified if the second learning rate $c_2$ is small and thus random effects have time to average out leaving only useful directions in the update. Also notice that the algorithm does not \\\"go back in time\\\" to perform the improved learning step. Instead it applies the updated learning law with the updated memory units. This shows that the efficiency of the RLLC method depends on a type of consistency property. Roughly speaking it assumes that the notion of a \\\"good learning law\\\" does not change too much in time and so improvements of the past give improvements of the future. For this reason the choice of the memory update rule is a crucial issue which is one of the main topics of the second part of this paper.\n\n::: remark\n*Remark 4*. The performance of the RLLC optimizer is dependent on the initialization of the learning law at the beginning. In practice it is not initialized to be $0$.\n:::\n\n::: remark\n*Remark 5*. To avoid numerical instability, in practice we use a relaxed version of Penrose inverse which has a parameter $\\epsilon$ set to a small number.\n:::\n\n  We close this chapter with a useful linear invariance property of the RLLC method. We will need the next two definitions.\n\n::: {#memconj .definition}\n**Definition 6**. Let $U$ be a memory update rule as above and let $Q\\in\\mathbb{R}^{k\\times k}$ be an invertible matrix. We define the new memory update rule $U^Q$ in the following way: Let $M\\in\\mathbb{R}^{n\\times k}$, $H\\in\\mathcal{H}$ and $U(MQ^{-1},H,g)=(M_2,H_2)$. Then $$U^Q(M,H,g):=(M_2Q,H_2).$$\n:::\n\n::: definition\n**Definition 7**. Two optimizers given by $(F,G)$ and $(F',G')$ with state spaces $\\mathcal{S}$ and $\\mathcal{S}'$ are called equivalent if there is a bijection $\\phi:\\mathcal{S}\\to\\mathcal{S}'$ (called an *isomorphism*) such that $\\phi(F(S,v))=F'(\\phi(S),v)$ and $G(S,v)=G'(\\phi(S),v)$. A *partial isomorphism* is a bijection between a subset of $\\mathcal{S}$ and a subset of $\\mathcal{S}'$ having the same property. If there is such a function we say that the two optimizers are partially equivalent on these two subsets. In particular, if two optimizers with memory $k$ states are partially equivalent on states with rank $k$ memory matrices then we call them *essentially equivalent*.\n:::\n\nIt is easy to see that if two optimizers are equivalent then they define the same optimization process if their initialization of internal states is isomorphic. If two optimizers are partially equivalent with partial isomorphism $\\phi$ then the optimization processes are identical as long as they operate on states in the domain (and image) of $\\phi$.\n\n::: {#RLLCinv .lemma}\n**Lemma 8** (Linear invariance of RLLC). *Let $U$ be a memory update rule as above and $Q\\in\\mathbb{R}^{k\\times k}$ be an arbitrary matrix. Then the RLLC optimizer corresponding to $U$ is essentially equivalent to the RLLC optimizer corresponding to $U^Q$.*\n:::\n\n::: proof\n*Proof.* We claim that the function $\\phi(M,H,L):=(MQ,H,Q^{-1}L)$ is a partial isomorphism on states with rank $k$ memory matrices. This follows trivially from formulas in definition [2](#RLLCdef){reference-type=\"ref\" reference=\"RLLCdef\"} and the fact that $(MQ)^+=Q^{-1}M^+$ holds if ${\\rm rank}(M)=k$. ◻\n:::\n\n# Linear memory updates {#linmemupdt}\n\nThroughout this chapter we investigate linear memory update rules with no hidden states. Such an update rule is given by $$\\label{linupdt}\nU(M,g):=MB+ga^T$$ where $M\\in\\mathbb{R}^{n\\times k}$ is the memory matrix, $g\\in\\mathbb{R}^n$ is a new gradient and $a\\in\\mathbb{R}^k,B\\in\\mathbb{R}^{k\\times k}$ are fixed parameters of the update rule. In an optimization process this means that the memory unit $m_i$ represented by the $i$-th column of $M$ is updated to $a_ig+\\sum_{j=1}^kB_{j,i}m_i$ when the new gradient $g$ is received.\n\n  Linearly updated memory units are interesting independently of the RLLC method. We can directly obtain powerful optimizers by using a fixed hand designed learning law $L\\in\\mathbb{R}^k$. This type of optimizer, denoted by $\\mathcal{L}(B,a,L)$ works by the equations: $$M_t=M_{t-1}B+g_ta^T~,~\\theta_t=\\theta_{t-1}-M_tL.$$ If $k=1$ then $B,a,L$ are single real numbers. The corresponding optimizer is a momentum SGD optimizer with decay parameter $B$ and learning rate $aL$. Another, important setting is described in the following lemma (for proof see [10](#SGD_Nesterov_proof){reference-type=\"ref+label\" reference=\"SGD_Nesterov_proof\"}).\n\n::: lemma\n**Lemma 9**. *Let*\n\n*$$B=\n\\begin{pmatrix}\n\\beta & 0\\\\\n0 & 0 \n\\end{pmatrix},~ a=\n\\begin{pmatrix}\n1 \\\\\n1 \n\\end{pmatrix},~L=\n\\begin{pmatrix}\nc\\beta \\\\\nc \n\\end{pmatrix}$$ Then the corresponding optimizer $\\mathcal{L}(B,a,L)$ is Nestorv Accelerated Gradient with decay parameter $\\beta$ and learning rate $c$.*\n:::\n\n   There is a useful observation which sheds more light on what information linear memory stores if this memory update is iterated in an optimization process started with initial value $0^{n\\times k}$ for $M$. By induction we have $$U(...U(U(0^{n\\times k},g_1),g_2),...,g_t)=\\sum_{i=1}^t g_ta^TB^{t-i}.$$ We obtain that at time $t$ (after $t$ iteration of the update rule) the value of the $i$-th memory unit is given by $$\\label{abstrule}\nm_j=\\sum_{i=0}^{t-1} g_{t-i}(a^TB^i)_j$$ where $(a^TB^i)_j$ denotes the $j$-th coordinate of the row vector $a^TB^i$. If we regard gradients with index $0$ or negative index as $0$ then the sum can be taken from $0$ to infinity. Informally speaking, this means that $m_i$ is a fixed (time independent) linear combination of previous gradients going backwards in time. This linear combination is represented by the infinite sequence $\\{(a^TB^i)_j\\}_{i=0}^\\infty$ for the $j$-th memory unit. We say that this infinite sequence is the *abstract rule* of the memory unit. To guarantee that older gradients are taken with decaying weight in ([\\[abstrule\\]](#abstrule){reference-type=\"ref\" reference=\"abstrule\"}) we need to assume that the spectral norm of $B$ is smaller than $1$.\n\n Memory updates in the case $k=1$ are determined by two numbers: $\\beta=B_{1,1}$ and $\\alpha=a_1$. The update rule of the single memory unit $m$ in the $t$-th step is $m\\longleftarrow \\alpha g_t+\\beta m$. (This is essentially the update rule of a momentum vector.) It follow from our formula that the abstract rule in this case is given by the geometric sequence $\\beta^i\\alpha$. In particular, in the $t$-th step we have that $m=\\sum_{i=0}^t \\alpha\\beta^i g_{t-i}$.\n\n The case\n\n$$\\label{comprep} B=\\begin{pmatrix}\n\\alpha & -\\beta\\\\\n\\beta & \\alpha \n\\end{pmatrix}$$\n\nhas a distinguished role because such matrices represent complex numbers $\\gamma=\\alpha+\\beta i$. In this special case the two memory units can be interpreted as the real and the complex parts of a single complex valued memory unit which describes a momentum vector with complex parameter $\\gamma$. More precisely $m_1$ and $m_2$ are the real and the complex parts of a memory unit $m\\in\\mathbb{C}^n$ which is updated according to $m\\longleftarrow g_t+\\gamma m$.\n\n[]{#jordan_prop label=\"jordan_prop\"}  Another interesting example for $k=2$ is given by $$B=\\begin{pmatrix}\n\\alpha & 1\\\\\n0 & \\alpha \n\\end{pmatrix}$$ which is the so called Jordan block of size $2$ with eigenvalue $\\alpha$. The first one of the two memory units is the momentum vector of the gradients with parameter $\\alpha$. However the second memory unit stores something new. It is the momentum vector of the first memory unit with parameter $a$. One can show that the abstract rule corresponding to this memory unit is given by the infinite sequence $0,1,2\\alpha,3\\alpha^2,4\\alpha^3,\\dots$.\n\n In general, we call $k$ memory units $m_1,m_2,\\dots,m_k$ connected by a joint linear update rule a *propagator* of dimension $k$. Recall that such an object is described by a matrix $B\\in\\mathbb{R}^{k\\times k}$ and a vector $a\\in\\mathbb{R}^k$. In the previous two examples each real number $\\beta$ is associated with a propagator denoted by $M(\\beta)$ of dimension $1$ while complex numbers $\\gamma$ are associated with a propagator denoted by $CM(\\gamma)$ of dimension $2$. We call such propagators *momentum propagators*. It will be important for us that there is a simple operation on propagators that we call *union* and denote by $\\oplus$. This is simply just taking the union of the corresponding memory units together and updating them independently. From a linear algebraic point of view, the matrix $B$ corresponding to the union of propagators is a block diagonal matrix whose blocks contain the matrices of the individual propagators. The vector $a$ corresponding to the union is the concatenation of the vectors of the propagators. Unions of momentum propagators will be called *multi momentum propagators*.\n\n# Optimizers with linear memory and RLLC\n\nIn this chapter we discuss the basic properties of optimizers which combine linear memory and the RLLC method. We use the term LM-RLLC optimizers for them. Based on definition [2](#RLLCdef){reference-type=\"ref\" reference=\"RLLCdef\"} and formula ([\\[linupdt\\]](#linupdt){reference-type=\"ref\" reference=\"linupdt\"}) one can produce the LM-RLLC optimizer $\\mathcal{F}(B,a,c_1,c_2)$ with hyperparameters $B,a,c_1,c_2$. The corresponding update functions are given by $$F(M,L,g)=(MB+ga^T,L+c_2M^+g)$$ $$G(M,L,g)=-c_1ML$$ For the sake of completeness we describe the recursive optimization process.\n\n::: definition\n**Definition 10**. (LM-RLLC optimization process) Let us fix the hyperparameters $B\\in\\mathbb{R}^{k\\times k},a\\in\\mathbb{R}^k,(c_1,c_2)\\in\\mathbb{R}^2.$ Then the LM-RLLC optimizer with these hyperparameters is given by the equations $$L_t=L_{t-1}+c_2M_{t-1}^+g_t$$ $$M_t=M_{t-1}B+g_ta^T$$ $$\\theta_t=\\theta_{t-1}-c_1M_tL$$ where $M_0$ is the $0$ matrix in $\\mathbb{R}^{n\\times k}$ and $L_0\\in\\mathbb{R}^k$ is a suitable (typically non $0$) vector.\n:::\n\nDeeper mathematical analysis reveals that LM-RLLC optimizers can be transformed into a simpler, canonical form if we look at them up to equivalence. The key observation is a \\\"basis independence\\\" property of LM-RLLC optimizer functions.\n\n::: {#LM-RLLCinv .theorem}\n**Theorem 11** (Basis independence of LM-RLLC optimizers). *Let $k\\in\\mathbb{N},a\\in\\mathbb{R}^k,B\\in\\mathbb{R}^{k\\times k},(c_1,c_2)\\in\\mathbb{R}^2$ and let $Q\\in\\mathbb{R}^{k\\times k}$ be an invertible matrix. Then $\\mathcal{F}(B,a,c_1,c_2)$ is essentially equivalent to $$\\mathcal{F}(Q^{-1}BQ,Qa,c_1,c_2).$$*\n:::\n\n::: proof\n*Proof.* The optimizer $\\mathcal{F}(B,a,c_1,c_2)$ is obtained from the linear memory update rule $U(M,g)=MB+ga^T$ with RLLC. Notice that $U^Q$ (in the sense of definition [6](#memconj){reference-type=\"ref\" reference=\"memconj\"}) is given by $U^G(M,g)=M(Q^{-1}BQ)+g(Qa)^T$. Then lemma [8](#RLLCinv){reference-type=\"ref\" reference=\"RLLCinv\"} finishes the proof. ◻\n:::\n\n  Theorem [11](#LM-RLLCinv){reference-type=\"ref\" reference=\"LM-RLLCinv\"} together with a variant of the Jordan decomposition theorem implies that we can transform LM-RLLC optimizers into a very special form without changing the optimization process. The original form of the Jordan decomposition theorem says that if $B\\in\\mathbb{C}^{k\\times k}$ is an arbitrary complex matrix then there is an invertible matrix $Q\\in\\mathbb{C}^{k\\times k}$ such that $Q^{-1}BQ$ has a block diagonal form with each block being a so-called Jordan block. A Jordan block $J_m(\\lambda)$ is a matrix of size $m\\times m$ with $\\lambda\\in\\mathbb{C}$ in the diagonal, $1$ above the diagonal and $0$ everywhere else. For example\n\n$$J_3(\\lambda)=\n\\begin{pmatrix}\n\\lambda & 1         & 0   \\\\\n0         & \\lambda & 1   \\\\\n0         & 0         & \\lambda \\\\\n\\end{pmatrix}$$ There is a similar, although somewhat more complicated statement (called real Jordan normal form) if $B$ and $Q$ are required to be real matrices. In this case there are two types of blocks $J_m(\\lambda)$ with $\\lambda\\in\\mathbb{R}$ and $CJ_m(\\alpha+\\beta i)$ with $\\alpha,\\beta\\in\\mathbb{R}$. The second type of block has size $2m\\times 2m$ and it \\\"imitates\\\" complex Jordan blocks with real matrices. This matrix is very similar to $J_m(\\lambda)$ with the main difference being that each entry is replaced by a $2\\times 2$ matrix. The $0$'s and $1's$ are replaced by $0$ matrices and identity matrices. The $\\lambda$ entries are replaced by the matrix in equation $(\\ref{comprep})$ which represents $\\alpha+\\beta i$ by a real matrix. For example $$CJ_2(\\alpha+\\beta i)=\n\\begin{pmatrix}\n\\alpha & -\\beta         & 1 & 0  \\\\\n\\beta  & \\alpha & 0 & 1  \\\\\n0      & 0      & \\alpha & -\\beta \\\\\n0      & 0      & \\beta  & \\alpha\n\\end{pmatrix}$$\n\n  Racall that an LM-RLLC optimizer is given by $B\\in\\mathbb{R}^{k\\times k},a\\in\\mathbb{R}^k$ and two learning rates. By transforming the matrix $B$ to its real Jordan normal form with a basis transformation given by $Q\\in\\mathbb{R}^{k\\times k}$, we can divide the memory units into groups belonging to single blocks of type $J_m(\\lambda)$ or $CJ_m(\\alpha+\\beta i)$. The block diagonal form of $Q^{-1}BQ$ guarantees that these groups do not interact with each other in memory updates and thus we can treat them as separate propagators. Recall that in this basis transformation considered in theorem [11](#LM-RLLCinv){reference-type=\"ref\" reference=\"LM-RLLCinv\"} the vector $a$ transforms into $Qa$. By applying a statement which is slightly stronger then the Jordan decomposition theorem we can also guarantee that the part of $a$ in each block contains at most one coordinate with $1$ and the rest is $0$. We can also assume that this coordinate is the first one otherwise there are trivial memory units which store $0$ in each step. By summarizing all of this we obtain propagators of very special type. Let $e_m\\in\\mathbb{R}^m$ denote the vector with $1$ in the first coordinate and $0$ in the rest. We denote the propagator corresponding to the pair $(J_m(\\lambda),e_m)$ by $M_m(\\lambda)$ and the propagator corresponding to $(CJ_m(\\alpha+\\beta i),e_{2m})$ by $CM_m(\\alpha+\\beta i)$. We call such propagators *Jordan block propagators*. If $m=1$ then we omit the index and simply write $M(\\lambda)$ and $CM(\\alpha+\\beta i)$. We obtain the next theorem.\n\n::: theorem\n**Theorem 12** (Normal forms of LM-RLLC optimizers). *Every LM-RLLC optimizer is essentially equivalent with another LM-RLLC optimizer where the memory update is of the form $P_1\\oplus P_2\\oplus\\dots\\oplus P_r$ where each $P_i$ is a Jordan block propagator.*\n:::\n\nBy slightly abusing the notation we will also use the formula $P_1\\oplus P_2\\oplus\\dots\\oplus P_r$ for the optimizer itself. For example $M(0.9)\\oplus M_2(0.6)\\oplus CM_2(0.3+0.2i)$ stands for a memory $7$ optimizer where the memory units are grouped and updated according to the propagators $M(0.9)$,$M_2(0.6)$ and $CM_2(0.3+0.2i)$.\n\n# Experiments\n\nFor our experiments, we used the Learned Optimization framework [@metz2022practical] as a starting point. The framework offers pre-trained and hyper parameter optimized optimizers. We compare our results with the most widely used optimizers as baseline: *Adam*, *SGD*, and *SGD with momentum*. We compare test loss and classification accuracy on MNIST[@deng2012mnist], Fashin-MNIST[@xiao2017fashionmnist], and CIFAR-10[@Krizhevsky2009LearningML] datasets. We experimented with dense, convolutional and residual neural networks. The source code of our work is available publicly[^1]. See implementation details in [12](#appendix:impleementation_details){reference-type=\"ref+label\" reference=\"appendix:impleementation_details\"}.\n\n::: table*\n                                         **MNIST**                                                                         **Fashion-MNIST**                                                                        **CIFAR-10**                                                                                                                          \n  ---------------------- ----------------------------------------- ---------------------------------------- -- ----------------------------------------- ----------------------------------------- -- ----------------------------------------- ----------------------------------------- ----------------------------------------- ------ -- -- -- -- -- --\n    2-3 (lr)5-6 (lr)8-10                  **MLP**                                  **Conv**                                     **MLP**                                  **Conv**                                      **MLP**                                  **Conv**                                **ResNet-20**                                     \n                   $SGD$                  0.0882                                    0.0331                                      0.3426                                    0.3673                                       1.4084                                    0.8359                                    0.6010                    Loss                 \n                                           98.16                                    98.56                                        88.65                                     86.97                                        52.18                                     71.37                                     80.93                    Acc                  \n                                          0.0856                                    0.0324                                      0.3476                                    0.2732                                       1.4108                      [0.7850]{style=\"color: BlueViolet\"}       [0.5757]{style=\"color: BlueViolet\"}     Loss                 \n                            [98.22]{style=\"color: BlueViolet\"}                      98.97                                        88.67                      [90.78]{style=\"color: BlueViolet\"}                          51.99                                     73.17                                     81.36                    Acc                  \n                          [**0.0758**]{style=\"color: BlueViolet\"}                   0.0304                        [0.3407]{style=\"color: BlueViolet\"}       [0.2704]{style=\"color: BlueViolet\"}          [1.3858]{style=\"color: BlueViolet\"}                     0.7920                                    0.5857                    Loss                 \n                                           97.83                      [98.99]{style=\"color: BlueViolet\"}          [88.78]{style=\"color: BlueViolet\"}                       90.76                         [52.43]{style=\"color: BlueViolet\"}        [73.70]{style=\"color: BlueViolet\"}        [81.48]{style=\"color: BlueViolet\"}      Acc                  \n                                          0.0844                                    0.0310                                      0.3408                      [0.2661]{style=\"color: OliveGreen\"}                        1.4021                                    0.8030                      [0.5301]{style=\"color: OliveGreen\"}     Loss                 \n                                           98.21                      [99.03]{style=\"color: OliveGreen\"}                         88.64                      [90.96]{style=\"color: OliveGreen\"}                          52.13                      [73.95]{style=\"color: OliveGreen\"}        [83.08]{style=\"color: OliveGreen\"}      Acc                  \n                                          0.0888                                    0.0323                                      0.3475                      [0.2678]{style=\"color: OliveGreen\"}                        1.3973                                    0.7977                      [0.5353]{style=\"color: OliveGreen\"}     Loss                 \n                          [**98.26**]{style=\"color: OliveGreen\"}                    98.95                         [88.82]{style=\"color: OliveGreen\"}        [90.98]{style=\"color: OliveGreen\"}                          51.71                      [74.11]{style=\"color: OliveGreen\"}      [**83.38**]{style=\"color: OliveGreen\"}    Acc                  \n                                          0.0829                                    0.0343                        [0.3359]{style=\"color: OliveGreen\"}     [**0.2563**]{style=\"color: OliveGreen\"}                      1.4142                      [0.7734]{style=\"color: OliveGreen\"}     [**0.5268**]{style=\"color: OliveGreen\"}   Loss                 \n                            [98.23]{style=\"color: OliveGreen\"}                      98.95                                        88.67                      [91.11]{style=\"color: OliveGreen\"}                          51.55                      [75.42]{style=\"color: OliveGreen\"}        [83.19]{style=\"color: OliveGreen\"}      Acc                  \n                                          0.0801                                    0.0800                      [**0.3220**]{style=\"color: OliveGreen\"}                   0.4488                       [**1.3444**]{style=\"color: OliveGreen\"}                   1.0404                                    0.5811                    Loss                 \n                                           98.17                                    97.60                                        88.75                                     84.31                       [**53.42**]{style=\"color: OliveGreen\"}                     63.59                                     81.35                    Acc                  \n                                          0.0861                                    0.0319                                      0.3536                      [0.2636]{style=\"color: OliveGreen\"}                        1.4155                      [0.7602]{style=\"color: OliveGreen\"}       [0.5354]{style=\"color: OliveGreen\"}     Loss                 \n                                           98.21                      [98.99]{style=\"color: OliveGreen\"}          [89.03]{style=\"color: OliveGreen\"}        [90.95]{style=\"color: OliveGreen\"}                          52.08                    [**75.87**]{style=\"color: OliveGreen\"}      [82.84]{style=\"color: OliveGreen\"}      Acc                  \n                                          0.0877                                    0.0287                                      0.3498                      [0.2596]{style=\"color: OliveGreen\"}                        1.4028                    [**0.7216**]{style=\"color: OliveGreen\"}     [0.5393]{style=\"color: OliveGreen\"}     Loss                 \n                            [98.23]{style=\"color: OliveGreen\"}      [**99.03**]{style=\"color: OliveGreen\"}        [89.14]{style=\"color: OliveGreen\"}      [**91.29**]{style=\"color: OliveGreen\"}                        51.83                      [75.76]{style=\"color: OliveGreen\"}        [82.73]{style=\"color: OliveGreen\"}      Acc                  \n                                          0.0797                                    0.0539                        [0.3282]{style=\"color: OliveGreen\"}                     0.3735                         [1.3798]{style=\"color: OliveGreen\"}                     0.9433                      [0.5445]{style=\"color: OliveGreen\"}     Loss                 \n                                           98.22                                    98.31                       [**89.25**]{style=\"color: OliveGreen\"}                     87.08                         [53.37]{style=\"color: OliveGreen\"}                       66.68                      [82.43]{style=\"color: OliveGreen\"}      Acc                  \n                                          0.0873                                    0.0334                                      0.3671                      [0.2624]{style=\"color: OliveGreen\"}                        1.4029                      [0.7647]{style=\"color: OliveGreen\"}       [0.5337]{style=\"color: OliveGreen\"}     Loss                 \n                                           98.09                                    98.97                                        88.57                      [91.06]{style=\"color: OliveGreen\"}                          52.10                      [75.44]{style=\"color: OliveGreen\"}        [83.07]{style=\"color: OliveGreen\"}      Acc                  \n:::\n\n![ Test accuracy graphs of RLLC and benchmark optimizers, measured on the CIFAR-10 dataset, with the ResNet-20 network. RLLC optimizers show faster convergence and better generalization. See related plots and error bars in [14](#appendix:Supplementary_plots){reference-type=\"ref+label\" reference=\"appendix:Supplementary_plots\"}.](images/figs/resnet_cifar4_3.png){#fig:resnet_acc width=\"\\\\textwidth/2\"}\n\n**RLLC based adaptive learning rate:**   One of the simplest case of the RLLC method is already interesting. If there is a single memory unit containing the momentum of previous gradients then RLLC yields an adaptive version of the momentum SGD optimizer. In this case the learning law contains a single coefficient, that defines an adaptively changing learning rate for the momentum SGD. Our experiments show that this upgrade outperforms the plain momentum SGD method, showcasing the power of RLLC. See $M(0.9)$ results in [\\[table:main_results\\]](#table:main_results){reference-type=\"ref+label\" reference=\"table:main_results\"} and on [1](#fig:resnet_acc){reference-type=\"ref+label\" reference=\"fig:resnet_acc\"}. Note that RLLC can be applied to an arbitrary optimizer by introducing a single memory unit storing the last learning step. In a similar way we obtain a version of the optimizer with an adaptive learning rate. However it may depend on the optimizer whether it leads to a performance boost or not.\n\n**Mixing SGD and momentum SGD:**   We observe an intriguing phenomenon when we enhance the memory unit of the previous method with the current gradient and monitor the learning law throughout the training process. As shown in [2](#fig:SGD_MOM_weights){reference-type=\"ref+label\" reference=\"fig:SGD_MOM_weights\"}, during the initial phase of training, the coefficient of the $M(0.9)$ memory unit is predominant. However, as training advances, the coefficient of the $M(0)$ unit increases, leading to a reversal in the significance of the two memory units. Our experiment supports [@SGD_MOM_paper] findings. [\\[table:main_results\\]](#table:main_results){reference-type=\"ref+label\" reference=\"table:main_results\"} and [1](#fig:resnet_acc){reference-type=\"ref+label\" reference=\"fig:resnet_acc\"} show results for $M(0.9) \\oplus M(0)$. An interesting additional detail is that in between the two extremal phases there exists a phase which emulates the Nesterov Accelerated Gradient (NAG) method. This occurs when the coefficient of the Momentum SGD memory unit, divided by the coefficient of the SGD unit, equals the decay parameter of the momentum SGD. (For more details, see [10](#SGD_Nesterov_proof){reference-type=\"ref+label\" reference=\"SGD_Nesterov_proof\"}).\n\n![ Analysis of $M(0.9) \\oplus M(0.0)$ optimizer's memory unit's coefficients over time. The figure illustrates the optimizer's transition between momentum SGD and SGD, briefly aligning with the NAG optimizer around the 2k step.](images/figs/SGD_MOM_metaW.png){#fig:SGD_MOM_weights width=\"8cm\"}\n\n**Multi-momentum propagators:**   In our experiment we investigated optimizers of the form $M(\\beta_1)\\oplus M(\\beta_2)\\oplus\\dots\\oplus M(\\beta_k)$. We have not optimized the hyperparameters $\\beta_i$ but we found very promising settings with a few trials. Our results are therefore illustrative and the fine tuning (depending on the type of network) is subject to further research. In [\\[table:main_results\\]](#table:main_results){reference-type=\"ref+label\" reference=\"table:main_results\"} we describe our experiments with $M(0.9)\\oplus M(0),M(0.9)\\oplus M(0.6),M(0.9) \\oplus M(0.8) \\oplus M(0.7)$ and $M(0.9)\\oplus M(0.6)\\oplus M(0)$ optimizers. Quite surprisingly the simplest one $M(0.9)\\oplus M(0)$ (which is mentioned earlier) is the most reliable. However on certain tasks it is outperformed by the memory $3$ settings. [3](#fig:resnet_metaW){reference-type=\"ref+label\" reference=\"fig:resnet_metaW\"} illustrates an interesting coupling between the coefficient of $M(0.8)$ and $M(0.7)$ memory units. See further details in [11](#appendix:additional_math_obs){reference-type=\"ref+label\" reference=\"appendix:additional_math_obs\"}.\n\n![ Analysis of the coefficients of $M(0.9) \\oplus M(0.8) \\oplus M(0.7)$ optimizer over time. The figure shows an interesting negative coupling between $M(0.8)$ and $M(0.7)$. See further details in [11](#appendix:additional_math_obs){reference-type=\"ref+label\" reference=\"appendix:additional_math_obs\"}.](images/figs/mom_mix_3_metaW2.png){#fig:resnet_metaW width=\"8cm\"}\n\n**$M_m(\\lambda)$ propagator for $m\\geq 2$:**   Jordan block propagators of the form $M_m(\\beta)$ and $CM_m(\\beta)$ with $m\\geq 2$ are easy to implement in our code. In our experiments we focused on type $M_m(\\beta)$ propagators with $m=2,3$. It is also interesting to combine them with other propagators. [\\[table:main_results\\]](#table:main_results){reference-type=\"ref+label\" reference=\"table:main_results\"} shows results for $M_2(0.6)$, $M_3(0.6)$ and $M(0.9) \\oplus M_2(0.6)$. These configurations surpass the baseline optimizers in many tasks and also surpass pure multi-momentum propagators in some specific tasks.\n\n**Complex-moment propagators:**   Another interesting possibility in our framework is the usage of complex-momentum propagators. One particular example that we experimented with is the case of $M(\\beta){\\oplus}M(-\\beta){\\oplus}CM(\\beta i)$. This choice in not random. It comes from the Jordan normal form of the permutation matrix corresponding to the cyclic shift on $4$ elements multiplied with $\\beta$. This particular propagator is closely related to Fourier analysis.\n\n# Limitations\n\nUsing memory units comes at a cost. Each memory unit is a vector in the parameter space $\\mathbb{R}^n$. In our experiments, we opted for relatively small or medium-sized architectures. However, for architectures with a vast parameter space, our approach with many memory units could prove to be too memory-intensive.\n\nIt's also worth noting that our experiments were conducted on relatively small datasets, and future work should explore experiments on larger datasets.\n\n# Conclusion\n\nOur experiments demonstrate that the RLLC method is capable of boosting the performance of classical optimizers (such as SGD and momentum SGD) by combining them and making them more adaptive. Furthermore the case of linearly updated memory units provides a mathematically elegant framework with many new types of promising optimizers such as the ones corresponding to larger Jordan blocks, complex numbers and their combinations. We regard this paper as a starting point for future research in the frame of which the full potential of our approach is explored. One possible research direction is to introduce adaptively changing memory update rules. In particular, in the linear setting the pair $B\\in\\mathbb{R}^{k\\times k},a\\in\\mathbb{R}^k$ (see [4](#linmemupdt){reference-type=\"ref+label\" reference=\"linmemupdt\"}) is fixed for the whole optimization process in the current version. It would be interesting to study a version were $B$ and $a$ are also adaptively changing throughout learning.\n\n# Acknowledgements\n\nThe first-named author received funding from the project KPP 133921 of the Hungarian Government. The research was also supported by the Hungarian Ministry of Innovation and Technology NRDI Office in the framework of the Artificial Intelligence National Laboratory Program.\n\n# Interpolations between SGD, momentum SGD and Nesterov Accelerated Gradient (NAG) {#SGD_Nesterov_proof}\n\nAs we have already explained, momentum SGD method in our interpretation is represented as the propagator $M(\\beta)$ where $0\\leq \\beta<1$ is the decay parameter. In particular $M(0)$ corresponds to a memory unit which stores the last gradient seen by the optimizer. In this sense, a memory $1$ optimizer with memory unit $M(0)$ is basically an SGD optimizer. The learning law in this case is a single real number which manifests as a learning rate.\n\nIf an optimizer has two memory units $M(\\beta)$ and $M(0)$ then by changing the learning law (described by a pair of real numbers) we can continuously interpolate between momentum SGD and pure SGD. In this section we prove that there is a third well known optimizer which can be represented in this setting: the so-called Nesterov accelerated gradient (NAG). In the next list we summarize the meanings of special learning laws for $M(\\beta),M(0)$.\n\n1.  $(0,c)$ : SGD\n\n2.  $(c,0)$ : momentum SGD with decay parameter $\\beta$\n\n3.  $(c\\beta,c)$ : NAG with decay parameter $\\beta$\n\nwhere the learning rate is $c*{\\rm lr}$. Notice that since $\\beta$ is a prescribed fix number, the above three cases don't cover the possible learning laws for the pair $M(\\beta),M(0)$.\n\nThe fact that the last learning law (if kept fixed!) is equivalent with NAG is an easy calculation from the equations describing the NAG optimizer:\n\n$$\\phi_{t+1}=\\theta_t-r\\nabla f(\\theta_t)$$ $$\\theta_{t+1}=\\phi_{t+1}+\\beta (\\phi_{t+1}-\\phi_t).$$\n\nLet $$v_{t+1}:=\\phi_t-\\phi_{t+1}.$$ With this notation we have the update rules: $$v_{t+1}=\\beta v_t+r\\nabla f(\\theta_t),$$ $$\\theta_{t+1}=\\theta_t-(r\\nabla f(\\theta_t)+\\beta v_{t+1}).$$ Observe that if we introduce the update rule $$m_{t+1}:=\\beta m_t+\\nabla f(\\theta_t)$$ then $v_t=rm_t$ holds at any given time $t$. Furthermore $m_t$ corresponds to the propagator $M(\\beta)$. With this notation we have that $$\\theta_{t+1}=\\theta_t-r(\\nabla f(\\theta_t)+\\beta m_{t+1}).$$ Assume now that $r=c*{\\rm lr}$ where ${\\rm lr}$ is a fixed learning rate. Then we have that $$\\theta_{t+1}=\\theta_t-{\\rm lr}(c\\nabla f(\\theta_t)+c\\beta m_{t+1})$$. This verifies our claim since $\\nabla f(\\theta_t)$ is the propagator $M(0)$.\n\n# Additional mathematical observations {#appendix:additional_math_obs}\n\n**On the memory units of $M_k(\\lambda)$ propagators:**   The $M_k(\\lambda)$ propagator can naturally be interpreted as an iterated momentum propagator. Let $m_1,m_2,\\dots,m_k$ denote the the memory units. The update rule of $M_k(\\lambda)$ is given by $$m_1\\leftarrow m_1\\beta+g$$ $$m_i\\leftarrow m_i\\beta+m_{i-1}~{\\rm for}~i\\geq 2.$$ Thus $m_1$ is the momentum vector of the gradient and $m_i$ (for $i\\geq 2$) is the momentum vector of $m_{i-1}$. One can compute that the abstract rule of $m_i$ is given by the sequence $\\{\\beta^{j-i+1}{{j}\\choose{i-1}}\\}_{j=0}^\\infty.$ It follows that the subspace generated by the abstract rules of the memory units is the space of all sequences of the form $\\{p(j)\\beta^j\\}_{j=0}^\\infty$ where $p$ is a polynomial of degree at most $k-1$. This means that we can associate such a polynomial with each learning law. The RLLC method for $M_k(\\lambda)$ basically adaptively navigates in this polynomial space.\n\n**Relation between Multi-momentum and $M_k(\\lambda)$ propagators:**   The progression of the learning law of $M(0.9) \\oplus M(0.8) \\oplus M(0.7)$ presents an interesting phenomenon. As [3](#fig:resnet_metaW){reference-type=\"ref+label\" reference=\"fig:resnet_metaW\"} shows, the coefficients of $M(0.8)$ and $M(0.7)$ memory units are noticeably coupled with opposite sign. One might assume that the algorithm is just trying to cancel their effects, but the performance improvement compared to $M(0.9)$ suggests that something more interesting is happening here. A deeper explanation relates this optimizer to another one of the form $M(0.9)\\oplus M_2(0.75)$. More precisely if we consider $\\mathcal{O}(\\epsilon)=M(0.9)\\oplus M(0.75+\\epsilon)\\oplus M(0.75-\\epsilon)$ we find that as $\\epsilon$ goes to $0$ the subspace spanned by the abstract rules of the memory units converges to the subspace spanned by the memory units of $M(0.9)\\oplus M_2(0.75)$. In theory this convergence means that the optimizers themselves converge. Note that in practice we can not model $M(0.9)\\oplus M_2(0.75)$ by $\\mathcal{O}(\\epsilon)$ because numerical instability arises if $\\epsilon$ is very small.\n\n# Implementation details {#appendix:impleementation_details}\n\n## Network architectures and training details\n\n**Dense network** Our dense network comprises three hidden layers, each with a width of 128 and followed by a ReLU activation function. We did not include any normalization layers.\n\n**Convolutional network** Our convolutional network features a depth of three, with channel widths of 32, 64, and 64, each followed by a ReLU activation function. We did not incorporate any normalization layers. Following the convolutional layers, we apply max pooling and then a final dense layer.\n\n**ResNet-20** Our ResNet-20 variant adheres to established conventions for CIFAR-10, employing a three-level architecture with three residual blocks at each level. Each residual block is composed of the following sequence of layers: Convolution-Batch Normalization-ReLU-Convolution-Batch Normalization. A ReLU operation is applied after the addition operation in each residual block. The convolution kernels are 3x3 in size.\n\n## Training details, hyperparameters\n\nIn all reported experiments, we employed a batch size of 128 and trained the models for 10,000 iterations. We did not use a learning rate scheduler, to avoid any potential variance in its effect across different optimizers. We run every experiment with 3 different seed, and reported the average of the results.\n\nDuring our hyperparameter optimization process, we tested the following potential values:\n\n-   **Benchmark optimizers** learning rate: 1e-7, 3e-7, 1e-6, 3e-6, 1e-5, 3e-5, 1e-4, 3e-4, 1e-3, 3e-3, 1e-2, 3e-2, 1e-1, 3e-1, 1\n\n-   **Our optimizers** learning rate: 0.001, 0.003, 0.01, 0.03, 0.1, 0.3 learning law - learning rate: 0.003, 0.01, 0.03\n\n## Trainind datasets\n\n**CIFAR-10** The CIFAR-10 dataset [@cifar10_dataset] consists of $60000$ $32x32$ colour images in $10$ classes, with $6000$ images per class. There are $50000$ training images, and $10000$ test images. We used the canonical train--validation-test split, with $45000$ train, $5000$ validation, and $10000$ test images. As a preprocessing, we normalized the images with the means $(0.4914, 0.4822, 0.4465)$ and standard deviations $(0.2023, 0.1994, 0.2010)$ for the three RGB channels, respectively. On the *ResNet* task we used random resized crop (with zoom scale $0.8$-$1.2$), horizontal flip, and random rotation.\n\n**Fashion-MNIST** The Fashion-MNIST dataset [@fashion_mnist_dataset] consists of $70000$ $28x28$ monochrom images in $10$ classes, with $7000$ images per class. There are $60000$ training images, and $10000$ test images. We used the canonical train--validation-test split, with $54000$ train, $6000$ validation, and $10000$ test images. As a preprocessing, we normalized the images with the mean $0.3$ and stadard deviation $0.3$.\n\n**MNIST** The MNIST dataset [@lecun2010mnist] consists of $60000$ $28x28$ monocrom images in $10$ classes, with $7000$ images per class. There are $60000$ training images, and $10000$ test images. We used the canonical train--validation-test split, with $54000$ train, $6000$ validation, and $10000$ test images. As a preprocessing, we normalized the images with the mean $0.1307$ and standard deviation $0.3081$.\n\n# Computational resources\n\nFor our experiments we used a server with 8 A10040GB GPUs. We reported outcomes from a total of 16 optimizers, each optimized for hyperparameters across seven distinct tasks (architecture - dataset pair). One round of hyperparameter optimization with three different random seeds took approximately 4 hours on our server, and we were able to run 16 paralelly.\n\nTherefore, all of our results can be replicated in about 28 hours using the same setup, or in 224 hours on a single A100 40GB GPU.\n\n# Supplementary Plots {#appendix:Supplementary_plots}\n\n[4](#fig:Supplementary_plots){reference-type=\"ref+label\" reference=\"fig:Supplementary_plots\"} shows additional accuracy plots for MLP and Convolutional tasks. [5](#fig:error_bar){reference-type=\"ref+label\" reference=\"fig:error_bar\"} demonstrates, that the test accuracy is consistent on different random seeds for the demonstrated experiments.\n\n![Test accuracy graph of some RLLC optimizer, comparing with benchmark optimizers. On most of the tasks RLLC optimizers perform better, than the benchmark optimizers.](images/figs/all_acc1.png){#fig:Supplementary_plots width=\"\\\\textwidth\"}\n\n![Test accuracy graph of some RLLC optimizer, with min-max interval, trained from $3$ random seed initialization. The accuracy does not vary a lot, suggesting, that RLLC is robust.](images/figs/confidence_plots.png){#fig:error_bar width=\"\\\\textwidth\"}\n\n[^1]: <https://anonymous.4open.science/r/easytrace-843E/README.md>",
    "rationale": "Summary: This paper proposes a general gradient descent framework for retrieving retrospective information from new gradients by multiple memory units. When specialized with linear update rules, the proposed framework recovers classic optimizers with one memory unit such as SGD and NAS, and can be further decomposed to Jordan blocks. This paper then compares optimizers generated from Jordan blocks to classic optimizers across various datasets and neural networks, and demonstrates a competitive competence. This paper also provides certain theoretical explorations.\n\nStrengths: (S1) The proposed framework can be decomposed into Jordan blocks when constrained to linear memory, providing a mathematical foundation for developing and studying such ‘linear’ optimizers.\n\nWeaknesses: Overall, this paper is not in proper writing and structure for publication, and there are certain flaws in both the theoretical analysis and experiments that might invalidate the whole paper. I will consider raising the score if concerns are addressed.\n\n(W1) The citation format is unclear (misuse of \\citet and \\citep across the whole paper, missing space before many citations).\n\n(W2) Some undefined notations, e.g., U_1 and U_2 at line 175, and typos exist, e.g. U_G at line 360.\n\n(W3) Experiments lack statistical significance: results are averaged from only three different seeds.\n\n(W4) Lemma 3.8 might be flawed or has a typo because the proof uses Q^{-1} where Q is an arbitrary k*k matrix in its statement. \n\n(W5) The paper is hard to read because of the mismatch between its linear focus and the opening abstract framework, I thus suggest putting the abstract framework to the end or to the appendix.\n\nQuestions: (Q1) I am curious about your ‘consistency property’ at line 218, could you provide some examples?\n\n(Q2) This paper aims to improve past updates by extracting retrospective information from current gradients. Instead of using crafted memory units, have you thought about using parameterized distributions to extract? Some evolution strategies papers might be of help, such as [1](a summarized one) and [2](has parts similar to the equation at line 285).\n\n[1]Ollivier, Y., Arnold, L., Auger, A., & Hansen, N. (2017). Information-geometric optimization algorithms: A unifying picture via invariance principles. Journal of Machine Learning Research, 18(18), 1-65.\n[2]Zhang, Z., Wei, Y., & Sui, Y. (2024). An Invariant Information Geometric Method for High-Dimensional Online Optimization. Proceedings of the 6th Annual Learning for Dynamics & Control Conference, PMLR 242:641-653.",
    "rating": 2,
    "label": false,
    "rationale_edited": "This paper proposes a general gradient descent framework for retrieving retrospective information from new gradients by multiple memory units. When specialized with linear update rules, the proposed framework recovers classic optimizers with one memory unit such as SGD and NAS, and can be further decomposed to Jordan blocks. This paper then compares optimizers generated from Jordan blocks to classic optimizers across various datasets and neural networks, and demonstrates a competitive competence. This paper also provides certain theoretical explorations.\n\nThe proposed framework can be decomposed into Jordan blocks when constrained to linear memory, providing a mathematical foundation for developing and studying such ‘linear’ optimizers.\n\nOverall, this paper is not in proper writing and structure for publication, and there are certain flaws in both the theoretical analysis and experiments that might invalidate the whole paper. I will consider raising the score if concerns are addressed.\n\nThe paper is hard to read because of the mismatch between its linear focus and the opening abstract framework, I thus suggest putting the abstract framework to the end or to the appendix.\n\nThis paper aims to improve past updates by extracting retrospective information from current gradients. Instead of using crafted memory units, have you thought about using parameterized distributions to extract? Some evolution strategies papers might be of help."
  },
  {
    "title": "Beyond Gradient and Priors in Privacy Attacks: Leveraging Pooler Layer Inputs of Language Models in Federated Learning",
    "abstract": "Federated learning (FL) emphasizes decentralized training by storing data locally and transmitting only model updates, underlining user privacy. However, a line of work on privacy attacks undermines user privacy by extracting sensitive data from large language models during FL.Yet, these attack techniques face distinct hurdles: some work chiefly with limited batch sizes (e.g., batch size of 1), and others can be easily defended or are transparently detectable. This paper introduces an innovative approach that is challenging to detect and defend, significantly enhancing the recovery rate of text in various batch-size settings. Building on fundamental gradient matching and domain prior knowledge, we enhance the recovery by tapping into the input of the Pooler layer of language models, offering additional feature-level guidance that effectively assists optimization-based attacks. We benchmark our method using text classification tasks on datasets such as CoLA, SST, and Rotten Tomatoes. Across different batch sizes and models, our approach consistently outperforms previous state-of-the-art results.",
    "text": "# Introduction\n\nLanguage models trained under the Federated Learning paradigm play a pivotal role in diverse applications such as next-word predictions on mobile devices and electronic health record analysis in hospitals [@ramaswamy2019federated; @li2020federated]. This training paradigm prioritizes user privacy by restricting raw data access to local devices and centralizing only the model's updates, such as gradients and parameters [@mcmahan2017communication]. While the FL framework is created to protect user privacy, vulnerabilities still persist. Previous works [@geiping2020inverting; @yin2021see; @jeon2021gradient] proved that attackers can almost perfectly recover image data with gradients, which also highlighted the potential risks in the realm of textual data.\n\nMany studies have investigated vulnerabilities of private data in FL when applied to language models [@zhu2019deep; @deng2021tag; @balunovic2022lamp; @gupta2022recovering]. @zhu2019deep and @deng2021tag leverage gradient information and well-designed objective functions to build an optimization-based pipeline that can recover certain training textual data in minimal batch sizes. @balunovic2022lamp and @gupta2022recovering further improve the recovery rate by incorporating prior knowledge embedded in LLM to provide additional optimization or retrieval signals. All of them mainly focus on algorithm design and external information but rarely notice inherent privacy leakage risks embedded in the language models themselves. In contrast to these approaches,  @fowl2022decepticons and  @boenisch2023curious assume a malicious central server, designing specific malicious parameters and architectures for language models to enhance attack performance. To a certain degree, their research is connected to the privacy vulnerabilities of specific module designs. However, their methods depend on matched tampered parameters, such as identity weight matrices. These approaches also violate the training objective, generating useless gradients and parameter updates.\n\n::: wrapfigure\nr0.5 ![image](assets/pdfs/simple_archi.pdf){width=\"48%\"} []{#fig:3 label=\"fig:3\"}\n:::\n\nRecently, @wang2023reconstructing provides a theoretical analysis that recovers training samples from gradient information for a two-layer fully connected network. A major limitation of their approach is the reliance on a randomly initialized network instead of an actual, pre-trained network. This reliance poses challenges in applying their insights to the prevalent training paradigm that fine-tuning pre-trained models. When applied to deeper networks, their method also relies on identity modules and other transparently detectable weight manipulations, limiting its practical utility. However, building upon this foundation, we have found that certain prevalent modules in contemporary language model architectures possess intrinsic privacy vulnerabilities.\n\nThis paper focuses on Transformer-based language models featuring a distinctive Pooler layer and proposes a two-stage privacy attack method that exploits vulnerabilities in this specific module. Specifically, in the first stage, we employ an analytics-based reconstruction method to initially recover the direction of features associated with this module. This feature information is not averaged over the number of samples in the same batch or the sequence length, enabling the extraction of more unique information for each token. Subsequently, we utilize this feature information as an additional (beyond gradient and priors) supervisory signal to guide the recovery of training data. This research differentiates itself in the following ways: ) Different from @wang2023reconstructing, the research is solidly based on textual data and deep language models in real-world scenarios. ) Different from @fowl2022decepticons and @boenisch2023curious, this research does not depend on trap weights, such as Identity module, and maintains adherence to an effective training roadmap. ) In contrast to the honest-but-curious works [@zhu2019deep; @deng2021tag; @balunovic2022lamp; @gupta2022recovering], this research provides feature-level supervisory signals that differ from conventional gradients and priors.\n\nThe contributions of this research are outlined as follows: **1**) Instead of directly recovering the training samples of the entire model, this paper proposes a two-stage attack method to first approximate the intermediate feature information and then recover the real input. **2**) We design a strategic weight initialization method coupled with a flexible tuning approach, thereby empowering an analytics-based method to accurately and efficiently deduce the direction of intermediate features in a specific module. **3**) When integrated with gradient inversion and prior knowledge, our method consistently surpasses previous methods in the attack performance across a variety of benchmark datasets and scenarios. We also propose using the distance between semantic embeddings as a more comprehensive metric for evaluation. **4**) This research brings to light the inherent privacy leakage risks that are embedded within the design of contemporary language model architectures.\n\n# Preliminaries\n\nIn this section, we describe the relevant background of federated learning, gradient inversion, priors knowledge, two-layer-networks-based reconstruction, as well as the threat model of our proposed attack method.\n\n## Federated Learning\n\nIntroduced by @mcmahan2017communication, federated learning solves data privacy concerns by promoting decentralized model training. In this approach, models are refined using local updates from individual clients, which are merged at a central server [@konečný2015federated; @konečný2016federated; @konečný2017federated]. This field has attracted significant attention due to its potential business applications, underlining its promise in academia and industry [@ramaswamy2019federated; @li2020federated].\n\n## Gradient Inversion\n\nGradient inversion is a significant technique that could potentially breach privacy in federated learning [@zhu2019deep; @zhao2020idlg]. Although federated learning is designed to provide a decentralized training mechanism ensuring local data privacy, gradient inversion shows that this privacy may not be infallible.\n\n**Problem definition:**\n\nConsider the supervised learning framework wherein a neural network $f(\\cdot; \\bm{\\Theta}) : \\bm{x} \\in \\mathbb{R}^d \\rightarrow f(\\bm{x}; \\bm{\\Theta}) \\in \\mathbb{R}$ is trained using the objective: $$\\min _{\\bm{\\Theta}} \\sum_{(\\bm{x}, y) \\in \\mathcal{D}} \\ell(f(\\bm{x} ; \\bm{\\Theta}), y)$$ where $\\ell$ is the loss function and $\\mathcal{D}$ denotes the dataset of input-output pairs. In the federated paradigm, during the communication between the central server and clients, each node reports an average gradient of its local batch $S$ [@mcmahan2017communication; @konečný2015federated]. This is mathematically formulated as: $$G := \\frac{1}{B} \\nabla_{\\bm{\\Theta}} \\sum_{i=1}^B \\ell\\left(f\\left(\\bm{x}_i, \\bm{\\Theta}\\right), y_i\\right)$$ where $B$ is the batch size of $S$. Given the above definition, the challenge posed by gradient inversion becomes apparent: With access to a once-queried gradient and a known model as well as its loss function, is it possible to reconstruct the input training data?\n\n**Objective of Gradient Inversion:** Based on the above problem, the objective of gradient inversion can be represented as: $$\\min_{\\{\\hat{\\bm{x}}_i, \\hat{y}_i\\}_{i=1}^B} d\\left(\\frac{1}{B} \\sum_{i=1}^B \\nabla_{\\bm{\\Theta}} \\ell\\left(f\\left(\\hat{\\bm{x}}_i ; \\bm{\\Theta}\\right), \\hat{y}_i\\right), G\\right)$$ Here, $d(\\cdot, \\cdot)$ quantifies the difference between the provided and deduced gradients, and ($\\hat{x_I}, \\hat{y_i}$) refers to the estimated input and its label. Prominent works have leveraged this objective to attempt the retrieval of private data [@zhu2019deep; @zhao2020idlg; @geiping2020inverting].\n\n## Prior Knowledge\n\nRelying solely on gradient inversion to recover textual data often proves challenging, especially when handling larger batch sizes and long sequences. Researchers often seek to acquire the prior knowledge encapsulated in pre-trained language models like GPT-2 [@radford2019language] to address this. These models are adept at predicting the probability of the next token based on the preceding sequence. This property aids in evaluating the quality of text searched through gradient inversion. Specifically, it introduces the perplexity as an evaluation metric to guide optimization by pinpointing optimal starting points or intermediate points of the attack [@balunovic2022lamp; @gupta2022recovering].\n\n## Two-layer Network-based Reconstruction {#section:3-3}\n\n@wang2023reconstructing identified a gap in existing literature regarding the capability of gradient information to unveil training data. Their study demonstrates that it might be possible to reconstruct training data solely from gradient data using a theoretical approach within a two-layer neural network. Consider a two-layer neural network: $f(x;\\Theta) = \\sum_{j=1}^{m} a_j \\sigma(w_j \\cdot x)$, with parameters defined as $\\Theta = (a_1, ... , a_m, w_1, ... , w_m)$. Here, $m$ represents the hidden dimension. The objective function is represented as: $L(\\Theta) = \\sum_{i=1}^{B} (y_i - f(x_i;\\Theta))^2$. A notable finding is that the gradient for $a_j$ is solely influenced by $w_j$, making it independent from other parameters. This gradient is represented as: $$g_{j}:=\\nabla_{a_{j}}L(\\Theta)=\\sum_{i=1}^{B}r_{i}\\sigma\\left(w_{j}^{\\mathsf{T}}x_{i}\\right)$$ where the residual $r_{i}$ is given by $r_{i}=f(x_{i};\\Theta)-y_{i}$. For wide neural networks with random initialization from a standard normal distribution, the residuals $r_{i}$ concentrate to a constant, $r^{*}_{i}$. By set $g_{(w)}:=\\sum_{i=1}^{B}r_{i}^{*}\\sigma(w^{\\top}x_{i})$, $g_{j}$ can be expressed as $g_{j} = g(w_{j}) + \\epsilon$, where $\\epsilon$ represents noise. Then the third derivative of $g(w)$ is represented as: $$\\nabla^{3}g(w) = \\sum_{i=1}^{B} r_{i}^{*} \\sigma^{(3)}(w^{\\textsf{T}} x_{i}) x_{i}^{\\otimes3}\n\\label{equation/collapse}$$ Here, $x_{i}^{\\otimes3}$ signifies the tensor product of vector $x_{i}$ with itself three times. The researchers postulated that if they can accurately estimate $\\nabla^{3}g(w)$, it is possible to determine $\\{x_{i}\\}_{i=1}^{B}$ by using tensor decomposition techniques, especially when these features are independent. They used Stein's Lemma, expressed as: $\\mathbb{E}[g(X)H_{p}(X)] = \\mathbb{E}[g^{(p)}(X)]$ to approximate $\\nabla^{3}g(w)$ as: $$\\begin{aligned}\nT &= \\mathbb{E}_{W}[\\nabla_{W}^{3}g(W)] = \\mathbb{E}_{W\\sim N(0,I)}[g(W)H_{3}(W)] \\nonumber \\\\\n  &\\approx \\frac{1}{m} \\sum_{j=1}^{m} g(w_{j})H_{3}(w_{j}) = \\hat{T} \\label{equation/concentration}\n\\end{aligned}$$ Where $H_{3}(w_j)$ is the p-th Hermite function of $w_j$. By leveraging this approach, they successfully reconstructed each unique $x_{i}$. Their approach is primarily theoretical, focusing on two-layer fully connected networks and largely confined to randomly initialized networks. When applied to deeper networks, their method uses identity modules and other transparently detectable weight manipulations, which also limits its practical use.\n\nInspired by @wang2023reconstructing, we have identified a vulnerable module commonly found in current language model architectures, such as BERT and RoBERTa [@devlin2018bert; @liu2019roberta]. The module, comprising the Pooler and Classifier layers, typically forms the final stages of these networks. Given this module does not have deeper topological connections, it can be treated as an independent two-layer network. Leveraging this insight, we have designed specific techniques to enhance the two-layer network-based reconstruction method, enabling it to recover the feature information associated with this special module in pre-trained language models. Further details of this methodology are discussed in Section [\\[section:4-2\\]](#section:4-2){reference-type=\"ref\" reference=\"section:4-2\"}.\n\n## Threat Model[]{#section:3-5 label=\"section:3-5\"}\n\nTo facilitate comparisons with previous works, our threat model is based on the following principles: **1)** We opt to freeze the gradients of both token and positional embeddings. This is because it is relatively easy to deduce the training text tokens or max sequence length without this restriction since only the tokens and positions in the current training batch will receive gradient updates on the embedding matrix. By freezing the updates of these embeddings, we create a more challenging attack scenario. **2)** We ensure that the training process maintains effective gradient aggregation and consistently aims to minimize the training loss. This principle is crucial for preserving the integrity and efficiency of the training process, ensuring it remains free from suspicion. **3)** We refrain from using trap weights, such as the Identity module, in our approach. However, instead of using pre-trained weights for initialization, we may opt to initialize randomly for partial weights of certain layers.\n\n# Methodology\n\nGradient inversion seeks to reconstruct the original training data by harnessing the gradients of a known network. A closer look at this method reveals several challenges. Central to these is the nonconvexity of the issue, marked by the presence of numerous local minima that complicate the pursuit of the global optimum. Additionally, the problem is over-determined because it has more equations to resolve than unknown parameters. While these equations remain consistent, they complicate the optimization process. This complexity persists even when reduced to a single-sample scenario. As a result, gradient inversion remains an NP-complete problem, implying that procuring an exact solution within a feasible time frame is difficult [@wang2023reconstructing]. From a broader perspective, it is crucial to recognize that text tokens represent discrete data, different from the continuous nature of images. Additionally, the gradients of language models are not just averaged over the number of samples in the same batch, but also across the sequence length of the longest sentence in that batch. These properties make the recovery of textual training data more challenging compared to image data.\n\nTo overcome these challenges, this paper endeavors to seek more valuable information that could guide the recovery or retrieval process. Instead of solely focusing on gradients and external information (priors), we shift our attention toward the inner workings of the model itself.\n\n## Vulnerable Module Identification\n\nBuilding upon the work of @wang2023reconstructing, which demonstrated the feasibility of recovering training data using gradient information from a two-layer fully connected network, we are reminded to scrutinize similar vulnerabilities embedded in contemporary language model architectures. However, the increasing complexity and depth of these models render it nearly impossible to apply a similar strategy without resorting to trap weights, such as an identity module. This leads us to a critical observation when examining the architecture of widely-used transformer-based language models like BERT and RoBERTa. Notably, these models contain a module consisting of a Pooler layer and a Classifier layer, with a non-linear activation function between them. By analyzing the topological operational order, we discern that this module can be treated as an independent two-layer fully connected network, given its position at the top of the language model, implying no deeper logical operations.\n\nWith this keen realization, instead of attempting to recover the training samples of the entire model in one step, we propose first recovering the input information for this special module using an analytics-based method. This additional information then aids the optimization-based method (gradient inversion) with extra supervisory signals, thereby enhancing privacy attack performance. we have successfully unearthed a novel source of valuable information, distinct from conventional gradients and priors, embedded within the internal module of the model itself. We identify this module as a **vulnerable module**, which harbors inherent privacy leakage risks. Nonetheless, @wang2023reconstructing's reconstruction method, as described in Section  [2.4](#section:3-3){reference-type=\"ref\" reference=\"section:3-3\"}, comes with its own set of assumptions and limitations. Most critically, it does not recover the actual features but their direction within the feature space. To ensure the efficacy of this analytics-based method, especially in its application to real pre-trained language models, we have designed several critical techniques to relax or remove these limitations.\n\n## First-stage Analytics-based Attack []{#section:4-2 label=\"section:4-2\"}\n\nIn the first-stage attack, we aim to first recover the feature information directed to the identified vulnerable module with an enhanced analytics-based method. To better elucidate our tailored design, let us first establish the notations used in this context: Let $X \\in \\mathbb{R}^{B \\times d}$ be the input to this vulnerable module, where $B$ is the batch size and $d$ is the feature dimension. Let $W_1 \\in \\mathbb{R}^{d \\times v}$ and $W_2 \\in \\mathbb{R}^{v \\times n}$ denote the weights of the Pooler layer and the Classifier layer, respectively, with $v$ being the pooler dimension and $n$ being number of classes. Let $\\sigma$ represent the non-linear activation function that is positioned after the Pooler layer.\n\n**Enlarge Pooler Dimension: []{#section:4-2-1 label=\"section:4-2-1\"}** The initial configuration of language models often sets the pooler dimension $v$ to match the hidden dimension $d$ (For BERT$_{\\text{BASE}}$, it's 768) [@devlin2018bert]. This setting is insufficient to promise the accuracy of tensor decomposition when applying the analytics-based reconstruction method. To address this limitation, we temporarily expand the pooler dimension to match the vocabulary size for BERT. To be explicit, we set $v = |V|$. The rationale behind this change is grounded in enhancing the model's expressiveness while ensuring our modification is subtle. It's important to note that, given the expanding width of state-of-the-art language models such as GPT-3 [@brown2020language], the hidden dimension has become sufficiently large to assure the accuracy of our method. Consequently, we can bypass this certain step that would otherwise be necessary in BERT.\n\n**Strategic Weight Initialization: []{#section:4-2-2 label=\"section:4-2-2\"}** As mentioned in Section [2.4](#section:3-3){reference-type=\"ref\" reference=\"section:3-3\"}, $m$ signifies the intermediate dimension in the two-layer network. In our setting, we should have $m = v$. However, during our computation of $\\hat{T}$ as outlined in Equation [\\[equation/concentration\\]](#equation/concentration){reference-type=\"ref\" reference=\"equation/concentration\"}, we noticed an anomaly in $g_j$. Due to the random initialization of $W_1$ by requirement, a substantial portion of gradients $g_j$ approached a value close to 0. This side effect impacts the subsequent decomposition procedure. To address this issue, rather than setting $m = v$, we set $m = v - d$. This approach ensures the original pre-trained weights $W_1^\\prime \\in \\mathbb{R}^{d \\times d}$ are retained in the new weight matrix $W_1 \\in \\mathbb{R}^{d \\times v}$, allowing us to obtain optimal gradients for $W_1$ and $W_2$. Simultaneously, the remaining dimensions $W_1^{\\prime\\prime} \\in \\mathbb{R}^{d \\times (v-d)}$ are randomly initialized and adequate to promise the accuracy of tensor decomposition. For the classifier layer, we utilize a strategy similar to that of the Pooler layer, adjusting the remaining dimensions to a constant ($i/m$, where $i$ represents the class index for the classification task). More details are in Appendix [\\[B-1\\]](#B-1){reference-type=\"ref\" reference=\"B-1\"}.\n\nFrom one perspective, it may appear that we have altered the parameters. However, it is important to clarify that we have not assigned any special properties to these weights. Our approach involves initializing part of the weight randomly, which is a standard operation in model initialization. Furthermore, this random initialization is confined only to the identified vulnerable module, allowing the rest of the language model to utilize pre-trained weights for initialization. Consequently, this approach avoids creating trap weights and ensures the normal training roadmap.\n\n**Flexible Tuning Framework: []{#section:4-2-3 label=\"section:4-2-3\"}** @wang2023reconstructing suggests significantly expanding the pooler dimension $m$ in comparison to the input dimension $d$ to reduce the tensor decomposition error. In our setting, the specific relationship between recovery dimension $d$ and pooler dimension $v = m + d$ remains undetermined. Recognizing these constraints, we keep $m$ constant and design an alternative method to tweak $d$. Specifically, instead of attempting to recover the full dimension $d$, our strategy focuses on recovering a dimension $d^{\\prime}$ and $d^{\\prime} \\leq d$. This approach sets the subweights ($d\\colon, d^{\\prime}\\colon$) of $W_1$ to zero. Then the gradient $g_j$ in Equation [\\[equation/concentration\\]](#equation/concentration){reference-type=\"ref\" reference=\"equation/concentration\"} remains functional but is exclusively tied to the subweights ($\\colon,\\colon{d^\\prime}$) of $W_1$. As a result, we embrace a more flexible and efficient methodology by centering our reconstruction on the feature subset $X \\in \\mathbb{R}^{B \\times d^\\prime}$. More details can be found in Appendix [\\[B-1\\]](#B-1){reference-type=\"ref\" reference=\"B-1\"}.\n\n**Reorder Feature Information: []{#section:4-2-4 label=\"section:4-2-4\"}** When applying tensor decomposition techniques to retrieve features from $\\hat{T}$, a significant issue arises when the batch size exceeds one: the exact order of the recovered features remains uncertain. Under adversarial conditions, one might try every conceivable permutation as a reference. However, we simplify the procedure by sequentially comparing each recovered feature to the actual input features with cosine similarity until the best order is discerned. In certain cases, a single recovered feature displayed a notably high cosine similarity with multiple actual inputs simultaneously. Interestingly, although a 1-m greedy relationship might exhibit a high correlation, it did not exceed the attack performance of a straightforward 1-1 match in the final outcome. Consequently, we adopted the 1-1 relationship to achieve the best attack result.\n\n**Activation Function Exploration: []{#section:4-2-5 label=\"section:4-2-5\"}** Our empirical observations have indicated that various activation functions result in differing effects on information retrieval. Consequently, rather than limiting our focus to the original activation function, we have experimented with a range of activation functions to assess their impact on the final attack performance. Currently, we include activation functions such as Tanh, ReLU, SeLU, and a custom function defined as $\\sigma(x) = x^3 + x^2$. A more detailed discussion of this design will be presented in Section [5](#section:5-3){reference-type=\"ref\" reference=\"section:5-3\"}.\n\n## Second-stage Optimization-based Attack []{#section:4-3 label=\"section:4-3\"}\n\nIn the second-stage attack, we aim to recover the real input for the entire language models with an optimization-based method. Specifically, following @balunovic2022lamp, we divide our entire optimization process into three phases: Initialization, Training, and Token Swap. In the initialization and token swap stages, we aim to leverage certain metrics to identify optimal starting or intermediary points for the subsequent training phase. This stage is also commonly recognized as discrete optimization. In this setting, we've chosen a mix of metrics to guide the choice, including gradient match loss and perplexity obtained from pre-trained language models. In the optimization stage, we optimize the embeddings derived from input IDs to minimize gradient matching loss and cosine distance between the input of the Pooler layer with the recovered feature information from our first-stage attack. This phase falls under the category of continuous optimization. We oscillate between continuous and discrete optimization phases to bolster the final attack performance. More details can be found in Appendix [\\[B-1\\]](#B-1){reference-type=\"ref\" reference=\"B-1\"}.\n\n# Experiments\n\nThis section initially presents the fundamental setup for our experiments. Subsequently, we demonstrate the results of experiments in various settings and provide an in-depth analysis from multiple perspectives.\n\n## Set Up []{#section:5-1 label=\"section:5-1\"}\n\n**Datasets:** Following previous work [@balunovic2022lamp], our experimental design incorporates three binary text classification datasets to ensure a comprehensive evaluation. Specifically, we utilize **CoLA** and **SST-2** from the **GLUE** benchmark [@warstadt2018neural; @socher2013recursive; @wang2019glue], with their sequences predominantly ranging between 5-9 and 3-13 words, respectively. Additionally, the **RottenTomatoes** dataset presents a more complex scenario with sequence lengths oscillating between 14 and 27 words [@pang2005seeing]. More details can be found in Appendix [\\[sec:C-3\\]](#sec:C-3){reference-type=\"ref\" reference=\"sec:C-3\"}. Within the scope of our experiments, we utilize a subset of 100 randomly selected sequences from the training sets of these datasets as our evaluation benchmark, a method also endorsed by @balunovic2022lamp.\n\n**Models:** Experiments are primarily based on **BERT$_{\\text{BASE}}$** [@devlin2018bert] architecture. Consistent with @balunovic2022lamp, we use models that have been fine-tuned on downstream tasks for two epochs. To ensure a fair comparison, we adopt the same fine-tuned models from @balunovic2022lamp. As for the auxiliary language model employed to extract prior knowledge, we choose GPT-2 [@radford2019language], a choice also used by @balunovic2022lamp.\n\n**Metrics:** Following @deng2021tag and @balunovic2022lamp, we evaluate attack performance with the ROUGE metric suite [@lin2004rouge]. Specifically, we present the F-scores of **ROUGE-1**, **ROUGE-2**, and **ROUGE-L**. These metrics respectively assess the retrieval of unigrams, bigrams, and the proportion of the longest continuous matching subsequence relative to the entire sequence. We omit all padding tokens in the reconstruction and evaluation phases.\n\n**Baselines:** We benchmark our approach against three baselines: **DLG**, **TAG**, and **LAMP**, with a similar definition of threat model described in Section [\\[section:3-5\\]](#section:3-5){reference-type=\"ref\" reference=\"section:3-5\"}. Among them, LAMP represents the state-of-the-art. We employ the open-sourced implementation from LAMP, which encompasses the implementations for all three baselines [@deng2021tag; @zhu2019deep; @balunovic2022lamp]. Following previous work, we assume the lengths of sequences are known for both baselines and our attacks, as an adversary can run the attack for all possible lengths [@balunovic2022lamp].\n\n**Implementation:** Our method is implemented based on LAMP's framework. To ensure a fair comparison, we standardized the experimental conditions and settings when comparing our approach with baselines. We adopt all of LAMP's hyperparameters, including the optimizer, learning rate, learning rate schedule, regularization coefficient, and optimization steps. For hyperparameters unique to our method, we made selections using a grid search on BERT$_{\\text{BASE}}$ and shared them in different settings. We also assume the prior knowledge of input labels. This is typical in text classification tasks with limited categories because we can easily iterate all the potentials. More details can be found in Appendix [\\[B-1\\]](#B-1){reference-type=\"ref\" reference=\"B-1\"}.\n\n:::: table*\n::: tabularx\nl\\|CCC\\|CCC\\|CCC\\|CCC\\|CCC Method & & & & &\\\n& R-1 & R-2 & R-L & R-1 & R-2 & R-L & R-1 & R-2 & R-L & R-1 & R-2 & R-L & R-1 & R-2 & R-L\\\n\\\nDLG & 59.3 & 7.7 & 46.2 & 36.9 & 2.6 & 31.4 & 35.3 & 1.4 & 31.9 & 16.5 & 0.8 & 7.9 & $\\downarrow$ 17.7 & $\\downarrow$ 16.8 & $\\downarrow$ 19.0\\\nTAG & 78.9 & 10.2 & 53.3 & 45.6 & 4.6 & 36.9 & 35.3 & 1.6 & 31.3 & 33.3 & 1.6 & 30.4 & $\\downarrow$ 6.4 & $\\downarrow$ 15.4 & $\\downarrow$ 10.4\\\nLAMP$~_{\\text{COS}}$ & 84.8 & 46.2 & 73.1 & 57.2 & 21.9 & 49.8 & 40.4 & 6.4 & 36.2 & 36.4 & 5.1 & 34.4 & --- & --- & ---\\\n**Ours**$_{\\text{~Tanh}^{\\star}}$ & 84.5 & 46.1 & 72.8 & 56.9 & 22.0 & 49.6 & 41.2 & 7.8 & 40.1 & 37.2 & 5.2 & 34.4 & $\\uparrow$ 0.25 & $\\uparrow$ 0.4 & $\\uparrow$ 0.9\\\n**Ours**$_{\\text{~ReLU}}$ & 84.5 & 45.9 & 72.6 & 57.3 & 19.3 & 49.8 & 42.3 & 8.4 & 40.1 & 37.6 & 5.6 & 34.5 & $\\uparrow$ 0.7 & $\\downarrow$ 0.1 & $\\uparrow$ 0.9\\\n**Ours**$_{\\text{~SeLU}}$ & **86.6** & **51.5** & **76.7** & **69.5** & **31.2** & **60.6** & **50.5** & **11.8** & **43.9** & **40.8** & **8.3** & **38.1** & **$\\uparrow$ 7.1** & **$\\uparrow$ 5.8** & **$\\uparrow$ 6.5**\\\n**Ours$_{~x^3+x^2}$** & 84.6 & 45.2 & 72.4 & 57.3 & 19.2 & 49.8 & 43.9 & 11.4 & 40.1 & 37.8 & 5.9 & 34.8 & $\\uparrow$ 1.2 & $\\uparrow$ 0.5 & $\\uparrow$ 1.0\\\n\\\nDLG & 57.7 & 11.7 & 48.2 & 39.1 & 7.6 & 37.2 & 38.7 & 6.5 & 36.4 & 36.6 & 4.7 & 35.5 & $\\downarrow$ 16.0 & $\\downarrow$ 19.3 & $\\downarrow$ 14.1\\\nTAG & 71.8 & 16.1 & 54.4 & 46.1 & 10.9 & 41.6 & 44.5 & 9.1 & 40.1 & 41.4 & 6.7 & 38.9 & $\\downarrow$`<!-- -->`{=html}8.0 & $\\downarrow$`<!-- -->`{=html}16.2 & $\\downarrow$`<!-- -->`{=html}9.7\\\nLAMP$~_{\\text{COS}}$ & 87.7 & 54.1 & 76.4 & 59.6 & 26.5 & 53.8 & 48.9 & 17.1 & 45.4 & 39.7 & 10.0 & 38.2 & --- & --- & ---\\\n**Ours**$_{\\text{~Tanh}^{\\star}}$ & 88.5 & 56.9 & 77.3 & 66.4 & 33.2 & 61.2 & 49.9 & 15.6 & 46.1 & 43.5 & 10.9 & 40.5 & $\\uparrow$ 3.1 & $\\uparrow$ 2.2 & $\\uparrow$ 2.8\\\n**Ours**$_{\\text{~ReLU}}$ & 88.5 & 56.1 & 77.1 & 67.3 & 32.6 & 60.8 & 50.4 & 14.1 & 46.2 & 43.5 & 11.2 & 40.8 & $\\uparrow$ 3.4 & $\\uparrow$ 1.6 & $\\uparrow$ 2.8\\\n**Ours**$_{\\text{~SeLU}}$ & 90.3 & 59.0 & 78.2 & 71.0 & 35.3 & 63.4 & 58.6 & **26.3** & 54.2 & 45.4 & 11.5 & 43.2 & $\\uparrow$ 7.3 & $\\uparrow$ 6.1 & $\\uparrow$ 6.3\\\n**Ours$_{~x^3+x^2}$** & **93.1** & **61.6** & **81.5** & **78.3** & **40.9** & **67.9** & **60.6** & 23.1 & **54.9** & **49.5** & **16.5** & **47.3** & **$\\uparrow$ 11.4** & **$\\uparrow$ 8.6** & **$\\uparrow$ 9.5**\\\n\\\nDLG & 20.1 & 0.4 & 15.2 & 18.9 & 0.6 & 15.4 & 18.7 & 0.4 & 15.7 & 20.0 & 0.3 & 16.9 & $\\downarrow$ 17.4 & $\\downarrow$ 5.4 & $\\downarrow$ 11.5\\\nTAG & 31.7 & 2.5 & 20.1 & 26.9 & 1.0 & 19.1 & 27.9 & 0.9 & 20.2 & 22.6 & 0.8 & 18.5 & $\\downarrow$ 9.5 & $\\downarrow$ 4.5 & $\\downarrow$ 7.8\\\nLAMP$~_{\\text{COS}}$ & 63.4 & 13.8 & 42.6 & 38.4 & 6.4 & 28.8 & 24.6 & 2.3 & 20.0 & 20.7 & 0.7 & 17.7 & --- & --- & ---\\\n**Ours**$_{\\text{~Tanh}^{\\star}}$ & 64.2 & 15.5 & 43.8 & 38.8 & 5.8 & 28.9 & 28.3 & 2.4 & 21.2 & 22.4 & 1.1 & 18.9 & $\\uparrow$ 1.7 & $\\uparrow$ 0.4 & $\\uparrow$ 1.0\\\n**Ours**$_{\\text{~ReLU}}$ & 64.1 & 15.7 & 44.2 & 40.2 & 5.4 & 28.8 & 31.1 & 2.6 & 23.6 & 22.8 & 1.3 & 18.8 & $\\uparrow$ 2.8 & $\\uparrow$ 0.5 & $\\uparrow$ 1.6\\\n**Ours**$_{\\text{~SeLU}}$ & 71.9 & 19.2 & 48.7 & **48.1** & **8.2** & **34.2** & **33.0** & **4.23** & **25.3** & **24.6** & **2.0** & **20.6** & **$\\uparrow$ 7.6** & **$\\uparrow$ 2.6** & **$\\uparrow$ 4.9**\\\n**Ours$_{~x^3+x^2}$** & **72.2** & **21.0** & **49.3** & 44.6 & 7.0 & 31.8 & 29.9 & 3.5 & 24.3 & 23.6 & 1.7 & 19.8 & $\\uparrow$ 5.8 & $\\uparrow$ 2.5 & $\\uparrow$ 4.0\\\n:::\n\n[]{#tab:1 label=\"tab:1\"}\n::::\n\n## Results and Analysis\n\nWe present experimental results in Table [\\[tab:1\\]](#tab:1){reference-type=\"ref\" reference=\"tab:1\"}. These findings demonstrate that our approach outperforms all baselines (DLG, TAG, and LAMP) across various datasets and batch sizes. Examining the impact of batch size variations, we notice that launching an attack becomes more challenging as the batch size increases. All attack methods, including ours, exhibit a decline in attack performance. However, our method brings a more noticeable improvement at batch sizes 2 and 4, surpassing its efficacy at batch sizes 1 and 8. We posit that for a batch size of 1, where the gradient is only averaged solely over tokens, the benefit of incorporating the feature information is less evident because the gradient information still plays a leading role in the optimization process. For a batch size of 8, the improvement scale is also not pronounced, we explore the background reason in Section [5](#section:5-3){reference-type=\"ref\" reference=\"section:5-3\"}.\n\n::: wrapfigure\nr0.5 ![image](assets/pdfs/fig2.pdf){width=\"\\\\linewidth\"} []{#fig:2 label=\"fig:2\"}\n:::\n\nTurning our attention to variations in sequence length across datasets, we notice a clear trend: as sequences get longer, the benefit from feature information at a batch size of 1 becomes more pronounced. Specifically, for the CoLA dataset with token counts between 5-9, we see an average improvement in ROUGE metrics of **3%**. This improvement grows to **5%** for the SST-2 dataset with token counts from 2 to 13. For the Rotten Tomatoes dataset, which features even longer sequences with token counts ranging from 14 to 27, the average ROUGE metric improvement further increases to **8%**. This suggests a correlation between sequence length and the extent of improvement observed. Moreover, when the batch size exceeds one, the benefits observed for these three datasets are consistently notable. Recall that gradient averaging occurs only over tokens at a batch size of 1, it implies that with longer sentences, the gradient information becomes less effective, leading to greater benefits from feature-level supervision signals. When batch sizes are larger than 1, averaging happens over the number of tokens and sentences simultaneously. This results in our method consistently yielding pronounced benefits across sequences with different lengths. Our findings further reinforce the idea that relying exclusively on gradient information diminishes efficacy with larger batch sizes and longer sequences.\n\nIn a word, the experiments consistently show that the use of feature information significantly enhances the success of privacy attacks on language models. This evidence further confirms that current language models, particularly in their Pooler and Classifier layers, inherently possess vulnerabilities that pose risks of privacy leakage.\n\n# Discussion {#section:5-3}\n\n**Impact of Activation Function:** As outlined in Section [\\[section:4-2-5\\]](#section:4-2-5){reference-type=\"ref\" reference=\"section:4-2-5\"}, we replaced the original Tanh activation function with ReLU, SeLU, and a custom activation function $\\sigma=x^3+x^2$ to investigate how different activation functions affect attack performance in our strategy. Table [\\[tab:1\\]](#tab:1){reference-type=\"ref\" reference=\"tab:1\"} presents the performance of these attacks under various settings. SeLU and $\\sigma=x^3+x^2$ consistently yield significant improvements in attack performance, while the enhancements seen with Tanh and ReLU are relatively less pronounced. We speculate that the latter activation functions' n-th derivative leads to a zero expectation ($\\mathbb{E}_{Z\\sim N(0,1)}[\\sigma^{(n)}(Z)]=0$), thereby affecting the estimation of $T$ as explained in Equation [\\[equation/concentration\\]](#equation/concentration){reference-type=\"ref\" reference=\"equation/concentration\"}. In contrast, the earlier activation functions, whose n-th derivatives are neither odd nor even, do not exhibit this issue, potentially resulting in a more pronounced risk of privacy leakage. Another interesting phenomenon observed is that for the CoLA dataset, all attack methods show only minor improvements, except for attacks utilizing the SeLU. This suggests the existence of an unknown correlation between datasets and the feature information recovered in different activation functions. Further details and discussions are provided in the Appendix [\\[B-3\\]](#B-3){reference-type=\"ref\" reference=\"B-3\"}.\n\n**Impact of Recovery Dimension:** In Section [\\[section:4-2-3\\]](#section:4-2-3){reference-type=\"ref\" reference=\"section:4-2-3\"}, we propose fixing $m$ and adjusting $d'$ to identify the optimal mapping for $d'$ (where $d' < d$) and $m$. Accordingly, we conduct experiments using **BERT$_{\\text{BASE}}$** with various batch sizes to investigate the quality of the recovered feature information by calculating their cosine similarity with the ground truth. The results are illustrated in Figure [\\[fig:2\\]](#fig:2){reference-type=\"ref\" reference=\"fig:2\"}. Our findings suggest that when the batch size is 1, the recovered quality gradually degrades as the recovery dimension $d'$ increases, yet it remains as high as 0.99 across all configurations. However, this pattern does not hold when the batch size exceeds 1. We also observed that the recovered quality consistently declines as the batch size increases. We hypothesize that multiple inputs might exhibit some undisclosed dependencies, particularly features within the deeper layers of language models, thereby affecting the efficacy of tensor decomposition. For simplicity, we set $d'=100$ across all experiments. However, under adversarial conditions, attackers might experiment with various $d'$ settings to enhance their attack performance.\n\n**Impact on Other Models:** To demonstrate the effectiveness of our attack method on various model architectures, we also apply our method on the RoBERTa [@liu2019roberta]. While RoBERTa shares similarities with BERT, it distinguishes itself through unique training configurations and datasets. Notably, unlike BERT$_{\\text{BASE}}$, RoBERTa does not have a Pooler layer. Instead, it employs a classifier composed of two linear layers in the head. In our experiments, we treat the first layer as an analogous Pooler layer and endeavor to reconstruct its input first. All the models used in this experiment are from Hugging Face, contributed by TextAttack [@morris2020textattack]. As for the auxiliary model, we employ RoBERTa itself due to a specific challenge: we can't locate another generative model using the same tokenizer with RoBERTa. However, it's essential to note that we use the exact same settings for baselines and our method. We present the experiment results in Table [\\[tab:3\\]](#tab:3){reference-type=\"ref\" reference=\"tab:3\"}. While the overall attack performance significantly decreases due to the auxiliary masked language model, our approach still outperforms the baseline. Furthermore, in numerous instances (as illustrated in Table [\\[tab:3\\]](#tab:3){reference-type=\"ref\" reference=\"tab:3\"}), our method appears to restore the essence of the reference sample almost flawlessly. However, due to the limitation of evaluation metrics, they may have equal or even worse evaluation metrics than some terrible cases. Hence, we also employ the cosine similarity metric between the embeddings generated by SBERT for both the reference and recovery texts to assess the attack performance [@reimers2019sentence].\n\n:::: table*\n::: tabularx\nl\\|p1cm\\|CCCC\\|p6.5cm Dataset & Method & R-1 & R-2 & R-L & Cos$_{\\text{S}}$ & Recovered Samples\\\n&\\\n& LAMP & 15.5 & 2.6 & 14.4 & 0.36 & likeTHETw box contains divPORa\\\n& **Ours** & **17.4** & **3.8** & **15.9** & **0.41** & [like Mess box contains contains balls]{style=\"color: orange\"}\\\n&\\\n& LAMP & **20.1** & **2.2** & 15.9 & 0.56 & likesmlightly disappointed a\\\n& **Ours** & 19.7 & 2.1 & **16.8** & **0.59** & [like lightly disappointed a]{style=\"color: orange\"}\\\n&\\\n& LAMP & 19.9 & 1.6 & 15.1 & 0.48 & vagueLY', interestingtooMuchbuttoojusta\\\n& **Ours** & **21.5** & **1.8** & **16.0** & **0.51** & vagueLY, interestingBut seemsMuch Toolaughs\\\n:::\n\n[]{#tab:3 label=\"tab:3\"}\n::::\n\n# Related Work[]{#section-2-2 label=\"section-2-2\"}\n\nWhile federated learning features with data privacy, recent studies show that model updates (gradients and parameters) can be intentionally leveraged to uncover sensitive data [@phong2017privacy; @zhao2020idlg; @zhu2020r; @zhu2019deep]. This susceptibility is especially pronounced in the field of CV [@huang2021evaluating; @geiping2020inverting; @yin2021see; @jeon2021gradient].\n\nTextual data poses unique challenges in the context of private data attacks, especially given the prevalence of Transformer architectures. In Transformer, gradients average across sequences and tokens, which inherently masks specific token details. Furthermore, the inputs, expressed as discrete token IDs, starkly contrast the continuous features found in image data. Nonetheless, numerous studies have highlighted the risks associated with textual information. @fowl2021robbing [@fowl2022decepticons; @boenisch2023curious] distribute networks with embedded backdoors or trap parameters that facilitate easy reconstruction of training data. However, one can employ prefixed, recognized architectures to counter the former attack and guard against potential backdoor threats. For the latter attack, consistently monitoring statistics of features and weights across different layers can help detect malicious parameter [@balunovic2022lamp]. Some approaches assume a trustworthy central server. Even with its integrity, the shared parameters and gradients could still be leveraged to extract private data [@zhu2019deep]. For example, methods introduced by @zhu2019deep and @deng2021tag employ optimization-based strategies using finely-tuned objective functions for data retrieval. @balunovic2022lamp leverages prior knowledge from extensive language models for data recovery. However, due to the self-imported limitation (Server is benign without doing any change to the model), these methods tend to be less effective with larger batch sizes. Notably, the method introduced by [@gupta2022recovering] remains effective even with considerable batch sizes. Nevertheless, this vulnerability can be easily defended by suspending updates of embedding matrix.\n\n# Conclusion\n\nThis paper introduces a two-stage privacy attack methodology that exposes the inherent privacy leakage risks in contemporary language model architectures. Rather than attempting to directly reconstruct the training samples of the language model, our approach focuses initially on retrieving certain feature information. Furthermore, our method distinguishes itself by not solely depending on gradients and prior knowledge; it also integrates unique feature-level data. Extensive empirical research, spanning a range of model architectures, datasets, and batch sizes, corroborates the efficacy of our approach.\n\n# Acknowledgments {#acknowledgments .unnumbered}\n\nThe authors would like to extend their sincere gratitude to the ARC (A Root Cluster for Research into Scalable Computer Systems) at the Computer Science Department of North Carolina State University. The invaluable computing resources provided by the ARC cluster ([https://arcb.csc.ncsu.edu/ mueller/cluster/arc/](https://arcb.csc.ncsu.edu/~mueller/cluster/arc/)) were instrumental in facilitating the research presented in this paper.\n\n# Appendix-A\n\nAs discussed in the main paper, our method distinguishes itself by uncovering the inherent privacy risks embedded in contemporary language model architectures. Additionally, this work also highlights its superior performance in privacy attacks compared to prior research. To more effectively demonstrate this superiority under comparatively fair conditions, we first discuss the difference between previous work and our research, and then clarify our threat model again.\n\n## Comparision with Previous Work []{#A-1 label=\"A-1\"}\n\nPrevious work classifies privacy attacks on textual data into two principal categories: **malicious** attacks and **eavesdropping** attacks, with the latter also known as **honest-but-curious** attacks [@gupta2022recovering]. However, this binary classification tends to oversimplify the complexity of the issue, failing to account for various nuanced intermediary stages.\n\nConventionally, **malicious** attacks are characterized by the assumption of a completely malicious server [@fowl2021robbing; @fowl2022decepticons; @boenisch2023curious]. Specifically, they may insert specially designed modules into the original model or create trap weights (such as Identity weight) to facilitate the recovery of training samples. Differently, instead of inserting a suspicious module, we opt to identify vulnerable modules that inherently possess privacy risks within contemporary language model architectures. Additionally, our approach does not depend on trap weights; instead, we merely initialize a subset of weights randomly in a specific layer.\n\n**Malicious** attacks fundamentally forget the primary objective of federated learning, which is to minimize training loss and develop an effective model. In contrast, our method does not impede the original training roadmap due to its effective gradient generation and weight updates.\n\n**Honest-but-curious** attacks can extract training samples by utilizing only gradient information and prior knowledge. However, these attacks typically demonstrate limited effectiveness, particularly in scenarios using a large batch size. To address this limitation, we propose an approach that initially recovers intermediate feature information, which is then employed as an additional supervisory signal in the optimization-based attack. Our method is the first to offer feature-level information that is distinct from conventional gradients and priors in the privacy attack on textual data.\n\n## Threat Model []{#A-2 label=\"A-2\"}\n\nWe clarify the **threat model** of our method as follows:\n\n-   We do not fine-tune the model's **token and positional embeddings**. The gradients for these embeddings are non-zero for words in the current training batch, enabling easy recovery of client sequences.\n\n-   The server should learn maximally from gradients without deviating from the fundamental objectives of federated learning, such as **effective gradient aggregation and training loss minimization**.\n\n-   The server should not distribute trap weights such as the **Identity module which are easily detected**.\n\nResearch in privacy attacks holds a unique importance in practice. This is because every discovery in this area, even with certain constraints, can cause tremendous destruction once it happens. The attacker will break out the constraints regardless of cost because of the massive profit. Therefore, the risks uncovered by these studies are often difficult to measure, making them crucial for understanding security vulnerabilities.\n\n![Architecture overview of our proposed attack mechanism on language models. **A$_1$**: Subtle modification of architecture and strategic weight initialization. **A$_2$**: Two-layer-neural-network-based reconstruction. **B**: Continuous optimization with gradient inversion and feature match. **C**: Discrete optimization with gradient matching loss and perplexity from pre-trained language models.](assets/pdfs/archi.pdf){#fig:achitecture width=\"\\\\textwidth\"}\n\n# Appendix-B\n\n## More Implementation Details []{#B-1 label=\"B-1\"}\n\nIn the first-stage attack, as detailed in Sections [\\[section:4-2-2\\]](#section:4-2-2){reference-type=\"ref\" reference=\"section:4-2-2\"} and [\\[section:4-2-3\\]](#section:4-2-3){reference-type=\"ref\" reference=\"section:4-2-3\"}, we develop a strategic weight initialization method in conjunction with a flexible tuning framework. This combination ensures that our analytics-based approach effectively and efficiently recovers feature information associated with the vulnerable module. Further specifics about these two techniques are illustrated in Figure [1](#fig:achitecture){reference-type=\"ref\" reference=\"fig:achitecture\"}-A. For the second-stage attack, we establish distinct optimization objectives for the discrete and continuous optimization phases, with the optimization formulas depicted in Figure [1](#fig:achitecture){reference-type=\"ref\" reference=\"fig:achitecture\"}-BC.\n\nAs outlined in Section [\\[section:5-1\\]](#section:5-1){reference-type=\"ref\" reference=\"section:5-1\"}, we employ a grid search strategy to determine the hyperparameters unique to our method. Generally, there are two key hyperparameters to consider. The first is the margin used when calculating the feature match loss. Due to inevitable errors in estimating feature information, we cannot directly treat the recovered feature as ground truth. Therefore, we introduce a margin in the feature match loss computation. Typically, we set this margin to 0.1 for attacks utilizing activation functions like Tanh and ReLU, and to 0 for attacks with activation functions such as SeLU and $\\sigma=x^2+x^3$. The second hyperparameter $\\lambda_2$ is used to scale the feature loss and is typically set to either 0.1 or 0.05.\n\n## Extend to Cross Entropy Loss []{#B-2 label=\"B-2\"}\n\n@wang2023reconstructing grounded their research on the assumption that the loss function of the neural network is Mean Square Error (MSE). Building upon this foundation, we extend the method to the scenario of classification tasks utilizing Cross-Entropy Loss (CEL). In the classification context, the gradient of $g_j$ is calculated for all class outputs. While a straightforward approach might only random choose the gradient for a single class to satisfy the equation [\\[equation/concentration\\]](#equation/concentration){reference-type=\"ref\" reference=\"equation/concentration\"}, we chose a more holistic method, leveraging the gradient of the Pooler layer to compute $\\hat{T}$ rather than the classifier layer. Based on this methodology, the gradient of $w_j$ we derived is as follows: $$\\hat{g}_{j} = \\nabla_{w_{j}}L(\\Theta) = \\sum_{i=1}^{B}r_{i}a_{j}\\sigma^{\\prime}\\left(w_{j}^{\\top}x_{i}\\right)x_{i}$$ Let $a_{j}={\\frac{1}{m}},\\forall j\\in[m]$ and $w_{j}\\in N(0,1)$, by Stein's lemma, we have:\n\n$$\\begin{aligned}\nT_{1} & = \\sum_{i=1}^{m}\\hat{g}_{j}{H}_{2}(w_{j}) \\\\\n& = \\frac{1}{m}\\sum_{i=1}^{B}r_{i}^{*}x_{i}\\otimes\\left[\\sum_{j=1}^{m}\\sigma^{\\prime}\\left(w_{j}^{\\top}x_{i}\\right)(w_{j}\\otimes w_{j}-I)\\right] \\\\\n& \\approx \\sum_{i=1}^{B}r_{i}^{*}x_{i}\\otimes\\mathbb{E}\\left[\\sigma^{\\prime}\\left(w_{j}^{\\top}x_{i}\\right)(w_{j}\\otimes w_{j}-I)\\right] \\\\\n& = \\sum_{i=1}^{B}r_{i}^{*}\\mathbb{E}\\left[\\sigma^{(3)}(w^{\\mathsf{T}}x_{i})\\right]x_{i}^{\\otimes3} \\\\\n& = T\n\\end{aligned}$$\n\nBy defining the tensors $T_2$ and $T_3$ such that: $T_2(i, j, k) = T_1(k, i, j) \\quad \\text{and} \\quad T_3(i, j, k) = T_1(j, k, i)$, we can deduce: $\\hat{T} = \\frac{T + T_{2} + T_{3}}{3} \\approx T$. This computation results in $\\hat{T}$ being symmetric. @wang2023reconstructing and we both observed that this method offers a more precise estimation when attempting to recover features. We also adopt this strategy in all our experiments.\n\n## Impact of Activation Function []{#B-3 label=\"B-3\"}\n\nWhen applying the two-layer network-based reconstruction method to the Pooler layer of language models, we also substitute the original Tanh activation function with ReLU. However, it's noteworthy that the n-order derivative of the ReLU and Tanh will lead to zero expectation $\\mathbb{E}_{Z\\sim N(0,1)}[\\sigma^{(n)}(Z)]=0$. This property renders more error for the estimation of $T$ and $x_i$. To address this challenge, we follow the approach proposed by @wang2023reconstructing, instead of using a third-order Hermite function to estimate $T$, we use a fourth-order function. The estimation is represented as: $$\\hat{T} := \\frac{1}{m} \\sum_{j=1}^{m} g_{j}(w_{j})H_{4}(w_{j})(I,I,I,a)$$ where $a$ is a unit vector, pointing in a specific direction in space.\n\nHowever, even in this way, compared with SeLU and $\\sigma=x^2+x^3$, the improvement for Tanh and ReLU is not significant. To remove the influence from the second-stage optimization-based attack, we directly calculate the average cosine similarity between recovered feature information with the ground truth of the entire dataset to check the attack performance. We present the results in Table [\\[tab:5\\]](#tab:5){reference-type=\"ref\" reference=\"tab:5\"}. Interestingly, while ReLU achieves the highest cosine similarity in feature information recovery, its overall attack performance is surpassed by SeLU. This aspect is a focus of our ongoing research efforts.\n\n::: tabularx\n0.52l\\|C\\|C\\|C\\|p2cm $d^\\prime$=100 & Tanh & ReLU & SeLU & $\\sigma=x^2+x^3$\\\nCoLA & 0.83 & 0.99 & 0.87 & 0.99\\\nSST-2 & 0.82 & 0.99 & 0.89 & 0.99\\\nRotten Tomato & 0.78 & 0.92 & 0.87 & 0.99\\\n:::\n\n## Impact of Feature Match in Different Optimization Phase\n\nIn Section [\\[section:4-3\\]](#section:4-3){reference-type=\"ref\" reference=\"section:4-3\"}, we propose a novel optimization objective: the cosine distance between the input of the Pooler layer and the recovered intermediate features from Section [\\[section:4-2\\]](#section:4-2){reference-type=\"ref\" reference=\"section:4-2\"}. It's worth noting that we can also apply this distance as a new metric like gradient match loss in the discrete optimization stage to select the best starting or intermediary points for the subsequent training phase. Therefore, we add the new metric to the discrete and continuous optimization phases separately to observe its impact on the final attack performance. The results are illustrated in Table [\\[tab:2\\]](#tab:2){reference-type=\"ref\" reference=\"tab:2\"}. Notably, our introduced metric has a positive effect on both phases. However, when the new metric is used in discrete and continuous optimization together, the results are not always two-win.\n\n::: tabularx\n0.48l\\|CCC Phase & R-1 & R-2 & R-L\\\n\\\nNon-use (LAMP) & 87.7 & 54.1 & 76.4\\\nOnly Discrete & 92.5 & 59.3 & 79.9\\\nOnly Continuous & **93.1** & **61.6** & **81.5**\\\nBoth & 90.0 & 53.9 & 76.8\\\n\\\nNon-use (LAMP) & 48.9 & 17.1 & 45.4\\\nOnly Discrete & 57.9 & 23.4 & 52.3\\\nOnly Continuous & 60.6 & 23.1 & 54.9\\\nBoth & **61.7** & 23.0 & **55.7**\\\n:::\n\n[]{#tab:2 label=\"tab:2\"}\n\n## Impact of Data Dependence []{#B-5 label=\"B-5\"}\n\nWe made a noteworthy observation during our implementation of the two-layer network-based reconstruction technique. When the batch size goes beyond a single data point, ensuring the independence of features across various data points becomes crucial. However, there's an inherent challenge in achieving this. Delving deeper into the language model, particularly close to the Pooler layer, we find that dominant features are those closely aligned with the downstream task. Using sentiment analysis as an example, features directed to the Pooler layer somewhat have characteristics that describe similar emotions. Unfortunately, this similarity can degrade the quality of the features we are trying to recover. As a result, the reliability of these recovered features might be diminished when they are used as ground truth during optimization.\n\n@wang2023reconstructing's analysis also underscores this puzzle: the reconstruction quality is closely tied to the condition number, defined by the data matrix's smallest singular value. To elaborate further, if a sample is heavily influenced by or dependent on other samples (like two sentences mirroring each other or belonging to identical classes), the assurance of accurate recovery falters. This decline is attributed to the inherent limitation of tensor decomposition when faced with almost identical data. For instance, with two strikingly similar sentences, tensor decomposition might only be able to discern the collective span of the sentences, failing to distinguish between them. Resorting to feature matching in such scenarios would invariably perform negatively.\n\n# Appendix-C\n\n## Clarification on Two-Layer Network-Based Reconstruction\n\nConsider a two-layer neural network: $f(x;\\Theta) = \\sum_{j=1}^{m} a_j \\sigma(w_j \\cdot x)$, with parameters defined as $\\Theta = (a_1, ... , a_m, w_1, ... , w_m)$. Here, $m$ represents the hidden dimension. The objective function is represented as: $L(\\Theta) = \\sum_{i=1}^{B} (y_i - f(x_i;\\Theta))^2$. A notable finding is that the gradient for $a_j$ is solely influenced by $w_j$, making it independent from other parameters. This gradient is represented as: $$g_{j}:=\\nabla_{a_{j}}L(\\Theta)=\\sum_{i=1}^{B}r_{i}\\sigma\\left(w_{j}^{\\mathsf{T}}x_{i}\\right)$$ where the residual $r_{i}$ is given by $r_{i}=f(x_{i};\\Theta)-y_{i}$. For wide neural networks with random initialization from a standard normal distribution, the residuals $r_{i}$ concentrate to a constant, $r^{*}_{i}$. By set $g_{(w)}:=\\sum_{i=1}^{B}r_{i}^{*}\\sigma(w^{\\top}x_{i})$, $g_{j}$ can be expressed as $g_{j} = g(w_{j}) + \\epsilon$, where $\\epsilon$ represents noise. This is to say by setting different $w$ we are able to observe a noisy version of $g(w)$, where we have the first order derivative of $g(w)$: $$\\nabla{g(w)} = \\sum_{i=1}^{B} r_{i}^{*} \\sigma^{\\prime}(w^{\\textsf{T}} x_{i}) x_{i}$$ Similarly, we have the second and third derivations of $g(w)$: $$\\nabla^{2}g(w) = \\sum_{i=1}^{B} r_{i}^{*} \\sigma^{\\prime\\prime}(w^{\\textsf{T}} x_{i}) x_{i} x_{i}^{\\textsf{T}}$$ $$\\nabla^{3}g(w) = \\sum_{i=1}^{B} r_{i}^{*} \\sigma^{(3)}(w^{\\textsf{T}} x_{i}) x_{i}^{\\otimes3}$$ Here, $x_{i}^{\\otimes3}$ signifies the tensor product of vector $x_{i}$ with itself three times. Given $E_{W} \\nabla^p g(W)$, where $p = 1, 2, 3$, we are able to recover the reweighted sum for $x_{i}^{\\otimes{p}}$. Especially when $p = 3$, the third order tensor $E_{w} \\nabla^3 g(W)$ has a unique tensor decomposition which will identify $\\{x_i\\}_{i=1}^B$ when they are independent. @wang2023reconstructing further take use of Stein's Lemma, expressed as: $\\mathbb{E}[g(X)H_{p}(X)] = \\mathbb{E}[g^{(p)}(X)]$ to approximate $E_{W}\\nabla^{3}g(W)$ as: $$T=\\mathbb{E}_{W}[\\nabla_{W}^{3}g(W)] = \\mathbb{E}_{W\\sim N(0,I)}[g(W)H_{3}(W)] \\approx \\frac{1}{m} \\sum_{j=1}^{m} g(w_{j})H_{3}(w_{j})=\\hat{T}$$ Where $H_{3}(w_j)$ is the p-th Hermite function of $w_j$. In this way, we have a very close estimation $\\hat{T} \\approx T$, and take use of the technique of tensor decomposition, we can recover the unique $x_{i}$. **However, we want to reinforce the directional component of the feature in the recovered information**. Yet, recovering the magnitude information of the feature remains a challenging task. For more details, please refer to the paper @wang2023reconstructing.\n\n## Intuition of Intermediate Features\n\nTwo previous works that utilize intermediate features to enhance privacy and adversary attacks also share a similar intuition with ours [@huang2019enhancing; @kariyappa2023cocktail]. However, @huang2019enhancing focuses on a completely different attack scenario and objective with different constraints and limitations in the community. In contrast, @kariyappa2023cocktail is a recent work that concentrates on the recovery of image data, employing intermediate features in the context of federated learning. Considering that attacking textual data presents unique challenges compared to image data and our method differs from these studies, the novelty and contribution remain distinct for our research.\n\n## Datasets []{#sec:C-3 label=\"sec:C-3\"}\n\n**CoLA**: The CoLA (Corpus of Linguistic Acceptability) dataset is a seminal resource for evaluating the grammatical acceptability of machine learning models in natural language processing. Consisting of approximately 10,657 English sentences, these annotations are derived from various linguistic literature sources and original contributions. The sentences are categorized based on their grammatical acceptability. Spanning a comprehensive range of linguistic phenomena, CoLA provides a robust benchmark for tasks requiring sentence-level acceptability judgments. Its diverse set of grammatical structures challenges models to demonstrate both depth and breadth in linguistic understanding, making it a popular choice in the field.\n\n**SST-2**: The SST-2 (Stanford Sentiment Treebank Version 2) dataset is a widely recognized benchmark for sentiment analysis tasks in natural language processing. Originating from the Stanford NLP Group, this dataset contains around 67,000 English sentences, drawn from movie reviews, annotated for their sentiment polarity. Unlike its predecessor which had fine-grained sentiment labels, SST-2 has been simplified to a binary classification task, where sentences are labeled as either positive or negative. This dataset not only provides sentence-level annotations but also contains a unique feature: a parsed syntactic tree for each sentence. By leveraging both sentiment annotations and syntactic information, we can investigate various dimensions of sentiment understanding and representation in machine learning models.\n\n**Rotten Tomatoes**: The Rotten Tomatoes dataset is a compilation of movie reviews sourced from the Rotten Tomatoes website. This dataset has been instrumental in sentiment analysis research. In its various versions, the most notable being SST-2, the dataset consists of sentences from these reviews, annotated for their sentiment polarity. These sentences are labeled either as positive or negative, making it a binary classification challenge. The dataset's value lies in its representation of real-world opinions, rich in diverse sentiment expressions, and has been a cornerstone for evaluating the performance of natural language processing models in sentiment classification tasks.\n\n  **Reference**                                                                                                                                                                                                                              **Recovery**\n  ------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------ -------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n  slightly disappointed                                                                                                                                                                                                                      slightly disappointed\n  splendidly                                                                                                                                                                                                                                 splendidly\n  gaining much momentum                                                                                                                                                                                                                      gaining much momentum\n  flawless film                                                                                                                                                                                                                              flawless film\n  tiresomely                                                                                                                                                                                                                                 tiresomely\n  enjoyable ease                                                                                                                                                                                                                             ease enjoyable\n  grayish                                                                                                                                                                                                                                    grayish\n  no cute factor here \\... not that i mind ugly ; the problem is he has no character , loveable or otherwise .                                                                                                                               he no problem is here i really love cute, not ugly the mind or no character ; the loveable love factor cute has.\n  of softheaded metaphysical claptrap                                                                                                                                                                                                        softhead of metaphysical clap claptrap\n  ably balances real-time rhythms with propulsive incident .                                                                                                                                                                                 time ably balances incident with real incident.ulsive rhythms.\n  was being attempted here that stubbornly refused to gel                                                                                                                                                                                    here was attempted stubbornly that being refused to gel\n  that will be seen to better advantage on cable , especially considering its barely                                                                                                                                                         , that better to barely advantage will be seen on cable considering its advantage\n  point at things that explode into flame                                                                                                                                                                                                    point things flame that explode into explode\n  undeniably intriguing film                                                                                                                                                                                                                 undeniably intriguing film\n  efficient , suitably anonymous chiller .                                                                                                                                                                                                   efficient, suitably anonymous chiller shady\n  all of this , and more                                                                                                                                                                                                                     this and all this more,\n  want to think too much about what s going on                                                                                                                                                                                               think want to think too much about what s going on\n  invigorating                                                                                                                                                                                                                               invigorating\n  to infamy                                                                                                                                                                                                                                  to infamy\n  the perverse pleasure                                                                                                                                                                                                                      the perverse pleasure\n  the way this all works out makes the women look more like stereotypical caretakers and moral teachers , instead of serious athletes .                                                                                                      the stereotypical this way all works out ( the more like oxygenmissible caretaker makes teachers of athletes instead look moral. women instead\n  a successful adaptation and an enjoyable film in its own right                                                                                                                                                                             a successful and enjoyable film adaptation right in its own right\n  while some will object to the idea of a vietnam picture with such a rah-rah , patriotic tone , soldiers ultimately achieves its main strategic objective : dramatizing the human cost of the conflict that came to define a generation .   will achieve object main while idea conflict drama with the such tone a political picture cost : vietnam thetih ra, vietnam insulted achieves objective objective, some patriotic dramazing a tone of soldiers generation that strategic its drama ultimately generation to define.\n  taken outside the context of the current political climate ( see : terrorists are more evil than ever ! )                                                                                                                                  the climate terrorists than outside the context of current political climate ( see : are evil ever taken! )\n  strange and beautiful film                                                                                                                                                                                                                 strange and beautiful film\n  this ) meandering and pointless french coming-of-age import from writer-director anne-sophie birot                                                                                                                                         this meander pointless director - anne french - coming from pointless importing of writer ) and ageing - -rot\n  are so generic                                                                                                                                                                                                                             are so generic\n  for only 71 minutes                                                                                                                                                                                                                        for 71 minutes only\n  i also believe that resident evil is not it .                                                                                                                                                                                              it is also i not.. believe resident evil\n  fizzability                                                                                                                                                                                                                                fizzability\n  a better vehicle                                                                                                                                                                                                                           a better vehicle\n  pull together easily accessible stories that resonate with profundity                                                                                                                                                                      hand together stories resonate with pullclundity easily accessible\n  higher                                                                                                                                                                                                                                     higher\n  build in the mind of the viewer and take on extreme urgency .                                                                                                                                                                              build urgency in the extreme of viewer urgency and take on mind.\n  we ve seen it all before in one form or another , but director hoffman , with great help from kevin kline , makes us care about this latest reincarnation of the world s greatest teacher .                                                thesegreatest of form seen beforeall reinnationdirector we, directorstand wele great hoffman in ve latest makes us help teacher care about greatestnation in this thelancenation, but one of\n  s horribly wrong                                                                                                                                                                                                                           shorribly wrong\n  eccentric and                                                                                                                                                                                                                              eccentric and\n  scare                                                                                                                                                                                                                                      scare\n  finds one of our most conservative and hidebound movie-making traditions and gives it new texture , new relevance , new reality .                                                                                                          gives our finds new finds, conservative newbound movie making traditions - and reality texture it hide. reality texture and one movie relevance\n  pummel us with phony imagery or music                                                                                                                                                                                                      imagery pummel us or phony with music\n  consistently sensitive                                                                                                                                                                                                                     consistently sensitive\n  the project s filmmakers forgot to include anything even halfway scary as they poorly rejigger fatal attraction into a high school setting .                                                                                               s scary filmmakers forgot anything forgot to include even halfway fatal attraction as they poorlyjigger regger into high school scary project setting\n  narcissistic                                                                                                                                                                                                                               narcissistic\n  has been lost in the translation \\... another routine hollywood frightfest in which the slack execution italicizes the absurdity of the premise .                                                                                          slack has the includesity in the executionalic translation. another frightfest. the absurd premise which lost, it routineizes the premise of hollywood.\n  -- bowel movements than this long-on-the-shelf , point-and-shoot exercise in gimmicky crime drama                                                                                                                                          movements - - than long - shoot - - this exercise, and this - the bowel shelf - on gimmick in crime drama point\n  visually striking and slickly staged                                                                                                                                                                                                       visually striking and slickly staged\n  downright transparent                                                                                                                                                                                                                      downright transparent\n  rotting underbelly                                                                                                                                                                                                                         underbelly rotting\n  could possibly be more contemptuous of the single female population .                                                                                                                                                                      could possibly be more contemptuous of the single female population.\n\n  **Reference**                                                                                                                                                                                  **Recovery**\n  ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------- ---------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------------\n  what the english call ' too clever by half                                                                                                                                                     what ' call call by clever english too half\n  sucks , but has a funny moment or two .                                                                                                                                                        has funny sucks but moment or two funny sucks.\n  trailer-trash                                                                                                                                                                                  trash trailer -\n  flinching                                                                                                                                                                                      flinching\n  hot topics                                                                                                                                                                                     hot topics\n  settles too easily                                                                                                                                                                             settles too easily\n  films which will cause loads of irreparable damage that years and years of costly analysis could never fix                                                                                     films which will cause loads ofparable damage that years and years of costly analysis irre could never fix\n  wears                                                                                                                                                                                          wears\n  is an inspirational love story , capturing the innocence and idealism of that first encounter                                                                                                  innocence is an inspirational story capturing the idealism of first encounter, and love that\n  has the charisma of a young woman who knows how to hold the screen                                                                                                                             has the the thea of char young who knows how hold of screen womanism\n  circuit is the awkwardly paced soap opera-ish story .                                                                                                                                          h - is awkwardly paced circuit story is the soap opera story\n  , beautiful scene                                                                                                                                                                              beautiful scene,\n  grace to call for prevention rather than to place blame , making it one of the best war movies ever made                                                                                       to call for prevention rather than to place blame, grace making it one of the best war movies ever made\n  looking for a return ticket                                                                                                                                                                    looking for a return ticket\n  the strange horror                                                                                                                                                                             the strange horror\n  , joyous romp of a film .                                                                                                                                                                      , a joyous romp of film.\n  a longtime tolkien fan                                                                                                                                                                         a longtime tolkien fan\n  heartwarming , nonjudgmental kind                                                                                                                                                              heartwarming, nonmingjugmental kind\n  uncouth , incomprehensible , vicious and absurd                                                                                                                                                absurdhensible, uncouth, vicious and incompmbled\n  a real winner -- smart , funny , subtle , and resonant .                                                                                                                                       a winner. resonant and funny - ami subtle, smart, real res\n  gets clunky on the screen                                                                                                                                                                      gets on screenunk clunky\n  there s not a single jump-in-your-seat moment and                                                                                                                                              there s not a single jump and seat in your seat - - - moment\n  has a tougher time balancing its violence with kafka-inspired philosophy                                                                                                                       acter has a tough time balancing itsfka philosophy with violence - inspired\n  bad filmmaking                                                                                                                                                                                 bad filmmaking\n  share                                                                                                                                                                                          share\n  this excursion into the epicenter of percolating mental instability is not easily dismissed or forgotten .                                                                                     this excursionenter is the mentalenter into instability or iserving easily dismissed or not easily forgotten.\n  s as if allen , at 66 , has stopped challenging himself .                                                                                                                                      as if regarding sums, allen has stopped s 66, challenging himself.\n  is its make-believe promise of life that soars above the material realm                                                                                                                        its promise that life is promiseence make soars above the material realm -\n  exit the theater                                                                                                                                                                               exit the theater\n  is fascinating                                                                                                                                                                                 fascinating is\n  wise , wizened                                                                                                                                                                                 wise, wizened\n  is not the most impressive player                                                                                                                                                              is not the most impressive player\n  it s undone by a sloppy script                                                                                                                                                                 its undone by a sloppy script\n  know what it wants to be when it grows up                                                                                                                                                      know what grows up when it wants it to be\n  people have lost the ability to think                                                                                                                                                          people have lost the ability to think\n  unfortunately , it s also not very good .                                                                                                                                                      . very, unfortunately it also s not very good\n  clarity and emotional                                                                                                                                                                          and emotional clarity\n  propulsive                                                                                                                                                                                     propulsive\n  p.t. anderson understands the grandness of romance and how love is the great equalizer that can calm us of our daily ills and bring out joys in our lives that we never knew were possible .   l of will understands joy is our romance. daily we ill of how of t a grand anderson. the anderson romanceing calms never at us lives guest bearings daily and ofness of coulds p the grand.\n  tactic to cover up the fact that the picture is constructed around a core of flimsy -- or , worse yet , nonexistent -- ideas                                                                   tactic to cover up the fact picture the core or the coreim constructed,' - none worse yet - - aroundum orstensyim. and central ideas\n  how ridiculous and money-oriented                                                                                                                                                              how ridiculous and - money oriented\n  muy loco , but no more ridiculous                                                                                                                                                              muy loco, but no more ridiculous\n  deceit                                                                                                                                                                                         deceit\n  in its understanding , often funny way                                                                                                                                                         understanding in its often funny way,\n  a caper that s neither original nor terribly funny                                                                                                                                             s that original a caper neither original nor terribly funny\n  ( denis ) story becomes a hopeless , unsatisfying muddle                                                                                                                                       denis use ) becomes a hopeless muddle story, unsatisfying (\n  force himself on people and into situations that would make lesser men run for cover                                                                                                           would himself / people run for cover of situations and make force on lesser men\n  and unforgettable characters                                                                                                                                                                   unforgettable and characters\n  unfulfilling                                                                                                                                                                                   unfulfilling\n  walked out muttering words like \" horrible and \" terrible , but had so much fun dissing the film that they did nt mind the ticket cost                                                         walked out muttering words words like di fun the' ' mind the horrible filmbut had so much fun that they did tired, the terriblenssing ticket the film cost\n                                                                                                                                                                                                 \n\n  : Recovery examples for SST2 datasets with BERT$_{\\text{BASE}}$ model.\n\n[^1]: Department of Computer Science at North Carolina State University, Email: `jli265@ncsu.edu`\n\n[^2]: Center for Data Science at New York University; Stanford University, Email: `shengl@stanford.edu`\n\n[^3]: Center for Data Science at New York University, Email: `ql518@nyu.edu`\n\n[^4]: Our code is available at <https://github.com/mexiQQ/privacy_attack_llm>",
    "rationale": "Summary: This paper proposes a new attack method to enhance the text recovery rate of language models under the Federated Learning setting. It is based on two techniques: (1) leveraging gradient data and prior knowledge to extract sensitive information (Zhu et al., 2019; Deng et al., 2021; Balunovic et al., 2022; Gupta et al., 2022), and (2) Two-Layer Neural Network-Based Reconstruction (Wang et al., 2023), whose results will be used as the prior knowledge. By combining these two techniques together, the proposed method tries to address existing challenges in enhancing the recovery rate of text in larger batch-size settings while being hard to detect and defend against. This paper compares the proposed method with existing baseline methods and proves its superiority.\n\nStrengths: 1. This paper studies a very interesting problem, which is the attacks on language models under the Federated Learning setting.\n2. The proposed methods achieve better results than existing baselines.\n\nWeaknesses: 1. The proposed method is based on the existing method and applies it to the language model under the federated learning setting, which fits the setting of the existing method well. The contribution is limited.\n2. This paper does not solve the batch size issue efficiently. Since the batch size (i.e., 8) that the proposed method can work well on is still very small compared to common settings for batch sizes.\n3. This paper does not demonstrate how existing defense methods work to defend against the proposed attack, or in other words, how the proposed attack performs against the defense methods.\n\nQuestions: 1. Does this method require the attacker to know the attacked model's structure, such as whether it has a Pooler layer or not, as a priori?\n2. In the text domain, what kind of information is considered private? For instance, if there's a phrase 'this food is … ,' and then 'delicious' is recovered, is this considered private information?",
    "rating": 2,
    "label": false,
    "rationale_edited": "This paper proposes a new attack method to enhance the text recovery rate of language models under the Federated Learning setting. It is based on two techniques: (1) leveraging gradient data and prior knowledge to extract sensitive information (Zhu et al., 2019; Deng et al., 2021; Balunovic et al., 2022; Gupta et al., 2022), and (2) Two-Layer Neural Network-Based Reconstruction (Wang et al., 2023), whose results will be used as the prior knowledge. By combining these two techniques together, the proposed method tries to address existing challenges in enhancing the recovery rate of text in larger batch-size settings while being hard to detect and defend against. This paper compares the proposed method with existing baseline methods and proves its superiority.\n\nhe proposed method is based on the existing method and applies it to the language model under the federated learning setting, which fits the setting of the existing method well. The contribution is limited.\n\nThis paper does not demonstrate how existing defense methods work to defend against the proposed attack, or in other words, how the proposed attack performs against the defense methods.",
    "chosen": false
  },
  {
    "title": "Does Deep Active Learning Work in the Wild?",
    "abstract": "Deep active learning (DAL) methods have shown significant improvements in sample efficiency compared to simple random sampling. While these studies are valuable, they nearly always assume that optimal DAL hyperparameter (HP) settings are known in advance, or optimize the HPs through repeating DAL several times with different HP settings. Here, we argue that in real-world settings, or _in the wild_, there is significant uncertainty regarding good HPs, and their optimization contradicts the premise of using DAL (i.e., we require labeling efficiency).  In this study, we evaluate the performance of eleven modern DAL methods on eight benchmark problems as we vary a key HP shared by all methods: the pool ratio.  Despite adjusting only one HP, our results indicate that eight of the eleven DAL methods sometimes underperform relative to simple random sampling and some frequently perform worse. Only three methods always outperform random sampling (albeit narrowly), and we find that these methods all utilize diversity to select samples - a relatively simple criterion.  Our findings reveal the limitations of existing DAL methods when deployed _in the wild_, and present this as an important new open problem in the field.",
    "text": "# Introduction {#sec:introduction}\n\nIn this work, we focus on the application of active learning to deep neural networks (DNNs), sometimes referred to as Deep Active Learning (DAL) [@roy2018deep]. Broadly speaking, the premise of DAL is that some training instances will yield superior performance compared to others. Therefore, we can improve the training sample efficiency of DNNs by selecting the best training instances. A large number of methods have been investigated in recent years for DAL [@settles2009active; @ren2021survey; @holzmuller2023framework], often reporting significant improvements in sample efficiency compared to simpler strategies, such as random sampling [@tsymbalov2018dropout; @kading2018active; @kee2018query]. While these studies provide valuable insights, they nearly always assume good DAL hyperparameter (HP) settings are known in advance, or alternatively, they optimize the HPs (e.g., by repeating DAL several times with different HP settings). To our knowledge however, there is little evidence that one can assume good hyperparameters are known in advance for novel problems (see [3](#sec:ProblemSetting){reference-type=\"ref+label\" reference=\"sec:ProblemSetting\"}, where we find HP settings in the literature vary widely across problems). Moreover, running a DAL method multiple times in search of good HP settings may result in significant label inefficiency, even when compared to random sampling. Therefore, in real-world settings where DAL is applied to a novel problem, or *in the wild* as we term it here, the best DAL HPs are not generally known in advance, and it is unclear whether DAL still offers advantages (e.g., compared to random sampling) when accounting for HP uncertainty. If DAL models do not reliably outperform simple random sampling in the presence of HP uncertainty, it greatly undermines their value, and the likelihood that they will be adopted. Despite the significance of this problem, it has received little attention in the literature.\n\n<figure id=\"img:pool_based\">\n<div class=\"center\">\n<img src=\"imgs/Pool_based_AL_subsample_from_big_pool_0124_two_row.png\" />\n</div>\n<figcaption>Schematic diagram for pool-based DAL procedure. In the input space X, the triangles represent labeled data (<span class=\"math inline\"><em>L</em></span>), and the circles represent unlabeled data (<span class=\"math inline\"><em>D</em></span> for the full set of unlabeled data, and<span class=\"math inline\"><em>U</em></span> for subsampled unlabeled pool). At each step, after the model is trained using the existing training set <span class=\"math inline\"><em>L</em></span>, a subset of unlabeled data <span class=\"math inline\"><em>U</em></span> is sampled and evaluated by the AL criteria q(x). Then, the top-k points according to q(x) are labeled by the oracle function.</figcaption>\n</figure>\n\n#### Contributions {#subsec:contributions_of_this_work}\n\nIn this work, we perform the first systematic evaluation of DAL *in the wild*. We focus our investigation on DAL for regression, where to our knowledge, most applicable DAL methods are pool-based, and therefore they share an important HP: the pool ratio, $\\gamma$ (see [3](#sec:ProblemSetting){reference-type=\"ref+label\" reference=\"sec:ProblemSetting\"}). Using this property of regression problems, we evaluate a large number of DAL models as we vary a single HP, their $\\gamma$ setting, thereby providing a *distribution* of performance that one can expect in real-world settings (i.e., in the wild), where the best setting for $\\gamma$ is uncertain. We note that most DAL models have several (often unique) HPs that exhibit uncertainty, and each can contribute to performance variability of DAL methods in the wild. However, examining variability with respect to all of these HPs would require a lengthy exposition, and would be computationally costly. Therefore we focus on $\\gamma$, which mitigates the aforementioned challenges, while still providing sufficient empirical evidence to support our main conclusions.\n\nTo support our investigation, we assembled eight scientific computing regression problems to examine the performance of DAL methods in this setting; to our knowledge, this is the first such benchmark of its kind. We then identified past and recent DAL methods that are suitable for regression, totaling eleven methods\n\nTo support our study, we identified eleven DAL methods that are suitable for regression. We then examined the performance of these DAL methods on each of eight benchmark problems, compared to simple random sampling, as we vary their $\\gamma$ settings. Our results indicate that their performance varies significantly with respect to $\\gamma$, and that the best HP varies for different DAL/dataset with no single $\\gamma$ value working best across all settings, confirming our hypothesis that there is significant uncertainty regarding the best HP setting for novel problems. We also find that most of the DAL methods sometimes underperform simple random sampling and some frequently perform much worse:\n\n-   We compile a large benchmark of eleven state-of-the-art DAL methods across eight datasets. For some of our DAL methods, we are the first to adapt them to regression. Upon publication, we will publish the datasets and code to facilitate reproducibility.\n\n-   Using our benchmark, we perform the first analysis of DAL performance *in the wild*. Using $\\gamma$ as an example, we systematically demonstrate the rarely-discussed problem that most DAL models are often outperformed by simple random sampling when we account for HP uncertainty.\n\n-   We analyze the factors that contribute to the robustness of DAL in the wild, with respect to $\\gamma$.\n\n# Related works {#sec:related_work}\n\n#### Active learning benchmarks\n\nThe majority of existing AL benchmarks are for classification tasks, rather than regression [@jose2024regression], and many AL methods for classification cannot be applied to regression. Some existing studies include [@zhan2021comparative], which benchmarked AL using a Support Vector Machine (SVM) with 17 AL methods on 35 datasets. [@yang2018benchmark] benchmarked logistic regression with 12 AL methods and 44 datasets. [@meduri2020comprehensive] benchmarked specific entity matching application (classification) of AL with 3 AL methods on 12 datasets, with 3 different types of classifiers (DNN, SVM, and Tree-based). [@trittenbach2021overview] benchmarked an AL application in outlier detection on 20 datasets and discussed the limitation of simple metrics extensively. [@hu2021towards] benchmarked 5 classification tasks (including both image and text) using DNN. [@beck2021effective] benchmarked multiple facets of DAL on 5 image classification tasks. For the regression AL benchmark, [@o2017model] benchmarked 5 AL methods and 7 UCI [^1] datasets, but they only employed linear models. [@wu2019active] compared 5 AL methods on 12 UCI regression datasets, also using linear regression models. Our work is fundamentally different from both, as we use DNNs as our regressors, and we employ several recently-published problems that also involved DNN regressors, making them especially relevant for DAL study. The recent study by [@holzmuller2023framework] is the only work that is similar to ours, in which the authors benchmarked 8 pool-based DAL methods for regression on 15 datasets. The primary focus of their work was to propose a novel DAL regression framework, termed LCMD; meanwhile the focus of our work is to investigate DAL in the wild. Consequently, [@holzmuller2023framework] presents different performance metrics and conclusions compared to our study.\n\n#### Active learning for regression problems\n\nRegression problems have received (relatively) little attention compared to classification [@ren2021survey; @guyon2011results]. For the limited AL literature dedicated to regression tasks, Expected Model Change (EMC) [@settles2008curious; @cai2013maximizing] was explored, where an ensemble of models was used to estimate the true label of a new query point using both linear regression and tree-based regressors. Gaussian processes were also used with a natural variance estimate on unlabeled points in a similar paradigm [@kading2018active]. [@smith2018less] used Query By Committee (QBC), which trains multiple networks and finds the most disagreeing unlabeled points of the committee of models trained. [@tsymbalov2018dropout] used the Monte Carlo drop-out under a Bayesian setting, also aiming for maximally disagreed points. [@yu2010passive] found $x$-space-only methods outperforming y-space methods in robustness. [@yoo2019learning] proposed an uncertainty-based mechanism that learns to predict the loss using an auxiliary model that can be used on regression tasks. [@ranganathan2020deep] and [@kading2016active] used Expected Model Output Change (EMOC) with Convolutional Neural Network (CNN) on image regression tasks with different assumptions. We included all these methods that used deep learning in our benchmark.\n\n#### DAL in the wild\n\nTo our knowledge, all empirical studies of pool-based DAL methods assume that an effective pool ratio hyperparameter, $\\gamma$, is known apriori. While the majority of works assumed the original training set as the fixed, unlabeled pool, [@yoo2019learning] limited their method to a subset of 10k instances instead of the full unlabeled set and [@beluch2018power] used subsampling to create the pool $U$ (and hence $\\gamma$). In real-world settings - in the wild - we are not aware of any method to set $\\gamma$ a priori, and there has been no study of DAL methods under this setting. Therefore, we believe ours is the first such study.\n\n# Problem Setting {#sec:ProblemSetting}\n\nIn this work, we focus on DAL for regression problems, which comprise a significant portion of DAL problems involving DNNs [@jose2024regression]. As discussed in [1](#sec:introduction){reference-type=\"ref+label\" reference=\"sec:introduction\"}, nearly all DAL methods for regression are pool-based, which is one of the three major paradigms of AL, along with stream-based and query synthesis. [@settles2009active]\n\n#### Formal description\n\nLet $L^i = (X^{i}, Y^{i})$ be the dataset used to train a regression model at the $i^{th}$ iteration of active learning. We assume access to some oracle, denoted $f : \\mathcal{X} \\rightarrow \\mathcal{Y}$, that can accurately produce the target values, $y \\in \\mathcal{Y}$ associated with input values $x \\in \\mathcal{X}$. Since we focus on DAL, we assume a DNN as our regression model, denoted $\\hat{f}$. We assume that some relatively small number of $N_{0}$ labeled training instances are available to initially train $\\hat{f}$, denoted $L^0$. In each iteration of DAL, we must choose $k$ query instances to be labeled by the oracle, yielding a set of labeled instances, denoted $Q$, that is added to the training dataset. Our goal is then to choose $Q$ that maximizes the performance of the DNN-based regression models over unseen test data at each iteration of active learning.\n\n#### Pool-based Deep Active Learning\n\nGeneral pool-based DAL methods assume that we have some pool $U$ of $N_{U}$ unlabeled instances from which we can choose the $k$ instances to label. The set $U$ is sampled from a larger and potentially-infinite set, denoted $D$, and $N_{U}$ is a HP chosen by the DAL user. We note that in some DAL applications, such as computer vision, it is conventional to utilize all available unlabeled data for $U$, and the pool size is not often explicitly varied or discussed. However, this convention is equivalent to setting $U=D$, and thereby implicitly setting the $N_{U}$ HP. Most pool-based methods rely upon some acquisition function $q: \\mathcal{X} \\rightarrow \\mathbb{R}$ to assign some scalar value to each $x \\in U$ indicating its \\\"informativeness\\\", or utility for training $\\hat{f}$. In each iteration of active learning, $q$ is used to evaluate all instances in $U$, and the top $k$ are chosen to be labeled and included in $L$.\n\n<figure id=\"img:pool_ratio_schematic\">\n<div class=\"center\">\n<img src=\"imgs/QBC_2by1.png\" />\n</div>\n<figcaption>Pool-based DAL for uncertainty-based mechanism. <span class=\"math inline\"><em>q</em>(<em>x</em>)</span> is the acquisition metric. (a, b) are two scenarios of the pool ratio (<span class=\"math inline\"><em>γ</em></span>) being too small (4 in a) or too large (32 in b) in <span class=\"math inline\"><em>k</em></span> (step size) of 2. </figcaption>\n</figure>\n\n#### The pool ratio hyperparameter, $\\gamma$\n\nWe define the *pool ratio* as $\\gamma = N_{U}/k$. By definition, $N_{U}$ and $k$ are hyperparameters of pool-based problems, and therefore $\\gamma$ also is. While one could, in principle, vary $N_{U}$ and $k$ independently, this is not often done in practice. Typically $k$ is set as small as possible, limited by computational resources. This leaves $N_{U}$ as the major free hyperparameter; however, prior research has found that its impact depends strongly on its size relative to $k$ [@kee2018query; @tsymbalov2018dropout; @kading2018active], encoded in $\\gamma$. Given a fixed value of $k$, increasing $N_{U}$ can lead to the discovery of points with higher values of $q(x)$ due to denser sampling of the input space. However, a larger $N_{U}$ also increases the similarity of the points, which provides redundant information to the model - a problem referred to as mode collapse [@burbidge2007active; @ren2021survey; @kee2018query]. In the limit as $N_{U} \\rightarrow \\infty$, all of the $k$ selected query points will be located near the same $x \\in \\mathcal{X}$ that has the highest value of $q(x)$. This tradeoff is illustrated in [2](#img:pool_ratio_schematic){reference-type=\"ref+label\" reference=\"img:pool_ratio_schematic\"} for a simple problem, and has also been noted in [@cacciarelli2024active].\n\nIn most real-world settings, there is a substantial quantity of unlabeled data (often infinite), and the user has the freedom (or burden) of choosing a suitable $\\gamma$ setting for their problem by varying the size of $U$. Crucially, and as we show in our experiments, choosing a sub-optimal $\\gamma$ value can result in poorer performance than naive random sampling. This is not necessarily a problem if either (i) one $\\gamma$ setting works across most problems or, alternatively, (ii) $\\gamma$ can be optimized on new problems without using labels. To the best of our knowledge, there is no method for optimizing $\\gamma$ on a new problem without running multiple trials of AL to find the best one (i.e., collecting labels), defeating the purpose of AL in real-world settings. Furthermore, the value of $\\gamma$ varies widely across the literature, suggesting that suitable settings for $\\gamma$ indeed vary across problems (see supplement for a list).\n\n# Benchmark Regression Problems {#sec:benchmark_problems}\n\n:::::: center\n::::: small\n:::: sc\n::: {#tbl:benchmark_dataset}\n  Dataset         Sine      Robo          Stack          ADM        Foil        Hydr       Bess       Damp\n  ----------- ------------ ------ --------------------- ------ --------------- ------ -------------- ------\n  $Dim_{x}$        1         4              5             14          5          6          2          3\n  $Dim_{y}$        1         2             201           2000         1          1          1         100\n  Oracle       Analytical          Numerical simulator   DNN    Random Forest          ODE solution  \n\n  : Benchmark datasets dimensionality and oracle functions. $Dim_{x, y}$ are the dimensionality of $x$ and y. Note that ODE solutions are implemented in the form of analytical functions as well.\n:::\n::::\n:::::\n::::::\n\nTo compose our benchmarks, we focused primarily upon problems in scientific computing, which is an important emerging problem setting [@subramanian2024towards; @takamoto2022pdebench; @majid2024mixture]. We propose eight regression problems to include in our benchmark set: two simple toy problems (SINE, ROBO), four contemporary problems from publications in diverse fields of science and engineering (STACK, ADM, FOIL, HYDR) and two problems solving ordinary differential equations (also prevalent in engineering). Summary details of our benchmark problems can be found in [1](#tbl:benchmark_dataset){reference-type=\"ref+label\" reference=\"tbl:benchmark_dataset\"} and [\\[tbl:oracle_details\\]](#tbl:oracle_details){reference-type=\"ref+label\" reference=\"tbl:oracle_details\"}.\n\nWe utilized four major selection criteria, beyond choosing scientific computing problems: (i) diversity: we sought to include a set of problems that span different disciplines (aero and fluid-dynamics, materials science), and problems that require physical experiments (e.g., FOIL, HYDRO) versus simulators (e.g., ADM); (ii) availability of labeled data: the problems we chose (unlike many high dimension ones) all had sufficiently large amount of labeled data, allowing us to easily study the impact of different pool ratios; (iii) dimensionality: we sought problems with relatively low dimensionality because they mitigate computational costs allowing for more extensive experimentation, while still being representative of many contemporary scientific computing problems (e.g., labeling can be highly expensive, severely limiting total labeled data, and making even low-dimensional problems challenging); (iv) difficulty: the problems in our dataset are also \"difficult\" in the sense that the accuracy of the learners (i.e., the DNN regressors) can vary significantly depending upon which data are labeled, making it possible to distinguish between more/less effective AL approaches. Although this is not the only notion of \"difficulty\" that may be relevant for selecting benchmark problems, we believe this is the most important one, and has been used in recent DAL studies [@holzmuller2023framework]. We now describe our benchmark problems:\n\n::::: center\n:::: small\n::: sc\n:::\n::::\n:::::\n\n**1D sine wave (SINE)** A noiseless 1-dimensional sinusoid with smoothly-varying frequency. **2D robotic arm (ROBO)** [@ren2020benchmarking] The goal is to predict the 2-D spatial location of the endpoint of a robotic arm based on its three joint angles. **Stacked material (STACK)** [@Chen2019] The goal is to predict the 201-D reflection spectrum of a material based on the thickness of its five layers. **Artificial Dielectric Material (ADM)** [@deng2021neural] The goal is to predict the 2000-D reflection spectrum of a material based on its 14-D geometric structure. Full wave electromagnetic simulations were utilized in [@deng2021benchmarking] to label data in the original work, requiring 1-2 minutes per input point. **NASA Airfoil (FOIL)** [@Dua:2019] The goal is to predict the sound pressure of an airfoil based on the structural properties of the foil, such as its angle of attack and chord length. This problem was published by NASA [@brooks1989airfoil] and the instance labels were obtained from a series of real-world aerodynamic tests in an anechoic wind tunnel. It has been used in other AL literature [@wu2018pool; @liu2020unsupervised; @jose2024regression]. **Hydrodynamics (HYDR)** [@Dua:2019] The goal is to predict the residual resistance of a yacht hull in water based on its shape. This problem was published by the Technical University of Delft, and the instance labels were obtained by real-world experiments using a model yacht hull in the water. It is also referred to as the \\\"Yacht\\\" dataset in some AL literature [@wu2019active; @cai2013maximizing; @jose2024regression]. **Bessel function (BESS)** The goal is to predict the value of the solution to Bessel's differential equation, a second-order ordinary differential equation that is common in many engineering problems. The inputs are the function order $\\alpha$ and input position $x$. The order $\\alpha$ is limited to non-negative integers below 10. **Damping Oscillator (DAMP)** The goal is to predict the full-swing trajectory of a damped oscillator in the first 100 time steps, of the solution to a second-order ordinary differential equation. The input is the magnitude, damping coefficient, and frequency of the oscillation.\n\n:::::: table*\n::::: center\n:::: small\n::: sc\n                Method                                                                  Acquisition function (q)\n  ---------------------------------- ------------------------------------------------------------------------------------------------------------------------------\n            Core-set (GSx)                                       $\\displaystyle \\min_{x\\in \\mathcal{L} \\cup \\mathcal{Q}} dist(x^*, x)$\n          [@sener2017active]         \n      Greedy sampling in y (GSy)                            $\\displaystyle  \\min_{y\\in \\mathcal{L} \\cup \\mathcal{Q}} dist(\\hat{f}(x^*), y)$\n           [@wu2019active]           \n   Improved greedy sampling (GSxy)                  $\\displaystyle  \\min_{(x,y)\\in \\mathcal{L} \\cup \\mathcal{Q}} dist(x^*, x)*dist(\\hat{f}(x^*), y)$\n           [@wu2019active]           \n       Query by committee (QBC)                                   $\\displaystyle  \\frac{1}{N}\\sum^N_{n=1}(\\hat{f}_n(x^*)-\\mu(x^*))^2$\n           [@kee2018query]           \n                                     \n           QBC w/ diversity                                                   $\\displaystyle  q_{QBC}(x^*) + q_{div}(x^*)$\n       (QBCDiv) [@kee2018query]                                             $\\displaystyle (q_{div}(x^*) =  q_{GSx}(x^*) )$\n      QBC w/ diversity & density                                      $\\displaystyle  q_{QBC}(x^*) + q_{div}(x^*) + q_{den}(x^*)$\n     (QBCDivDen) [@kee2018query]                            $\\displaystyle  (q_{den}(x^*) = \\dfrac{1}{k} \\sum_{x\\in N_k(x^*)} sim(x^*, x) )$\n   bayesian by disagreement (BALD)                                             $\\displaystyle  q_{QBC}(x^*)$ with dropout\n       [@tsymbalov2018dropout]       \n     Expected model output change                  $\\displaystyle \\mathbb{E}_{y'|x'} \\mathbb{E}_{x} || \\hat{f}(x^*; \\phi') - \\hat{f}(x^*; \\phi)||_1$\n    (EMOC) [@ranganathan2020deep]     $\\displaystyle \\approx \\mathbb{E}_{x} || \\nabla_{\\phi} \\hat{f}(x; \\phi) * \\nabla_{\\phi} \\mathcal{L}(\\phi; (x^{*'}, y'))||_1$\n   Learning Loss [@yoo2019learning]                                                  $\\displaystyle f_{loss}(x^*)$\n           Cluster-Variance                                                            $q_{QBC}(x)^*$ in clusters\n         [@citovsky2021batch]        \n        Density-Aware Core-Set                                                       $q_{GSx}(x^*) + q_{den}(x^*)$\n       (DACS)[@kim2022defense]       \n:::\n::::\n:::::\n\nin\n::::::\n\nFrom the literature, we found eleven AL methods that are (i) applicable to regression problems, (ii) with DNN-based regressors, making them suitable for benchmark regression problems. Due to space constraints, we list each method in [\\[tbl:benchmark_method\\]](#tbl:benchmark_method){reference-type=\"ref+label\" reference=\"tbl:benchmark_method\"} along with key details, and refer readers to the supplement for full details. Some of the methods have unique HPs that must be set by the user. In these cases, we adopt HP settings suggested by the methods' authors, shown in [\\[tbl:benchmark_method\\]](#tbl:benchmark_method){reference-type=\"ref+label\" reference=\"tbl:benchmark_method\"}. Upon publication, we will publish code for all of these methods to support future benchmarking.\n\n# Benchmark Experiment Design {#sec:exp_design}\n\nIn our experiments, we compare eleven state-of-the-art DAL methods on eight scientific computing problems. We evaluate the performance of our DAL methods as a function of $\\gamma$ on each of our benchmark problems, with $\\gamma \\in [2,4,8,16,32,64]$ (i.e., at each step we sample our $U$ with $k*\\gamma$ points). Following convention [@kee2018query; @tsymbalov2018dropout], we assume a small training dataset is available at the outset of active learning, $T^{0}$, which has $N_{0} = 80$ randomly sampled training instances. We then run each DAL model to $T^{50}$ AL steps, each step identifying $k=40$ points to be labeled from a fresh, randomly generated pool of size $k*\\gamma$. For each benchmark problem, we assume an appropriate neural network architecture is known apriori. Each experiment (i.e., the combination of dataset, DAL model, and $\\gamma$ value) is run 5 times to account for randomness. The MSE is calculated over a set of 4000 test points that are uniformly sampled within the $x$-space boundary. To reduce unnecessary noise related to our core hypothesis, we use the same (randomly sampled) unlabeled pools across different DAL methods.\n\nWe must train a regression model for each combination of problem and DAL method. Because some DAL methods require an ensemble model (e.g., QBC), we use an ensemble of 10 DNNs as the regressor for all of our DAL algorithms (except for the ADM problem, which is set to 5 due to the GPU RAM limit). More details on the models used and training procedures can be found in the supplement. Following convention [@kading2018active; @wu2018pool; @o2017model], we summarize our DAL performance by the area under curve (AUC) of the error plot. We report the full MSE vs \\# labeled point plots in the supplement. For the AUC calculation, we use 'sklearn.metrics.auc' [@scikit-learn] then further normalize by such AUC of random sampling method for easier visualization. All reported results are given in the unit of normalized AUC of MSE ($nAUC_{MSE})$.\n\n# Experimental Results {#sec:result}\n\n<figure id=\"img:main_perf\">\n<div class=\"center\">\n<img src=\"imgs/agg_mid_bar_plot_touched.png\" />\n</div>\n<figcaption>Performance of each DAL method (x-axis) in terms of <span class=\"math inline\"><em>n</em><em>A</em><em>U</em><em>C</em><sub><em>M</em><em>S</em><em>E</em></sub></span> (y-axis). For each DAL method, we report a bar indicating the <em>range</em> of <span class=\"math inline\"><em>n</em><em>A</em><em>U</em><em>C</em><sub><em>M</em><em>S</em><em>E</em></sub></span> values obtained as we vary the pool ratio, <span class=\"math inline\"><em>γ</em> ∈ [2, 4, ..., 64]</span>; for a given DAL method, we report one bar for each of the eight benchmark problems, indicated by a unique color in the legend. Each bar is bisected by a solid black and magenta line, respectively. The black line represents the average <span class=\"math inline\"><em>n</em><em>A</em><em>U</em><em>C</em><sub><em>M</em><em>S</em><em>E</em></sub></span> value across all settings of <span class=\"math inline\"><em>γ</em></span>. The magenta line represents the performance using <span class=\"math inline\"><em>γ</em><sub><em>p</em><em>r</em><em>i</em><em>o</em><em>r</em></sub></span> (see <a href=\"#sec:result\" data-reference-type=\"ref+label\" data-reference=\"sec:result\">6</a> for details). The dashed red line at <span class=\"math inline\"><em>n</em><em>A</em><em>U</em><em>C</em><sub><em>M</em><em>S</em><em>E</em></sub> = 1</span> corresponds to the performance obtained using random sampling. Note that some vertical bars are intentionally clipped at the top to improve the visualization overall.</figcaption>\n</figure>\n\nThe performance of all eleven DAL methods on all eight benchmark datasets is summarized in [3](#img:main_perf){reference-type=\"ref+label\" reference=\"img:main_perf\"}. The y-axis is the normalized $nAUC_{MSE}$, the x-axis is the DAL methods of interest, and the color code represents the different benchmark datasets. The horizontal red dashed line represents the performance of random sampling, which by definition is equal to one (see [5](#sec:exp_design){reference-type=\"ref+label\" reference=\"sec:exp_design\"}). Further details about [3](#img:main_perf){reference-type=\"ref+label\" reference=\"img:main_perf\"} are provided in its caption. We next discuss the results, with a focus on findings that are most relevant to DAL in the wild.\n\nThe results in [3](#img:main_perf){reference-type=\"ref+label\" reference=\"img:main_perf\"} indicate that *all* of our benchmark DAL methods are sensitive to their setting of $\\gamma$ - a central hypothesis of this work. As indicated by the vertical bars in [3](#img:main_perf){reference-type=\"ref+label\" reference=\"img:main_perf\"}, the $nAUC_{MSE}$ obtained by each DAL method varies substantially with respect to $\\gamma$. For most of the DAL methods, there exist settings of $\\gamma$ (often many) that cause them to perform worse than random sampling. This has significant implications for DAL in the wild since, to our knowledge, there is no general method for estimating a good $\\gamma$ setting prior to collecting large quantities of labeled data (e.g., to run trials of DAL with different $\\gamma$ settings), and DAL methods may perform worse, and unreliably, when accounting for the uncertainty of $\\gamma$.\n\n<figure id=\"img:best_pr_hist\">\n<div class=\"center\">\n<img src=\"imgs/best_pr_hist.png\" style=\"width:65.0%\" />\n</div>\n<figcaption>Frequency histogram of the best pool ratio values found in each DAL. For a given DAL method, this figure shows the frequency (% out of 8) that a particular pool ratio (x-axis) performs the best in terms of average <span class=\"math inline\"><em>n</em><em>A</em><em>U</em><em>C</em><sub><em>M</em><em>S</em><em>E</em></sub></span> metric. </figcaption>\n</figure>\n\n## DALs are sensitive to their pool ratio, $\\gamma$ {#sec:results_dals_are_sensitive_to_pool_ratio}\n\nThe sensitivity of DAL regression models to $\\gamma$ may be less significant if there exist $\\gamma$ settings that tend to perform well across most problems (for a given DAL method). [4](#img:best_pr_hist){reference-type=\"ref+label\" reference=\"img:best_pr_hist\"} presents a histogram of the best-performing $\\gamma$ settings for each DAL method. The results indicate that for each method there is no setting of $\\gamma$ that performs best across all problems. This corroborates our observations from the literature where we found a wide range of $\\gamma$ settings used across studies. However, we do see that some methods tend to have similar $\\gamma$ settings across all problems. For example, DACS has its best performance near $\\gamma=2$, although DACS performs poorly overall. GSxy, however, is one of the best-performing methods overall, and its best-performing settings cluster around $\\gamma = 16$. Given this observation, we investigate how well we can perform if we use historical results for a given method to choose a $\\gamma$ value for future problems. We emulate this scenario by evaluating the performance of each DAL method when adopting the best single $\\gamma$ setting from [4](#img:best_pr_hist){reference-type=\"ref+label\" reference=\"img:best_pr_hist\"} (i.e., the setting that wins across the most benchmarks), which we term $\\gamma_{prior}$, and then apply it across all benchmarks. The result of this strategy is given by the magenta line in [3](#img:main_perf){reference-type=\"ref+label\" reference=\"img:main_perf\"}. In most (but not all) cases, $\\gamma_{prior}$ yields lower MSE than the average MSE of all $\\gamma$ settings (the black lines). In some cases, $\\gamma_{prior}$ yields substantial overall performance improvements, such as for GSx and GSxy, suggesting that this is a reasonable $\\gamma$ selection strategy, although the benefits seem to vary across DAL models. However, even when using $\\gamma_{prior}$, the performance of DAL models still varies greatly, and many models still perform worse than random sampling. Therefore, while $\\gamma_{prior}$ may often be beneficial, it does not completely mitigate $\\gamma$-uncertainty.\n\n## Do any DAL methods outperform random sampling in the wild? {#sec:results_do_dal_outperform_random_sampling}\n\nThe results indicate that several DAL methods *tend* to obtain much lower $nAUC_{MSE}$ (i.e., they are better) than random sampling. This includes methods such as GSx, GSxy, GSy, QBC-x (variations of QBC) and ClusterVar. The results therefore suggest that these methods are beneficial more often than not, compared to random sampling - an important property. However, as discussed in [6.1](#sec:results_dals_are_sensitive_to_pool_ratio){reference-type=\"ref+label\" reference=\"sec:results_dals_are_sensitive_to_pool_ratio\"}, all DAL methods exhibit significant performance variance with respect to $\\gamma$, and some of the aforementioned methods still sometimes perform worse than random sampling. For example, this is the case of QBC, GSy, and QBCDivDen on the SINE problem. In settings where DAL is useful, the cost of collecting labels tends to be high, and therefore the risk of poor DAL performance (e.g., relative to simple random sampling) may strongly deter its use. Therefore, another important criteria is performance robustness: do any DAL methods consistently perform better than random sampling, in the wild? Our results indicate that GSx, GSxy, and QBCDiv always perform at least as well as random sampling, and often substantially better, regardless of the problem or $\\gamma$ setting. Note that all three robust DALs (GSx, GSxy, QBCDiv) employ x-space diversity in their loss function, which we discuss further in [6.3](#sec:results_why_some_methods_are_better){reference-type=\"ref+label\" reference=\"sec:results_why_some_methods_are_better\"}.\n\n<figure id=\"img:mode_collapse\">\n<div class=\"center\">\n<img src=\"imgs/negative_correlation_robo_touched-01.png\" />\n</div>\n<figcaption>A representative, combined plot with <span class=\"math inline\"><em>n</em><em>A</em><em>U</em><em>C</em><sub><em>M</em><em>S</em><em>E</em></sub></span> performance (bottom, y-axis at left, solid) and collapse metric, nDiv (upper, y-axis at right, more transparent) for each of the eleven DAL models at all pool ratios (color coded) for <em>robotic arm</em> dataset (ROBO). Dashed horizontal red lines starting from both <span class=\"math inline\"><em>y</em></span> axes represent the random sampling’s average <span class=\"math inline\"><em>n</em><em>A</em><em>U</em><em>C</em><sub><em>M</em><em>S</em><em>E</em></sub></span> and nDiv at 1. </figcaption>\n</figure>\n\n## Sample diversity is important for DAL in the wild {#sec:results_why_some_methods_are_better}\n\nOur results indicate that the best-performing DAL methods are GSx, GSxy, and QBCDiv. We say these methods are \\\"best\\\" because they are both robust (see [6.2](#sec:results_do_dal_outperform_random_sampling){reference-type=\"ref+label\" reference=\"sec:results_do_dal_outperform_random_sampling\"}), and they also usually yield lower MSEs, than other DAL methods. These methods share the common property that they encourage training data diversity, as measured by $x$-space distance between points. Interestingly, GSx *only* relies on x-space diversity. These results suggest that $x$-space diversity is a highly effective DAL acquisition criterion. Furthermore, and in contrast to other criteria, seeking points that maximize $x$-space diversity does not (by definition) increase the risk of mode collapse. Consequently, increasing $\\gamma$ results in greater diversity but without any increased risk of mode collapse (more details in [5](#img:mode_collapse){reference-type=\"ref+label\" reference=\"img:mode_collapse\"}). This may be a major reason why GSx, GSxy, and QBCDiv are less sensitive to $\\gamma$, and provide much more robust performance in the wild than other DAL methods. While sampling methods that use diversity have been found to be promising [@jose2024regression], our work provides evidence, for the first time, that sampling based upon diversity *may* be robust to hyperparameter uncertainty (we only examine uncertainty of $\\gamma$) while other popular sampling criteria (e.g., estimated model error) seem to be much less reliable in the wild.\n\nTo corroborate these findings, we evaluated the $x$-space diversity of each DAL method as a function of $\\gamma$. In particular, we calculated the diversity metric as the average nearest neighbor distance $$Div = \\dfrac{1}{|T|} \\sum_t^T \\dfrac{1}{K} \\sum_{i}^K \\min_{x^* \\in \\mathcal{Q^T}} dist(x^*, x^i)$$ where $\\mathcal{Q}^t$ represents the queried batch at active learning step $t$ and $|T|=50$ is the total number of active learning steps. Note that this metric is similar to, but not a simple average of $q_{GSx}(x)$ as $Div$ only focuses on per batch diversity and does not take the labeled set into consideration. It is also further normalized ($nDiv$) by the value of random sampling for each dataset separately. The lower this metric's value, the more severe the mode collapse issue would be.\n\nThe $nDiv$ is plotted in the top half of [5](#img:mode_collapse){reference-type=\"ref+label\" reference=\"img:mode_collapse\"} using the inverted right y-axis. For the obvious failure cases (BALD, EMOC and Learning Loss) in this particular dataset (their $nAUC_{MSE}$ exceeds 1), a clear trend of mode collapse can be observed in the upper half of the plot (nDiv much lower than 1). Meanwhile, a strong correlation between the pool ratio and the diversity metric can be observed: (i) For GSx and GSxy methods, which seek to maximize diversity, their diversity increases monotonically with larger pool ratio. (ii) For uncertainty-based methods (BALD, EMOC, LearningLoss, QBC, MSE), which seek to maximize query uncertainty, their diversity decreases monotonically with larger pool ratios. (iii) For combined methods like QBCDiv and QBCDivDen, the relationship between pool ratio and diversity shows a weak correlation, consistent with the benefits of having diversity as a selection criterion. (iv) Lastly, we observe that all top-performing methods have high diversity, regardless of $\\gamma$, suggesting it is an important condition for effective DAL.\n\n# Conclusions {#sec:conclusions}\n\nFor the first time, we evaluated eleven state-of-the-art DAL methods on eight benchmark datasets for regression *in the wild*, where we assume that the best pool ratio hyperparameter, $\\gamma$, is uncertain. We summarize our findings as follows:\n\n-   *DAL methods for regression often perform worse than simple random sampling, when evaluated in the wild*. Using $\\gamma$ as an example, we systematically demonstrate the rarely-discussed problem that most DAL models are often outperformed by simple random sampling when we account for HP uncertainty.\n\n-   *Some DAL methods were relatively robust, and outperformed random sampling robustly in the wild (e.g., GSx, GSxy, QBCDiv)*.\n\n-   *Insofar as robustness to pool ratio is concerned, our results suggest that DAL approaches utilizing sample diversity tend to be much more robust in the wild than other popular selection criteria.*\n\n## Limitations\n\nOne limitation of this work is that we focused on scientific computing benchmark problems, and problems with relatively low dimensionality. Including higher dimensional problems is an especially important opportunity for future work due to the importance of vision problems in the DAL community, and also because sensitivity to pool ratio has been noted in that setting as well [@yoo2019learning; @sener2017active], but not studied systematically. Another important limitation is that we constrained our evaluation of DAL methods to uncertainty in their pool ratio. Future studies would benefit from evaluating each DAL approach with respect to uncertainty in all of its relevant DAL HPs (i.e., those that require labeled data to be optimized), providing a more comprehensive assessment of modern DAL methods in the wild.\n\n### Acknowledgments {#acknowledgments .unnumbered}\n\nSimiao Ren thanks the support of the Duke aiM trainee program, by the NSF grant DGE-2022040.\n\n# Details of the benchmarking methods\n\n**Core-set (GSx: Greedy sampling in $x$ space)** [@sener2017active]. This approach only relies upon the diversity of points in the input space, $\\mathcal{X}$, when selecting new query locations. A greedy selection criterion is used, given by $$q_{GSx}(x^*) = \\min_{x\\in \\mathcal{L} \\cup \\mathcal{Q}} dist(x^*, x)$$ where $\\mathcal{L}$ is the labeled set, $\\mathcal{Q}$ is the already selected query points and $dist$ being L2 distance.\n\n**Greedy sampling in $y$ space (GSy)** [@wu2019active]. Similar to GSx which maximizes diversity in the $x$ space in a greedy fashion, GSy maximizes the diversity in the $y$ space in a greedy fashion: $$q_{GSy}(x^*) = \\min_{y\\in \\mathcal{L} \\cup \\mathcal{Q}} dist(f(x^*), y)$$ where $f(x)$ is the current model prediction of the $x$ and $y$ is the labels in the already labeled training set plus the predicted labels for the points (to be labeled) selected in the current step.\n\n**Greedy sampling in xy space (GSxy)** [@wu2019active]. Named as 'Improved greedy sampling (iGS)' in the original paper [@wu2019active], this approach combines GSx and GSy and uses multiplication of the distance of both $x$ and $y$ space in its acquisition function: $$q_{GSxy}(x^*) = \\min_{(x,y)\\in \\mathcal{L} \\cup \\mathcal{Q}} dist(x^*, x)*dist(f(x^*), y)$$\n\n**Query-by-committee (QBC)** [@seung1992query] The QBC approach is pure uncertainty sampling if we set $q(x) = q_{QBC}(x)$: $$q_{QBC}(x) = \\frac{1}{N}\\sum^N_{n=1}(\\hat{f}_n(x)-\\mu(x))^2$$ Here $\\hat{f}_{n}$ denotes the $n^{th}$ model in an ensemble of $N_{ens}$ models (DNNs in our case), and $\\mu(x)$ is the mean of the ensemble predictions at $x$. In each iteration of AL these models are trained on all available training data at that iteration.\n\n**QBC with diversity (Div-QBC)** [@kee2018query]. This method improves upon QBC by adding a term to $q$ that also encourages the selected query points to be diverse from one another. This method introduces a hyperparameter for the relative weight of the diversity and QBC criteria and we use an equal weighting ($\\alpha = 0.5$ [@kee2018query]). $$\\begin{aligned}\n    q_{QBCDiv}(x) &= (1-\\alpha)* q_{QBC}(x) + \\alpha * q_{div}(x)\\\\\n    q_{div}(x^*) &=  q_{GSx}(x^*) \n\\end{aligned}$$\n\n**QBC with diversity and density (DenDiv-QBC)** [@kee2018query]. This method builds upon Div-QBC by adding a term to $q(x)$ that encourages query points to have uniform density. This method introduces two new hyperparameters for the relative weight ($\\alpha = \\beta = \\dfrac{1}{3}$) of the density, diversity, and QBC criteria, and we use an equal weighting as done in the original paper [@kee2018query]. $$\\begin{aligned}\n    q_{QBCDivDen}(x) &= (1-\\alpha - \\beta)* q_{QBC}(x) \\\\\n    & + \\alpha * q_{div}(x) + \\beta * q_{den}(x)\\\\\n    q_{den}(x^*) &= \\dfrac{1}{k} \\sum_{x\\in N_k(x^*)} sim(x^*, x)\n\\end{aligned}$$ where $N_k(x^*)$ is the k nearest neighbors of an unlabeled point, $sim(x^*, x)$ is the cosine similarity between points.\n\n**Bayesian active learning by disagreement (BALD)** [@tsymbalov2018dropout]. BALD uses the Monte Carlo dropout technique to produce multiple probabilistic model output to estimate the uncertainty of model output and uses that as the criteria of selection (same as $q_{QBC}(x)$). We used 25 forward passes to estimate the disagreement.\n\n**Expected model output change (EMOC)** [@kading2018active; @ranganathan2020deep]. EMOC is a well-studied AL method for the classification task that strives to maximize the change in the model (output) by labeling points that have the largest gradient. However, as the true label is unknown, some label distribution assumptions must be made. Simple approximations like uniform probability across all labels exist can made for classification but not for regression tasks. [@ranganathan2020deep] made an assumption that the label is simply the average of all predicted output in the unlabeled set ($y'(x') = \\mathbb{E}_{x \\in \\mathcal{U}} f(x)$) and we use this implementation for our benchmark of EMOC. $$\\begin{aligned}\n    q_{EMOC}(x') &= \\mathbb{E}_{y'|x'} \\mathbb{E}_{x} || f(x; \\phi') - f(x; \\phi)||_1\\\\\n    &\\approx \\mathbb{E}_{x} || \\nabla_{\\phi} f(x; \\phi) * \\nabla_{\\phi} \\mathcal{L}(\\phi; (x', y'))||_1\n\\end{aligned}$$ where $f(x; \\phi)$ is the current model output for point $x$ with model parameter $\\phi$, $\\phi'$ is the updated parameter after training on labeled point x' with label y' and $\\mathcal{L}(\\phi; (x', y')$ is the loss of the model with current model parameter $\\phi$ on new labeled data $(x', y')$.\n\n**Learning Loss** [@yoo2019learning]. Learning Loss is another uncertainty-based AL method that instead of using proxies calculated (like variance), learns the uncertainty directly by adding an auxiliary model to predict the loss of the current point that the regression model would make. The training of the auxiliary model concurs with the main regressor training and it uses a soft, pair-wise ranking loss instead of Mean Squared Error (MSE) loss to account for the fluctuations of the actual loss during the training. $$q_{LL}(x) = f_{loss}(x)$$ where $f_{loss}(x)$ is the output of the loss prediction auxiliary model. In this AL method, there are multiple hyper-parameters (co-training epoch, total auxiliary model size, auxiliary model connections, etc.) added to the AL process, all of which we used the same values in the paper if specified [@yoo2019learning].\n\n**Density Aware Core-set (DACS)** [@kim2022defense] A diversity-based AL method that not only considers core-set metric but also considers the density and strives to sample low-density regions. The original DACS also encodes the image space into feature space and uses locality-sensitive hashing techniques to accelerate the nearest neighbor calculation and prevent computational bottlenecks. As our scientific computing tasks does not involve high dimensional image as well as having much lower dataset size in general, instead of encoded feature space distance, we used input space distance and locality-sensitive hashing was dropped as we don't face such computational bottleneck for the nearest neighbor calculation with our smaller pool compared to theirs.\n\n**Cluster Margin adapted to regression problem: Cluster Variance (ClusterVar)** [@citovsky2021batch] To alleviate the robustness issue arising in larger batch AL scenarios, Cluster-Margin [@citovsky2021batch] method is proposed to add necessary diversity to the uncertainty sampling. The original method used margin as its uncertainty metric as it was demonstrated on image classification tasks, and we adapted it into a variance metric in a regression setting. During Cluster Margin, Hierarchical Agglomerative Clustering (HAC) is run once on the unlabeled pool before the AL process and during each round a round-robin selection is carried out from the smallest cluster to the largest cluster, each time selecting the unlabeled sample with the highest uncertainty metric.\n\n# Details of benchmark datasets used\n\n**1D sine wave (Wave).** A noiseless 1-dimensional sinusoid with varying frequency over $x$, illustrated in [6](#img:sup_data_toy){reference-type=\"ref+label\" reference=\"img:sup_data_toy\"}. $$y = x * sin(a_1 * sin(a_2 * x)),$$ where $a_1 = 3$ and $a_2 = 30$ is chosen to make a relative complicated loss surface for the neural network to learn while also having a difference in sensitivity in the domain of x.\n\n**2D robotic arm (Arm) [@ren2020benchmarking]** In this problem we aim to predict the 2D spatial location of the endpoint of a robotic arm based on its joint angles. Illustrated in [6](#img:sup_data_toy){reference-type=\"ref+label\" reference=\"img:sup_data_toy\"}. The Oracle function is given by $$y_0 = \\sum_{i=1}^3 \\cos(\\dfrac{pi}{2}x_i)*l_i, \n    y_1 = x_0 + \\sum_{i=1}^3 \\sin(\\dfrac{pi}{2}x_i)*l_i$$ where $y$ is the position in the 2D plane, $x_0$ is the adjustable starting horizontal position, $x_{i=1,2,3}$ are the angles of the arm relative to horizontal reference and $l_{i=0,1,2} = [0.5, 0.5, 1]$ represents the i-th length of the robotic arm component. The dataset is available under the MIT license.\n\n<figure id=\"img:sup_data_toy\">\n<div class=\"center\">\n<img src=\"imgs/sup_figs/dataset_toy.png\" />\n</div>\n<figcaption>Schematic illustration of sine wave (a) and robotic arm (b) datasets</figcaption>\n</figure>\n\n**Stacked material (Stack) [@Chen2019].** In this problem, we aim to predict the reflection spectrum of a material, sampled at 201 wavelength points, based upon the thickness of each of the 5 layers of the material, illustrated in [7](#img:sup_data_mm){reference-type=\"ref+label\" reference=\"img:sup_data_mm\"}. It was also benchmarked in [@ren2022inverse]. An analytic Oracle function is available based upon physics[@Chen2019].\n\n**Artificial Dielectric Material (ADM) [@deng2021neural]** This problem takes the geometric structure of a material as input, and the reflection spectrum of the material, as a function of frequency, illustrated in [7](#img:sup_data_mm){reference-type=\"ref+label\" reference=\"img:sup_data_mm\"}. It was also benchmarked in [@deng2021benchmarking]. This dataset -released under CC BY 4.0 License - consists of input space of 3D geometric shape parameterized into 14 dimension space and the output is the spectral response of the material. The oracle function is a DNN [@deng2021benchmarking].\n\n<figure id=\"img:sup_data_mm\">\n<div class=\"center\">\n<img src=\"imgs/sup_figs/dataset_material.png\" />\n</div>\n<figcaption>(a, c) are schematic illustration of two material design datasets (Stack &amp; ADM). (b, d) are example spectra of their material property after simulations from their geometric parameterization (typically from Maxwell equation solvers that are slow and hence can benefit from active learning)</figcaption>\n</figure>\n\n**NASA Airfoil (Foil) [@Dua:2019]** NASA dataset published on <https://archive.ics.uci.edu/dataset/291/airfoil+self+noise> UCI ML repository under CC BY 4.0 License [@Dua:2019] obtained from a series of aerodynamic and acoustic tests of 2D/3D airfoil blade sections conducted in an anechoic wind tunnel, illustrated in [8](#img:sup_UCI){reference-type=\"ref+label\" reference=\"img:sup_UCI\"}. The input is the physical properties of the airfoil, like the angle of attack and chord length and the regression target is the sound pressure in decibels. We use a well-fitted random forest fit to the original dataset as our oracle function following prior work[@trabucco2022design]. The fitted random forest architecture and its weights are also shared in our code repo to ensure future work makes full use of such benchmark datasets as we did.\n\n**Hydrodynamics (Hydro) [@Dua:2019]** Experiment conducted by the Technical University of Delft, illustrated in [8](#img:sup_UCI){reference-type=\"ref+label\" reference=\"img:sup_UCI\"}, (hosted on <https://archive.ics.uci.edu/ml/datasets/Yacht+Hydrodynamics> UCI ML repository under CC BY 4.0 License [@Dua:2019]), this dataset contains basic hull dimensions and boat velocity and their corresponding residuary resistance. Input is 6 dimensions and output is the 1 dimension. We use a well-fitted random forest fit to the original dataset as our oracle function. The fitted random forest architecture and its weights are also shared in our code repository to ensure future work makes full use of such benchmark dataset as we did.\n\n<figure id=\"img:sup_UCI\">\n<div class=\"center\">\n<img src=\"imgs/sup_figs/Appendix_UCI_dataset.png\" />\n</div>\n<figcaption>Schematic illustration of Airfoil and Hydro experiments. Reproduced from the original source of experiment reports from NASA and Delft University of technology. (a) The Hydro experiment with an actual yacht being built and resistance was measured in a water flow experiment as the regression target y. (b) Airfoil experiment where input is the parameters of the airfoil and the sound pressure level is measured as target <span class=\"math inline\"><em>y</em></span> of the regression task.</figcaption>\n</figure>\n\n**Bessel equation** The solution to the below single dimension second-order differential equation: $$x^2\\dfrac{d^2y}{dx^2} + x\\dfrac{dy}{dx} + (x^2 - \\alpha^2)y=0$$ where input is $\\alpha$ and $x$ position given. $\\alpha$ is limited to non-negative integers smaller than 10 and $x \\in [0, 10]$. The solution examples can be visualized in [9](#img:sup_ode){reference-type=\"ref+label\" reference=\"img:sup_ode\"}. Our choice of $\\alpha$ values makes the Bessel functions cylinder harmonics and they frequently appear in solutions to Laplace's equation (in cylindrical systems). The implementation we used is the python package 'scipy.special.jv(v,z)' [@2020SciPy-NMeth].\n\n**Damping oscillator equation** The solution to the below ordinary second-order differential equation: $$m\\dfrac{dx^2}{d^2t} + b\\dfrac{dx}{dt} + \\dfrac{mg}{l}x = 0$$ where m is the mass of the oscillator, b is the air drag, g is the gravity coefficient, l is the length of the oscillator's string and it has analytical solution of form $$x = a e^{-bt} cos(\\alpha - \\psi)$$ where a is the amplitude, b is the damping coefficient, $\\alpha$ is the frequency and $\\psi$ is the phase shift. We assume $\\psi$ to be 0 and let a,b,$\\alpha$ be the input parameters. The output, unlike our previous ODE dataset, is taken as the first 100 time step trajectory of the oscillator, making it a high dimensional manifold (nominal dimension of 100 with true dimension of 3). The trajectory is illustrated in [9](#img:sup_ode){reference-type=\"ref+label\" reference=\"img:sup_ode\"}. We implement the above solution by basic python math operations.\n\n<figure id=\"img:sup_ode\">\n<div class=\"center\">\n<img src=\"imgs/sup_figs/ODE.png\" />\n</div>\n<figcaption>Schematic illustration of Bessel function solution and the damping oscillator solutions.</figcaption>\n</figure>\n\n# List of pool ratio used in existing literature\n\n17000 [@mccallumzy1998employing], 20 to 2000 [@kee2018query], 300 to 375[@santos2020modeling], 11-20 [@roy2018deep], 1000 [@burbidge2007active], and 1 to 11 [@tan2019batch].\n\n# Details of models training and architecture\n\nIn the below [2](#tbl:model_architectures){reference-type=\"ref+label\" reference=\"tbl:model_architectures\"}, we present the model architecture for each of our benchmarked datasets. Unless otherwise noted, all of them are fully connected neural networks.\n\n:::::: center\n::::: small\n:::: sc\n::: {#tbl:model_architectures}\n  Feat     Sine   Robo   Stack     ADM     Foil   Hydr   BESS   DAMP\n  ------- ------ ------ ------- --------- ------ ------ ------ ------\n  NODE      20    500     700     1500     200     50     50    500\n  LAYER     9      4       9     $4^{*}$    4      6      6      6\n\n  : Regression model, $\\hat{f}$ architecture details for each problem. \\*: for ADM, there are 3 layers of convolutions after the linear layer)\n:::\n::::\n:::::\n::::::\n\nWe implemented our models in PyTorch [@NEURIPS2019_9015]. Beyond the above architectural differences, the rest of the model training settings are the same across the models: Starting labeled set of size 80, in each step DAL finds 40 points to be labeled for 50 active learning steps. Each regression model is an ensemble network of 10 models of size illustrated in [2](#tbl:model_architectures){reference-type=\"ref+label\" reference=\"tbl:model_architectures\"} except the ADM dataset (5 instead of 10 due to RAM issue). The test dataset is kept at 4000 points uniformly sampled across the $x$-space and they are fixed the same across all experiments for the same dataset. No bootstrapping is used to train the ensemble network and the only source of difference between networks in the ensemble (committee) is the random initialization of weights.\n\nThe batch size is set to be 5000 (larger than the largest training set) so that the incomplete last batch would not affect the training result (as we sample more and more data, we can't throw away the last incomplete batch but having largely incomplete batch de-stabilizes training and introduce noise into the experiment. Adam optimizer is used with 500 training epochs and the model always retrains from scratch. (We observe that the training loss is much higher if we inherit from the last training episode and do not start from scratch, which is consistent with other literature [@beck2021effective]). The learning rate is generally 1e-3 (some datasets might differ), and the decay rate of 0.8 with the decay at the plateau training schedule. The regularization weight is usually 1e-4 (dataset-dependent as well). The hyper-parameters only change with respect to the dataset but never with respect to DAL used.\n\nThe hyperparameters are tuned in the same way as the model architecture: Assume we have a relatively large dataset (2000 randomly sampling points) and tune our hyperparameter on this set. This raises another robustness problem of deep active learning, which is how to determine the model architecture before we have enough labels. This is currently out of the scope of this work as we focused on how different DALs behave with the assumption that the model architectures are chosen smartly and would be happy to investigate this issue in future work.\n\nFor the BALD method, we used a dropout rate of 0.5 as advised by previous work. As BALD requires a different architecture than other base methods (a dropout structure, that is capable of getting a good estimate even with 50% of the neurons being dropped), the model architecture for the active learning is different in that it enlarges each layer by a constant factor that can make it the relatively same amount of total neurons like other DAL methods. Initially, the final trained version of the dropout model is used as the regression model to be evaluated. However, we found that an oversized dropout model hardly fits as well as our ensembled counterpart like other DAL methods. Therefore, to ensure the fairness of comparison, we trained another separate, ensembled regression model same as the other DALs and reported our performance on that.\n\nFor the LearningLoss method, we used the same hyper-parameter that we found in the cited work in the main text: relative weight of learning loss of 0.001 and a training schedule of 60% of joint model training and the rest epoch we cut the gradient flow to the main model from the auxiliary model. For the design of the auxiliary model, we employed a concatenation of the output of the last three hidden layers of our network, each followed by a fully connected network of 10 neurons, before being directed to the final fully connected layer of the auxiliary network that produces a single loss estimate.\n\nFor the EMOC method, due to RAM limit and time constraint, we can not consider all the model parameters during the gradient calculation step (For time constraint, [3](#tbl:time_performance){reference-type=\"ref+label\" reference=\"tbl:time_performance\"} gives a good reference of how much longer EMOC cost, even in this reduced form). Therefore, we implemented two approximations: (i) For the training set points where the current model gradients are evaluated, instead of taking the ever-growing set that is more and more biased towards the DAL selection, we fixed it to be the 80 original, uniformly sampled points. (ii) We limit the number of model parameters to evaluate the EMOC criteria to 50k. We believe taking the effect of 50 thousand parameters gives a good representation of the model's response (output change) for new points. We acknowledge that these approximations might be posing constraints to EMOC, however, these are practical, solid challenges for DAL practitioners as well and these are likely the compromise to be made during application.\n\n## Computational resources\n\nHere we report the computational resources we used for this work: AMD CPU with 64 cores; NVIDIA 3090 GPU x4 (for each of the experiment we used a single GPU to train); 256GB RAM.\n\n# Additional performance plots\n\nAs the benchmark conducts a huge set of experiments that are hard to fit in the main text, here we present all the resulting figures for those who are interested to dig more takeaways.\n\n## Time performance of the benchmarked DAL methods\n\nWe also list the time performance of each DAL method, using the ROBO dataset as an example in the below [3](#tbl:time_performance){reference-type=\"ref+label\" reference=\"tbl:time_performance\"}. Note that this is only the sampling time cost, not including the model training time, which is usually significantly larger than the active learning sampling time at each step. The only DAL method that potentially has a time performance issue is the EMOC method, which requires the calculation of each of the gradients with respect to all parameters and therefore takes a much longer time than other DAL methods. However, as it is shown in the main text that it is not a robust method in our setting, there is no dilemma of performance/time tradeoff presented here.\n\n:::::: center\n::::: small\n:::: sc\n::: {#tbl:time_performance}\n  Dataset    Random   GSx     GSxy       GSy      BALD       EMOC              \n  --------- -------- ------ -------- ----------- ------- ------------ -- -- -- --\n  Time        2.15    4.96   10.27      6.85      9.06    **756.6**            \n  Dataset      LL     QBC    QBCDiv   QBCDivDen   DACS    ClusterVar           \n  Time        6.53    4.29    9.04      10.38     21.70      4.31              \n\n  : Time performance for average time spent during the sampling process for ROBO dataset per active learning step (40 points) in ms for pool ratio of 2. LL: LearningLoss\n:::\n::::\n:::::\n::::::\n\n## Combined plot with $nAUC_{MSE}$ and nDiv\n\n<figure>\n<div class=\"center\">\n<img src=\"imgs/sup_figs/combined_plot/negative_correlation_sine.png\" />\n</div>\n<figcaption><span class=\"math inline\"><em>n</em><em>A</em><em>U</em><em>C</em><sub><em>M</em><em>S</em><em>E</em></sub></span> and nDiv plot for SINE</figcaption>\n</figure>\n\n<figure>\n<div class=\"center\">\n<img src=\"imgs/sup_figs/combined_plot/negative_correlation_Stack.png\" />\n</div>\n<figcaption><span class=\"math inline\"><em>n</em><em>A</em><em>U</em><em>C</em><sub><em>M</em><em>S</em><em>E</em></sub></span> and nDiv plot for STACK</figcaption>\n</figure>\n\n<figure>\n<div class=\"center\">\n<img src=\"imgs/sup_figs/combined_plot/negative_correlation_ADM.png\" />\n</div>\n<figcaption><span class=\"math inline\"><em>n</em><em>A</em><em>U</em><em>C</em><sub><em>M</em><em>S</em><em>E</em></sub></span> and nDiv plot for ADM</figcaption>\n</figure>\n\n<figure>\n<div class=\"center\">\n<img src=\"imgs/sup_figs/combined_plot/negative_correlation_airfoil.png\" />\n</div>\n<figcaption><span class=\"math inline\"><em>n</em><em>A</em><em>U</em><em>C</em><sub><em>M</em><em>S</em><em>E</em></sub></span> and nDiv plot for FOIL</figcaption>\n</figure>\n\n<figure>\n<div class=\"center\">\n<img src=\"imgs/sup_figs/combined_plot/negative_correlation_hydro.png\" />\n</div>\n<figcaption><span class=\"math inline\"><em>n</em><em>A</em><em>U</em><em>C</em><sub><em>M</em><em>S</em><em>E</em></sub></span> and nDiv plot for HYDR</figcaption>\n</figure>\n\n<figure>\n<div class=\"center\">\n<img src=\"imgs/sup_figs/combined_plot/negative_correlation_bessel.png\" />\n</div>\n<figcaption><span class=\"math inline\"><em>n</em><em>A</em><em>U</em><em>C</em><sub><em>M</em><em>S</em><em>E</em></sub></span> and nDiv plot for BESS</figcaption>\n</figure>\n\n<figure>\n<div class=\"center\">\n<img src=\"imgs/sup_figs/combined_plot/negative_correlation_damp.png\" />\n</div>\n<figcaption><span class=\"math inline\"><em>n</em><em>A</em><em>U</em><em>C</em><sub><em>M</em><em>S</em><em>E</em></sub></span> and nDiv plot for DAMP</figcaption>\n</figure>\n\n## MSE vs active learning step plot\n\nWe also present the traditional plot of the MSE vs active learning step for reference. For each of the plots below, the MSE are smoothed with a smoothing parameter of 0.5 using the tensorboard smoothing visualizing function [@tensorflow2015-whitepaper]. The $x$ labels are from 0 - 49, where 0 measures the end of the first active learning step.\n\n<figure>\n<div class=\"center\">\n<img src=\"imgs/sup_figs/MSE_plots/sine_all_method_combined_agg_mode_None.png\" />\n</div>\n<figcaption>MSE plot for SINE</figcaption>\n</figure>\n\n<figure>\n<div class=\"center\">\n<img src=\"imgs/sup_figs/MSE_plots/robo_all_method_combined_agg_mode_None.png\" />\n</div>\n<figcaption>MSE plot for ROBO</figcaption>\n</figure>\n\n<figure>\n<div class=\"center\">\n<img src=\"imgs/sup_figs/MSE_plots/Stack_all_method_combined_agg_mode_None.png\" />\n</div>\n<figcaption>MSE plot for STACK</figcaption>\n</figure>\n\n<figure>\n<div class=\"center\">\n<img src=\"imgs/sup_figs/MSE_plots/ADM_all_method_combined_agg_mode_None.png\" />\n</div>\n<figcaption>MSE plot for ADM</figcaption>\n</figure>\n\n<figure>\n<div class=\"center\">\n<img src=\"imgs/sup_figs/MSE_plots/hydro_all_method_combined_agg_mode_None.png\" />\n</div>\n<figcaption>MSE plot for HYDR</figcaption>\n</figure>\n\n<figure>\n<div class=\"center\">\n<img src=\"imgs/sup_figs/MSE_plots/airfoil_all_method_combined_agg_mode_None.png\" />\n</div>\n<figcaption>MSE plot for FOIL</figcaption>\n</figure>\n\n<figure>\n<div class=\"center\">\n<img src=\"imgs/sup_figs/MSE_plots/bessel_all_method_combined_agg_mode_None.png\" />\n</div>\n<figcaption>MSE plot for BESS</figcaption>\n</figure>\n\n<figure>\n<div class=\"center\">\n<img src=\"imgs/sup_figs/MSE_plots/damp_all_method_combined_agg_mode_None.png\" />\n</div>\n<figcaption>MSE plot for DAMP</figcaption>\n</figure>\n\n[^1]: University of California Irvine Machine Learning Repository",
    "rationale": "Summary: This is a benchmark paper, the author evaluates eleven state-of-the-art DAL methods across eight datasets in the wild.\n\nStrengths: 1. It tests eleven different DAL methods across various problems.\n\n2. It highlights that methods using sample diversity are more reliable.\n\n3. The findings might be useful for people who want to use DAL in practical settings.\n\nWeaknesses: 1. Setting the pool ratio itself is not meaningful, since we can first use diversity-based measures to determine the subset and then use uncertainty-based measures to get better AL performance.\n\n2. Datasets are too simple.\n\n3. The conclusions of this paper rely on empirical evidence to support their findings. For example, it lacks a formal theoretical framework or mathematical proof to explain why diversity-based methods are inherently more robust.\n\n4. As the author mentioned \"The recent study by Holzmüller et al. (2023) is the only work that is similar to ours\", however, the comparisons described in this paper are not strong enough to explain the novelty and necessity of this paper.\n\nQuestions: 1. These figures are really hard to read, like Figure 3, and MSE plots in supplementary materials.",
    "rating": 2,
    "label": false,
    "chosen": true,
    "rationale_edited": "This is a benchmark paper, the author evaluates eleven state-of-the-art DAL methods across eight datasets in the wild.\n\nDatasets are too simple.\n\nThe conclusions of this paper rely on empirical evidence to support their findings. For example, it lacks a formal theoretical framework or mathematical proof to explain why diversity-based methods are inherently more robust.\n\nAs the author mentioned \"The recent study by Holzmüller et al. (2023) is the only work that is similar to ours\", however, the comparisons described in this paper are not strong enough to explain the novelty and necessity of this paper."
  }
]
