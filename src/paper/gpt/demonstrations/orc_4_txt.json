[
  {
    "title": "dMel: Speech Tokenization made Simple",
    "abstract": "Large language models have revolutionized natural language processing by leveraging self-supervised pretraining on vast textual data.\n  Inspired by this success, researchers have investigated complicated speech tokenization methods to discretize continuous speech signals so that language modeling techniques can be applied to speech data.\n  However, existing approaches either model semantic (content) tokens, potentially losing acoustic information, or model acoustic tokens, risking the loss of semantic (content) information. \n  Having multiple token types also complicates the architecture and requires additional pretraining.\n  Here we show that discretizing mel-filterbank channels into discrete intensity bins produces a simple representation (dMel), that performs better than other existing speech tokenization methods.\n  Using an LM-style transformer architecture for speech-text modeling, we comprehensively evaluate different speech tokenization methods on speech recognition (ASR) and speech synthesis (TTS).\n  Our results demonstrate the effectiveness of dMel in achieving high performance on both tasks within a unified framework, paving the way for efficient and effective joint modeling of speech and text. The code is available at anonymous_url, while generation samples are in the supplementary materials.",
    "text": "# Introduction\n\nLarge language models (LLMs) have achieved remarkable success in various natural language processing tasks by leveraging self-supervised pretraining on massive amounts of textual data [@brown2020language]. Inspired by this success, numerous works [@borsos2023audiolm; @rubenstein2023audiopalm; @zhang2023speechgpt; @wang2023neural] have sought to extend the language modeling approach to speech processing, aiming to build unified models capable of both speech understanding and generation tasks. However, a key challenge lies in the continuous nature of speech signals, necessitating effective tokenization methods to discretize the input for language model-based processing.\n\nCurrent speech tokenization approaches can be broadly categorized into two types: semantic (content) tokens and acoustic tokens[^1]. Semantic tokens, extracted from self-supervised (SSL) pretrained speech models [@baevski2020wav2vec; @hsu2021hubert], where the speech signal is first encoded into speech representations and then clustered into semantic tokens with $k$-means method. However, such SSL pretrained models are not useful for high fidelity speech synthesis as speaker identity and other details of raw speech are lost in training [@borsos2023audiolm]. Conversely, acoustic tokens can be obtained from audio compression models that are trained to compress the speech signal into codebook indices with residual vector quantization (RVQ) and reconstruction objectives [@zeghidour2021soundstream; @defossez2022high]. These tokens prioritize acoustic reconstruction but lose semantic information which can lead to poorer results in generating audio [@wang2023neural].\n\nTo combine the advantages of both semantic and acoustic tokens, AudioLM [@borsos2023audiolm] proposed to model both semantic tokens and acoustic tokens with 3 stages: semantic modeling, coarse acoustic modeling, and fine acoustic modeling. The coarse-to-fine modeling strategy is designed to match the residual structure of RVQ based acoustic tokens. This solution addresses both content and speech quality, but its multi-stage hierarchical structure complicates the model and can lead to slower training and inference. Another solution is to combine the semantic and acoustic features together. @zhang2023speechtokenizer proposed to distill the semantic tokens into the acoustic token's first residual channel during the training of the RVQ model in a teacher-student manner. In this way, the new feature can preserve the semantic information better and also reconstruct high quality speech signals.\n\nIn this paper, we raise the following fundamental question -- ***do we really need to separate speech into semantic and acoustic tokens first, and process them with idiosyncratic architectures?*** We propose a simple alternative called `dMel`(see Figure [1](#fig:dmel){reference-type=\"ref\" reference=\"fig:dmel\"}) that discretizes log mel-filterbanks (Mel) energies directly into ordinal bins. Intriguingly, we find that discretizing Mel has little impact on the ability of off-the-shelf Mel vocoders to reconstruct waveforms[^2]. In Table [\\[tab:teaser\\]](#tab:teaser){reference-type=\"ref\" reference=\"tab:teaser\"} we show different vocoders to reconstruct waveforms from Mel and discretized Mel (`dMel`) computed on them, as well as ASR models trained on Mel and `dMel`.\n\n::: wraptable\nr7.5cm\n\n  -------------- ------------------------ ---------------- --------------------- ------------\n                  Reconstruction WER (%)                    Recognition WER (%)  \n                     **P-WaveGAN^1^**      **HifiGAN^2^**     **Seq2seq^3^**      **CTC^4^**\n  Ground-truth             2.02                                     \\-           \n  Mel                      2.13                 2.08                2.4              2.1\n  `dMel`                   2.23                 2.11                2.5              2.1\n  -------------- ------------------------ ---------------- --------------------- ------------\n\n^\\*^ [@yamamoto2020parallel]^1^, [@kong2020hifi]^2^, [@dong2018speech]^3^, [@graves2006connectionist]^4^ Configurations are detailed in Sec. [3](#sec:exp){reference-type=\"ref\" reference=\"sec:exp\"}.\n:::\n\nWe find that the word error rate (WER) of an ASR system run on the reconstructed waveforms, is quite similar to the WER of the same system run on the ground-truth audio, showing that `dMel` captures the acoustic information needed to reconstruct good waveforms. Similarly, we find that the WER of ASR models trained on Mel and `dMel` are similar, indicating that `dMel` are good at preserving semantic content that can easily be found by ASR models. This shows that discretizing Mel has limited impact on information content.\n\nBy operating on the log mel-filterbanks and preserving the frequency and intensity information (with some loss of resolution from discretization), `dMel`*inherently preserves both semantic and acoustic information in a unified representation*, without the need for separate tokenization or additional pretraining of a tokenization model. There are many advantages to discretizing log mel-filterbanks:\n\n-   Log mel-filterbanks is an interpretable representation of speech signal, where both the semantic and acoustic information is preserved. As discretization has little impact, `dMel` inherits the properties.\n\n-   `dMel` is a *model-free representation grounded in raw acoustic space*. As a result it can be converted to waveforms by any mel-filterbank vocoder, unlike other tokenization schemes that have feature representations that are intricately coupled to both the encoder and the decoder.\n\n-   Different channels of `dMel` do not have the complex hierarchical dependencies on each other that is typical of coarse-to-fine acoustic tokens; we find that they can be ***modeled independently*** in each frame using a simple decoder-only (LM-style) transformer architecture.\n\nThrough comprehensive evaluations, we show that using `dMel` allows us to use a single decoder-only model, and achieve high performance on both automatic speech recognition (ASR) and text-to-speech (TTS) tasks. The ASR task validates that `dMel` preserves semantic information, while the TTS task shows that `dMel` are useful for high-fidelity acoustic reconstruction of speech. We also compare `dMel` to other tokenization methods and find that `dMel` achieves the best WER for ASR task, which indicates that the semantic information is well preserved. Also, `dMel` achieves the lower WER score for TTS task when using WhisperX [@bain2022whisperx] for automatic evaluation and we find model trained with `dMel` can generate long and natural speech samples (see Supplementary Material).\n\n![Prior works on speech tokenization use either heavy self-supervised pretrained encoders [@baevski2020wav2vec; @hsu2021hubert] to extract semantic tokens (and train a separate decoder for it [@lakhotia2021generative]) or learn compression encoder-decoder models with residual vector quantizations [@zeghidour2021soundstream; @defossez2022high] to obtain acoustic tokens. By contrast we eliminate the encoder and simply discretize mel-filerbanks (`dMel`) to encode audio, and use a simple mel-filterbank vocoder [@yamamoto2020parallel] to reconstruct speech signals. ](figs/dmel.pdf){#fig:dmel width=\"80%\"}\n\n# Method\n\nIn this section, we first introduce our proposed `dMel` speech tokenization method, which discretizes log mel-filterbanks energies directly into bins. We then describe our unified LM-style transformer model for ASR and TTS tasks, which leverages `dMel` for speech tokenization. The model architecture is illustrated in Figure [2](#fig:arch){reference-type=\"ref\" reference=\"fig:arch\"}.\n\n## `dMel` Speech Tokenizer\n\nDifferent from existing VQ-VAE [@borsos2023audiolm; @zhang2023speechtokenizer; @kim2024clamtts; @zeghidour2021soundstream] based speech tokenizers, we propose a discretized log mel-filterbanks based speech tokenizer. The outline of the discretization method is shown in Figure [1](#fig:dmel){reference-type=\"ref\" reference=\"fig:dmel\"}. Later in the paper, we show that this tokenizer allows the model to process the input speech signal efficiently and capture the relevant acoustic features for both ASR and TTS tasks.\n\nWe denote tensors as $\\mathbf{X}$ while $\\mathbf{X}_{i, ...}$ denote the $(i, ...)$-th component of tensor $\\mathbf{X}$. First, the speech tokenizer takes the input speech signal $\\mathbf{x}$ and computes the log mel-filterbanks representation $\\mathbf{M}$: $$\\mathbf{M} = \\text{Mel}(\\mathbf{x}),$$ where $\\text{Mel}(\\cdot)$ represents the function that computes the log mel-filterbanks, $\\mathbf{M}\\in\\mathbb{R}^{T\\times N}$, $N$ is the number of log mel-filterbanks and $T$ is the number of frames in the spectrogram.\n\n#### Tokenization\n\nTo discretize the log mel-filterbanks representation $\\mathbf{M}$ into speech tokens, we adopt a codebook $\\mathbf{C}$. In this paper, we apply a simple linear discretization, so that the codebook $\\mathbf{C}\\in\\mathbb{R}^{2^K}$ and its values are evenly spaced in the range of the log mel-filterbanks values: $$m=\\min_{t, i}(\\mathbf{M}_{t, i}), \n   \\qquad\n   M=\\max_{t, i}(\\mathbf{M}_{t, i})\n   \\qquad\n   \\delta=\\frac{M - m}{2^K},$$ $$\\mathbf{C} = \\left[ m,\\, m + \\delta, \\, m + 2 \\delta,\\,  \\dots, \\, m + (2^K - 1) \\delta \\right].$$ In practice, we compute the minimum $m$ and maximum $M$ values of log mel-filterbanks across the entire dataset to define the codebook $\\mathbf{C}$. Then we map a magnitude $\\mathbf{M}_{t, i}$ of every frequency channel $i=1\\dots N$ for the time frame $t=1\\dots T$ into a bin index of the codebook $\\mathbf{C}$ in the following way: $$\\mathbf{S}_{t,i} = \\text{Discretize}(\\mathbf{M}_{t, i}) \n                = \\text{argmin}_j |\\mathbf{M}_{t, i} - \\mathbf{C}_j|$$ where $\\mathbf{S}\\in\\mathbf{B}^{T\\times N}$ represents the discretized log mel-filterbanks (`dMel`) with $\\mathbf{B}=\\{j|j=1,2,3, \\dots 2^K\\}$ and $\\mathbf{S}_{t}\\in\\mathbf{B}^{N}$ being the $t$-th speech token. As the codebook $\\mathbf{C}$ has $2^K$ distinct values and thus number of bins $|\\mathbf{B}|=2^K$, each speech token is represented by $N\\cdot K$ bits where every $K$ bits are used to represent one of $N$ frequency channels.\n\n#### Detokenization\n\nTo reconstruct the speech signal $\\mathbf{x}$ from the speech tokens $\\mathbf{S}$, we first transform bin indices back to the log mel-filterbanks representation via the codebook $\\mathbf{C}$: $$\\hat{\\mathbf{M}}_{t,i} = \\mathbf{C}_{\\mathbf{S}_{t,i}}.$$ Then, we apply a vocoder [@yamamoto2020parallel] to transform reconstructed log mel-filterbanks $\\hat{\\mathbf{M}}_{t,i}$ back into the time domain signal $\\mathbf{x}$. The vocoder is trained independently and is not part of the transformer decoder-based model.\n\n## Unified Speech-Text Transformer Decoder\n\n![Unified Speech-Text Transformer Decoder with speech tokens as `dMel`.](figs/decoder.pdf){#fig:arch width=\"88%\"}\n\nModeling speech and text sequences jointly is essential for a model to understand and generate both modalities. However, it is challenging to design a unified model that can handle both speech-to-text and text-to-speech effectively. In this work, we apply a unified LM-style transformer model that takes speech and text tokens as input and generates the output tokens in the target sequence. The model is trained in end-to-end on a combined dataset of speech and text pairs, enabling it to learn the joint representations for ASR and TTS tasks. As we show in the rest of the paper, the crucial part for the joint model training is the proper speech tokenization which `dMel` provides.\n\n#### Token Representation\n\nFor text data, we apply a character-level tokenizer to convert the input text into a sequence of text tokens. The text tokens are passed through an embedding layer, $\\text{Embed}(\\cdot): \\{j|j=1,2,3\\dots L\\} \\to \\mathbb{R}^{D}$, where $D$ is the embedding dimension and $L$ is the vocabulary size. The dimension of the speech token embedding is set to be the same as the text token embedding $D$ and no further mapping is required. The motivation for using a character-level tokenizer is to reduce the vocabulary size $L$ and improve the model's generalization ability. Also, character tokens can capture the fine-grained linguistic features that are essential for both ASR and TTS tasks.\n\nFor speech signal, we apply the `dMel` speech tokenizer to convert the input speech signal into a sequence of speech tokens. Then, the speech tokens $\\mathbf{S}\\in\\mathbf{B}^{T\\times N}$ are passed through a learnable embedding layer, $\\text{Embed}(\\cdot): \\mathbf{B} \\to \\mathbb{R}^{d}$, and a learnable linear layer, $\\text{Linear}(\\cdot): \\mathbb{R}^{N \\times d} \\to \\mathbb{R}^{D}$, to obtain the speech token representation $\\mathbf{E}\\in\\mathbb{R}^{T\\times D}$: $$\\begin{aligned}\n  \\mathbf{E}_{t}=\\text{Linear}(\\mathbf{E^{\\prime}}_{t}), \\, \\text{and}\\,\\,\\,\n\\mathbf{E^{\\prime}}_{t}=\\text{Concatenate}([\\text{Embed}(\\mathbf{S}_{t,1}), \\text{Embed}(\\mathbf{S}_{t,2}), \\ldots, \\text{Embed}(\\mathbf{S}_{t,N})]),\n\\end{aligned}$$ where $\\mathbf{E}_t\\in\\mathbb{R}^D$ is the speech token representation. Here, for every time frame $t$, a speech token $\\mathbf{S}_{t}$ is processed *in parallel and independently* for every frequency channel $i$ by $\\text{Embed}(\\mathbf{S}_{t,i})$ mapping, and then embeddings of all frequency channels are stacked together to form one vector representation $\\mathbf{E^{\\prime}}_{t}$ for the frame $t$. Finally, the speech token embeddings $\\mathbf{E}_t$ are fed into the LM-style transformer models for further processing.\n\nWe also implemented other popular speech tokenizers including HuBERT-KM [@lakhotia2021generative] and SpeechTokenizer [@zhang2023speechtokenizer] for comparison. The main difference among these speech tokenizers is the codebook size and codes dimension, shown in Table [1](#tab:tokenizer){reference-type=\"ref\" reference=\"tab:tokenizer\"}. For both HuBERT-KM and SpeechTokenizer the speech tokens are mapped via a learnable linear layer from their dimension to the text embedding dimension $D$ before feeding into the LM-style transformer model.\n\n::: {#tab:tokenizer}\n                    `dMel`   HuBERT-KM   SpeechTokenizer\n  ---------------- -------- ----------- -----------------\n  Codebook Size       16        200           1024\n  Code Dimension      80         1              8\n  Training-free?                        \n\n  : Comparison between different speech tokenizers: `dMel`(ours), HuBERT-LM and SpeechTokenizer. For `dMel` we use $N=80$ log mel-filterbanks and $2^K=16$ values of the codebook $\\mathbf{C}$.\n:::\n\n#### Speaker Representation\n\nTo properly model multi-speaker data, we also include speaker embeddings as input to the transformer decoder. The speaker embeddings are extracted from an independent dvector [@variani2014deep] model[^3]. We use a learnable linear layer to map the speaker embeddings to the same dimension as the speech and text token embeddings $D$. The speaker representation is optional for ASR task, but required for TTS task. Hence, during the training, it is applied for text-to-speech and ignored for speech-to-text.\n\n#### Transformer Decoder\n\nThe transformer decoder is trained end-to-end on a combined dataset of speech and text pairs. For TTS training, the input sequence is constructed by concatenating the speaker embedding (extracted from a random audio for the same speaker of the current sample), text tokens, and speech tokens. For ASR training, the input sequence is constructed by concatenating the speech tokens and text tokens. Both tasks are trained with causal masking, where the model is trained to predict the next token based on the previous tokens. The loss is calculated using the cross-entropy loss between the predicted tokens and the ground-truth tokens. Loss calculation is skipped on the speech tokens for ASR task and on the text tokens for TTS task. ***Note, that all frequency channels at time frame $t$ for `dMel` tokenizer are predicted independently and in parallel.***\n\nTo capture the relative distances between tokens in the input sequence, we apply multiplicative relative positional embedding RoPE [@su2024roformer]. This allows the model to learn the positional relationships between speech tokens, text tokens, and speaker embeddings, enhancing its ability to generate coherent output sequences. For positional embeddings we do not distinguish between text, speech and speaker tokens and thus having global positions notation across all of them, see Figure [2](#fig:arch){reference-type=\"ref\" reference=\"fig:arch\"}.\n\n#### Robust Training\n\nCompared to LMs, audio frames are highly redundant with strong local correlations. This makes longform generation difficult for models due to exposure bias [@scheduled_sampling]. To mitigate exposure bias during training, we apply span-masking [@raffel2020exploring] to the speech token context, masking out multiple random spans of speech frames. The model is trained to predict the next token based on the masked context. This context-masking strategy helps the model learn to generate accurate speech tokens in the presence of missing information, improving its robustness and generalization. It forces the model to attend to the text rather than copying previously inferred speech tokens due to learnt correlations. We also find that span-masking text tokens improves the ASR task.\n\n# Experiments {#sec:exp}\n\nIn this section, we begin by evaluating different speech tokenizers through a common practice in the literature: tokenizing speech into discrete units and then reconstructing the speech to assess the quality of the reconstruction. This approach helps gauge the effectiveness of various tokenization techniques. Following this, we present both TTS and ASR results using an LM-style (decoder-only) model with different speech tokens. While most related work focuses solely on speech synthesis, our study encompasses both speech generation and recognition, providing a more comprehensive evaluation of the tokenization methods. We evaluate the performance of our model mainly on the LibriSpeech dataset and compare it with state-of-the-art speech tokenizers, ASR and TTS models.\n\n## Training Data\n\nWe use several open-sourced datasets with paired speech and text transcription to conduct experiments: i) LibriSpeech [@panayotov2015librispeech] dataset (CC BY 4.0) consists of English speech recordings (960h, 16kHz) from various speakers ($\\sim$`<!-- -->`{=html}2k) and conditions; ii) LibriTTS [@zen2019libritts] (CC BY 4.0) dataset (500h) derived from LibriSpeech improves on it with the proper sentence split, text normalization and keeping samples 24kHz; iii) VCTK [@vctk2019] contains 44h of English speech (108 speakers); iv) LJSpeech [@ljspeech17] (public domain in US) is a single speaker English audio recordings of 16kHz with read speech from LibriVox[^4]. While LibriSpeech is used to train ASR and TTS models, LibriTTS, VCTK and LJSpeech are only used to train the TTS.\n\n## Training Configuration\n\nWe train the LM-style transformers in three different sizes: Small, Base, and Large (see Appendix Table [10](#tab:model-size){reference-type=\"ref\" reference=\"tab:model-size\"}). Unless stated otherwise, the Base model is used in all experiments if not stated otherwise. All models use pre-LayerNorm with dropout set to 0.1 for residual, attention and embedding layers and 0.3 for positional embedding. `dMel` uses 16 discrete bins for each channel while text is tokenized with a character vocabulary; the speaker embedding dvector has 512 dimensions (see Appendx [10](#app:training-details){reference-type=\"ref\" reference=\"app:training-details\"} for details). In all experiments, training data are sampled to 16kHz.\n\n## Main Results\n\n### Speech Reconstruction\n\nFollowing [@zhang2023speechtokenizer], we randomly sample 300 speech utterances and their ground truth transcriptions from the LibriSpeech *test-clean* dataset. We use the speech2unit and unit2speech modules to convert the speech signal to speech tokens and then reconstruct the speech signal from the speech tokens. We compute the WER between the ASR outputs from HuBERT-Large [@hsu2021hubert][^5] on the audio samples and their ground truth transcripts. We also report MOS-LQO (Mean Opinion Score -- Listening Quality Objective) score to measure the reconstruction quality using ViSQOL [@hines2012visqol]. Finally, we use human evaluation to measure the naturalness of the reconstructed speech using a MOS score with 95% confidence interval. We instruct the human evaluators to rate the naturalness of the reconstructed speech on a scale of 1 to 5, where 1 is the worst and 5 is the best. The results are shown in Table [2](#tab:reconstruction){reference-type=\"ref\" reference=\"tab:reconstruction\"}.\n\n::: {#tab:reconstruction}\n  **Tokenizer**                         **WER↓**   **MOS-LQO↑**  \n  ----------------- ----- ----- ------ ---------- -------------- --------------------------------\n  GroundTruth          \\-    \\-   \\-      2.02          \\-        3.91$\\pm$`<!-- -->`{=html}0.12\n  HuBERT-KM            95   111  50Hz     8.71         2.06       2.74$\\pm$`<!-- -->`{=html}0.14\n  EnCodec               7     7  75Hz     2.03         4.03       3.69$\\pm$`<!-- -->`{=html}0.13\n  SpeechTokenizer      65    34  50Hz     2.41         4.19       3.77$\\pm$`<!-- -->`{=html}0.13\n  Mel-HifiGAN         n/a    12  80Hz     2.08         4.52       3.80$\\pm$`<!-- -->`{=html}0.12\n  `dMel`-HifiGAN      n/a    12  80Hz     2.11         4.47       3.68$\\pm$`<!-- -->`{=html}0.13\n  Mel-PWG             n/a     1  80Hz     2.13         4.40       3.27$\\pm$`<!-- -->`{=html}0.14\n  `dMel`-PWG          n/a     1  80Hz     2.23         4.37       3.23$\\pm$`<!-- -->`{=html}0.14\n  Mel-PWG             n/a     1  40Hz     2.36         4.34       2.99$\\pm$`<!-- -->`{=html}0.15\n  `dMel`-PWG          n/a     1  40Hz     2.51         4.29       2.97$\\pm$`<!-- -->`{=html}0.15\n\n  : Speech reconstruction results on 300 random samples from LibriSpeech *test-clean* set.\n:::\n\nFrom Table [2](#tab:reconstruction){reference-type=\"ref\" reference=\"tab:reconstruction\"}, we can see that semantic tokenization (HuBERT-KM) is not good for speech reconstruction. Meanwhile, acoustic tokenizers that are optimized to reconstruct the signal directly (EnCodec and SpeechTokenizer) do well.\n\nWe apply different vocoders to reconstruct the speech signal from log mel-filterbanks, and find that the WER of the reconstructed speech signal is comparable to the acoustic tokenization methods with a fraction of the parameters. Also, log mel-filterbanks achieve a better MOS-LQO score, which indicates that the reconstructed audio is more similar to the original audio. By comparing Mel and `dMel`, we can see that discretization has little impact on WER and MOS-LQO scores. We also find that the exact vocoder matters much less than the frame rate of tokenization: the WER goes from 2.08 to 2.13 when switching from HifiGAN to ParallelWaveGAN, but it falls from 2.13 to 2.36 when the frame rate is changed from 80Hz to 40Hz. However, even a 1M parameter vocoder operating at a 40Hz frame rate is comparable to the much larger SpeechTokenizer on WER and MOS-LQO metrics.\n\n*Considering the efficiency and performance, we choose the `dMel` speech tokenizer in 40Hz with ParallelWaveGAN vocoder for the following experiments.*\n\n### LM-Style Text-to-Speech\n\nHere we compare the accuracy and naturalness of speech synthesized by LM-style text-to-speech (TTS) models trained on different tokenization methods. For TTS evaluation, we utilize WhisperX [@bain2022whisperx] (\"base.en\" from [@radford2023robust]) to transcribe our generated speech into text and calculate the WER and the character error rate (CER). We report both WER and CER to facilitate comparisons to prior works which have reported only one or the other.\n\nWe trained the TTS model using the same architecture but with three different tokenization methods: HuBERT+KM (with 200 clusters), SpeechTokenizer, and `dMel`. Additionally, we present the results from VOXTLM [@maiti2024voxtlm] and USLM [@zhang2023speechtokenizer] for comparison. VOXTLM is a larger model trained on more data that is initialized from a pretrained LLM (OPT) using HuBERT-KM as the speech tokenizer. USLM comprises an autoregressive (AR) model and a non-autoregressive (NAR) model, both trained with the SpeechTokenizer.\n\nAs shown in Table [3](#tab:main-tts){reference-type=\"ref\" reference=\"tab:main-tts\"} for training on LibriSpeech dataset, our LM-style model with `dMel` tokenization achieves a WER of 4.3 and a CER of 1.8, significantly outperforming the baseline methods. This indicates that our model can generate more accurate speech with less hallucination and distortion. Furthermore, we observed that the AR model trained on SpeechTokenizer tokens exhibits a much higher WER compared to the idiosyncratic coarse to fine models (labeled AR+NAR) developed for these residual tokenizers -- indicating that `dMel` lies on a simpler data manifold.\n\nGiven the success of our LM-style `dMel` TTS model, dubbed `RichTTS`, we further evaluate it on various datasets, including LJSpeech, VCTK, and LibriTTS, and compare it with popular open-sourced TTS models, including Tacotron2 [@shen2018natural], FastSpeech2 [@ren2020fastspeech], and VITS [@kim2021conditional]. We conduct human evaluation to measure the naturalness of 50 randomly sampled synthesized speech from VCTK test set. `RichTTS` achieves competitive performance on the TTS task in terms of both MOS and WER demonstrating its effectiveness in generating high-quality synthesized speech, see Table [4](#tab:tts){reference-type=\"ref\" reference=\"tab:tts\"}. Interestingly, we find that VITS performs poorly on the VCTK WER. We suspect this is because VITS tends to make more mistakes at the beginning of each sequence, and since VCTK comprises short sequences, even one or two word errors can lead to a high WER.\n\n::: {#tab:main-tts}\n  **Model**                                                       **WER↓** (%)   **CER↓** (%)  **Params**\n  ------------------------------------------------------------- -------------- -------------- ------------\n  VOXTLM (HuBERT+KM), [@maiti2024voxtlm]                                    \\-            3.5     350M\n  USLM (SpeechTokenizer), AR+NAR, [@zhang2023speechtokenizer]              6.5             \\-     356M\n  `RichTTS`(HuBERT+KM)                                                     9.5            4.3     258M\n  `RichTTS`(SpeechTokenizer), AR                                          11.4            5.9     258M\n  `RichTTS`(`dMel`)                                                    **4.3**        **1.8**     258M\n\n  : Text-to-speech results for different tokenizers. `RichTTS` is trained on LibriSpeech 960h. WER (%) and CER (%) are evaluated with WhisperX ASR (\"base.en\") and reported on *test-clean*.\n:::\n\n::: {#tab:tts}\n                                        **WER↓ (%)**                           \n  ----------------------------------- -------------- -------------- ---------- ------------------------------------\n  2-4 (l)5-5 **Model**                  **LJSpeech**   **LibriTTS**   **VCTK**               **VCTK**\n  GroundTruth                                    2.6            3.8        3.4    4.18$\\pm$`<!-- -->`{=html}0.10\n  Tacotron2, [@shen2018natural]                  4.4            7.3        4.2    2.91$\\pm$`<!-- -->`{=html}0.15\n  FastSpeech2, [@ren2020fastspeech]              6.1           10.2        3.8    3.03$\\pm$`<!-- -->`{=html}0.14\n  VITS, [@casanova2022yourtts]                   6.4            8.3       11.1  **3.56$\\pm$`<!-- -->`{=html}0.12**\n  `RichTTS`(`dMel`)                          **4.0**        **4.5**    **2.2**    3.34$\\pm$`<!-- -->`{=html}0.14\n\n  : WER (%) (evaluated with WhisperX ASR \"base.en\") and MOS of different TTS models' generations using transcriptions from each evaluation set that correponds to data used for training.\n:::\n\n::: {#tab:tts-length}\n  **Sequence Length**     **Tacotron2**   **FastSpeech2**   **VITS**   **`RichTTS`**\n  --------------------- --------------- ----------------- ---------- ---------------\n  Total WER↓ (%)                    4.4               6.1        6.4         **4.0**\n  10-20 words                       5.5           **3.1**        7.4             3.5\n  20+ words                         3.3               9.1        5.3         **3.0**\n\n  : results for TTS models trained on LibriSpeech 960h and evaluated on LJSpeech test set.\n:::\n\nFurthermore, we observed that our model with `dMel` tokenization can generate long audio sequences with high quality. Here, we evaluate the performance of our model on different lengths of text sequences using the LJSpeech test set. Table [5](#tab:tts-length){reference-type=\"ref\" reference=\"tab:tts-length\"} shows the WER results for our model on text sequences with 10-20 words and more than 20 words. We ignore text sequences with fewer than 10 words, as they are too short and not robust for WER evaluation. From Table [5](#tab:tts-length){reference-type=\"ref\" reference=\"tab:tts-length\"}, we observe that our model achieves competitive performance across different text lengths, demonstrating its robustness and generalization ability in generating synthesized speech for varying text inputs lengths. Additionally, we find that the non-autoregressive (NAR) model FastSpeech2 achieves the lowest WER on shorter sequences but the highest WER on longer sequences. This suggests that NAR models may not be well-suited for generating long audio sequences.\n\n### LM-Style Speech-to-Text\n\n::: {#tab:main-asr}\n  **Model**                                        **dev-clean↓**                      **dev-other↓**                    **test-clean↓**                     **test-other↓** **Params**\n  ---------------------------- ---------------------------------- ----------------------------------- ---------------------------------- ----------------------------------- ------------\n  `RichASR`(SpeechTokenizer)         6.5$\\pm$`<!-- -->`{=html}0.3       16.9$\\pm$`<!-- -->`{=html}0.7       6.9$\\pm$`<!-- -->`{=html}0.4       17.5$\\pm$`<!-- -->`{=html}0.5 258M\n  `RichASR`(HuBERT+KM)               5.3$\\pm$`<!-- -->`{=html}0.1       13.7$\\pm$`<!-- -->`{=html}0.2       5.8$\\pm$`<!-- -->`{=html}0.1       13.8$\\pm$`<!-- -->`{=html}0.1 258M\n  `RichASR`(`dMel`)              **3.8**$\\pm$`<!-- -->`{=html}0.1   **10.3**$\\pm$`<!-- -->`{=html}0.1   **4.2**$\\pm$`<!-- -->`{=html}0.2   **10.4**$\\pm$`<!-- -->`{=html}0.1 258M\n\n  : Speech recognition results for different tokenizers measured with WER (%). All models are trained on LibriSpeech 960h.\n:::\n\n::: {#tab:main-asr-sota}\n  **Model**            **Data** (h)    **dev-clean↓**   **dev-other↓**   **test-clean↓**   **test-other↓**  **Params**\n  ------------------- -------------- ---------------- ---------------- ----------------- ----------------- ------------\n  VOXTLM                   280k                    \\-               \\-               6.5              17.6     350M\n  VOXTLM                   280k                    \\-               \\-               4.6              12.1     1.3B\n  [@chen2024loss]          960                    3.6          **7.8**               3.8           **8.3**     355M\n  `RichASR`(`dMel`)        960                **3.1**              8.4           **3.4**               8.6     355M\n\n  : Comparison of WER (%) for best `RichASR` trained with `dMel` tokenization and prior work with LM-style ASR models and HuBERT+KM with subword modeling on top as tokenization.\n:::\n\nTraining an LM-style speech-to-text (ASR) model can test if the speech tokens can preserve the semantic information in the speech signal and support the speech content-based task. Table [6](#tab:main-asr){reference-type=\"ref\" reference=\"tab:main-asr\"} shows results of our model dubbed `RichASR`, trained with different tokenizations including `dMel` for the ASR task. Our LM-style model with `dMel` speech tokenization achieves 4.2% WER on the *test-clean* and 10.4% WER on the *test-other* sets outperforming both HuBERT-KM and SpeechTokenizer. We also observe that our model with HuBERT-KM [@lakhotia2021generative] outperforms the SpeechTokenizer [@zhang2023speechtokenizer] for ASR, which is reasonable as semantic tokens are more suitable for the ASR task.\n\nIn Table [7](#tab:main-asr-sota){reference-type=\"ref\" reference=\"tab:main-asr-sota\"}, we further compare `RichASR` with `dMel` speech tokenizer trained with GPT-2-meduim architecture [@radford2019language] on LibriSpeech 960h with prior work: VOXTLM [@maiti2024voxtlm] that uses larger model trained with more data and initialized from a pretrained LLM (OPT [@zhang2022opt]), and HuBERT-KM with additional subword modeling on top as the speech tokenizer; [@chen2024loss] that also uses GPT-2 architecture trained on LibriSpeech 960h and HuBERT-KM with additional subword modeling on top as the speech tokenizer[^6]. `RichASR` with `dMel` outperforms VOXTLM; it also outperforms [@chen2024loss] on clean sets and a bit behind it on other sets.\n\nThe ASR results clearly demonstrate the benefit of using our `dMel` speech tokenizer for the content-related tasks in speech, as it better preserves the semantic information in the speech signal. Further details and ablations can be found in Appendix [10](#app:training-details){reference-type=\"ref\" reference=\"app:training-details\"} and [11](#app:ablations){reference-type=\"ref\" reference=\"app:ablations\"}.\n\n### Ablations\n\nWe first investigate the impact of the codebook sizes, shown in Table [8](#tab:nbins-ablation){reference-type=\"ref\" reference=\"tab:nbins-ablation\"}. The 16-bin configuration used in the paper demonstrates the best overall performance across tasks. While the 32-bin setup slightly outperforms on the ASR *test-other* set, it shows degraded performance in TTS. This trade-off likely stems from the increased speech vocabulary size, which may pose challenges for accurate prediction. The results may get better with increased data and model size. And 8-bin configuration looses too much information with discretization.\n\nWe then ablate the ASR results to understand why ASR LM-style model is behind the state-of-the-art ASR results on LibriSpeech. We take two existing transformer ASR baselines, Seq2Seq and CTC, that use 80 log mel-filterbanks and characters as targets. We then modify these baselines by using `dMel` instead (the discretization, embedding layer and linear layer) while keeping all other hyper-parameters the same (we adjust only the SpecAugment time masking max width accordingly to keep total masking in *ms* the same). Our results (Table [9](#tab:asr-ablation){reference-type=\"ref\" reference=\"tab:asr-ablation\"}) suggest: i) `dMel` brings only small degradation compared to Mel; ii) additional discrepancy is coming from different hop distance in featurization; iii) ***the main and significant performance degradation is coming from switching to LM-style model.*** The latter is in line with [@maiti2024voxtlm] and [@chen2024loss], though was not discussed in detail by any prior work. We hypothesise this gap is due to observed overfitting of the LM-style models.\n\n::: {#tab:nbins-ablation}\n   **N-Bins**   **ASR test-clean↓**   **ASR test-other↓**   **TTS WER↓ (%)**\n  ------------ --------------------- --------------------- ------------------\n       8                6.6                  16.5                 7.3\n       16               4.4                  10.7                 4.8\n       32               4.7                  10.2                 5.7\n\n  : ASR and TTS results (WER, %) with `dMel` speech tokenizer and different number of bins (codebook size) for discretization in `dMel`. All models are trained on LibriSpeech 960h.\n:::\n\n::: {#tab:asr-ablation}\n  **Model**                                          **Features**  **dev-clean↓**   **dev-other↓**  \n  ------------------------------------------------ -------------- ---------------- ---------------- --\n  [@gulati2020conformer] (RNN-T -- Conformer)            Mel-10ms       1.9              4.1        \n  [@kim2022squeezeformer] (CTC -- Squeezeformer)         Mel-10ms       2.3              5.8        \n  Seq2Seq [@dong2018speech]                              Mel-10ms       2.4              5.4        \n                                                      `dMel`-10ms       2.5              5.9        \n                                                         Mel-25ms       2.8              6.5        \n                                                      `dMel`-25ms       2.7              6.2        \n  CTC [@graves2006connectionist]                         Mel-10ms       2.1              5.4        \n                                                      `dMel`-10ms       2.1              5.6        \n                                                         Mel-25ms       2.1              5.4        \n                                                      `dMel`-25ms       2.3              6.1        \n  LM-style                                            `dMel`-25ms       3.4              9.5        \n\n  : WER (%) comparison for CTC, Seq2Seq, and LM-style ASR models ($\\sim$`<!-- -->`{=html}260M) trained on LibriSpeech 960h with `dMel` and Mel features. We compute 80 log mel-filterbanks with 25ms (50ms) window and 10ms (25ms) hop distance, denoted as '10ms' ('25ms').\n:::\n\n### Unlocking Joint Speech-Text Modeling\n\nOur model design allows us to train a single model for both ASR and TTS tasks leading to a simpler setup. We train a single model with the same architecture and tokenization as `RichTTS`, by constructing the training data with \\<text, speech\\> and \\<speech, text\\> pairs for ASR and TTS tasks, respectively. By mixing these two types of data, we can train a single model for both tasks.\n\nTable [\\[tab:joint-asr\\]](#tab:joint-asr){reference-type=\"ref\" reference=\"tab:joint-asr\"} shows that the joint model is worse on both tasks, but ASR is affected more than TTS. Comparing our results to VOXTLM, which initializes its model from pretrained LLM (OPT) and finetunes it with multiple tasks and datasets, we speculate that our joint model needs text-only training to learn a good LM for better ASR performance. Our model structure trivially allows for this text-only training, but we leave those experiments for future work (for further discussion see Appendix [11.3](#app:joint){reference-type=\"ref\" reference=\"app:joint\"}).\n\n# Related Work\n\n#### Speech Tokenization\n\nRecent advancements in speech tokenization have primarily focused on two approaches: semantic tokens and acoustic tokens. This section examines these methods, their combinations, and their limitations, highlighting the need for more efficient and generalizable solutions. Semantic tokens, extracted from self-supervised pretrained speech models, have shown promise in capturing high-level content information. Methods like wav2vec [@baevski2020wav2vec] and HuBERT [@hsu2021hubert] employ $k$-means clustering on speech representations to generate these tokens. While effective in capturing semantic content, these approaches often struggle with preserving fine-grained acoustic details crucial for high-quality speech synthesis. In contrast, acoustic tokens, derived from pretrained audio compression models, excel at preserving low-level acoustic information. Techniques such as SoundStream [@zeghidour2021soundstream] and EnCodec [@defossez2022high] utilize residual vector quantization (RVQ) with reconstruction objectives. These methods achieve high-quality audio compression but may not capture higher-level semantic structures effectively.\n\nRecognizing the complementary nature of semantic and acoustic tokens, recent works have attempted to combine these approaches. AudioLM [@borsos2023audiolm] introduced a three-stage model: semantic modeling, coarse acoustic modeling, and fine acoustic modeling. While comprehensive, this approach introduces complexity and computational overhead. AudioPalm [@rubenstein2023audiopalm] further demonstrated the critical importance of large-scale training data and model parameters for effective multi-stage modeling, highlighting potential generalization issues in low-resource scenarios. An alternative hybrid approach, proposed by @zhang2023speechtokenizer, attempts to distill semantic information into acoustic tokens during RVQ model training. However, this method still requires additional pretraining and does not fully achieve a single-stage model architecture.\n\nDespite these advancements, several challenges persist in the field of speech tokenization: i) balancing semantic and acoustic information in a unified representation; ii) reducing model complexity and computational requirements; iii) improving generalization to low-resource / out-of-domain data e.g. with mixed speech from multiple speakers, or multiple languages, or changing characteristics of recording equipment/sampling rate etc.; iv) developing truly single-stage tokenizers. Our proposed method, `dMel`, addresses these challenges by offering a training-free speech tokenization approach. By directly discretizing log mel-filterbanks into bins, it inherently preserves both semantic and acoustic information in a unified representation, while significantly reducing computational complexity.\n\n#### Speech-Text Modeling\n\nModeling speech and text jointly is a challenging task, as speech signals are continuous and while text is discrete. Existing works have explored various approaches to address this challenge, including usage of separate encoders for different modalities [@ao2021speecht5; @bapna2021slam]. @bai20223 proposed an encoder-only model A3T for speech-text modeling, by introducing alignment embedding to encourage cross-modal transfer between text and speech. Although A3T achieved good performance on speech synthesis and editing tasks, it cannot generate text and cannot generalize to longform generation because of its encoder-only architecture and mask-reconstruction training strategy. VioLA [@wang2023viola] also targets a unified speech-text model which can generate speech and text with a single model, but it is specifically designed for the Encodec [@defossez2022high] style feature, and compelled to model speech tokens in a multi-stage hierarchical manner. @maiti2024voxtlm proposed a LM-style model VOXTLM, to model speech and text jointly. However, VOXTLM is only models the HuBERT semantic tokens, and relies on an external generation model to transform semantic tokens into waveform, but the speaker and acoustic information are lost. In comparison, the model architecture in this paper is a simple, single stage LM-style transformer model, and can handle both the speech generation and text generation tasks.\n\n# Conclusion\n\nIn this work, we proposed `dMel`, a novel train-free speech tokenization method that discretizes log mel-filterbank energies directly into bins. By operating on the authentic log mel-filterbank representation, `dMel` inherently preserves both semantic and acoustic information in a unified tokenized representation. Our key contribution is the evaluation of `dMel` within a unified LM-style transformer architecture for speech recognition (ASR) and speech synthesis (TTS) tasks. Our `dMel`-based ASR model, `RichASR`, achieved the lowest word error rate among tokenization methods, robustly preserving semantic content. For TTS, `dMel`'s generation yielded the lowest WER, accurately reconstructing speech waveforms. Our `dMel`-based TTS model, `RichTTS`, achieved competitive naturalness, lowest error rates, and long audio generation capabilities.\n\n`dMel`'s simplicity circumvents separate tokenizers or multi-stage modeling, reducing computational overhead and dependence on pretrained models. By unifying semantic and acoustic modeling, `dMel` enables efficient speech-text modeling frameworks. While initial joint TTS-ASR training showed promise, further work is needed. Our primary contribution demonstrates `dMel`'s effectiveness for high-performing separate TTS and ASR models within a unified LM-style architecture.\n\n# Ethics Statement {#ethic}\n\nThe development and deployment of speech technologies carry important ethical considerations. While our proposed `dMel` method aims to advance the state-of-the-art in speech-text modeling, it is crucial to highlight potential ethical risks and raise the awareness so that new methods may be developed to mitigate these risks.\n\nOur first main concern is the potential dual-use of speech synthesis technologies for nefarious purposes such as impersonation, misleading audio-visual content generation, or voice spoofing attacks. Proactive measures, including watermarking techniques and robust speaker verification methods, should be explored to counter such risks. The former attempts to build markers into the generated speech that make it easy to detect, while the latter focusses on distinguishing synthetic from real data. Prior work [@le2023voicebox] has shown that neural networks can be trained to distinguish speech synthesized from their model from real speech, probably because of artifacts from the use of mel spectral vocoders. While we did not train a network to do so in our work yet (we will create one before code release), the vocoders we use are similar to their work -- going from mel spectrogram to raw waveforms. Our model also does not use prosody, phoneme duration and other predictions that more sophisticated TTS systems use to allow the model to perform very well on imitating speaker styles in zero-shot settings. However our model can probably mimic the styles of training speakers very well. It is our hope that releasing our methods will facilitate more research on fake speech verification and watermarking techniques -- even if current classifiers are able to perform this detection, the quality of the generative models is improving. It is also our hope that future works will attempt to perform more credit assignment -- by providing metrics that show which real data samples a synthetic speech example copies its style and substance from.\n\nAnother concern is the perpetuation of societal biases encoded within training data. Speech datasets may exhibit biases along dimensions such as gender, race, age, or socioeconomic status, which could be propagated or amplified by trained models. Rigorous debiasing techniques and careful curation of representative training data are essential to mitigate these risks. On the mitigating side of this equation, we also hope that with better, more controllable TTS systems, ASR systems can improve because more data can be generated for underrepresented segments of the distribution from the TTS models.\n\nFurthermore, the development and deployment of speech technologies should prioritize accessibility and inclusivity. Models should be evaluated for performance across diverse demographics, accents, and language varieties to ensure equitable access and quality of service.\n\nFinally, it is important to foster transparency and accountability in the research and development process. Clear documentation of model capabilities, limitations, and potential failure modes should be provided to enable informed decision-making and responsible usage.\n\nAddressing these ethical considerations requires a multistakeholder approach involving researchers, developers, policymakers, and end-users. By prioritizing ethical principles such as fairness, privacy, and accountability, we can work towards realizing the benefits of speech technologies while mitigating potential risks and adverse societal impacts.\n\n# Limitations {#app:limitations}\n\nBecause TTS work is tremendously fragmented and clear protocols are not often available for training and evaluation, we reimplemented other tokenizers within our code base using publicly available, official implementations where available: e.g. we used Hubert-KM and speech tokenizer features extraction from the public codebases and pluged them into our LM-style model training. While we made the best effort to tune the tokenization methods and the models, there is always a possibility we missed some details. However, our results seem to tell a consistent story when viewed from multiple angles, and when viewed on multiple datasets. We also did not train on larger model sizes (\\>1B parameters), larger datasets (\\>1k hours), or using pretrained models.\n\nThe real challenge for modern multimodal LLMs is complex semantic understanding tasks. While our current experiments focus on text-to-speech and speech-to-text tasks, these encompass critical aspects of speech processing. `dMel`'s effective performance within a decoder-only architecture for both tasks suggests potential for broader applications. We recognize the importance of more sophisticated speech understanding tasks and view our work as a foundation for future research leaving other tasks out of scope of the paper. Scaling up pretraining and exploring complex semantic understanding tasks could further validate our approach's versatility across a wider range of multimodal language processing challenges.\n\nWe acknowledge that our current scope targets only speech on purpose, as indicated in our title. While `dMel` may potentially support non-speech tasks, ***our current exploration and verification focus solely on speech, not general audio.*** Regarding the \"speaker variations\" -- mel-spectrogram is used for speaker recognition widely, thus it preserves necessary speaker information on which we thus rely in `dMel` too.\n\n# Data, Code, Reproducibility {#app:repro}\n\nWe made the best effort to use publicly available data and official implementations of prior works where it is possible. All data we used are under permissive license for research. We provided as much as detail as is possible without code such as details on our model training and hyperparameters throughout the paper and in the Appendix. We plan to open-source our code upon paper acceptance.\n\nWe do not plan to open-source any pre-trained models for sake of privacy, safety and misuse.\n\n# Subjective Evaluation for TTS {#subjective_evals}\n\nWe use crowd-sourcing to collect subjective ratings to compare the naturalness of the reconstructed speech from the different tokenizers. We evaluate the quality of the same (randomly sampled) 50 utterances for each model by collecting around seven ratings per sample. Overall, we collect 3500 ratings from 65 raters. The raters were English-speaking and were paid at least the minimum wage.\n\nWe present the raters with a generated speech sample and instruct them to rate how natural it sounds on a five-point Likert scale, where 1 corresponds to very unnatural and 5 corresponds to very natural. Figure [3](#fig:crowd_screenshot){reference-type=\"ref\" reference=\"fig:crowd_screenshot\"} shows a screenshot of our subjective test as seen by the rater.\n\n![A screenshot of the assessment task, as the crowd-sourced rater sees it.](figs/crowdsource_screenshot.png){#fig:crowd_screenshot width=\"80%\"}\n\nWe noticed human annotators have bias over audio volume so we do volume normalization on top of all reconstructed or generated audio before giving them to human annotators.\n\nWe report Mean Opinion Score (MOS) results throughout the paper with confidence intervals calculated using bootstrap resampling with 1000 iterations, providing a reliable estimate of the variability MOS results.\n\n# Training Details {#app:training-details}\n\n## Baselines\n\nFor reproducibility, we provide the HuggingFace model cards used in our experiments in Table [4](#tab:tts){reference-type=\"ref\" reference=\"tab:tts\"}:\n\n-   Tacotron2 [@shen2018natural], <https://huggingface.co/espnet/espnet/kan-bayashi_vctk_tts_train_xvector_tacotron2_raw_phn_tacotron_g2p_en_no_space_train.loss.ave>\n\n-   FastSpeech2 [@ren2020fastspeech], <https://huggingface.co/espnet/kan-bayashi_vctk_gst_fastspeech2>\n\n-   VITS [@casanova2022yourtts], <https://huggingface.co/espnet/kan-bayashi_vctk_multi_spk_vits>\n\n## `RichASR` and `RichTTS`\n\nFor our LM-style model we stack together speaker embedding, speech tokens and text tokens. Both speech and text tokens have prepended begin of sentence token (\\<bos\\>) and appended end of sentence token (\\<eos\\>).\n\nWe train all models using the Adam optimizer with a learning rate of 1e-3, learning rate warmup of 4k steps for ASR and 5k for TTS, cosine learning rate schedule and gradient clipping of 1.0 for TTS and 0.1 for ASR and joint models. We use dynamic batching to optimize the data packing with total batch size of 1.4h/1.4h/0.7h for ASR training and 1h/2h/2h for TTS training for Small/Base/Large models. We train TTS models for 100k steps and ASR models 80k steps with mixed precision training and BF16 on A100 and H100 GPUs with 80GB. Both ASR models and TTS models are trained with 8GPUs for less than a day and for 2-4 days for ASR and TTS respectively.\n\n::: {#tab:model-size}\n                               **Small**   **Base**   **Large**\n  -------------------------- ----------- ---------- -----------\n  \\# of layers                        18         36          48\n  \\# of attention heads                2          4           8\n  \\# of hidden units ($D$)           512        768        1536\n  \\# of parameters                   59M       258M        1.3B\n\n  : LM-style transformer model configurations for ASR, TTS and joint models training.\n:::\n\n## LM-Style Speech-to-Text\n\nFor ASR training as an augmentation we apply SpecAugment [@DBLP:conf/interspeech/ParkCZCZCL19] with 2 frequency masks with max width 30 and 10 time masks with max width 50 and ratio 0.1. With ablations we found that SpecAugment masking with average value instead of zero is slightly better. Without applying SpecAugment performance of ASR is 7.3% WER on *dev-clean* and 20.3% WER on *dev-other*, which is further can be improved with usage of frequency masking only to 6.4% WER on *dev-clean* and 16.6% WER on *dev-other*. Usage of both frequency masking and time masking results in the best performance of Table [6](#tab:main-asr){reference-type=\"ref\" reference=\"tab:main-asr\"}.\n\nWe found that span masking is key part of model training to enforce slef-attention to attend to speech part as well as to reduce exposure bias. The masking strategy is similar to the one used for TTS training: for every training step with probability $p$ the sample in the minibatch is masked with the mean span of 3 tokens with masking ration of 0.5. We found that the mean span of 1 token or 5 tokens gives the same results; while the mask probability $p$ is the most important hyper-parameter. The optimimal value for ASR is found to be 0.8, which is used in all final models.\n\nAs we found one best model configuration for the Base model with `dMel` we then change only i) model size ii) speech tokenization iii) training data (here we increase model dropout to 0.3 for training on *train-clean-360* and to 0.5 for training on *train-clean-100* as otherwise models drastically overfit); the rest of hyper-parameters stay the same.\n\n# Ablations {#app:ablations}\n\n## LM-Style Text-to-Speech\n\nScaling results for `RichTTS` are shown in Table [11](#tab:abl-tts){reference-type=\"ref\" reference=\"tab:abl-tts\"}.\n\n::: {#tab:abl-tts}\n                               **WER↓ (%)** \n  -------------------------- -------------- --\n  `RichTTS`(`dMel`), Small              8.1 \n  `RichTTS`(`dMel`), Base               4.3 \n  `RichTTS`(`dMel`), Large              5.4 \n\n  : Text-to-speech results for different model sizes with `dMel`. All models are trained on LibriSpeech 960h dataset. Evaluation is done via speech generation on the full *test-clean* transcriptions and speakers, and then evaluated WER with WhisperX base.en.\n:::\n\n## LM-Style Speech-to-Text\n\nASR ablations for different model sizes, data sizes, and tokenizers are shown in Table [\\[tab:extended-asr\\]](#tab:extended-asr){reference-type=\"ref\" reference=\"tab:extended-asr\"}.\n\nWe noticd the results in [@chen2024loss] seems to be the SOTA for LM-style ASR model to the best of our knowledge. However, as many ablations are missed in [@chen2024loss], we took their open-sourced code and run ablations ourselves to have proper comparison with it. The final results, including ablation with `dMel` are shown in Table [\\[tab:chen-ablation\\]](#tab:chen-ablation){reference-type=\"ref\" reference=\"tab:chen-ablation\"}:\n\n-   We successfully reproduced [@chen2024loss] results (row 1 and 2).\n\n-   Without pretraining (rows 3, 4, 5):\n\n    `dMel` outperforms HuBERT-KM on both clean and other datasets; `dMel` surpasses BPE on top of HuBERT-KM on clean data, while BPE on HuBERT-KM performs better on other.\n\n-   Without pretraining and without speed perturbation (rows 6, 7, 8):\n\n    BPE on HuBERT-KM performance decreases significantly after diabling speed perturbation (compare rows 3 and 6), raising questions about its generalizability to other domains, given that BPE tokens are trained on speed-perturbed LibriSpeech data.\n\n    Our `dMel`(row 8) achieves substantially better results than both HuBERT-KM and BPE on HuBERT-KM (rows 7 and 6), demonstrating robust performance even without speed augmentation.\n\nNote that in `dMel`, we use SpecAugment (masking across time and channels) and [@chen2024loss] also use SpecAugment. According to their code, the time masking is 30%, while channel masking is impossible as there is only 1 channel).\n\nWe believe these results demonstrate the effectiveness, simplicity in use, and robustness of our `dMel` tokenization method, particularly in scenarios where extensive pretraining or domain-specific augmentations may not be feasible.\n\nNote that [@chen2024loss] did not show applicability of BPE on HuBERT-KM or HuBERT-KM to TTS task, while in VOXTLM (also uses BPE on HuBERT-KM) it is shown that this tokenization is not suited for TTS (the performance is poor). `dMel` in contrary is shown to perform well on TTS task too in addition to ASR.\n\n## Joint Speech-Text Modeling Discussion {#app:joint}\n\nWe found it to be challenging to train joint model for ASR and TTS, similar to observations as in [@maiti2024voxtlm] and e.g. [@shi2022learning; @anonymous2024avcpl] for joint audio-visual speech recognition. Also, there is a very recent research work [@toyin2024unified], that also shows training TTS and ASR jointly is challenging, and needs carefully designed model architecture and training loss fusion technique.\n\nOne of the reasons is the different pace of learning. Careful consideration of training strategies can mitigate some of the challenges in joint modeling of TTS and ASR tasks, highlighting the complexities inherent in combining these distinct but related tasks within a single model.\n\nAnother reason we suspect is the mismatch between train and test time, which is more pronounced for the joint modeling: if we compare individual validation losses per task in joint model to their one-task training counterparts we see they match each other (so training is fine), however the generation (test time which mismatches how the train loss is defined) for both tasks is broken: longer sequences has hallucination and high repetition issues. This could be due to different length of sequences between text and audio and thus learnt attention pattern could be different which creates longer sequences generation issue for the joint model.\n\nLast but not the least, the two tasks have opposite modalities in the input and output, making it rather difficult to model. Most previously researched multi-task work have the same modality in the output. The combination of ASR and TTS is a rather recent phenomenon, such as Viola and VOXTLM.\n\n[^1]: We use a word 'semantic' with the meaning of 'content' to keep prior work notation [@borsos2023audiolm].\n\n[^2]: We used vocoders from <https://github.com/kan-bayashi/ParallelWaveGAN.>\n\n[^3]: We use a pretrained model \"Speaker Encoder\" from the YourTTS [@casanova2022yourtts] repository <https://github.com/Edresson/YourTTS>.\n\n[^4]: <https://librivox.org/pages/public-domain/>.\n\n[^5]: We use checkpoint [https://huggingface.co/facebook/hubert-large-ls960-ft\n    ](https://huggingface.co/facebook/hubert-large-ls960-ft\n    ){.uri}.\n\n[^6]: We use official codebase to train this model w/o text pretraining as [@chen2024loss] report results only with text pretraining.",
    "rationale": "Summary: This work propose to solve the problem of codec: it is hard for one codebook to cover both semantic and acoustic information, but multiple codebook will complicate the architecture and require additional pretraining. Therefore, this work proposes to discretize mel-filterbank channels into discrete intensity bins, which produces a simple representation that outperforms existing speech\ntokenization methods.\n\nI believe this is a pioneering work that may open a potentially new track of TTS research --> use continous mel to replace discrete codec in lm base TTS.\n\nOne question:\n- How is it compared to another similar work MELL-E (https://arxiv.org/abs/2407.08551) that also use continous mel tokens for lm based TTS?\n\nStrengths: see above\n\nWeaknesses: see above\n\nQuestions: see above",
    "rating": 3,
    "label": "novel",
    "rationale_edited": "Summary: This work propose to solve the problem of codec: it is hard for one codebook to cover both semantic and acoustic information, but multiple codebook will complicate the architecture and require additional pretraining. Therefore, this work proposes to discretize mel-filterbank channels into discrete intensity bins, which produces a simple representation that outperforms existing speech\ntokenization methods.\n\nI believe this is a pioneering work that may open a potentially new track of TTS research --> use continous mel to replace discrete codec in lm base TTS.\n\n- How is it compared to another similar work MELL-E (https://arxiv.org/abs/2407.08551) that also use continous mel tokens for lm based TTS?",
    "chosen": true
  },
  {
    "title": "Scaling Diffusion Language Models via Adaptation from Autoregressive Models",
    "abstract": "Diffusion Language Models (DLMs) have emerged as a promising new paradigm for text generative modeling, potentially addressing limitations of autoregressive (AR) models. However, current DLMs have been studied at a smaller scale compared to their AR counterparts and lack fair comparison on language modeling benchmarks. Additionally, training diffusion models from scratch at scale remains challenging. Given the prevalence of open-source AR language models, we propose adapting these models to build text diffusion models. We demonstrate connections between AR and diffusion modeling objectives and introduce a simple continual pre-training approach for training diffusion models. Through systematic evaluation on language modeling, reasoning, and commonsense benchmarks, we show that we can convert AR models ranging from 127M to 7B parameters (GPT2 and LLaMA) into diffusion models DiffuGPT and DiffuLLaMA, using less than 200B tokens for training. Our experimental results reveal that these models outperform earlier DLMs and are competitive with their AR counterparts. We release a suite of DLMs (127M-355M-7B) capable of generating fluent text, performing in-context learning, filling in the middle without prompt re-ordering, and following instructions.",
    "text": "# Introduction\n\nLarge language models (LLMs) have ushered in a new era of artificial intelligence, demonstrating remarkable capabilities in generating high-quality text, in-context learning, and following complex instructions [@gpt4; @touvron2023llama]. These advancements are primarily rooted in the scaling up of autoregressive (AR) language models. During both training and inference, these models leverage vast datasets and billions of parameters, employing a strict left-to-right sequential process for memorization and generation. This approach has resulted in the emergence of intelligence capable of tackling diverse tasks [@wei2022emergent; @10.5555/3600270.3602446]. However, the ultimate upper limit of intelligence achievable through this paradigm remains an open question. While AR mechanisms form the foundation of current LLMs, they are not without limitations [@lin2020limitations]. Notable challenges include difficulties in future planning [@bachmannpitfalls; @hu2024amortizing; @Xie2024TravelPlannerAB] and self-correction [@huang2024large]. These constraints have spurred researchers to explore alternative architectures for next-generation LLMs.\n\nA compelling direction in current research focuses on the development of text diffusion models [@diffusion2023survey]. Building upon the rapid evolution of diffusion models in various domains [@ho2020denoising; @nichol2021improved; @pmlr-v139-ramesh21a], innovative text diffusion models [@li2022diffusion; @lou2023discrete] have opened up new possibilities for text generation. A unifying insight across these models is the potential of diffusion language models (DLMs) for controllable [@venkatraman2024amortizing], any-order, and parallel text generation [@gong-etal-2023-diffuseq]. Notably, DLMs exhibit promising capabilities in intermediate token correction [@ye2024diffusion] and global planning [@zhang2023planner], thereby addressing key limitations inherent in the AR approach.\n\nDespite the promising potential of text diffusion models, the relatively small model size limits the competitiveness of DLMs compared to AR models. Existing state-of-the-art DLMs such as Plaid 1B [@gulrajani2023likelihoodbased] and SEDD [@lou2023discrete] are relatively small in size (127M-1B parameters) and under-trained, with less than 400B tokens of training data. This substantial gap in scale prevents fair comparisons with larger AR language models on many advanced capabilities and tasks, such as chain-of-thought reasoning abilities on complex mathematical benchmarks. Recent approaches [@ye2023diffusion] attempt adapt LLaMA models to DLMs based on masked language modeling [@he-etal-2023-diffusionbert]. However, they find that the base model capabilities are lost during their adaptation stage. Pre-training at such a scale is extremely resource-intensive, and the challenge is even more pronounced for diffusion models. These models lack the computational optimizations that have been developed for LLMs [@samragh2024scaling] and require significantly more resources than their AR counterparts, as noted by @gulrajani2023likelihoodbased.\n\nGiven these scaling challenges, pre-trained LLMs emerge as an invaluable resource that we can leverage, considering the extensive computational efforts already invested in their development. This strategy aligns with recent trends where new models are scaled up or adapted to new architectures using existing LLMs [@wang2024mamba; @zhang2024gated]. However, building DLMs through adaptation from AR models is non-trivial due to fundamental differences in their language modeling objectives. Two key distinctions present significant hurdles. First, AR models employ causal masking to prevent future information leakage, whereas diffusion models utilize bi-directional attention masks. Second, an AR LM processes clean inputs to predict subsequent tokens at each step, while a diffusion model operates on noisy inputs to predict their denoised versions.\n\nTo overcome these challenges, we propose a simple adaptation approach that bridges these discrepancies. We unify their modeling objectives (§[3.2](#sec:uni){reference-type=\"ref\" reference=\"sec:uni\"}) and address the architectural differences by breaking the causal masking bias in AR models through attention mask annealing (§[3.3.0.1](#sec:mask-anneal){reference-type=\"ref\" reference=\"sec:mask-anneal\"}). Additionally, we inherit the shift operation from AR models (§[3.3.0.2](#sec:shift-ope){reference-type=\"ref\" reference=\"sec:shift-ope\"}). This streamlined adaptation recipe enables us to construct a pre-trained DLM that can effectively compete in the arena of LLMs. Building on this approach, we leverage the FineWeb [@penedo2024finewebdatasetsdecantingweb] and SlimPajama [@cerebras2023slimpajama] pre-training corpora to continue training small and medium-sized DLMs based on GPT2 [@Brown2020LanguageMA], and further train up to a 7B model based on LLaMA2 [@touvron2023llama2].\n\nOur experiments provide a comprehensive comparison between AR LMs and DLMs across language modeling, reasoning, and infilling tasks. The evaluation encompasses diverse settings, including zero-shot, few-shot, and fine-tuning scenarios, addressing the limitations of relying solely on perplexity in previous works [@shi2024simplified]. Our contributions and empirical findings include:\n\n-   We demonstrate that by narrowing the gap between AR models and DLMs, it is possible to convert 127M-7B AR models (GPT2 and LLaMA2) into DiffuGPT and DiffuLLaMA with training on less than 200B tokens. Notably, DiffuGPT outperforms GPT2 in most tasks.\n\n-   We adapt 7B AR models to DLMs, greatly expanding the expertise compared to smaller-sized diffusion models. DiffuLLaMA emerges as the state-of-the-art DLM, exhibiting in-context learning, code generation, and strong infilling capabilities. Its generation speed is competitive with AR counterparts for unconditionally generating $1024$ tokens using $256$ diffusion timesteps.\n\n-   We provide a comprehensive benchmark for DLMs and release our adapted diffusion models (127M, 355M and 7B) along with open-source adaptation code, efficient fine-tuning scripts, and evaluation toolkits.\n\n# Preliminary and Notation {#sec:prel}\n\nDiffusion models [@sohl2015deep; @song2019generative; @ho2020denoising; @song2021scorebased] are latent variable generative models characterized by a forward and a reverse Markov process. We denote $\\mathbf{x}_0\\sim p_{data}(\\mathbf{x}_0)$ as the variable following the data distribution, and $\\mathbf{x}_t\\sim q(\\mathbf{x}_t)$ as the noisy variable of $\\mathbf{x}_0$ at time $t$, where the maximum time is $T$. The forward process $q(\\mathbf{x}_{1:T}|\\mathbf{x}_0)=\\prod_{t=1}^Tq(\\mathbf{x}_t|\\mathbf{x}_{t-1})$ corrupts the initial data $\\mathbf{x}_0$ into a sequence of increasingly noisy variables $\\mathbf{x}_{1:T}$. Accordingly, the backward Markov process models the joint probability as $p_{\\theta}(\\mathbf{x}_{0:T})=p_{\\theta}(\\mathbf{x}_T)\\prod_{t=1}^Tp_{\\theta}(\\mathbf{x}_{t-1}|\\mathbf{x}_t)$, which gradually denoises $\\mathbf{x}_t$ to reconstruct the original data $\\mathbf{x}_0$. Parameters $\\theta$ are learned by minimizing the negative log-likelihood of $\\mathbf{x}_0$, which can be optimized through the evidence lower bound (ELBO), $$\\label{eq:ori-loss}\n    -\\log p_{\\theta}(\\mathbf{x}_0) \\leq \\mathbb{E}_{q(\\mathbf{x}_1|\\mathbf{x}_0)}[-\\log p_{\\theta}(\\mathbf{x}_0|\\mathbf{x}_1)] + D_{\\mathrm{KL}}(q(\\mathbf{x}_T|\\mathbf{x}_0)||p_{\\theta}(\\mathbf{x}_T))+\\mathcal{L}_T,$$ with $\\mathcal{L}_T=\\sum_{t=2}^T\\mathbb{E}_{q(\\mathbf{x}_t|\\mathbf{x}_0)}[D_{\\mathrm{KL}}(q(\\mathbf{x}_{t-1}|\\mathbf{x}_t, \\mathbf{x}_0)||p_{\\theta}(\\mathbf{x}_{t-1}|\\mathbf{x}_t))]$. For continuous text diffusion [@li2022diffusion; @gong2022diffuseq], at each forward step, perturbations are applied according to $q(\\mathbf{x}_{t} \\vert \\mathbf{x}_{t-1}) = \\mathcal{N}(\\mathbf{x}_{t};\\sqrt{1-\\beta_t}\\mathbf{x}_{t-1}, {\\beta}_t \\mathbf{I})$, where $\\beta_t \\in (0,1)$ represents different scales across time steps such that $\\mathbf{x}_T \\sim \\mathcal{N}(0, \\mathbf{I})$. In the case of discrete denoising models [@ho2020denoising; @austin2021structured; @Zheng2023ARD], the forward process is defined as a categorical distribution $q(\\mathbf{x}_{t} \\vert \\mathbf{x}_{t-1})=\\text{Cat}(\\bm{x}_t;\\bm{Q}_t^\\top\\bm{x}_{t-1})$, where each $\\mathbf{x}_t \\in \\{0, 1\\}^K$ is a one-hot vector with vocabulary size $K$, $\\bm{Q}_t\\in[0,1]^{K\\times K}$ is the transition matrix, and each entry $[\\bm{Q}_t]_{ij}$ denotes the probability of transition from the state $i$ to $j$. We build on the formulation of *absorbing discrete diffusion* [@austin2021structured], which specifies $\\bm{Q}_t=(1-\\beta_t)I+\\beta_t\\mathbf{1}\\bm{m}^\\top$. We denote $\\mathbf{1}$ as an all-one vector of size $K$ and $\\bm{m}$ as the one-hot encoding of a special `[MASK]` token in the vocabulary. Therefore, the transition matrix, $\\bm{Q}_t$ indicates that with probability $1 - \\beta_t$, $\\bm{x}_t$ remains unchanged; otherwise, it transitions to $\\bm{m}$, becoming absorbed into `[MASK]`. Letting $\\bm{\\overline{Q}}_t \\coloneqq \\prod_{i=1}^t\\bm{Q}_i=\\alpha_tI+(1-\\alpha_t)\\mathbf{1}\\bm{m}^\\top$ and $\\alpha_t \\coloneqq \\prod_{i=1}^t(1-\\beta_t)$, the distribution of $\\bm{x}_t$ conditional on $\\bm{x}_0$ is given by $$\\label{eq:qxt}\n    q(\\bm{x}_t|\\bm{x}_0)=\\text{Cat}(\\bm{x}_t;\\bm{\\overline{Q}}_t^\\top\\bm{x}_0) = \\alpha_t I\\bm{x}_0 + (1-\\alpha_t)\\bm{m}\\mathbf{1}^\\top\\bm{x}_0 = \\alpha_t \\bm{x}_0 + (1-\\alpha_t)\\bm{m},$$ since $\\bm{x}_0$ is a one-hot vector and thus $\\mathbf{1}^\\top\\bm{x}_0 = 1$. We expect $\\alpha_T$ to approach $0$ such that the full noise data $\\bm{x}_T$ equals $\\bm{m}$ with probability $1$.\n\nThe discrete time representation of $t \\in [0, T]$, restricts $\\bm{x}_t$ to fixed noise ratios. To avoid this bias and enable sampling from any noisy representation, we use continuous-time sampling, allowing $t$ to span any point within $[0, 1]$  [@kingma2021variational; @shi2024simplified; @zhao2024improving; @ou2024your]. Continuous-time sampling is equivalent to dividing $[0, 1]$ into $T$ intervals and where $T \\rightarrow \\infty$. For any $0 \\leq s < t \\leq 1$, the forward process generalizes to $q(\\mathbf{x}_{t} \\vert \\mathbf{x}_{s})$. We will use this continuous-time notation in the following sections.\n\n# Model {#sec:model}\n\n![The overview of our approach to adapt autoregressive (AR) models to diffusion models. **Left**: The shift operation in AR models enables the output layer $h_i$ to approximate the distribution of next tokens $x_{i+1}$ in hidden representations through the cross entropy (CE) loss. **Middle**: We remove the causal mask gradually during training eventually making our model bi-directional. **Right**: inside the diffusion models we shift the logits to compute the loss with the next token (i.e., the loss on $h_i$ would be with respect to $x_{i+1}$), while perceptually, the diffusion models are still functioning as recovering the original signals (since $h_i$ corresponds to $x_{i+1}$ in AR loss).](fig/pipeline_v3.pdf){#fig:pipeline width=\"97%\"}\n\nWe begin by formulating the continuous-time discrete diffusion process (§[3.1](#sec:continuous){reference-type=\"ref\" reference=\"sec:continuous\"}) and establishing a connection between the discrete diffusion and autoregressive objectives (§[3.2](#sec:uni){reference-type=\"ref\" reference=\"sec:uni\"}). Based on this equivalence, we propose an adaptation approach (§[\\[sec:ada\\]](#sec:ada){reference-type=\"ref\" reference=\"sec:ada\"}) and a sampling algorithm (§[3.4](#sec:sampling){reference-type=\"ref\" reference=\"sec:sampling\"}) for diffusion models adapted from AR models. The whole process is illustrated in Figure [1](#fig:pipeline){reference-type=\"ref\" reference=\"fig:pipeline\"}.\n\n## Continuous-time Discrete Diffusion Processes {#sec:continuous}\n\nFollowing Eq.[\\[eq:qxt\\]](#eq:qxt){reference-type=\"ref\" reference=\"eq:qxt\"} and $q(\\bm{x}_t|\\bm{x}_0)=\\sum_{\\bm{x}_s}q(\\bm{x}_t|\\bm{x}_s)q(\\bm{x}_s|\\bm{x}_0)$, the forward transition distribution between arbitrary points $s < t$ can be derived as $$q(\\bm{x}_t|\\bm{x}_s)=\\text{Cat}(\\bm{x}_t;\\bm{\\overline{Q}}_{s|t}^\\top \\bm{x}_s) =\\frac{\\alpha_t}{\\alpha_s}\\bm{x}_s+(1-\\frac{\\alpha_t}{\\alpha_s})\\bm{m},$$ with $\\bm{\\overline{Q}}_{s|t} \\coloneqq \\bm{\\overline{Q}}_s^{-1}\\bm{\\overline{Q}}_t=\\frac{\\alpha_t}{\\alpha_s}I+(1-\\frac{\\alpha_t}{\\alpha_s})\\mathbf{1}\\bm{m}^\\top$. The corresponding backward transition distribution conditional on $\\bm{x}_0$ is also available in closed form, $$\\label{eq:qxs}\n    q(\\bm{x}_s|\\bm{x}_t, \\bm{x}_0) = \\frac{q(\\bm{x}_t|\\bm{x}_s)q(\\bm{x}_s|\\bm{x}_0)}{q(\\bm{x}_t|\\bm{x}_0)}=\n    \\begin{cases}\n        \\frac{\\alpha_s-\\alpha_t}{1-\\alpha_t}\\bm{x}_0+\\frac{1-\\alpha_s}{1-\\alpha_t}\\bm{m} & \\text{if } \\bm{x}_t=\\bm{m}, \\\\\n        \\bm{x}_0 & \\text{if }\\bm{x}_t\\neq\\bm{m}. \\\\\n    \\end{cases}$$ In discrete diffusion processes, we aim to approximate the backward transition distribution $q(\\bm{x}_s|\\bm{x}_t,\\bm{x}_0)$ using a denoising model $p_{\\theta}(\\bm{x_s}|\\bm{x}_t, f_{\\theta}(\\bm{x}_t))$, where $f_{\\theta}(\\bm{x}_t)$, an approximation of $\\bm{x}_0$, is usually the output of neural networks such as a transformer [@NIPS2017_3f5ee243]. We can define the denoising model to have a similar form of backward transitions as $p_{\\theta}(\\bm{x_s}|\\bm{x}_t)=\\frac{\\alpha_s-\\alpha_t}{1-\\alpha_t}f_{\\theta}(\\bm{x}_t)+\\frac{1-\\alpha_s}{1-\\alpha_t}\\bm{m}$. According to the training objective in Eq.[\\[eq:ori-loss\\]](#eq:ori-loss){reference-type=\"ref\" reference=\"eq:ori-loss\"}, the KL-divergence of $\\mathcal{L}_T$ at each step $t$ can be simplified to a reweighted cross-entropy function, $$D_{\\mathrm{KL}}(q(\\bm{x}_s|\\bm{x}_t,\\bm{x}_0)||p_{\\theta}(\\bm{x}_s||\\bm{x}_t))=-\\frac{\\alpha_s-\\alpha_t}{1-\\alpha_t}\\delta_{\\bm{x}_t,\\bm{m}}\\bm{x}_0^\\top\\log f_{\\theta}(\\bm{x}_t),$$ where $\\delta_{a,b}$ is the indicator function for $a=b$. If we take the limit and let $T\\rightarrow \\infty$, the first two terms of Eq.[\\[eq:ori-loss\\]](#eq:ori-loss){reference-type=\"ref\" reference=\"eq:ori-loss\"} will approach 0 and some constant, respectively. Thus the evidence lower bound (ELBO) effectively becomes $\\mathcal{L}_T$ and $$\\label{eq:L_T}\n    \\lim_{T\\rightarrow \\infty}\\mathcal{L}_T = \\int_{0}^{1}\\frac{\\alpha_t^\\prime}{1-\\alpha_t}\\mathbb{E}_{q(\\mathbf{x}_t|\\mathbf{x}_0)}[\\delta_{\\bm{x}_t,\\bm{m}}\\bm{x}_0^\\top\\log f_{\\theta}(\\bm{x}_t)]\\,dt.$$ The full derivation is listed in Appendix [7.2](#appendix:loss){reference-type=\"ref\" reference=\"appendix:loss\"}. The same form of ELBO which is invariant to noise schedule but related to the signal-to-noise ratio (SNR) is also introduced in @kingma2021variational [@shi2024simplified]. Following @austin2021structured, we choose the noise schedule $\\alpha_t=1-t$, then $\\frac{-\\alpha_t^\\prime}{1-\\alpha_t}=\\frac{1}{t}$. The previous discussion focused on the single token $\\bm{x}_t$, and can be applied independently to a text sequence of $N$ tokens $\\mathbf{x}_t=[\\bm{x}_t^{1}, \\bm{x}_t^{2}\\dots, \\bm{x}_t^{N}]$. During training, we do not compute integral loss in Eq.[\\[eq:L_T\\]](#eq:L_T){reference-type=\"ref\" reference=\"eq:L_T\"} for efficiency consideration; instead, we sample $t$ for each data point. The final loss at $t$ is $$\\label{eq:dm-loss}\n    \\mathcal{L}_{t}^{1:N} = \\frac{1}{t}\\mathbb{E}_{q(\\mathbf{x}_t|\\mathbf{x}_0)}\\left[-\\sum_{n=1}^N\\delta_{\\mathbf{x}_t^n,\\bm{m}}(\\mathbf{x}_0^{n})^\\top\\log f_{\\theta}(\\mathbf{x}_t^{1:N})_n\\right],$$ where $f_{\\theta}(\\mathbf{x}_t^{1:N})_n$ denotes the whole input sequence is fed into the transformer model and the $n$-th output token is indexed.\n\n## Unifying Language Modeling Objectives {#sec:uni}\n\nThe training objective of autoregressive (AR) language models is the negative log-likelihood of each ground-truth token provided the preceding tokens, $$\\label{eq:ce}\n    \\mathcal{L}_{AR}^{1:N} = -\\sum_{n=1}^N (\\mathbf{x}_0^{n})^\\top \\log f_{\\theta}(\\mathbf{x}_0^{1:n-1})_{n-1}.$$ Comparing Eq.[\\[eq:ce\\]](#eq:ce){reference-type=\"ref\" reference=\"eq:ce\"} against Eq.[\\[eq:dm-loss\\]](#eq:dm-loss){reference-type=\"ref\" reference=\"eq:dm-loss\"}, we note that while both take the form of cross-entropy functions, Eq.[\\[eq:dm-loss\\]](#eq:dm-loss){reference-type=\"ref\" reference=\"eq:dm-loss\"} includes an additional reweighting term $\\frac{1}{t}$ and an indicator function $\\delta_{\\mathbf{x}_t^n,\\bm{m}}$. They result from the definition of discrete diffusion processes (§[3.1](#sec:continuous){reference-type=\"ref\" reference=\"sec:continuous\"}). The reweighting emphasizes smaller $t$ where $\\mathbf{x}_t$ contains fewer masked tokens, and this can be regarded as the importance sampling [@nichol2021improved]. The indicator specifies which tokens are masked for prediction. The AR training objective Eq.[\\[eq:ce\\]](#eq:ce){reference-type=\"ref\" reference=\"eq:ce\"}, on the other hand, constrains the context to be unidirectional via attention masking and shifts the targets so that each token predicts the next token instead of itself. These discrepancies form the basis of our adaptation framework, which is detailed in §[\\[sec:ada\\]](#sec:ada){reference-type=\"ref\" reference=\"sec:ada\"}.\n\nIn fact, an alternative way to understand AR modeling, through the lens of diffusion models, is to consider a diffusion process where the forward pass deterministically masks right-to-left and token-by-token [@austin2021structured; @hoogeboom2022autoregressive]. This yields a backward process generating one token at a time from left to right, running with $T=N$ denoising steps in total. As discussed in @austin2021structured, the loss objective of this diffusion process is equivalent to standard cross-entropy (Eq.[\\[eq:ce\\]](#eq:ce){reference-type=\"ref\" reference=\"eq:ce\"}) commonly used to train AR language models. This crafted diffusion process for AR models represents a special case of discrete diffusion (§[3.1](#sec:continuous){reference-type=\"ref\" reference=\"sec:continuous\"}), yet it is limited to unidirectional context and sequential token generation. In contrast, general discrete diffusion processes can leverage bidirectional context and support parallel generation in arbitrary orders.\n\n## Adaptation\n\nBuilding on the connection between AR modeling and discrete diffusion processes, we construct an adaptation recipe next. Figure [1](#fig:pipeline){reference-type=\"ref\" reference=\"fig:pipeline\"} shows an overview of our adaptation approach. We use attention mask annealing, shift operations, and a time-embedding free architecture to narrow the differences between AR and DLMs.\n\n[]{#sec:ada label=\"sec:ada\"}\n\n<figure id=\"algo:line-sample-shift\">\n<div class=\"minipage\">\n<div class=\"algorithm\">\n<div class=\"algorithmic\">\n<p><strong>Input:</strong> network <span class=\"math inline\"><em>f</em><sub><em>θ</em></sub></span> initialized by existing models, training corpus <span class=\"math inline\"><em>p</em><sub><em>d</em><em>a</em><em>t</em><em>a</em></sub>(<strong>x</strong><sub>0</sub><sup>1 : <em>N</em></sup>)</span>, mask token <span class=\"math inline\"><strong>m</strong></span>. <strong>Output:</strong> model parameters <span class=\"math inline\"><em>θ</em></span>. Draw <span class=\"math inline\"><strong>x</strong><sub>0</sub><sup>1 : <em>N</em></sup> ∼ <em>p</em><sub><em>d</em><em>a</em><em>t</em><em>a</em></sub></span> and set <span class=\"math inline\"><em>labels</em> ← <strong>x</strong><sub>0</sub><sup>1 : <em>N</em></sup></span> Sample <span class=\"math inline\"><em>t</em> ∈ <em>Uniform</em>(0, 1)</span> Sample <span class=\"math inline\"><strong>x</strong><sub><em>t</em></sub><sup>1 : <em>N</em></sup> ∼ <em>q</em>(<strong>x</strong><sub><em>t</em></sub>|<strong>x</strong><sub>0</sub>)</span> Anneal the attention mask <span class=\"math inline\"><em>a</em><em>t</em><em>t</em><em>n</em>_<em>m</em><em>a</em><em>s</em><em>k</em></span> Forward <span class=\"math inline\"><em>logits</em> ← <em>f</em><sub><em>θ</em></sub>(<strong>x</strong><sub><em>t</em></sub><sup>1 : <em>N</em></sup>)</span> with <span class=\"math inline\"><em>a</em><em>t</em><em>t</em><em>n</em>_<em>m</em><em>a</em><em>s</em><em>k</em></span> Right shift <span class=\"math inline\"><em>logits</em></span> by one position <span id=\"algo:line-loss\" data-label=\"algo:line-loss\"></span> <span class=\"math inline\">$\\mathcal{L}_t=\\frac{1}{t}\\delta_{x_t, m}$</span>CE<span class=\"math inline\">(<em>logits</em>, <em>labels</em>)</span> <span class=\"math inline\">⊳</span> Eq.<a href=\"#eq:dm-loss\" data-reference-type=\"ref\" data-reference=\"eq:dm-loss\">[eq:dm-loss]</a> Backprop with <span class=\"math inline\">ℒ<sub><em>t</em></sub></span> and update <span class=\"math inline\"><em>θ</em></span></p>\n</div>\n</div>\n</div>\n<div class=\"minipage\">\n<div class=\"algorithm\">\n<div class=\"algorithmic\">\n<p><strong>Input:</strong> Trained diffusion model <span class=\"math inline\"><em>f</em><sub><em>θ</em></sub></span>, sampling algorithm <span class=\"math inline\"><em>τ</em></span>, mask token <span class=\"math inline\"><strong>m</strong></span>, start token <span class=\"math inline\"><strong>s</strong></span>. <strong>Output:</strong> generated sample <span class=\"math inline\"><strong>x</strong><sub>0</sub></span>. <strong>Initialize</strong> <span class=\"math inline\"><strong>x</strong><sub><em>T</em></sub><sup>1 : <em>N</em></sup> = <strong>m</strong></span>. Forward <span class=\"math inline\"><em>logits</em> ← <em>f</em><sub><em>θ</em></sub>(<strong>x</strong><sub><em>t</em></sub><sup>1 : <em>N</em></sup>)</span> Sample <span class=\"math inline\">$\\tilde{\\bm{x}}_0^{1:N}\\sim \\textit{Categorical}(\\tau(\\textit{logits}))$</span> <span class=\"math inline\">$\\bm{x}_{t-1}^n=q(\\bm{x}_{t-1}^n|\\bm{x}_{t}^n,\\tilde{\\bm{x}}_0^n)$</span> <span class=\"math inline\">⊳</span> Eq.<a href=\"#eq:qxs\" data-reference-type=\"ref\" data-reference=\"eq:qxs\">[eq:qxs]</a> Right shift <span class=\"math inline\"><strong>x</strong><sub><em>t</em> − 1</sub><sup>1 : <em>N</em></sup> = [<strong>s</strong>, <strong>x</strong><sub><em>t</em> − 1</sub><sup>1 : <em>N</em> − 1</sup>]</span> <span id=\"algo:line-sample-shift\" data-label=\"algo:line-sample-shift\"></span> <strong>Return</strong> <span class=\"math inline\"><strong>x</strong><sub>0</sub><sup>2 : <em>N</em></sup></span></p>\n</div>\n</div>\n</div>\n<figcaption>Sampling</figcaption>\n</figure>\n\n#### Attention Mask Annealing {#sec:mask-anneal}\n\nThe prediction of the $n$-th token, given all preceding tokens, $f_{\\theta}(\\mathbf{x}_0^{1:n-1})$, is usually implemented by causal attention masking in transformer-based AR language models. As shown in Figure [1](#fig:pipeline){reference-type=\"ref\" reference=\"fig:pipeline\"}, causal attention masks set all entries in the upper triangle of the self-attention matrices to zero, so each token cannot attend to its respective future tokens. Such causal masking prevents the model from learning right-to-left dependencies for more general diffusion processes. To address this limitation while preserving left-to-right conditionals during adaptation, we introduce an incremental annealing process from causal masks to full attention matrices. During annealing, the causal mask is not immediately removed; instead, it is retained at a controlled ratio, as shown in the middle part of Figure [1](#fig:pipeline){reference-type=\"ref\" reference=\"fig:pipeline\"}. At each training step, we sample the amount of context from the right side and progressively increase this amount till we obtain the full attention mask.\n\n#### Shift Operation {#sec:shift-ope}\n\nAR models also apply a shifting operation, where the target output is the input sequence shifted left by one position. In other words, the prediction target of the $(n\\!-\\!1)$-th token is the $n$-th token, contrasting with typical diffusion models that try to predict masked tokens at their original positions. When initializing text diffusion models with AR model parameters, the model would tend to output the hidden representations of the shifted input sequence. If we continue to optimize the cross-entropy objective based on the original token positions, the model struggles to adapt due to misalignment between input and output. Instead, we maintain the shift operation (Algo.[\\[alg:training\\]](#alg:training){reference-type=\"ref\" reference=\"alg:training\"}, line [\\[algo:line-loss\\]](#algo:line-loss){reference-type=\"ref\" reference=\"algo:line-loss\"}), treating the output logits at each position as corresponding to the next token. When calculating the objective, we align prediction targets so that the diffusion model learns to recover the original signals. This process is illustrated in the right panel of Figure [1](#fig:pipeline){reference-type=\"ref\" reference=\"fig:pipeline\"}.\n\n#### Time-Embedding-Free Architecture\n\nMany diffusion models for text generation [@li2022diffusion; @dieleman2022continuous; @gulrajani2023likelihoodbased; @lou2023discrete; @shi2024simplified] incorporate time embedding layers to represent the information of current timesteps $t$, which can explicitly indicate the noise scale of the input noisy data. While inferring these timesteps can be challenging for image diffusion models [@ho2020denoising; @li2024autoregressive], some discrete text diffusion models [@he-etal-2023-diffusionbert] assert that timesteps $t$ can be easily learned implicitly based on the number of mask tokens. Since AR models are not equipped with time embedding layers, we also choose not to use the time embedding, resulting in no additional parameters compared to previous diffusion models.\n\n## Sampling {#sec:sampling}\n\nFollowing @shi2024simplified, we initialize $\\bm{x}_T$ with all `[MASK]` tokens and then sample tokens according to the time reversal $q(\\bm{x}_s|\\bm{x}_t, \\bm{x}_0)$ in Eq.[\\[eq:qxs\\]](#eq:qxs){reference-type=\"ref\" reference=\"eq:qxs\"}. At each timestep, if $\\bm{x}_t$ is a mask, it will jump to the predicted $\\bm{x}_0$ at time $s$ with probability $\\frac{\\alpha_s-\\alpha_t}{1-\\alpha_t}$. After $T$ iterations, the model generates the full sequence. Since our adapted models are trained with the shift operation, at each sampling iteration, we shift back the generated sentence and prepend a start token before the next forward pass (Algo.[\\[alg:sampling\\]](#alg:sampling){reference-type=\"ref\" reference=\"alg:sampling\"}, line [2](#algo:line-sample-shift){reference-type=\"ref\" reference=\"algo:line-sample-shift\"}). Usually larger $T$ requires more interactions of computation, and can yield texts in higher quality, and this trade-off can be controlled easily through $T$. Through experiments, we find that the output generated by diffusion models is diverse and scattered. Therefore, for conditional generation tasks, we improve the sampling procedure to ensure that only tokens with high probabilities from neural networks are denoised [@ghazvininejad-etal-2019-mask; @maskgit; @Zheng2023ARD], so that the model could predict tokens mostly relevant to the input. In addition, existing sampling techniques for AR language models, including top-$k$ and nucleus sampling [@holtzmancurious], can be seamlessly applied to diffusion models as well.\n\n# Experiment\n\n## Adaptation setup\n\n**DiffuGPT** We use the 30 billion tokens[^1] random split from the FineWeb dataset [@penedo2024finewebdatasetsdecantingweb], an improved corpus than OpenWebText [@Gokaslan2019OpenWeb] used in prior DLMs [@lou2023discrete], to continue training GPT2 base [@Radford2019LanguageMA]. We use sequence packing, logits shifting, and $10$K-step attention mask annealing to transform GPT2 to DiffuGPT.\n\n::: wrapfigure\nr0.32 ![image](fig/train-loss.pdf){width=\"32%\"}\n:::\n\n**DiffuLLaMA** We continue pre-training [llama-2-7-hf]{.smallcaps} [@touvron2023llama] on a mixture of SlimPajama (70%) [@cerebras2023slimpajama] and Starcoder (30%) [@li2023starcoder] data following TinyLLaMA [@zhang2024tinyllama]. We randomly sample 65 billion tokens from this mixture and use sequence packing with context length of $2048$. For efficient implementation we enable flash-attention 2 [@dao2023flashattention2] and directly use bi-directional attention without attention mask annealing.\n\nFor both adaptation settings, we employ full parameter finetuning with `bf16`. Please refer to Appendix [\\[appendix:train\\]](#appendix:train){reference-type=\"ref\" reference=\"appendix:train\"} for details. We plot the training loss curve in Figure [\\[fig:train-loss\\]](#fig:train-loss){reference-type=\"ref\" reference=\"fig:train-loss\"}. We train DiffuLLaMA on 60B tokens and achieve a lower loss compared to 127M and 335M models, suggesting a scaling trend similar to that of AR LLMs [@kaplan2020scaling]. We also note that there is still scope for training more, since the model does not show signs of saturation.\n\n## Evaluation setup\n\nPreviously developed diffusion language models [@gulrajani2023likelihoodbased; @lou2023discrete; @shi2024simplified; @ou2024your] evaluate model performance using zero-shot perplexity on benchmark datasets. However, this metric alone does not fully capture a model's capabilities for several reasons. First, lower perplexity does not always correlate with human-like content, even in autoregressive models [@kuribayashi-etal-2021-lower]. Additionally, the loss from text diffusion models only indicates an upper bound on negative log-likelihood. While @kingma2021variational [@shi2024simplified] demonstrate that the ELBO is invariant to the noise scheduler, discrepancies between continuous diffusion, discrete diffusion, and autoregressive loss still hinder fair comparisons across different model types. Given the ample evaluation benchmarks [@gu2024olmes] for LLMs, we propose a more comprehensive evaluation for diffusion models.\n\n[]{#tab:eval label=\"tab:eval\"}\n\n**Tasks and Metrics** We consider TriviaQA [@JoshiTriviaQA2017] to test the reading comprehension of models and last word completion task Lambada [@paperno-etal-2016-lambada] to test how models capture long-range dependencies in text. These two tasks are measured by exact match accuracy. We also test for common sense reasoning tasks HellaSwag [@zellers2019hellaswag], Winogrande [@WinoGrande2021], SIQA [@sap-etal-2019-social] and PIQA [@Bisk2020], all of which involve multiple-choice questions assessed by accuracy. On grade school math problems GSM8K [@Cobbe2021TrainingVT], we follow @ye2024diffusion in finetuning setting using the augmented symbolic data to test the CoT [@NEURIPS2022_9d560961] math reasoning abilities of diffusion models. Following @shen2023film, we also test the story infilling tasks using ROCStories [@mostafazadeh-etal-2016-corpus] and evaluate using ROUGE score [@lin2004rouge]. To test the code infilling, we adopt Humaneval [@bavarian2022efficient] single line infilling task, which is evaluated by pass@1 rate. We evaluate DiffuLLaMA's math reasoning and in-context learning ability by evaluating on MAWPS [@koncel-kedziorski-etal-2016-mawps] consisting of math word problems and SATMATH from AGI-eval consisting of math problems from SAT exam [@zhong-etal-2024-agieval]. We base our implementation on `lm-evaluation-harness` [@eval-harness] and re-implement all tasks across models to ensure a fair comparison.\n\n**Implementation Details** For pre-trained diffusion language models, we mainly use continuous diffusion (CD) model Plaid 1B [@gulrajani2023likelihoodbased], discrete diffusion (DD) model SEDD [@lou2023discrete] with different sizes as baselines. MD4 [@shi2024simplified] and RADD [@ou2024your] are based on and compared with SEDD, so we mainly compare SEDD. For autoregressive (AR) baselines, we consider the base models from which our models adapt. We implement infilling tasks for AR models by feeding the prefix and cutting off the generation length using the oracle length, considering that these AR models are not supporting infilling. For the sentence completion task, $T$ is the exact number of ground truth tokens for DD and $32$ for CD. For 4 multi-choices tasks from commonsense reasoning, we compute the loss (Eq.[\\[eq:L_T\\]](#eq:L_T){reference-type=\"ref\" reference=\"eq:L_T\"}) of each choice (averaged by token) and choose the one with lowest loss (perplexity). For GSM8K finetuning, we use parameter-efficient LoRA tuning [@hu2022lora] for DiffuLLaMA. The decoding $T$ are set to $32$ by default. The detailed settings are in Appendix [8.3](#appendix:imple-eval){reference-type=\"ref\" reference=\"appendix:imple-eval\"}.\n\n## Language modeling capacities\n\n#### Benchmark performance\n\nAccording to Table [\\[tab:eval\\]](#tab:eval){reference-type=\"ref\" reference=\"tab:eval\"}, the results on diverse tasks demonstrate that our adapted diffusion models achieve the state-of-the-art results among all existing diffusion language models (DLMs). We observe that diffusion models with larger parameters show improved performance, likely due to better base AR models. DiffuLLaMA's performance still falls short of the LLaMA2 model. This drop in performance is likely because DiffuLLaMa is trained on a small subset of SlimPajama and Starcoder data. We believe more training tokens can help improve these numbers. TriviaQA and PIQA are significant challenging for DLMs, probably because they require specific physical knowledge, such as *the capital of a city* or *the boiling point of water*; while our models are trained on 30B-70B tokens, which may be insufficient to preserve the general knowledge in the original LMs [@ke2023continual].\n\nIn tasks that require more extensive global reasoning, such as complex mathematics and coding, DLMs consistently exhibit better performance compared to AR models that rely solely on left-to-right modeling capabilities. Remarkably, DLMs demonstrate their strengths in infilling tasks. Regular LLMs like LLaMA2 are not trained for filling-in-the-middle (FIM) tasks like those in  @roziere2023code, making them incapable of handling infilling. Considering this, we do not provide the suffix information to the model, which might result in an unfair comparison. But the FIM requires re-arranging the order of pre-training/inference sequence with special tokens [@zheng2024selfinfilling], while diffusion training naturally supports this in its objective modeling.\n\nTasks in Table [\\[tab:eval\\]](#tab:eval){reference-type=\"ref\" reference=\"tab:eval\"} mainly measure conditional modeling abilities, where Plaid 1B performs unsatisfactorily for conditional generation tasks even though with 1B parameters. We attribute this result to the gap between the continuous diffusion modeling and discrete text representation; in contrast, discrete diffusion models align more closely with AR modeling, naturally supporting conditional generation. Despite this, as illustrated in Figure [\\[fig:ungen-ppl\\]](#fig:ungen-ppl){reference-type=\"ref\" reference=\"fig:ungen-ppl\"}, Plaid 1B demonstrates its strength in unconditional generation, highlighting its language modeling capabilities as a generative model. These findings reveal that the previous evaluation based on the perplexity of test data is too general to accurately assess the model's true capabilities, while our evaluation offers a more nuanced benchmark.\n\n#### Unconditional Generation\n\n::: wrapfigure\nr0.4 ![image](fig/ungen-ppl.pdf){width=\"40%\"}\n:::\n\nWe evaluate the quality of text unconditionally generated by DLMs in Figure [\\[fig:ungen-ppl\\]](#fig:ungen-ppl){reference-type=\"ref\" reference=\"fig:ungen-ppl\"}. The perplexity is measured using GPT2 large, consistent with the prior work [@lou2023discrete], where the data of MD4 [@shi2024simplified] is sourced from its original paper. To make sure low perplexity is not brought by repeated content, we assess the distinct 2-gram diversity of the generated text. Our model achieves low perplexity while maintaining a high level of diversity, validating that the DiffuGPT series excels in fluent text generation. As the number of decoding steps increases, thereby extending the test computing time, the fluency of unconditional generation improves. Similarly, increasing model size also contribute to better performance. An increase in generation perplexity is often associated with a slight decrease in diversity, which is a common phenomenon. Notably, DiffuGPT outperforms both SEDD and MD4 models, particularly at lower step counts (e.g., 64 steps), while as the continuous diffusion models, Plaid 1B needs more decoding steps to generate more fluent texts. DiffuGPT thus exhibits a significant advantage on less sampling time. We outline the decoding hyperparameters and show the diversity changes across different settings in Appendix [9.1](#appendix:un-gen){reference-type=\"ref\" reference=\"appendix:un-gen\"}, which also includes generation cases.\n\n## Analysis on DiffuLLaMA\n\n::: wraptable\nr0.5\n\n[]{#tab:diffullama label=\"tab:diffullama\"}\n:::\n\nWe validate that increasing the size of adapted DLMs significantly enhances the performance of downstream tasks in Table [\\[tab:eval\\]](#tab:eval){reference-type=\"ref\" reference=\"tab:eval\"}. Further, we aim to assess if the 7B model demonstrates in-context learning and reasoning capabilities similar to AR LLMs. Table [\\[tab:diffullama\\]](#tab:diffullama){reference-type=\"ref\" reference=\"tab:diffullama\"} presents the exact match accuracy between gold labels and predictions generated by DiffuLLaMA across zero-shot (ZS), few-shot (FS), and FS with chain-of-thought (CoT) scenarios. Besides, we deploy the self-consistency approach [@wang2023selfconsistency], considering that small DLMs can indeed benefit from this technique [@ye2024diffusion]. We use majority vote to choose the best answer from $3$ individual predictions, and also report the hit rate @$k$ with $k=3$, which measures whether any of the $k$ predictions include the correct answer, serving as a reference for the model's upper bound. For in-context learning (ICL) evaluations, we give $4$-shot on math tasks and $2$-shot on TriviaQA.\n\nThe performance improvement from zero-shot to few-shot settings suggests that DiffuLLaMA can learn from ICL examples, particularly in following to the format of answers as we observe. We hypothesize that the adapted model retains some of the abilities from the base AR model. We randomly select the ICL demonstration here and anticipate that advanced ICL strategies in LLMs [@wu-etal-2023-self] could yield potentially higher results. The self-consistency offers LMs with an effective approach to test-time scaling [@snell2024scaling], and DiffuLLaMA shows that it can also leverage this method. Furthermore, we report the hit rate results in generated candidate answers, highlighting the model's potential to produce the correct answer. This reveals that the current model exhibits high uncertainty about its responses, leading to temporarily suboptimal performance. We also observe that adding step-wise solutions in the in-context example (CoT) leads to a drop in performance, likely due to the absence of instruction tuning, similar to the findings in LLMs [@ouyang2022training]. We will leave instruction tuning as the future work as @ye2023diffusion show that text diffusion model can benefit from instruction tuning. In summary, we show the potential capabilities of DiffuLLaMA, which motivates us to further investigate the scaling of diffusion models.\n\n## Discussions\n\n#### Ablation Test on GSM8K-symbolic\n\n::: wraptable\nr0.4\n\n[]{#tab:gsm label=\"tab:gsm\"}\n:::\n\nDirect ablation on adaptation training is costly; hence, we conduct preliminary experiments to determine the adaptation recipes. Following @ye2024diffusion, we finetune models on the augmented GSM8K symbolic dataset using various base models and training objectives. The models are either trained from scratch (random initialization) or initialized with GPT2-S/M weights. Training objectives includes autoregressive training with a causal mask, continuous diffusion loss (CD), and discrete diffusion loss (DD). As shown in Table [\\[tab:gsm\\]](#tab:gsm){reference-type=\"ref\" reference=\"tab:gsm\"}, different training objectives yield comparable results when training from scratch. However, when using GPT2 as the base model, the CD loss performs worse than both the DD and AR losses. We attribute this to the better alignment of DD and AR losses as discussed in §[3.2](#sec:uni){reference-type=\"ref\" reference=\"sec:uni\"}. Previous continuous diffusion models [@dieleman2022continuous; @gulrajani2023likelihoodbased] has reparameterized the estimation of embeddings into the CE loss. However, adapting diffusion models from an AR model in continuous space necessitates an additional projection from the embedding to a categorical distribution, increasing the difficulty of adaptation.\n\nFor DD loss, removing attention mask annealing and shift operations both degrade performance, indicating the efficacy of our approaches. The mask annealing has minimal impact, so we choose to omit it for 7B adaptation to simplify implementation using flash-attention 2.\n\n::: wrapfigure\nr0.38 ![image](fig/speed.pdf){width=\"38%\"}\n:::\n\nDirect DD loss finetuning on GPT2 achieves accuracy of $45.4$ and $49.7$ for small and medium models, respectively, outperforming GPT2 AR finetuning. However, finetuning from already adapted diffusion language models (DiffuGPT) yields accuracy of $50.2$ and $61.8$ (Table [\\[tab:eval\\]](#tab:eval){reference-type=\"ref\" reference=\"tab:eval\"}). This demonstrates the superiority of DiffuGPT as the current best diffusion base model at this size and highlights that a better base model leads to improved results. Even with the same DD loss, DiffuGPT's finetuning converges faster and achieves lower loss, as shown in Appendix [9.2](#appendix:gsm){reference-type=\"ref\" reference=\"appendix:gsm\"}.\n\n**Inference Speed** AR models usually utilize key-value caching (incremental decoding; @ott-etal-2019-fairseq) to enhance throughput during decoding. However, due to the nature of sequential token generation, they are highly memory-bound and cannot fully exploit modern accelerators [@chen2023accelerating]. In contrast, diffusion models, despite not having a concept of caching and requiring self-attention over the entire sequence at each iteration, can operate with fewer iterations than the sequence length and exhibit less memory-bound behavior. Their performance can be further boosted with hardware-aware optimizations like flash-attention [@dao2022flashattention1; @dao2023flashattention2; @shah2024flashattention3]. In Figure [\\[fig:speed\\]](#fig:speed){reference-type=\"ref\" reference=\"fig:speed\"}, we evaluate the decoding latency with batch size 1 using flash-attention 2 and illustrate that our DiffuLLaMA achieves better inference efficiency using $T=256$ when generating sequences of length $1024$ or longer. This underscores the significant potential of diffusion models for efficient inference. Further decreasing $T$ can lead to faster decoding but may sacrifice quality. Additional latency comparisons are provided in Appendix [9.5](#appendix:speed){reference-type=\"ref\" reference=\"appendix:speed\"}.\n\n# Related Work\n\n**Continue Pre-training** Continue pre-training is commonly used in adapting an existing language model (LM) to a domain-specific LM [@ke2023continual] or enabling new abilities of LM, such as for longer context [@chen2024longlora] or code generation [@xu2024lemur]. Pre-training LMs is non-trivial and expensive [@samragh2024scaling], thus in exploring of new architectures of LMs such as Mamba [@gu2023mamba] and gated attention, @wang2024mamba [@zhang2024gated] choose to transfer from LMs to save the training cost. However, all these continue pre-training works follow the autoregressive (AR) language modeling, while adapting LMs into diffusion language model is more challenging due to discrepancies between their modeling objectives.\n\n**Text Diffusion Models** Diffusion models have demonstrated significant diversity and controllability in image generation [@ho2020denoising; @song2020denoising; @ramesh2022hierarchical]. Building on this success, line of research [@li2022diffusion; @gong2022diffuseq; @gong-etal-2023-diffuseq; @dieleman2022continuous] build continuous diffusion models for text generation tasks. Among them, @genie2023 experiment with a pre-training and finetuning framework under a small scale; @gulrajani2023likelihoodbased highlight the scaling law of continuous diffusion models, revealing that the compute-optimal requires longer training than their AR counterparts. To address the discrete nature of text, @austin2021structured [@hoogeboom2021argmax; @Zheng2023ARD] incorporate an absorbing `[MASK]` state as noise, laying the foundation for discrete diffusion models, which are further developed by @lou2023discrete [@shi2024simplified; @ou2024your; @zhao2024improving]. By connecting text diffusion models with pre-trained masked language models (MLMs; @devlin-etal-2019-bert), @ye2023diffusion [@he-etal-2023-diffusionbert] initialize discrete diffusion models using MLMs. Besides, the unification between diffusion and AR generation is also discussed in image generation [@li2024autoregressive]. However, the adaptation of diffusion models from AR LLMs remains unexplored.\n\n**Non-autoregressive Generation** Non-autoregressive (NAR) models, introduced by @gu2017non, break free from the left-to-right generation constraint, allowing for new capabilities like planning with future tokens [@wu2024do]. Current diffusion language models are a notable part of the NAR family [@gong2022diffuseq]. Given the challenges of developing NAR models, researchers often seek to find a trade-off. For instance, SSD-LM [@han-etal-2023-ssd] leverages diffusion models to iteratively generate text blocks, facilitating a semi-NAR generation process. Similarly, CLLM [@kou2024cllms] enhances LLMs by enabling the parallel generation of $n$ tokens, thereby improving decoding speed. FiLM [@shen2023film] adapts language models to generate tokens in any order, which is particularly useful for infilling tasks. [@guo2020fine] trains a NAR using a curriculum for the attention mask on translation tasks with seq2seq labels. Additionally, @gloeckle2024better focus on training models to achieve better and faster multi-token predictions as they scale up. These NAR approaches provide compelling alternatives to traditional AR LLMs, yet few have thoroughly explored training large NAR models on large-scale unlabeled data.\n\n# Conclusion\n\nBuilding on existing DLMs, we present a recipe for building DLMs by continuing training on off-the-shelf autoregressive LLMs. Our adaptation technique involves using 1) attention mask annealing to enable bidirectional modeling and 2) shift operation to allow similar training dynamics like AR models. By unifying the language modeling objectives of autoregressive and diffusion models, we train diffusion models up to 7B parameters. Through experiments on common sense reasoning, language modeling, math reasoning and code generation, we show that DiffuGPT and DiffuLLaMA have better performance compared to existing DLMs. We find that DiffuLLaMA is capable of following in-context demonstrations to some extent on math problems. In the future, we aim to instruction tune our DLMs and explore inference time planning methods. We release DiffuLLaMA and DiffuGPT for further exploration of diffusion models as an alternative language modeling method.\n\n### Author Contributions {#author-contributions .unnumbered}\n\nShansan Gong: Project lead, methodology development, DiffuGPT training and model evaluation, major writing. Shivam Agarwal: Methodology exploration, discussion, DiffuLLaMA training, writing. Yizhe Zhang: Discussion, DiffuLLaMA training, writing suggestions. Jiacheng Ye: Initial methodology exploration. Lin Zheng: Discussion, writing. Mukai Li & Chenxin An: Discussion, writing suggestions. Others: Mentorship and supervision.\n\n### Acknowledgments {#acknowledgments .unnumbered}\n\nResearch was supported in part by US DARPA INCAS Program No. HR0011-21-C0165 and BRIES Program No. HR0011-24-3-0325, National Science Foundation IIS-19-56151, the Molecule Maker Lab Institute: An AI Research Institutes program supported by NSF under Award No. 2019897, and the Institute for Geospatial Understanding through an Integrative Discovery Environment (I-GUIDE) by NSF under Award No. 2118329. This work used Delta AI at University of Illinois Urbana-Champaign through allocation CIS230229, CIS240488 from the Advanced Cyberinfrastructure Coordination Ecosystem: Services & Support (ACCESS) program, which is supported by U.S. National Science Foundation grants #2138259, #2138286, #2138307, #2137603, and #2138296.\n\nThis research was supported in part by the joint research scheme of the National Natural Science Foundation of China (NSFC) and the Research Grants Council (RGC) under grant number N_HKU714/21.\n\nThis work was also in part supported by research awards from Apple and the Allen Institute for AI.\n\n# Objective Derivations\n\nThis section provides detailed preliminary and loss derivations of §[2](#sec:prel){reference-type=\"ref\" reference=\"sec:prel\"} and §[3.1](#sec:continuous){reference-type=\"ref\" reference=\"sec:continuous\"} in the main paper.\n\n## Background of Diffusion Models\n\nWe denote $\\mathbf{x}_0\\sim p_{data}(\\mathbf{x}_0)$ as the variable following the data distribution, and $\\mathbf{x}_t\\sim q(\\mathbf{x}_t)$ as the noisy variable of $\\mathbf{x}_0$ at time $t$, where the maximum time is $T$. The forward process $$q(\\mathbf{x}_{1:T}|\\mathbf{x}_0)=\\prod_{t=1}^Tq(\\mathbf{x}_t|\\mathbf{x}_{t-1})$$ corrupts the initial data $\\mathbf{x}_0$ into a sequence of increasingly noisy variables $\\mathbf{x}_{1:T}$. Accordingly, the reverse Markov process models the joint probability as $$p_{\\theta}(\\mathbf{x}_{0:T})=p_{\\theta}(\\mathbf{x}_T)\\prod_{t=1}^Tp_{\\theta}(\\mathbf{x}_{t-1}|\\mathbf{x}_t),$$ which gradually denoises $\\mathbf{x}_t$ to reconstruct the original data $\\mathbf{x}_0$. Parameters $\\theta$ are learned by minimizing the negative log-likelihood of $\\mathbf{x}_0$, which can be optimized through the variational lower bound (VLB): $$\\begin{aligned}\n\\label{eq:ori-loss-appen}\n    -\\log p_{\\theta}(\\mathbf{x}_0) & \\leq \\mathbb{E}_{q(\\mathbf{x}_1|\\mathbf{x}_0)}[-\\log p_{\\theta}(\\mathbf{x}_0|\\mathbf{x}_1)] + D_{\\mathrm{KL}}(q(\\mathbf{x}_T|\\mathbf{x}_0)||p_{\\theta}(\\mathbf{x}_T))+\\mathcal{L}_T, \\\\\n    \\label{eq:ori-loss-line2-appen}\n    \\text{with }\n\\mathcal{L}_T & =\\sum_{t=2}^T\\mathbb{E}_{q(\\mathbf{x}_t|\\mathbf{x}_0)}[D_{\\mathrm{KL}}(q(\\mathbf{x}_{t-1}|\\mathbf{x}_t, \\mathbf{x}_0)||p_{\\theta}(\\mathbf{x}_{t-1}|\\mathbf{x}_t))].\n\\end{aligned}$$\n\nFor continuous text diffusion [@li2022diffusion; @gong2022diffuseq], at each forward step, perturbations are applied according to $$q(\\mathbf{x}_{t} \\vert \\mathbf{x}_{t-1}) = \\mathcal{N}(\\mathbf{x}_{t};\\sqrt{1-\\beta_t}\\mathbf{x}_{t-1}, {\\beta}_t \\mathbf{I}),$$ where $\\beta_t \\in (0,1)$ represents different scales. In the end, $\\mathbf{x}_T \\sim \\mathcal{N}(0, \\mathbf{I})$. In the case of discrete denoising models [@ho2020denoising; @austin2021structured; @Zheng2023ARD], $\\mathbf{x}_t$ follows a categorical distribution which naturally aligns with discrete text data. Let $\\bm{x}$ be the one-hot encoded sample of variable $\\mathbf{x}$ and $\\mathbf{x}_t\\sim \\text{Cat}(\\bm{x}_t;\\bm{p})$ represent a categorical distribution over vector $\\bm{x}$ with probabilities given by $\\bm{p}$. Here, $K$ represents the vocabulary size, $\\bm{x}\\in \\{\\bm{e}_1,\\dots,\\bm{e}_K\\}$, and $\\bm{e}_k \\in \\{0, 1\\}^K$ is the one-hot encoding of the $k$-th word category. The forward process can be formulated through a transition matrix $\\bm{Q}_t\\in[0,1]^{K\\times K}$ such that $$q(\\mathbf{x}_{t} \\vert \\mathbf{x}_{t-1})=\\text{Cat}(\\bm{x}_t;\\bm{Q}_t^\\top\\bm{x}_{t-1}); \\bm{Q}_t=(1-\\beta_t)I+\\beta_t\\mathbf{1}\\bm{e}_K^\\top,$$ with $\\mathbf{1}$ as an all-one vector of size $K$ and we assume $\\bm{e}_K$ as the special \\[mask\\] state, also defined as the absorbing state in discrete diffusion $\\bm{m}$. Each entry in $[\\bm{Q}_t]_{ij}$ denotes the probability of transition from the state $\\bm{e}_i$ to $\\bm{e}_j$, and thus the previously defined $\\bm{Q}_t$ means with probability $1-\\beta_t$, $\\bm{x}_t$ will stay unchanged and otherwise it will jump to the mask state $\\bm{e}_K$.\n\nStarting from $\\bm{x}_0$, the $t$-step marginal distribution and the posterior at previous time $t-1$ is respectively $$q(\\bm{x}_t|\\bm{x}_0)=\\text{Cat}(\\bm{x}_t;\\bm{p}=\\bm{\\overline{Q}}_t^\\top\\bm{x}_0); \\; q(\\bm{x}_{t-1}|\\bm{x}_t,\\bm{x}_0)=\n    \\frac{q(\\bm{x}_{t}|\\bm{x}_{t-1},\\bm{x}_0)q(\\bm{x}_{t-1}|\\bm{x}_0)}{q(\\bm{x}_t|\\bm{x}_0)}$$ where cumulative products $\\bm{\\overline{Q}}_t = \\prod_{i=1}^t\\bm{Q}_i=\\alpha_tI+(1-\\alpha_t)\\mathbf{1}\\bm{m}^\\top$, and $\\alpha_t=\\prod_{i=1}^t(1-\\beta_t)$. We expect $\\alpha_T$ approaches $0$ such that the full noise data $\\bm{x}_T$ is equal to $\\bm{e}_K$ with probability $1$. In the following sections, we primarily takes the discrete diffusion formulation.\n\n## Loss Derivation {#appendix:loss}\n\nPrevious discrete time $t\\in[0,T]$ restricts $\\bm{x}_t$ to fixed time points whereas the continuous-time sampling allows for more flexibility covering any point in the range [@kingma2021variational; @shi2024simplified; @zhao2024improving; @ou2024your]. In this case, $t$ runs from $0$ to $1$, corresponding to dividing $[0,1]$ into $T$ intervals and let $T\\rightarrow\\infty$. For any two arbitrary time points, $0\\leq s< t\\leq 1$, the forward modeling can be generalized from $q(\\mathbf{x}_{t} \\vert \\mathbf{x}_{t-1})$ to $q(\\mathbf{x}_{t} \\vert \\mathbf{x}_{s})$. We uniformly adopt the notation of continuous-time in following sections.\n\nFollowing the previous definition, after simplification, we have $q(\\bm{x}_t|\\bm{x}_0)=\\alpha_t\\bm{x}_0+(1-\\alpha_t)\\bm{m}$, referring to the probability of transition to absorbing mask state. Given $q(\\bm{x}_t|\\bm{x}_0)=q(\\bm{x}_t|\\bm{x}_s)q(\\bm{x}_s|\\bm{x}_0)$, we can derive the transition distribution between two arbitrary times $s$ and $t$: $$q(\\bm{x}_t|\\bm{x}_s)=\\text{Cat}(\\bm{x}_t;\\bm{\\overline{Q}}_{s|t}^\\top \\bm{x}_s), \\;\\text{with }\\bm{\\overline{Q}}_{s|t} = \\bm{\\overline{Q}}_s^{-1}\\bm{\\overline{Q}}_t=\\frac{\\alpha_t}{\\alpha_s}I+(1-\\frac{\\alpha_t}{\\alpha_s})\\mathbf{1}\\bm{m}^\\top.$$ Similarly, after simplification, $$q(\\bm{x}_t|\\bm{x}_s)=\\frac{\\alpha_t}{\\alpha_s}\\bm{x}_s+(1-\\frac{\\alpha_t}{\\alpha_s})\\bm{m}.$$ Following @Zheng2023ARD [@shi2024simplified] and extend the formulation to continuous time, we have the backward transition probability: $$\\label{eq:qxs-appen}\n    q(\\bm{x}_s|\\bm{x}_t, \\bm{x}_0) = \\frac{q(\\bm{x}_t|\\bm{x}_s)q(\\bm{x}_s|\\bm{x}_0)}{q(\\bm{x}_t|\\bm{x}_0)}=\n    \\begin{cases}\n        \\frac{1\\cdot(1-\\alpha_s)}{1-\\alpha_t} = \\frac{1-\\alpha_s}{1-\\alpha_t} = 1- \\frac{\\alpha_s-\\alpha_t}{1-\\alpha_t} & \\text{if } \\bm{x}_t=\\bm{x}_s=\\bm{m}, \\\\\n        \\frac{(1-\\frac{\\alpha_t}{\\alpha_s})\\cdot\\alpha_s}{1-\\alpha_t} = \\frac{\\alpha_s-\\alpha_t}{1-\\alpha_t} & \\text{if }\\bm{x}_t=\\bm{m}\\neq \\bm{x}_s. \\\\\n    \\end{cases}$$ For $\\bm{x}_t\\neq\\bm{m}$, the $q(\\bm{x}_s|\\bm{x}_t, \\bm{x}_0)$ will stick to the observed data. For $\\bm{x}_t=\\bm{m}$, we get the simplified $$\\label{eq:appen-qxs}\nq(\\bm{x}_s|\\bm{x}_t, \\bm{x}_0)=\\frac{\\alpha_s-\\alpha_t}{1-\\alpha_t}\\bm{x}_0+\\frac{1-\\alpha_s}{1-\\alpha_t}\\bm{m}.$$ In diffusion process, the generative model aims to approximate the reverse transitions using a denoising model $p_{\\theta}(\\bm{x_s}|\\bm{x}_t, f_{\\theta}(\\bm{x}_t))\\rightsquigarrow q(\\bm{x}_s|\\bm{x}_t,\\bm{x}_0)$, where $f_{\\theta}(\\bm{x}_t)$ represents the probability vector obtained from the softmax applied to the logits generated by the neural network, usually using transformer networks [@NIPS2017_3f5ee243] in text domain. We can similarly have $$\\label{eq:appen-px}\np_{\\theta}(\\bm{x_s}|\\bm{x}_t)=\\frac{\\alpha_s-\\alpha_t}{1-\\alpha_t}f_{\\theta}(\\bm{x}_t)+\\frac{1-\\alpha_s}{1-\\alpha_t}\\bm{m}.$$ Given Eq.[\\[eq:appen-qxs\\]](#eq:appen-qxs){reference-type=\"ref\" reference=\"eq:appen-qxs\"} and Eq.[\\[eq:appen-px\\]](#eq:appen-px){reference-type=\"ref\" reference=\"eq:appen-px\"}, the KL-divergence loss is optimized by $$D_{\\mathrm{KL}}(q(\\bm{x}_s|\\bm{x}_t,\\bm{x}_0)||p_{\\theta}(\\bm{x}_s||\\bm{x}_t)) = \n    \\begin{cases}\n        \\frac{\\alpha_s-\\alpha_t}{1-\\alpha_t}D_{\\mathrm{KL}}(\\bm{x}_0||f_{\\theta}(\\bm{x}_t)), & \\text{for } \\bm{x}_t =\\bm{m};\\\\\n        0, & \\text{for } \\bm{x}_t \\neq \\bm{m}.\\\\\n    \\end{cases}$$ We can use the indicator function $\\delta_{\\bm{x}_t,\\bm{m}}$ to unify the conditional cases. In addition, given $\\bm{x}_0$, we have $D_{\\mathrm{KL}}(\\bm{x}_0||f_{\\theta}(\\bm{x}_t))=-\\bm{x}_0^\\top\\log f_{\\theta}(\\bm{x}_t)$ which corresponds to the cross-entropy widely used in the classification. Therefore, we have $$D_{\\mathrm{KL}}(q(\\bm{x}_s|\\bm{x}_t,\\bm{x}_0)||p_{\\theta}(\\bm{x}_s||\\bm{x}_t))=-\\frac{\\alpha_s-\\alpha_t}{1-\\alpha_t}\\delta_{\\bm{x}_t,\\bm{m}}\\bm{x}_0^\\top\\log f_{\\theta}(\\bm{x}_t).$$ Following Eq.[\\[eq:ori-loss-line2-appen\\]](#eq:ori-loss-line2-appen){reference-type=\"ref\" reference=\"eq:ori-loss-line2-appen\"}, if we set a small timestep $\\Delta_t=t-s=\\frac{1}{T}\\in(0,1)$, $$\\mathcal{L}_T = \\sum_{t=2}^T [-\\frac{\\alpha_s-\\alpha_t}{(t-s)(1-\\alpha_t)}\\delta_{\\bm{x}_t,\\bm{m}}\\bm{x}_0^\\top\\log f_{\\theta}(\\bm{x}_t)\\Delta_t].$$ By taking the limit as $T\\rightarrow \\infty$, we have $\\alpha_t^\\prime=\\frac{\\alpha_t-\\alpha_s}{t-s}$, and the sum is transformed into an integral: $$\\label{eq:final-imit-loss-app}\n    \\lim_{T\\rightarrow \\infty}\\mathcal{L}_T = \\int_{0}^{1}\\frac{\\alpha_t^\\prime}{1-\\alpha_t}\\mathbb{E}_{q(\\mathbf{x}_t|\\mathbf{x}_0)}[\\delta_{\\bm{x}_t,\\bm{m}}\\bm{x}_0^\\top\\log f_{\\theta}(\\bm{x}_t)]\\,dt.$$ Also, the first two terms in Eq.[\\[eq:ori-loss-appen\\]](#eq:ori-loss-appen){reference-type=\"ref\" reference=\"eq:ori-loss-appen\"} are $\\rightarrow 0$ and a constant, respectively. Thus we can formulate the evidence lower bound (ELBO) of $-\\log p_\\theta(\\mathbf{x}_0)$ as Eq.[\\[eq:final-imit-loss-app\\]](#eq:final-imit-loss-app){reference-type=\"ref\" reference=\"eq:final-imit-loss-app\"}.\n\nThe same form of ELBO which is invariant to noise schedule but related to the signal-to-noise ratio (SNR) is also introduced in @kingma2021variational [@shi2024simplified]. Following @austin2021structured [@Zheng2023ARD], we choose the noise schedule $\\alpha_t=1-t$, then $\\frac{-\\alpha_t^\\prime}{1-\\alpha_t}=\\frac{1}{t}$.\n\nThe previous discussion focused on the single token $\\bm{x}_t$, and can be easily extended to a text sequence of length $N$ represented as $\\mathbf{x}_t=[\\bm{x}_t^{1}, \\bm{x}_t^{2}\\dots, \\bm{x}_t^{N}]$. The final loss of the whole sequence is $$\\label{eq:dm-loss-appen}\n    \\mathcal{L}_{t}^{1:N} = \\frac{1}{t}\\mathbb{E}_{q(\\mathbf{x}_t|\\mathbf{x}_0)}\\left[-\\sum_{n=1}^N\\delta_{\\mathbf{x}_t^n,\\bm{m}}(\\mathbf{x}_0^{n})^\\top\\log f_{\\theta}(\\mathbf{x}_t^{1:N})_n\\right],$$ where $f_{\\theta}(\\mathbf{x}_t^{1:N})_n$ denotes the whole input sequence is fed into the transformer model and the $n$-th output token is indexed. During training, we sample $t$ for each data point to optimize the expectation in $\\mathcal{L}_t^{1:N}$ instead of the integral $\\mathcal{L}_T$, while for evaluation, we use integral $\\mathcal{L}_T$.\n\n# Implementation Details\n\n## Training data\n\n#### DiffuGPT\n\nPrevious diffusion language models such as Plaid 1B [@gulrajani2023likelihoodbased], SEDD [@lou2023discrete] and MD4 [@shi2024simplified] use OpenWebText [@Gokaslan2019OpenWeb] to pre-train from scratch, referring to GPT2 [@Radford2019LanguageMA]. We choose the advanced FineWeb[^2] corpus [@penedo2024finewebdatasetsdecantingweb], which is also derived from Common Crawl. We randomly sample 30 billion tokens from subset `sample-100BT`.\n\n#### DiffuLLaMA\n\nFollowing [@zhang2024tinyllamaopensourcesmalllanguage][^3] we construct the training data for DiffuLLaMA by mixing SlimPajama [@cerebras2023slimpajama] and Starcoder data [@li2023starcoder]. We randomly sample 65 billion tokens in the ratio of 7:3 from SlimPajama and Starcoder, respectively. We use sequence packing and pre-tokenize the dataset for efficient computing.\n\n#### Data Selection Consideration\n\nOur dataset selection aims to align with their respective pre-training objectives. Since we train on a relatively smaller number of tokens and do not intend to introduce new capabilities, we prioritize maintaining continuity with the models' pre-training distributions to minimize distributional shift. For GPT2-based DiffuGPT, we use the FineWeb dataset, which closely resembles OpenWebText (the dataset used in GPT2's pre-training). Considering that SEDD [@lou2023discrete] iterates OpenWebText for more than 1 epoch and the total training amount is around 200B tokens, we also iteratively train DiffuGPT on 30B FineWeb data to more than 100B training tokens. In contrast, LLaMA2 is pre-trained over 1T tokens within one epoch on web and code data. To align with this, we follow TinyLLaMA [@zhang2024tinyllama] and use a mixture of SlimPajama and Starcoder data, designed to reflect LLaMA2's pre-training data.\n\n## Model optimization and hyperparameters\n\n#### DiffuGPT\n\nWe implement DiffuGPT using LLaMA-Factory[^4] with DeepSpeed Zero-2 parallelization [@rajbhandari2020zero]. The hyperparameter setting compared with previous work is listed in Table [1](#tab:training-appen){reference-type=\"ref\" reference=\"tab:training-appen\"}. The global batch size is calculated by multiplying the single GPU batch size, the number of gradient accumulation steps, and the number of GPUs, where we use 8 A100 80G. We use learning rate of $3e-4$ with cosine scheduler. The warm up steps are set to 2K and attention mask annealing steps are 10K. As shown in Table [1](#tab:training-appen){reference-type=\"ref\" reference=\"tab:training-appen\"}, our effective training tokens are less or equal than SEDD and MD4, while DiffuGPT exhibits better performance according to Table [\\[tab:eval\\]](#tab:eval){reference-type=\"ref\" reference=\"tab:eval\"} and Figure [\\[fig:ungen-ppl\\]](#fig:ungen-ppl){reference-type=\"ref\" reference=\"fig:ungen-ppl\"}.\n\n::: {#tab:training-appen}\n  Models                      Training steps   Global batch size   Context length     \n  -------------------------- ---------------- ------------------- ---------------- -- --\n  SEDD [@lou2023discrete]          400k               512               1024          \n  MD4 [@shi2024simplified]        1000k               512               1024          \n  DiffuGPT-S                      1000k               256               512           \n  DiffuGPT-M                       160k              1280               1024          \n\n  : Training settings for different diffusion language models.\n:::\n\n[]{#tab:training-appen label=\"tab:training-appen\"}\n\n#### DiffuLLaMA\n\nFor a more efficient pre-training, we implement DiffuLLaMA using huggingface[^5]. We use DeepSpeed Zero-3 parallelization with CPU offloading [@rajbhandari2020zero] to efficiently scale DiffuLLaMA to multiple GPUs and nodes. Furthermore, we use flash-attention 2 and fused cross-entropy loss for optimized GPU memory usage and compute time [@dao2023flashattention2]. With these settings, we get set a batch size of 60 per GPU with context length 2048 on a GH200 96GB GPU. We use AdamW [@loshchilov2018decoupled] to optimize our models with a constant learning rate of $2e-5$ and accumulate gradients every $4$ steps. We train our model for 65 billion tokens on $16$ 4xGH200 nodes. []{#appendix:train label=\"appendix:train\"}\n\n#### Tokenizer\n\nDuring adaptation, we do not change the tokenizer of the base model. In theory, we should expand the original vocabulary by adding an additional dimension to include a special token as `[MASK]` token. However, considering practical issues on implementation, we can alternatively select an existing word from the vocabulary to serve as the `[MASK]` token. It is preferable that this chosen word has a particularly low frequency of occurrence in corpus. For DiffuGPT-S we use `tokenid=10541` and for DiffuGPT-M we set a new `[MASK]` token with `tokenid=50257`. For DiffuLLaMA, we set `tokenid=811`.\n\n## Evaluation Details {#appendix:imple-eval}\n\n#### Generation tasks\n\nFor the TriviaQA and Lambada sentence completion tasks, we generate $n$-tokens for continue-writing. In triviaQA, we set $n$ to the oracle length plus an additional $10$ tokens, and we only evaluate the first 2000 cases in this dataset for efficiency. For Lambada, which requires the completion of the last word, we set $n$ to oracle length of that word's tokens, which might be larger than $1$ based on the tokenizer. For DLMs, we set the diffusion timesteps $T$ to the required generation length. For AR baselines, we cut off maximum new tokens. For SATMATH and MAWPS, we integrate our model into `math-evaluation-harness`[^6].\n\n#### CommonSense Reasoning tasks\n\nThe 4 commonSense reasoning tasks are multiple-choices questions with 4 options. Instead of open generation, we calculate the diffusion loss for each `Question+choice` pair using Eq.[\\[eq:dm-loss-appen\\]](#eq:dm-loss-appen){reference-type=\"ref\" reference=\"eq:dm-loss-appen\"}. A lower loss (perplexity) indicates the model thinks that choice most suitable. This approach is commonly employed in ICL of LLMs [@wu-etal-2023-self]. We also use this for AR baselines.\n\n#### Finetune GSM8K-symbolic\n\nThe setting of finetune GSM8K-symbolic dataset is following @ye2024diffusion[^7], which enables the diffusion model to perform chain-of-thought reasoning. For DiffuLLaMA, we use parameter-efficient-finetune: LoRA Tuning [@hu2022lora]. We set rank to 8 and enable the finetuning of the word embedding layer, with 151 million (2%) parameters involved. For this task, we use $T=64$ for the decoding of DLMs.\n\n#### Infilling tasks\n\nFor ROCstories, where each case is a $5$-sentence story, we setup evaluation referring @shen2023film.The model is tasked with infilling the third sentence based on the first two and last two sentences. We evaluate the first 1000 cases in this dataset for efficiency. For code infilling, we use humaneval-single-line infilling [^8] and their evaluation toolkit, which contains 1033 test cases. We implement infilling tasks for AR models by feeding the prefix and cutting off the generation length using the oracle length, considering that these AR models are not supporting infilling. We also try to feed the suffix information using the instruction like `Given prefix and suffix please infill the middle`, however, LLaMA2 can not follow this instruction. For AR LLMs, to perform infilling tasks requires additional FIM training [@roziere2023code] or carefully instruction tuning.\n\n#### Unconditional Generation\n\nFor unconditional generation in Figure [\\[fig:ungen-ppl\\]](#fig:ungen-ppl){reference-type=\"ref\" reference=\"fig:ungen-ppl\"}, we set the temperature of top-$k$ to 0.98 and top-p to 0.9 for the medium-sized model, while using top-$k$ of 1.0 and top-p of 0.9 for the small model. We generate 64 samples and evaluate the perplexity using the GPT-2 large model, aligning with @lou2023discrete [@shi2024simplified].\n\n# Additional Results\n\n## Unconditional generation {#appendix:un-gen}\n\nThe generation quality is different for different hyperparameters, shown in Figure [3](#fig:sample-ppl-appen){reference-type=\"ref\" reference=\"fig:sample-ppl-appen\"}. Lowering the temperature increases fluency but reduces diversity, leading to noticeable repetition in sentences.\n\n![The unconditional generation quality for different diffusion time steps $T$ and sampling algorithms. We annotate the temperature of top-$k$ sampling and top-p sampling.](ppl.pdf){#fig:sample-ppl-appen width=\"50%\"}\n\nWe randomly selected samples generated by our DiffuGPT-M models with 1024 tokens for various $T$, as shown in Table [\\[tab:sample1-appen\\]](#tab:sample1-appen){reference-type=\"ref\" reference=\"tab:sample1-appen\"}, Table [\\[tab:sample2-appen\\]](#tab:sample2-appen){reference-type=\"ref\" reference=\"tab:sample2-appen\"}, and Table [\\[tab:sample3-appen\\]](#tab:sample3-appen){reference-type=\"ref\" reference=\"tab:sample3-appen\"}. Lower $T$ values result in less fluent text.\n\n## Ablation on GSM8K-symbolic {#appendix:gsm}\n\nUsing the same discrete diffusion loss, if we direct finetune on GPT2 achieves accuracy of $45.4$ and $49.7$ for small and medium models, respectively. In contrast, Finetuning from DiffuGPT yields accuracy of $50.2$ and $61.8$ (Table [\\[tab:eval\\]](#tab:eval){reference-type=\"ref\" reference=\"tab:eval\"}). Comparing with GPT2, DiffuGPT, as the base model, converges faster and attains a lower loss, as shown in Figure [4](#fig:gsm-loss){reference-type=\"ref\" reference=\"fig:gsm-loss\"}. This indicates that a better base model leads to improved results and also demonstrates the superiority of DiffuGPT as the current best diffusion base model.\n\nTraining with the initial weightings of GPT2, we evaluate three loss functions: DD, DD (no shift), and DD (no annealing) when finetuning on GSM8K-symbolic data. The corresponding loss and accuracy are shown in Table [2](#tab:elbo-gsm){reference-type=\"ref\" reference=\"tab:elbo-gsm\"}. Additionally, results for DiffuGPT and DiffuLLaMa are presented. All results highlight the negative correlation between the loss and accuracy.\n\n::: {#tab:elbo-gsm}\n  Models                    Loss (ELBO)    Acc\n  ------------------------- ------------- ------\n  GPT2-M + DD               0.015          49.7\n  GPT2-M + DD (no shift)    0.028          34.5\n  GPT2-M + DD (no anneal)   0.019          47.2\n  DiffuGPT                  0.009          61.8\n  DiffuLLaMA                0.003          63.1\n\n  : The training loss (ELBO) and test accuracy on the GSM8K-symbolic dataset.\n:::\n\n[]{#tab:elbo-gsm label=\"tab:elbo-gsm\"}\n\n![Finetune GSM8K data with discrete diffusion objectives, using a base model of either GPT2-S/M or DiffuGPT-S/M. DiffuGPT converges faster and attains a lower loss.](train-loss-gsm.pdf){#fig:gsm-loss width=\"50%\"}\n\n## Advantages of DLMs\n\nTo explore the self-correction advantages of DLMs noted by @ye2024diffusion, we perform a qualitative analysis and find a similar self-correction capability in DiffuGPT. Our observation of the final steps of sampling trajectories, as shown in Table [3](#tab:self-correct){reference-type=\"ref\" reference=\"tab:self-correct\"}, indicates that DLMs refine intermediate numbers without adhering to a left-to-right constraint.\n\n::: {#tab:self-correct}\n   Steps ($t/T$)                                              DoT rationales\n  --------------- -------------------------------------------------------------------------------------------------------\n       \\...                                                        \\...\n       9/32                      `<<3*15=45>> <<4*45=180>> <<180+300=`[`00>> #### 00`]{style=\"color: red\"}\n       8/32                  `<<3*15=45>> <<4*45=180>> <<180+`[`400=580000 #### #### 000`]{style=\"color: red\"}\n       7/32                    `<<3*15=45>> <<4*45=180>> <<180+`[`400=400`]{style=\"color: red\"}`>> #### 480`\n       6/32        `<<3*15=45>> <<4*45=180>> <<180+300=`[`500`]{style=\"color: red\"}`>> #### `[`580`]{style=\"color: red\"}\n       5/32        `<<3*15=45>> <<4*45=180>> <<180+300=`[`580`]{style=\"color: red\"}`>> #### `[`580`]{style=\"color: red\"}\n       4/32                     `<<3*15=45>> <<4*45=180>> <<180+300=480>> #### `[`580`]{style=\"color: red\"}\n       3/32                     `<<3*15=45>> <<4*45=180>> <<180+300=480>> #### `[`580`]{style=\"color: red\"}\n       2/32                                 `<<3*15=45>> <<4*45=180>> <<180+300=480>> #### 480`\n       1/32                                 `<<3*15=45>> <<4*45=180>> <<180+300=480>> #### 480`\n\n  : A Case study to show self-correction capacity of DiffuGPT. $t/T$ refers to the current decoding step over the total diffusion steps. The incorrect rationales are marked in red.\n:::\n\n[]{#tab:self-correct label=\"tab:self-correct\"}\n\nFor global planning, we follow @ye2024autoregressiondiscretediffusioncomplex to finetune DLMs on counting down (CD) datasets. CD is a mathematical reasoning challenge and a generalized version of the game 24, which many AR models struggle with [@gandhi2024stream]. We compare DiffuGPT with other AR baselines with different model sizes in Table [4](#tab:sos){reference-type=\"ref\" reference=\"tab:sos\"}, demonstrating the advantages of DLMs.\n\n::: {#tab:sos}\n  Models                    Size    CD4\n  ------------------------- ------ ------\n  GPT2-scratch              85M     45.8\n  LLaMA FT                  13B     51.1\n  SoS [@gandhi2024stream]   250M    54.2\n  DiffuGPT                  355M    87.5\n\n  : The finetuning results (accuracy) on the CD4 dataset.\n:::\n\n[]{#tab:sos label=\"tab:sos\"}\n\nFor infilling tasks, we attempt to query the LLaMA model with the prompt `given the <prefix> and <suffix>, please answer the <middle> part`, which includes both prefix and suffix information. However, this approach is no better than simply completing the prefix, likely because the LLaMA model needs tuning for filling in the middle (FIM; @bavarian2022efficienttraininglanguagemodels). Additionally, @bavarian2022efficienttraininglanguagemodels notes that using AR models for infilling presents challenges, such as prompting difficulties and repetition. In contrast, DLMs are naturally suited for this task, as they are trained to handle masked inputs, which is a key advantage.\n\nAdditionally, we conduct a controlled experiment by training both AR and DLMs on 100M tokens from the Starcoder dataset, using CodeLLaMA as the base model and evaluating performance on HumanEval infilling. We finetune CodeLLaMA autoregressively with FIM in both suffix-prefix-middle (SPM) and prefix-suffix-middle (PSM) formats. Our results in Table [5](#tab:code){reference-type=\"ref\" reference=\"tab:code\"} show that DiffuCodeLLaMA outperforms PSM, suggesting that prompt format affects AR models but not DLMs. We believe that training on more than 100M tokens, which is relatively small, could enhance performance.\n\n::: {#tab:code}\n  Models                    Pass@1 HumanEval Infilling\n  ------------------------ ----------------------------\n  CodeLLaMA FT (FIM-SPM)               0.80\n  CodeLLaMA FT (FIM-PSM)               0.74\n  Diffu-CodeLLaMA (Ours)               0.76\n\n  : Models finetuned on 100M tokens of Starcoder and their results on HumanEval Infilling.\n:::\n\n[]{#tab:code label=\"tab:code\"}\n\n## Continual pre-training AR models\n\nWe conduct a continual pre-training of GPT2 on the same corpus under the same settings as DiffuGPT. However, the zero-shot performance, shown in Table [6](#tab:tasks){reference-type=\"ref\" reference=\"tab:tasks\"}, indicates no improvement. This may be due to the stability gap introduced by continual pre-training [@guo2024efficientcontinualpretrainingmitigating], leading to performance degradation. Additionally, since our used corpus is similar to the one used for GPT2's initial pre-training, continual pre-training may offer limited new knowledge.\n\n::: {#tab:tasks}\n  Models                        HSwag   Wino   SIQA   PIQA   Code\n  ---------------------------- ------- ------ ------ ------ ------\n  GPT2-M                        38.3    50.7   37.7   67.4   2.6\n  GPT2-M (continue pretrain)    36.7    49.4   37.9   66.7   2.6\n  DiffuGPT-M                    37.2    52.6   39.0   59.6   2.9\n\n  : Performance of different models on various tasks.\n:::\n\n[]{#tab:tasks label=\"tab:tasks\"}\n\n## Decoding Speed Testing {#appendix:speed}\n\nWe evaluate the inference time of LLaMA2 and DiffuLLaMA for unconditional text generation across various lengths. Our tests include vanilla attention, flash attention 2, and the torch version of flash attention SDPA, as shown in Table [7](#tab:appen-speed){reference-type=\"ref\" reference=\"tab:appen-speed\"}.\n\n::: {#tab:appen-speed}\n   **Length**  **Attention**        **DiffuLLaMA (sec)**   **LLaMA (sec)**\n  ------------ ------------------- ---------------------- -----------------\n      512      flash-attention 2            12.5                 9.2\n      1024     SDPA                         13.2                16.3\n      1024     flash-attention 2            13.3                17.5\n      1024     vanilla                      16.2                17.2\n      2048     SDPA                         28.5                29.5\n      2048     flash-attention 2            23.5                35.7\n      2048     vanilla                      38.1                32.8\n\n  : Single batch inference time for different attention implementation and generation lengths.\n:::\n\n[]{#tab:appen-speed label=\"tab:appen-speed\"}\n\nYet smaller $T$ leads to faster generation, in downstream tasks like multiple-choices, this may slightly impact accuracy. Examples are provided in Table [8](#tab:diffullama-t){reference-type=\"ref\" reference=\"tab:diffullama-t\"}.\n\n::: {#tab:diffullama-t}\n  Models             HSwag   Wino   SIQA   PIQA\n  ----------------- ------- ------ ------ ------\n  DiffuLLaMA T=32    58.7    56.4   43.2   63.3\n  DiffuLLaMA T=8     47.1    52.6   41.9   57.1\n\n  : Performance of DiffuLLaMA models on various tasks.\n:::\n\n[]{#tab:diffullama-t label=\"tab:diffullama-t\"}\n\n[]{#tab:sample1-appen label=\"tab:sample1-appen\"}\n\n[]{#tab:sample2-appen label=\"tab:sample2-appen\"}\n\n[]{#tab:sample3-appen label=\"tab:sample3-appen\"}\n\n[^1]: This is the total number of tokens used; however, our effective training tokens exceed this count, meaning that we train for more than one epoch.\n\n[^2]: <https://huggingface.co/datasets/HuggingFaceFW/fineweb>\n\n[^3]: <https://github.com/jzhang38/TinyLlama>\n\n[^4]: <https://github.com/hiyouga/LLaMA-Factory>\n\n[^5]: <https://github.com/huggingface/transformers>\n\n[^6]: <https://github.com/ZubinGou/math-evaluation-harness>\n\n[^7]: <https://github.com/HKUNLP/diffusion-of-thoughts>\n\n[^8]: <https://github.com/openai/human-eval-infilling>",
    "rationale": "Summary: The paper addresses limitations in autoregressive (AR) models, particularly issues with global planning and intermediate token correction. In response, Diffusion Language Models (DLMs) were introduced but face scalability challenges due to the computational costs of training with only 400 billion tokens. This work proposes a novel training mechanism that redefines DLM objectives. The approach also addresses architectural limitations in AR models, using attention mask annealing and a shift operation to remove causal masking bias. Scaled models ranging from 127 million to 7 billion parameters, specifically DiffuGPT and DiffuLLaMA, demonstrate better or comparable performance to existing AR models. Additionally, the authors release open-source code for these models, providing valuable resources to the research community for faster inference speeds with minimal iterations.\n\nStrengths: - Well-written and easy to understand.\n- Comprehensive evaluation on multiple tasks, including word completion, reading comprehension, commonsense reasoning, math problems, and coding.\n- Ablation study conducted to emphasize the importance of shift operation and attention mask annealing.\n- Mathematically grounded and novel approach.\n- Demonstrates increased token diversity without compromising quality.\n\nWeaknesses: - Missing qualitative examples and human evaluation.\n- No statistical significance testing across tasks.\n- The current T is set to 32. It would help to see the quality performance across different T, since it was mentioned in section 4.5 that decreasing T leads to faster decoding but with loss of quality.\n\nQuestions: The current T is set to 32. It would help to see the quality performance across different T, since it was mentioned in section 4.5 that decreasing T leads to faster decoding but with loss of quality.\nPlease include this.",
    "rating": 3,
    "label": "novel",
    "rationale_edited": "The paper addresses limitations in autoregressive (AR) models, particularly issues with global planning and intermediate token correction. In response, Diffusion Language Models (DLMs) were introduced but face scalability challenges due to the computational costs of training with only 400 billion tokens. This work proposes a novel training mechanism that redefines DLM objectives. The approach also addresses architectural limitations in AR models, using attention mask annealing and a shift operation to remove causal masking bias. Scaled models ranging from 127 million to 7 billion parameters, specifically DiffuGPT and DiffuLLaMA, demonstrate better or comparable performance to existing AR models. Additionally, the authors release open-source code for these models, providing valuable resources to the research community for faster inference speeds with minimal iterations.",
    "chosen": true
  },
  {
    "title": "Behind the Myth of Exploration in Policy Gradients",
    "abstract": "Policy-gradient algorithms are effective reinforcement learning methods for solving control problems with continuous state and action spaces. To compute near-optimal policies, it is essential in practice to include exploration terms in the learning objective. Although the effectiveness of these terms is usually justified by an intrinsic need to explore environments, we propose a novel analysis and distinguish two different implications of these techniques. First, they enable a smooth learning objective and eliminate local optima while preserving the global maximum. Second, they modify the gradient estimates, increasing the probability that the stochastic parameter update eventually provides an optimal policy. In light of these effects, we discuss and illustrate empirically exploration strategies based on entropy bonuses, highlighting their limitations and opening avenues for future works in the design and analysis of such strategies.",
    "text": "# Introduction {#sec:introduction}\n\nMany practical problems require making sequential decisions in environments. Reinforcement learning (RL) is a framework for solving such decision-making problems that has been successful on complex tasks, including playing games [@mnih2015human; @silver2017mastering], operating power systems [@aittahar2014optimal], controlling robots [@kalashnikov2018qt], or interacting with electricity markets [@boukas2021deep].\n\nReinforcement learning algorithms interact with an environment to gather information about this environment, which in turn enables to compute and follow an optimal policy. This creates a trade-off between exploration and exploitation. In short, in order to eventually compute a good policy, it is necessary to obtain additional information about the environment by taking actions that are likely not optimal. In algorithms where the trade-off is explicit, exploration is well-understood and has been the subject of many works [@dann2017unifying; @azar2017minimax; @neu2020unifying]. In policy-gradient algorithms, one can most often not distinguish exploration from exploitation. Nevertheless, a main theoretical requirement to converge towards globally (or even locally) optimal solutions is that policies remain sufficiently stochastic during the learning procedure [@bhandari2019global; @bhatt2019policy; @agarwal2020optimality; @zhang2021sample; @bedi2022hidden]. Interestingly, neither softmax nor Gaussian policies guarantee enough stochasticity for ensuring (fast) convergence [@mei2020escaping; @mei2021understanding; @bedi2022hidden]. This requirement of stochasticity in policy-gradient algorithms is often abusively called exploration and understood as the need to infinitely sample all states and actions.\n\nPractitioners have tried to meet the theoretical requirement of sufficient randomness of policies in policy gradient via reward-shaping strategies, whereby a learning objective that promotes or hinders behaviors by providing reward bonuses for some states and actions is optimized as a surrogate to the return of the policy. These bonuses typically promote actions that reduce the uncertainty of the agent about its environment [@pathak2017curiosity; @burda2018large; @zhang2021noveld], or that maximize the entropy of states and/or actions [@williams1991function; @bellemare2016unifying; @haarnoja2019soft; @hazan2019provably; @lee2019efficient; @islam2019marginalized; @guo2021geometric; @zhang2021made]. Optimizing a surrogate objective is particularly effective for solving tasks with complex dynamics and reward functions, or with sparse rewards.\n\nThe differences between theory and practical implementations of exploration has led to common folklore seeking to provide intuition for the efficiency of policy-gradient methods. This work is part of the research line that studies the maximization of practical surrogate learning objective functions from a mathematical optimization perspective. Close to our work, studies of the learning objective with entropy regularization (an exploration-based reward shaping technique where the entropy of the policy is added in the learning objective) were conducted. It includes the study by @ahmed2019understanding concluding that it helps to provide smooth learning objective functions. The same exploration strategy was reinterpreted as a robust optimization method by @husain2021regularized and equivalently as a two-player game by @brekelmans2022your. @bolland2023policy furthermore argued that optimizing an entropy regularized objective is equivalent to optimizing the return of another policy with larger variance. More general studies on the learning dynamics have focused on the influence of baselines in policy gradient [@chung2021beyond], and reward-shaping strategies that do not modify the learning objective, called potential based [@ng1999policy; @wiewiora2003principled; @harutyunyan2015expressing; @forbes2024potential]. All these studies are too restrictive and the literature lacks unified explanations and interpretations about exploration in policy gradients.\n\nBefore delving into our contributions, we recall that the convergence of stochastic ascent methods is driven by the objective function and how the ascent directions are estimated. First, the objective function shall be (pseudo) concave to find its global maximum [@leon1998online]. Second, the convergence rate is influenced by the distribution of the stochastic ascent estimates [@chen2018stochastic; @ajalloeian2020convergence]. In this paper, we rigorously study policy-gradient methods with exploration-based reward shaping through the lens of these two optimization theory aspects. To that end, we first introduce two new criteria that relate the return of a policy to the learning objective with exploration bonuses, and their respective optima. Second, we introduce two additional criteria on the distribution of the gradient estimates of the learning objective and their likelihood of providing directions in which the learning objective and the return increase. Importantly, these criteria are general to any reward-shaping strategy, and highlight the importance of reward shaping that modify the optimal control behavior, in opposition to the literature on potential-based reward shaping. The influence of some exploration bonuses are illustrated and discussed in the light of these four criteria. In practice, finding good exploration strategies is problem specific and we thus introduce a general framework for the study and interpretation of exploration in policy-gradient methods instead of trying to find the best exploration method for a given task.\n\nThe paper is organized as follows. In Section [2](#sec:background){reference-type=\"ref\" reference=\"sec:background\"}, we provide the background about policy gradients and exploration. Section [3](#sec:learning_objective){reference-type=\"ref\" reference=\"sec:learning_objective\"} focuses on the effect of exploration on the learning objective while Section [4](#sec:ascent_direction){reference-type=\"ref\" reference=\"sec:ascent_direction\"} is dedicated to the effect on the gradient estimates used in the policy-gradient algorithms[^1]. Conclusions and future works are discussed in Section [5](#sec:conclusion){reference-type=\"ref\" reference=\"sec:conclusion\"}.\n\n# Background {#sec:background}\n\nIn this section, we introduce Markov decision processes and policy gradients with intrinsic exploration.\n\n## Markov Decision Processes\n\nWe study problems in which an agent makes sequential decisions in a stochastic environment [@sutton2018reinforcement]. The environment is modeled with an infinite-time Markov decision process (MDP) composed of a state space $\\mathcal{S}$, an action space $\\mathcal{A}$, an initial state distribution with density $p_0$, a transition distribution (modeling the dynamics) with conditional density $p$, a bounded reward function $\\rho$, and a discount factor $\\gamma \\in [0, 1)$. When an agent interacts with the MDP, first, an initial state $s_0 \\sim p_0(\\cdot)$ is sampled, then, the agent provides at each time step $t$ an action $a_t \\in \\mathcal{A}$ leading to a new state $s_{t+1} \\sim p(\\cdot|s_t, a_t)$. Such a sequence of states and actions $h_t = (s_0, a_0, \\dots, s_{t-1}, a_{t-1}, s_t) \\in \\mathcal{H}$ is called a history and $\\mathcal{H}$ is the set of all histories of any arbitrary length. In addition, after an action $a_t$ is executed, a reward $r_{t} = \\rho(s_t, a_t) \\in \\mathbb{R}$ is observed.\n\nA policy $\\pi \\in \\Pi = \\mathcal{S} \\rightarrow \\mathcal{P}(\\mathcal{A})$ is a mapping from the state space $\\mathcal{S}$ to the set of probability measures on the action space $\\mathcal{P}(\\mathcal{A})$, where $\\pi(a|s)$ is the associated conditional probability density of action $a$ in state $s$. The function $J:\\Pi \\rightarrow \\mathbb{R}$ is defined as the function mapping any policy $\\pi$ to the expected discounted sum of rewards gathered by an agent interacting in the MDP by sampling actions from the policy $\\pi$. We call return of the policy $\\pi$ the value provided by that function $$\\begin{aligned}\n    J(\\pi)\n    = \\frac{1}{1 - \\gamma} \\underset{\n    \\begin{subarray}{c}\n    s  \\sim d^{\\pi, \\gamma}(\\cdot) \\\\\n    a \\sim \\pi(\\cdot|s) \n    \\end{subarray}}{\\mathbb{E}} \\left [ \\rho(s, a) \\right ] \\; , \\label{eq:def_j}\n\\end{aligned}$$ where $d^{\\pi, \\gamma}(\\cdot)$ is the discounted state-visitation probability [@manne1960linear]. In reinforcement learning, we seek to find an optimal policy $\\pi^*$ maximizing the expected discounted sum of rewards $J$.\n\n## Policy-Gradient Algorithms\n\nPolicy-gradient algorithms (locally) optimize a parameterized policy $\\pi_\\theta$ to find the optimal parameter $\\theta^*$ for which the return of the policy $J(\\pi_{\\theta^*})$ is maximized. Naively maximizing the return may provide sub-optimal results. This problem is mitigated in practice with exploration strategies, which consist in optimizing a surrogate learning objective $L$ that intrinsically encourages certain behaviors. In this work, we consider reward-shaping strategies where the expected discounted sum of rewards is extended by $K$ additional reward terms $\\rho_i^{int}$, called intrinsic motivation terms, and optimize the learning objective $$\\begin{aligned}\n    L(\\theta)\n    &= \\frac{1}{1 - \\gamma}  \\underset{\n    \\begin{subarray}{c}\n    s  \\sim d^{\\pi_\\theta, \\gamma}(\\cdot) \\\\\n    a \\sim \\pi_\\theta(\\cdot|s) \n    \\end{subarray}}{\\mathbb{E}} \\left [ \\rho(s, a) + \\sum_{i=0}^{K-1} \\lambda_i \\rho_i^{int}(s, a) \\right ] \\nonumber \\\\\n    &= J(\\pi_\\theta) + J^{int}(\\pi_\\theta) \\label{eq:objective_rl} \\; ,\n\\end{aligned}$$ where $\\lambda_i$ are non-negative weights for each intrinsic reward and where $J^{int}(\\pi_\\theta)$ is the intrinsic return of the policy. The parameter maximizing the learning objective is denoted by $\\theta^\\dagger$, which we distinguish from the optimal policy parameter $\\theta^*$. Most of the intrinsic motivation terms can be classified in the two following groups.\n\n**Uncertainty-based motivations.** It is common to provide bonuses for performing actions that reduce the uncertainty of the agent about its environment [@pathak2017curiosity; @burda2018large; @zhang2021noveld]. The intrinsic motivation terms are then proportional to the prediction errors of a model of the MDP dynamics. The latter model is usually learned.\n\n**Entropy-based motivations.** It is also common to provide bonuses for visiting states and/or playing actions that are less likely in histories [@haarnoja2019soft; @hazan2019provably]. In this work, we focus on two of these bonuses $$\\begin{aligned}\n    \\rho^{s}(s, a) &= - \\log d^{\\pi_\\theta, \\gamma}(\\phi(s)) \\label{eq:int_rew_h} \\\\\n    \\rho^{a}(s, a) &= - \\log \\pi_\\theta(a| s) \\label{eq:int_rew_pi} \\; ,\n\\end{aligned}$$ where $\\phi(s)$ is a feature built from the state $s$. The corresponding intrinsic returns are maximized for policies that visit uniformly every feature, and for policies with uniformly distributed actions in each state, respectively. Note that these rewards require to estimate the distribution over the states and/or actions. Furthermore, they implicitly depend on the policy parameter $\\theta$. The second technique is usually referred to as entropy regularization.\n\nIn this work, we consider on-policy policy-gradient algorithms, which were among others reviewed by [@duan2016benchmarking] and [@andrychowicz2020matters]. These algorithms optimize differentiable parameterized policies with gradient-based local optimization. They iteratively approximate an ascent direction $\\hat d$ relying on histories sampled from the policy in the MDP and update the parameters in the ascent direction, or in a combination of the previous ascent directions [@hinton2012neural; @kingma2014adam]. For the sake of simplicity and without loss of generality, we consider that the ascent direction $\\hat d$ is composed of the sum of an estimate of the gradient of the return $\\hat g \\approx \\nabla_\\theta J(\\pi_\\theta)$ and an estimate of the gradient of the intrinsic return $\\hat i \\approx \\nabla_\\theta J^{int}(\\pi_\\theta)$. In practice, the first is usually unbiased while the second is computed neglecting some partial derivatives of $\\theta$ and is thus biased, typically neglecting the influence of the policy on the intrinsic reward.\n\n# Study of the Learning Objective {#sec:learning_objective}\n\nIn this section, we study the influence of the exploration terms on the learning objective defined in equation [\\[eq:objective_rl\\]](#eq:objective_rl){reference-type=\"eqref\" reference=\"eq:objective_rl\"}. We define two criteria under which the learning objective can be globally optimized by ascent methods, and such that the solution is close to an optimal policy. We then graphically illustrate how exploration modifies the learning objective to remove local extrema.\n\n## Policy-Gradient Learning Objective\n\nPolicy-gradient algorithms using exploration maximize the learning objective function $L$, as defined in equation [\\[eq:objective_rl\\]](#eq:objective_rl){reference-type=\"eqref\" reference=\"eq:objective_rl\"}. We introduce two criteria related to this learning objective for studying the performance of the policy-gradient algorithm. First, we say that a learning objective $L$ is $\\epsilon$-coherent when its global maximum is in an $\\epsilon$-neighborhood of the return of an optimal policy. Second, we call learning objectives that have a unique maximum and no other stationary point pseudoconcave.\n\n**Coherence criterion.** A learning objective $L$ is $\\epsilon$-coherent if, and only if, $$\\begin{aligned}\n    J(\\pi_{\\theta^*}) - J(\\pi_{\\theta^\\dagger})\\leq \\epsilon \\; ,\n\\end{aligned}$$ where $\\theta^* \\in \\text{argmax}_\\theta J(\\pi_\\theta)$ and where $\\theta^\\dagger \\in \\text{argmax}_\\theta L(\\theta)$.\n\n**Pseudoconcavity criterion.** A learning objective $L$ is pseudoconcave if, and only if, $$\\begin{aligned}\n    \\exists!\\: \\theta^\\dagger : \\nabla L(\\theta^\\dagger) = 0 \\land L(\\theta^\\dagger) = \\max_\\theta L(\\theta) \\; .\n\\end{aligned}$$ If the pseudoconcavity criterion is respected, there is a single optimum, and it is thus possible to globally optimize the learning objective function by (stochastic) gradient ascent [@bottou2010large][^2]. If the learning objective is furthermore $\\epsilon$-coherent, the latter solution is also a near-optimal policy, where $\\epsilon$ is the bound on the suboptimality of its return.\n\nLet us finally remind a theorem from @ng1999policy.\n\n**Consistency Theorem.** The learning objective $L$ is $\\epsilon$-coherent, with $\\epsilon = 0$, in any MDP with state space $\\mathcal{S}$, action space $\\mathcal{A}$ and factor $\\gamma$, if, and only if, $J(\\theta) = L(\\theta)$ for all $\\theta$. The intrinsic rewards are furthermore potential based.\n\nThis theorem states that there is no MDP-agnostic exploration method that guarantees consistency with $\\epsilon$ equal to zero and that modifies the objective function. This type of exploration is only possible with potential-based reward shaping [@ng1999policy]. In conclusion, if the return is not pseudoconcave, there is a trade-off between the two criteria, which can not be resolved by potential-based exploration.\n\n## Illustration of the Effect of Exploration on the Learning Objective\n\nExploration is of paramount importance in environments with complex dynamics and reward functions, where many locally optimal policies may exist [@lee2019efficient; @liu2021behavior; @zhang2021made]. In the following, we first define such an environment and a policy parameterization that will serve as an example to illustrate the effect of exploration on the optimization process. For the sake of the analysis, we then represent the learning objectives associated to different exploration strategies, and depict their global and local optima. Learning objectives with a single optimum respect the pseudoconcavity criterion. In addition, we represent the neighborhood $\\Omega$ of the optimal policy parameters, such that any learning objective with its global maximum within this region is coherent for a given $\\epsilon$. In light of the coherence and the pseudoconcavity criteria, we finally elaborate on the policy parameter computed by stochastic gradient ascent algorithms.\n\nWe consider the environment illustrated in Figure [1](#fig:hill_env){reference-type=\"ref\" reference=\"fig:hill_env\"} where a car moves in a valley [@bolland2023policy]. We denote by $x$ and $v$ the position and speed of the car, both composing its state $s=(x, v)$. The valley contains two separate low points, positioned in $x_{initial}=-3$ and $x_{target}=3$, separated by a peak. The car starts at rest $v_0=0$ at the highest low point $x_0=x_{initial}$ and receives rewards proportional to the depth of the valley at its current position. The reward function is provided in Figure [2](#fig:hill_reward){reference-type=\"ref\" reference=\"fig:hill_reward\"}. We consider a policy $\\pi_{K, \\sigma}(a| s) = \\mathcal{N}(a| \\mu_K(s), \\sigma)$, namely a normally disturbed proportional controller with $\\mu_K(s) = K\\times (x - x_{target})$, parameterized by the vector $\\theta=(K, \\sigma)$. Figure [4](#fig:hill_return_pcontrollers){reference-type=\"ref\" reference=\"fig:hill_return_pcontrollers\"} illustrates the contour map of the return of the policy as a function of the parameters $K$ and $\\sigma$. The optimal parameters are represented by black dots and correspond to policies that drive the car to pass the peak and reach the lowest valley point in $x_{target}$. The green area represents the set of parameters $\\Omega = \\left \\{ \\theta' | \\max_\\theta J(\\pi_\\theta) - J(\\pi_{\\theta'})\\leq \\epsilon \\right \\}$ for $\\epsilon = 1$.\n\n<figure id=\"fig:hill_return_pcontrollers\">\n<figure id=\"fig:hill_env\">\n<embed src=\"figures/hill_picture.pdf\" />\n<figcaption>Hill environment.</figcaption>\n</figure>\n<figure id=\"fig:hill_reward\">\n<embed src=\"figures/hill.pdf\" />\n<figcaption>Reward function <span class=\"math inline\"><em>ρ</em></span>.</figcaption>\n</figure>\n<figure id=\"fig:hill_return_pcontrollers\">\n<embed src=\"figures/return_reward_shaping_contour/return_contour_1.0_0.0_0.0_0.0.pdf\" />\n<figcaption>Return <span class=\"math inline\"><em>J</em>(<em>π</em><sub><em>K</em>, <em>σ</em></sub>)</span>.</figcaption>\n</figure>\n<figcaption>Illustration of the <em>hill environment</em> in Figure <a href=\"#fig:hill_env\" data-reference-type=\"ref\" data-reference=\"fig:hill_env\">1</a> and its reward function in Figure <a href=\"#fig:hill_reward\" data-reference-type=\"ref\" data-reference=\"fig:hill_reward\">2</a>. In Figure <a href=\"#fig:hill_return_pcontrollers\" data-reference-type=\"ref\" data-reference=\"fig:hill_return_pcontrollers\">4</a>, the return of the policy <span class=\"math inline\"><em>π</em><sub><em>K</em>, <em>σ</em></sub></span> with the global and local maximum represented in black and grey, together with their respective return values.</figcaption>\n</figure>\n\nFigure [11](#fig:return_int){reference-type=\"ref\" reference=\"fig:return_int\"} illustrates the learning objective with the intrinsic rewards $\\rho^s(s, a) = -\\log d^{\\pi_{K, \\sigma}, \\gamma}(\\phi(s))$, from equation [\\[eq:int_rew_h\\]](#eq:int_rew_h){reference-type=\"eqref\" reference=\"eq:int_rew_h\"}, and $\\rho^a(s, a) = - \\log \\pi_{K, \\sigma}(a|s)$, from equation [\\[eq:int_rew_pi\\]](#eq:int_rew_pi){reference-type=\"eqref\" reference=\"eq:int_rew_pi\"}, for different values of the corresponding weights $\\lambda_s$ and $\\lambda_a$. Here, the feature is the position in the valley $\\phi(s) = x$. First, we observe that for weights approaching zero, the parameter $\\theta^\\dagger$ maximizing the learning objective, represented by a black dot, corresponds to a policy with a high return. More precisely, it is in the green set $\\Omega$ such that $\\epsilon$-coherence is guaranteed for the value $\\epsilon=1$. Larger weights require larger values of $\\epsilon$ for guaranteeing the $\\epsilon$-coherence criterion. Nevertheless, when increasing the weights, we observe that the learning objective eventually becomes pseudoconcave. There appears to be a trade-off between the two criteria. In Figure [6](#fig:return_int_010){reference-type=\"ref\" reference=\"fig:return_int_010\"}, we observe that in this environment, there is a learning objective that respects the pseudoconcavity criterion and the $\\epsilon$-coherence criterion for $\\epsilon=1$. Indeed, there is a single global maximum in Figure [6](#fig:return_int_010){reference-type=\"ref\" reference=\"fig:return_int_010\"} represented by a black dot that is furthermore part of the set $\\Omega$.\n\nShaping the reward function with an exploration strategy based on the state-visitation entropy appears to be a good solution for optimizing the policy. However, a notable drawback is that the reward depends on the policy and its (gradient) computation requires to estimate a complex probability measure. In this example, the intrinsic reward function itself was estimated by Monte-Carlo sampling for every parameter, which would not scale for complex problems and requires approximations and costly evaluation strategies [@islam2019marginalized]. In Appendix [6](#apx:reward_shaping){reference-type=\"ref\" reference=\"apx:reward_shaping\"} we present an alternative problem-dependent intrinsic reward, independent of the policy parameters and thus simple to compute efficiently, that still respects the pseudoconcavity and $\\epsilon$-coherence criteria, and in Appendix [7](#apx:minigrid_experiments){reference-type=\"ref\" reference=\"apx:minigrid_experiments\"} we extend the study to more complex environments from the MiniGrid library [@MinigridMiniworld23] where the policy is a deep neural network and the state-visitation probability is approximated.\n\n<figure id=\"fig:return_int\">\n<figure id=\"fig:return_int_005\">\n<embed src=\"figures/return_reward_shaping_contour/return_contour_1.0_0.05_0.0_0.0.pdf\" />\n<figcaption><span class=\"math inline\"><em>λ</em><sub><em>s</em></sub> = 0.05</span> and <span class=\"math inline\"><em>λ</em><sub><em>a</em></sub> = 0</span></figcaption>\n</figure>\n<figure id=\"fig:return_int_010\">\n<embed src=\"figures/return_reward_shaping_contour/return_contour_1.0_0.1_0.0_0.0.pdf\" />\n<figcaption><span class=\"math inline\"><em>λ</em><sub><em>s</em></sub> = 0.1</span> and <span class=\"math inline\"><em>λ</em><sub><em>a</em></sub> = 0</span></figcaption>\n</figure>\n<figure id=\"fig:return_int_100\">\n<embed src=\"figures/return_reward_shaping_contour/return_contour_1.0_1.0_0.0_0.0.pdf\" />\n<figcaption><span class=\"math inline\"><em>λ</em><sub><em>s</em></sub> = 1</span> and <span class=\"math inline\"><em>λ</em><sub><em>a</em></sub> = 0</span></figcaption>\n</figure>\n<figure id=\"fig:return_regul_001\">\n<embed src=\"figures/return_reward_shaping_contour/return_contour_1.0_0.0_0.0_0.01.pdf\" />\n<figcaption><span class=\"math inline\"><em>λ</em><sub><em>s</em></sub> = 0</span> and <span class=\"math inline\"><em>λ</em><sub><em>a</em></sub> = 0.01</span></figcaption>\n</figure>\n<figure id=\"fig:return_regul_010\">\n<embed src=\"figures/return_reward_shaping_contour/return_contour_1.0_0.0_0.0_0.1.pdf\" />\n<figcaption><span class=\"math inline\"><em>λ</em><sub><em>s</em></sub> = 0</span> and <span class=\"math inline\"><em>λ</em><sub><em>a</em></sub> = 0.1</span></figcaption>\n</figure>\n<figure id=\"fig:return_regul_050\">\n<embed src=\"figures/return_reward_shaping_contour/return_contour_1.0_0.0_0.0_0.5.pdf\" />\n<figcaption><span class=\"math inline\"><em>λ</em><sub><em>s</em></sub> = 0</span> and <span class=\"math inline\"><em>λ</em><sub><em>a</em></sub> = 0.5</span></figcaption>\n</figure>\n<figcaption>Contour map of (scaled) learning objective functions for different values of <span class=\"math inline\"><em>λ</em><sub><em>s</em></sub></span> and <span class=\"math inline\"><em>λ</em><sub><em>a</em></sub></span>. The darker the map, the larger the learning objective value. The green area represents the set <span class=\"math inline\"><em>Ω</em> = {<em>θ</em>′|max<sub><em>θ</em></sub><em>J</em>(<em>π</em><sub><em>θ</em></sub>) − <em>J</em>(<em>π</em><sub><em>θ</em>′</sub>) ≤ <em>ϵ</em> = 1}</span>, such that when the parameter maximizing the learning objective is part of <span class=\"math inline\"><em>Ω</em></span>, then the learning objective function is <span class=\"math inline\"><em>ϵ</em></span>-coherent with <span class=\"math inline\"><em>ϵ</em> = 1</span>. The black dot is the parameter <span class=\"math inline\"><em>θ</em><sup>†</sup></span> globally maximizing the learning objective and the grey dot is the local (non-global) maximum of the learning objective if it exists. Both are labeled with the return values of the corresponding policies.</figcaption>\n</figure>\n\nThe observations suggest that well-chosen exploration strategies can lead to learning objective functions that satisfy the two criteria defined in the previous section, thereby guaranteeing that policies suboptimal by at most $\\epsilon$ can be computed by local optimization. When designing exploration strategies, it is essential to keep in mind that we modify the learning objective for the algorithms to converge to optimal policy parameters, which can be achieved when both criteria are respected. While strategies such as enforcing entropy can be effective in some environments, they are only heuristic strategies and not to be relied upon exclusively. Furthermore, as illustrated, both criteria may be subject to a trade-off. In more complex environments, an efficient exploration strategy may require to balance both criteria, e.g., through a schedule on the learning objective weights.\n\n# Study of the Ascent Direction Distribution {#sec:ascent_direction}\n\nOptimizing pseudoconcave functions with stochastic ascent methods are guaranteed to converge (at a certain rate) under assumptions on the distribution of the gradient estimates at hand [@bottou2010large; @chen2018stochastic; @ajalloeian2020convergence]. In this section, we study the influence of the exploration terms on this distribution in the context of policy gradients. More precisely, we study the probability of improving the learning objective and the return with stochastic ascent steps. Intuitively, they shall be sufficiently large for the algorithm to be efficient. We formalize this intuition and illustrate how exploration strategies can increase these probabilities, leading to more efficient algorithms.\n\n## Policy-Gradient Estimated Ascent Direction\n\nIn general, gradient ascent algorithms update parameters in a direction $\\hat d$ in order to locally improve an objective function $f$. The quality of these algorithms can therefore be studied (for a small step size $\\alpha \\rightarrow 0$) through the random variable representing the quantity by which the objective increases for each $\\theta$ $$\\begin{aligned}\n    X &= f(\\theta + \\alpha \\hat d) - f(\\theta)\n    = \\alpha \\: \\langle \\hat d, \\nabla_\\theta f(\\theta) \\rangle \\: , \\label{eq:rv_local_search}\n\\end{aligned}$$ where $\\langle \\cdot, \\cdot \\rangle$ is the Euclidean scalar product. This variable depends on the random event $\\hat d$ estimated by Monte-Carlo simulations in practice.\n\nThe (asymptotic) convergence of a gradient ascent algorithm is usually studied by bounding the expectation of $X$. Such bounds depend, among others, on the expectation of $\\hat d$, which equals $\\nabla_\\theta f(\\theta)$ when unbiased, and depend on the variance of $\\hat d$, which deteriorates the expected convergence rate. Finding rates depending on the algorithm is an active field of research. In parallel, a slightly more general problem is to quantify if the expectation of $X$ is driven by rare events. Intuitively, an algorithm with unbiased gradient estimates has positive expected improvements $\\mathbb{E} [ X ] > 0$, and should theoretically converge, but may be inefficient in practice if positive events $X > 0$ rarely occur. We illustrate in the next section that this phenomenon makes reinforcement learning in sparse-reward environments particularly hard. To the best of our knowledge, no existing result fits to the study of policy gradients. We therefore introduce two new criteria on the probability of improvement $P(X > 0)$, which we empirically validate afterwards.\n\nFirst, we define an exploration strategy as $\\delta$-efficient if, and only if, following the ascent direction $\\hat d \\approx \\nabla_\\theta L(\\theta)$ has a probability at least $\\delta$ to increasing the learning objective $L(\\theta)$ almost everywhere. Second, an exploration strategy is $\\delta$-attractive if, and only if, there exists a neighborhood of $\\theta^\\dagger$ containing the parameter $\\theta^{int}$ maximizing the intrinsic return $J^{int}$, where the probability of increasing the return by following **$\\hat d$** is almost everywhere at least equal to $\\delta$. Note that each probability measure and random variable is a function of $\\theta$, which we do not explicitly write for the sake of keeping notations simple.\n\n**Efficiency criterion.** An exploration strategy is $\\delta$-efficient if, and only if, $$\\begin{aligned}\n    \\forall^\\infty \\theta : \\mathbb{P}(D > 0) \\geq \\delta \\: ,\n\\end{aligned}$$ where $D = \\: \\langle \\hat d, \\nabla_\\theta L(\\theta) \\rangle$.\n\n**Attraction criterion.** An exploration strategy is $\\delta$-attractive if, and only if, $$\\begin{aligned}\n    \\exists B(\\theta^\\dagger) &: \\theta^{int} \\in B(\\theta^\\dagger) \\: ,\n\\end{aligned}$$ such that $$\\begin{aligned}\n    \\forall^\\infty \\theta \\in B(\\theta^\\dagger) : \\mathbb{P}(G > 0) \\geq \\delta \\: ,\n\\end{aligned}$$ where $\\theta^{int} = \\text{argmax}_\\theta J^{int}(\\pi_\\theta)$, $B(\\theta^\\dagger)$ is a ball centered in $\\theta^\\dagger$, and $G = \\: \\langle \\hat d, \\nabla_\\theta J(\\pi_\\theta) \\rangle$.\n\nThe efficiency criterion quantifies how often a stochastic gradient ascent step improves the learning objective. The larger, the better the learning objective and its stochastic ascent direction approximations. The rationale behind the attraction criterion is that in many exploration strategies, the intrinsic reward is dense, and it is then presumably easy to optimize the intrinsic return in the sense that $\\mathbb{P}(\\: \\langle \\hat i, \\nabla_\\theta J^{int}(\\pi_\\theta) \\rangle > 0)$ is large. It implies that it is easy to locally improve the learning objective by (solely) increasing the value of the intrinsic motivation terms. It furthermore implies that policy-gradient algorithms may be subject to converging towards $\\theta^{int}$ rather than $\\theta^\\dagger$ when $\\mathbb{P}(\\:  \\langle \\hat d, \\nabla_\\theta J(\\pi_\\theta) \\rangle > 0)$ is small. If the criterion is respected for large $\\delta$, the latter is less likely to happen as policy gradients will eventually tend to improve the return of the policy if the parameter approaches $\\theta^{int}$ and enters the ball $B(\\theta^\\dagger)$; eventually converging towards $\\theta^\\dagger$.\n\nThese two new criteria on $\\hat d$ are independent of the previous ones on $L$, which only captured the quality of the deterministic learning objective functions. In the particular cases where the learning objectives $L$ are $\\epsilon$-coherent, for $\\epsilon=0$, and pseudoconcave, e.g., with potential-based intrinsic rewards, only the distribution of estimates $\\hat d$ can explain why some algorithms succeed and others fail. Finally, the value of $\\delta$ in the new criteria we introduce can be related to the variance of the estimate $\\hat d$ under some assumptions, e.g., with Cantelli's concentration inequalities.\n\n## Illustration of the Effect of Exploration on the Estimated Ascent Direction\n\nExploration is usually promoted and tested for problems where the reward function is sparse, typically in maze-environments [@islam2019marginalized; @liu2021behavior; @guo2021geometric]. In this section, we first introduce a new maze-environment with sparse rewards where we illustrate the influence of exploration on the gradient estimates of the learning objective. To this end, we present two learning objective functions and elaborate on the influence of exploration on the performance of policy-gradient algorithms in the light of the efficiency and attraction criteria.\n\nLet us consider a maze-environment consisting of a horizontal corridor composed of $S \\in \\mathbb{N}$ tiles. The state of the environment is the index of the tile $s \\in \\left \\{ 1, \\dots, S \\right \\}$, and the actions consist in going left $a=-1$ or right $a=+1$. When an action is taken, the agent stays idle with probability $p=0.7$, and moves with probability $1-p = 0.3$ in the direction indicated by the action, then $s' = \\min(S, \\max(1, s + a))$. The agent starts in state $s = 1$ and the target state $s = S = 15$ is absorbing. Zero rewards are observed except when the agent reaches the target state where a reward $r=100$ is observed. A discount factor of $\\gamma=0.99$ is considered. Finally, we study the policy going with probability $\\theta$ to the right and probability $1-\\theta$ to the left, and with density $$\\begin{aligned}\n    \\pi_\\theta(a|s) =\n    \\left\\{ \n        \\begin{array}{cl}\n        \\theta      & \\quad \\textrm{if } a = 1 \\\\\n        1 - \\theta  & \\quad \\textrm{if } a = -1 \\; .\n        \\end{array}\n    \\right.\n\\end{aligned}$$\n\nThe return $J(\\pi_\\theta)$ is represented in black in Figure [12](#fig:maze_return){reference-type=\"ref\" reference=\"fig:maze_return\"} as a function of $\\theta$ along with two intrinsic returns, $J^a(\\pi_\\theta)$ in orange and $J^s(\\pi_\\theta)$ in blue. The intrinsic reward $\\rho^{a}(s, a) = - \\log \\pi_\\theta(a| s)$, from equation [\\[eq:int_rew_pi\\]](#eq:int_rew_pi){reference-type=\"eqref\" reference=\"eq:int_rew_pi\"}, and the intrinsic reward $\\rho^{s}(s, a) = - \\log d^{\\pi_\\theta, \\gamma}(s)$, from equation [\\[eq:int_rew_h\\]](#eq:int_rew_h){reference-type=\"eqref\" reference=\"eq:int_rew_h\"}, are used respectively. In Figure [13](#fig:maze_learning){reference-type=\"ref\" reference=\"fig:maze_learning\"}, we illustrate the return of the policy without exploration $J(\\pi_\\theta)$, along with two learning objective functions, $L^a(\\theta)$ and $L^s(\\theta)$, using as exploration strategies the intrinsic returns $J^a(\\pi_\\theta)$ and $J^s(\\pi_\\theta)$. We observe that the return is a pseudoconcave function with respect to $\\theta$ and the optimal parameter is $\\theta^*=1$. In addition, the two learning objectives respect the $\\epsilon$-coherence criterion for $\\epsilon=0$, implying that $\\theta^* = \\theta^\\dagger$, and respect the pseudoconcavity criterion. It is important to note that with regard to the discussion from Section [3](#sec:learning_objective){reference-type=\"ref\" reference=\"sec:learning_objective\"}, there is no interest in optimizing the learning objectives rather than directly optimizing the return, as the latter is already pseudoconcave. In the following we illustrate how choosing a correct exploration strategy still deeply influences the policy-gradient algorithms when it comes to building gradient estimates.\n\n<figure id=\"fig:maze_positive_direction\">\n<figure id=\"fig:maze_return\">\n<embed src=\"figures/maze_return.pdf\" />\n<figcaption>Return functions.</figcaption>\n</figure>\n<figure id=\"fig:maze_learning\">\n<embed src=\"figures/maze_learning.pdf\" />\n<figcaption>Learning objective functions.</figcaption>\n</figure>\n<figure id=\"fig:maze_positive_direction\">\n<embed src=\"figures/maze_proba_improvement.pdf\" />\n<figcaption>Improvement probabilities.</figcaption>\n</figure>\n<figcaption>Figure <a href=\"#fig:maze_return\" data-reference-type=\"ref\" data-reference=\"fig:maze_return\">12</a> represents the return of the policy along with two intrinsic return functions. In Figure <a href=\"#fig:maze_learning\" data-reference-type=\"ref\" data-reference=\"fig:maze_learning\">13</a> the return is also represented together with two learning objective functions, corresponding to the two intrinsic returns. Figure <a href=\"#fig:maze_positive_direction\" data-reference-type=\"ref\" data-reference=\"fig:maze_positive_direction\">15</a> illustrates the probability (estimated by Monte-Carlo) of positive stochastic gradient (derivative) estimates <span class=\"math inline\"><em>J</em>(<em>π</em><sub><em>θ</em></sub>)</span>, <span class=\"math inline\"><em>L</em><sup><em>a</em></sup>(<em>θ</em>)</span>, and <span class=\"math inline\"><em>L</em><sup><em>s</em></sup>(<em>θ</em>)</span>. At the top of the figure, the intervals <span class=\"math inline\"><em>B</em><sup><em>a</em></sup> = [<em>θ</em><sup><em>i</em><em>n</em><em>t</em>, <em>a</em></sup>, <em>θ</em><sup>†, <em>a</em></sup>]</span> and <span class=\"math inline\"><em>B</em><sup><em>s</em></sup> = [<em>θ</em><sup><em>i</em><em>n</em><em>t</em>, <em>s</em></sup>, <em>θ</em><sup>†, <em>s</em></sup>]</span> are represented. These intervals represent the smallest balls containing the parameters maximizing the intrinsic return and the learning objective, for both exploration strategies.</figcaption>\n</figure>\n\nLet us compute the estimate $\\hat g$ and $\\hat d$ relying on REINFORCE [@williams1992simple] by sampling $8$ histories of length $T = 100$. In this particular environment, $\\mathbb{P}(D > 0)$ equals $\\mathbb{P}(G > 0)$, and equal the probability that the derivative is positive. We represent in Figure [15](#fig:maze_positive_direction){reference-type=\"ref\" reference=\"fig:maze_positive_direction\"} this probability for the return and for both learning objectives. First, we see that the learning objectives are more efficient than the return, meaning they are $\\delta$-efficient for larger values of $\\delta$. Depending on the parameter value, the objective $L^a(\\theta)$ or $L^s(\\theta)$ is best in that regard. Second, concerning the attraction criterion, we represent at the top of Figure [15](#fig:maze_positive_direction){reference-type=\"ref\" reference=\"fig:maze_positive_direction\"} the intervals $B^a = [\\theta^{int, a}, \\theta^{\\dagger, a}]$ and $B^s = [\\theta^{int, s}, \\theta^{\\dagger, s}]$. They correspond to the smallest balls containing the maximizers of the intrinsic return and of the learning objective. Let the minima of the orange and blue curves over these intervals be denoted by $\\delta^a$ and $\\delta^s$. By definition of the attraction criterion, it is thus respected for any values of $\\delta$ at most equal to $\\delta^a$ and $\\delta^s$, for $L^a(\\theta)$ and $L^s(\\theta)$, respectively. All these observations can eventually be explained as the computation of $\\hat g$ is always zero when the target is not sampled in the histories, which is highly likely for policies with small values of $\\theta$. Policy-gradient algorithms relying on intrinsic exploration would compute optimal policies efficiently where naive optimization without exploration would fail or be sample inefficient.\n\nWe have empirically shown that a well-chosen exploration strategy in policy gradients may not only remove local extrema from the objective function, but may also increase the probability that stochastic ascent steps improve the objective function. Under the previous assumptions, this probability measures the efficiency of algorithms. Furthermore, among different learning objectives respecting the coherence and pseudoconcavity criteria, it is best to choose one that has high values for $\\delta$ in both the efficiency and attraction criteria. In Appendix [6](#apx:reward_shaping){reference-type=\"ref\" reference=\"apx:reward_shaping\"} we use these criteria to study other reward-shaping strategies, and in Appendix [7](#apx:minigrid_experiments){reference-type=\"ref\" reference=\"apx:minigrid_experiments\"} we extend the study to more complex environments from the MiniGrid library [@MinigridMiniworld23] where the policy is a deep neural network.\n\nThe problem discussed in this section strongly relates to overfitting or generalization in reinforcement learning. In situations where the same state and action pairs are repeatedly sampled with high probability, the policy may appear optimal by neglecting the rewards observed in state and action pairs sampled with low probability. The gradient estimates will then be zero with high probability, and the gradient updates will not lead to policy improvements. In the previous example, gradient estimates computed from policies with a small parameter value $\\theta$ wrongly indicate that a stationary point has been reached as they equal zero with high probability. We quantify this effect with a novel definition of local optimality. We define as locally optimal policies over a space with probability $\\Delta$ the policies that maximize the reward on expectation over a set of states and actions observed in a history with probability at least $\\Delta$. Formally, a policy $\\pi$ is locally optimal over a space with probability $\\Delta$ if, and only if, $$\\begin{aligned}\n    \\exists\\: \\mathcal{E}\n    &\\in \\left \\{ \\mathcal{X} \\Big | \\int_\\mathcal{X} \\: d^{\\pi, \\gamma}(s) \\pi(a| s) \\: da ds \\geq \\Delta \\right \\} : \\nonumber \\\\\n    &\\pi \\in \\underset{\\pi'}{\\text{argmax}} \\int_{\\mathcal{E}} \\: d^{\\pi', \\gamma}(s) \\pi'(a| s) \\rho(a, s) \\: da ds \\; . \\label{eq:local_optimal_pi}\n\\end{aligned}$$ In the typical case of environments with sparse rewards, many policies observe with high probability state and action pairs with zero rewards and are locally optimal for large probabilities $\\Delta$. Typically, in the previous example, the joint set $\\{1, \\dots, S-2\\} \\times \\{-1, 1\\}$ is a set of state and action pairs $\\mathcal{E}$ that respects the definition equation [\\[eq:local_optimal_pi\\]](#eq:local_optimal_pi){reference-type=\"eqref\" reference=\"eq:local_optimal_pi\"} for large values $\\Delta$ when $\\theta$ is small. As we have shown, exploration mitigates the convergence of policy-gradient algorithms towards these locally optimal policies. Note that assuming a non-zero reward is uniformly distributed over the state and action space, exploration policies with uniform probabilities over visited states and actions are the best choice for sampling non-zero rewards with high probability. It can thus also be considered as the best choice of exploration to reduce the probability that the stochastic gradient ascent steps do not increase the objective value. Such initial policy may be learned from the framework developed by @lee2019efficient.\n\n# Conclusion {#sec:conclusion}\n\nIn conclusion, this research takes a step towards dispelling misunderstandings about exploration through the study of its effects on the performance of policy-gradient algorithms. More particularly, we distinguished two effects exploration has on the optimization. First, it modifies the learning objective in order to remove local extrema. Second, it modifies the gradient estimates and increases the likelihood that the update steps lead to improved returns. These two phenomena were studied through four criteria that we introduced and illustrated.\n\nThese ideas apply to other direct policy optimization algorithms. Indeed, the four criteria do not assume any structure on the learning objective and can thus be straightforwardly applied to any objective function optimized by a direct policy search algorithm. In particular, for off-policy policy gradient, we may simply consider that the off-policy objective is itself a surrogate or that the gradients of the return are biased estimates based on past histories. Ideas introduced in this work also apply to other reinforcement learning techniques. Typically, for value-based RL with sparse-reward environments, convergence towards a value function outputting zero is expected with high probability. This is mostly due to the low probability of sampling non-zero rewards by Monte-Carlo. The discussions from Section [4](#sec:ascent_direction){reference-type=\"ref\" reference=\"sec:ascent_direction\"} then apply, and a similar analysis can be performed.\n\nOur framework opens the door for further theoretical analysis, and the potential development of new criteria. We believe that deriving practical conditions on the exploration strategies, and the scheduling of the intrinsic return, for guaranteeing fast convergence should be the focus of attention. It could be achieved by bounding the policy improvement on expectation, which is nevertheless usually a hard task without strong assumptions. We furthermore believe that we provide a new lens on exploration necessary for interpreting and developing exploration strategies, in the sense of optimizing surrogate learning objective functions.\n\n# Acknowledgments {#acknowledgments .unnumbered}\n\nThe authors thank Arnaud Delaunoy, Pascal Leroy, and Mathias Berger for valuable comments on this manuscript. Adrien Bolland and Gaspard Lambrechts gratefully acknowledge the financial support of the F.R.S.-FNRS.\n\n# Reward Shaping and Exploration Strategies {#apx:reward_shaping}\n\nAs discussed in the manuscript, exploration strategies are reward-shaping strategies where the intrinsic reward bonuses are, among others, dependent on the policy parameters. This dependency makes the shaping strategies adaptive but makes the computation of gradients and the study of the learning objectives more complex. In this section, we study handcrafted reward-shaping strategies that have pseudoconcave and dense reward functions in the hill and maze environments. We then illustrate that the same criteria can be used to study these expert-knowledge based shaped rewards.\n\nFor the hill environment from Section [3](#sec:learning_objective){reference-type=\"ref\" reference=\"sec:learning_objective\"}, we illustrate in Figure [16](#fig:modif_hill_surface){reference-type=\"ref\" reference=\"fig:modif_hill_surface\"} an intrinsic reward bonus making the sum of rewards in equation [\\[eq:objective_rl\\]](#eq:objective_rl){reference-type=\"eqref\" reference=\"eq:objective_rl\"} concave. The corresponding learning objective has a unique maximum, which is part of the set $\\Omega = \\left \\{ \\theta' | \\max_\\theta J(\\pi_\\theta) - J(\\pi_{\\theta'})\\leq \\epsilon \\right \\}$ with $\\epsilon=1$ and $\\theta = (K, \\sigma)$. It can be seen in Figure [18](#fig:return_logconcenvlp){reference-type=\"ref\" reference=\"fig:return_logconcenvlp\"} where the global maximum in black is within the set $\\Omega$ in green. Both, the $\\epsilon$-coherence and the pseudoconcavity criteria are thus respected for $\\epsilon=1$. Here, the intrinsic reward function is a simple function independent of the policy $\\pi_\\theta$. Finding such an intrinsic reward may be complex for other environments but the example underlines that exploration and reward shaping are mostly equivalent and that designing reward functions that are concave may help converging towards optimal policies.\n\n<figure id=\"fig:return_logconcenvlp\">\n<figure id=\"fig:modif_hill_surface\">\n<embed src=\"figures/hill_concave.pdf\" />\n<figcaption>Alternative reward function.</figcaption>\n</figure>\n<figure id=\"fig:return_logconcenvlp\">\n<embed src=\"figures/return_reward_shaping_contour/return_contour_1.0_0.0_1.0_0.0.pdf\" />\n<figcaption>Learning objective.</figcaption>\n</figure>\n<figcaption>In Figure <a href=\"#fig:modif_hill_surface\" data-reference-type=\"ref\" data-reference=\"fig:modif_hill_surface\">16</a>, an alternative intrinsic reward function ensuring that the sum of rewards is a pseudoconcave function. In Figure <a href=\"#fig:return_logconcenvlp\" data-reference-type=\"ref\" data-reference=\"fig:return_logconcenvlp\">18</a>, the contour function of the learning objective.</figcaption>\n</figure>\n\nFor the maze environment, the return $J(\\pi_\\theta)$ is represented in black in Figure [19](#fig:maze_return_dense){reference-type=\"ref\" reference=\"fig:maze_return_dense\"} together with the intrinsic return $J^d(\\pi_\\theta)$ in green. The latter is the return of the dense handcrafted reward function $\\rho^d(s, a) = (a - 1) / 2$ penalizing actions moving away from the target. In Figure [20](#fig:maze_learning_dense){reference-type=\"ref\" reference=\"fig:maze_learning_dense\"}, the corresponding learning objective function is shown. In the same experimental setting as in Section [4](#sec:ascent_direction){reference-type=\"ref\" reference=\"sec:ascent_direction\"}, we observe that the objective function is $\\delta$-efficient for higher values of $\\delta$ compared to the already-discussed learning objectives. Furthermore, the attraction criterion is respected for any value of $\\delta$ as the unique global maxima of the learning objective, intrinsic return, and return are all equals.\n\n<figure id=\"fig:maze_proba_improvement_dense\">\n<figure id=\"fig:maze_return_dense\">\n<embed src=\"figures/maze_return_dense.pdf\" />\n<figcaption>Return.</figcaption>\n</figure>\n<figure id=\"fig:maze_learning_dense\">\n<embed src=\"figures/maze_learning_dense.pdf\" />\n<figcaption>Learning objectives.</figcaption>\n</figure>\n<figure id=\"fig:maze_proba_improvement_dense\">\n<embed src=\"figures/maze_proba_improvement_dense.pdf\" />\n<figcaption>Improvement probabilities.</figcaption>\n</figure>\n<figcaption>In Figure <a href=\"#fig:maze_return_dense\" data-reference-type=\"ref\" data-reference=\"fig:maze_return_dense\">19</a> the return of the maze environment is represented together with the intrinsic return of a dense handcrafted reward function. Figure <a href=\"#fig:maze_learning_dense\" data-reference-type=\"ref\" data-reference=\"fig:maze_learning_dense\">20</a> represents the corresponding learning objective and Figure <a href=\"#fig:maze_proba_improvement_dense\" data-reference-type=\"ref\" data-reference=\"fig:maze_proba_improvement_dense\">22</a> the probability that the REINFORCE estimates are positive.</figcaption>\n</figure>\n\n# Minigrid Experiments {#apx:minigrid_experiments}\n\nIn this section, we introduce complex environments and parameterize policies with neural networks. In this context, it is impractical to naively compute and represent the objective functions and probability distributions for the different criteria. Therefore, we only evaluate the criteria along parameter trajectories, and extend the previous experimental setting.\n\nWe consider seven environments from the MiniGrid suite of environments [@MinigridMiniworld23], among others, designed for evaluating exploration strategies. In these environments, an agent moves in a maze and aims to reach a target position. To do so, the agent may choose actions that consist of turning left, turning right, moving forward, or staying idle. We consider two reward settings: the dense setting and the sparse setting. In the first, rewards of $-1$ are received for every non-idle move, and a reward of $1000$ is received upon reaching the target position. In the second setting, zero rewards are received everywhere, except upon reaching the target position, where a bonus of $1000$ is provided. In the dense setting, due to the action penalization incurred when moving, a policy outputting the idle action with probability one is locally optimal and has a return equal to zero. This is not (necessarily) the case in the second setting. We consider a discount factor of $\\gamma = 0.98$ and optimize a fully connected neural network taking as input the position pair and the orientation of the agent, and outputting a categorical distribution over actions. The network is composed of three hidden layers of $64$ neurons with ReLU activation functions.\n\nIn the dense reward setting, we optimize policies by maximizing three learning objective functions: $J(\\pi_\\theta)$, $L^a(\\theta)$, and $L^s(\\theta)$, respectively with $\\lambda_a = 0.5$ and $\\lambda_s = 0.25$. For the last objective, the state-visitation density estimator is a ten-component Gaussian mixture model maximizing the likelihood of the sampled batch. The optimization is performed using the Adam update rule [@kingma2014adam], with REINFORCE ascent directions computed over 32 histories of constant length $T = 100$, and with learning rate (step size) equal to $0.0005$. The length $T$ of the histories is chosen such that the realization value $T$ from a geometric distribution with success probability parameter $1 - \\gamma$ has at least a cumulative probability of $0.85$. In this setting, we illustrate the quasiconcavity criterion and the $\\epsilon$-coherence criterion. In Figure [23](#fig:minigrid_dense_return){reference-type=\"ref\" reference=\"fig:minigrid_dense_return\"}, we provide the evolution of the return of the policies when optimizing the three objectives $J(\\pi_\\theta)$, $L^a(\\theta)$, and $L^s(\\theta)$ for the different environments. For the `MiniGrid-Empty-8x8-v0` and the `MiniGrid-FourRooms-v0` environments, optimizing the return results in high-performance policies that do not stay idle. The other objectives also manage to find high-performing policies, but with a lower return. This phenomenon (assuming that the global optimum of each objective is found) illustrates the $\\epsilon$-coherence criterion, where this $\\epsilon$ value is the bound on the best policy that can be found when optimizing the learning objective. For the other environments, the policies resulting from the optimization of the return fall into local optima, namely ones where the policy chooses the idling action with probability one. When optimizing the learning objectives with exploration bonuses, the resulting policies no longer fall into the previous local optima. This result suggests that, along these parameter trajectories, the return $J(\\pi_\\theta)$ has a local optimum (or saddle point), in opposition to the learning objective functions $L^a(\\theta)$ and $L^s(\\theta)$. The latter illustrates the validity of the pseudoconcavity criterion in that region of the parameter space. For the learning objective $L^s(\\theta)$, the $\\epsilon$-coherence criterion is respected for a small value of $\\epsilon$, and the resulting policy manages to reach the target position. For the learning objective $L^a(\\theta)$, the resulting policy does not reach the target position. We nevertheless hypothesize that it is not a real local optimum of the learning objective but a local optimum in the sense of equation [\\[eq:local_optimal_pi\\]](#eq:local_optimal_pi){reference-type=\"eqref\" reference=\"eq:local_optimal_pi\"}.\n\n<figure id=\"fig:minigrid_dense_return\">\n<figure>\n<embed src=\"figures/minigrid/with_action_cost/MiniGrid-Empty-8x8-v0_return.pdf\" />\n</figure>\n<figure>\n<embed src=\"figures/minigrid/with_action_cost/MiniGrid-Empty-16x16-v0_return.pdf\" />\n</figure>\n<figure>\n<embed src=\"figures/minigrid/with_action_cost/MiniGrid-SimpleCrossingS9N1-v0_return.pdf\" />\n</figure>\n<figure>\n<embed src=\"figures/minigrid/with_action_cost/MiniGrid-SimpleCrossingS9N2-v0_return.pdf\" />\n</figure>\n<figure>\n<embed src=\"figures/minigrid/with_action_cost/MiniGrid-SimpleCrossingS9N3-v0_return.pdf\" />\n</figure>\n<figure>\n<embed src=\"figures/minigrid/with_action_cost/MiniGrid-SimpleCrossingS11N5-v0_return.pdf\" />\n</figure>\n<figure>\n<embed src=\"figures/minigrid/with_action_cost/MiniGrid-FourRooms-v0_return.pdf\" />\n</figure>\n<figcaption>Evolution of the return of policies during optimization in the dense minigrid environments. In blue, the return <span class=\"math inline\"><em>J</em>(<em>π</em><sub><em>θ</em></sub>)</span> is optimized; in orange, the learning objective <span class=\"math inline\"><em>L</em><sup><em>a</em></sup>(<em>θ</em>)</span> is optimized; and in green, the learning objective <span class=\"math inline\"><em>L</em><sup><em>s</em></sup>(<em>θ</em>)</span> is optimized by performing Adam steps in REINFORCE directions. Note that the median, worst, and best cases over five runs are represented for the different curves. For the environments <code>MiniGrid-Empty-8x8-v0</code> and <code>MiniGrid-FourRooms-v0</code>, optimizing each objective results in a policy that does not stay idle. These policies are initialized outside of the basin of attraction of the local optimum of the return. The coherence criteria can be observed as optimizing the learning objective with intrinsic exploration bonuses results in suboptimal policies. For the other environments, optimizing the return directly leads to policies that always choose to stay idle, and are thus locally optimal. Optimizing the learning objective with exploration allows us to escape from these local optima, illustrating the quasiconcavity criterion. It can be noted that when optimizing the objective <span class=\"math inline\"><em>L</em><sup><em>a</em></sup>(<em>θ</em>)</span>, the <span class=\"math inline\"><em>ϵ</em></span>-coherence criterion is respected for a large value of <span class=\"math inline\"><em>ϵ</em></span>, making the resulting policy worse than when optimizing the return directly. On the contrary, the objective <span class=\"math inline\"><em>L</em><sup><em>s</em></sup>(<em>θ</em>)</span> appears to have <span class=\"math inline\"><em>ϵ</em></span>-coherence for a reasonable value of <span class=\"math inline\"><em>ϵ</em></span>.</figcaption>\n</figure>\n\nIn the previous experiments with the dense setting, the local optima exist due to the negative rewards associated to idle-actions. If we consider the sparse setting, we could then assume that directly optimizing the return is sufficient to find high-performing policies. It is not always the case and it can be justified using the efficiency and attraction criteria. We use the same parameters as in the previous set of experiments and we provide the evolution of the return of the policies when optimizing the three objectives $J(\\pi_\\theta)$, $L^a(\\theta)$, and $L^s(\\theta)$, for the different environments in Figure [24](#fig:minigrid_sparse_return){reference-type=\"ref\" reference=\"fig:minigrid_sparse_return\"}. On the one hand, for most environments, whatever the learning objective, the resulting policy has a high return. Note that the $\\epsilon$-coherence can again be illustrated where the policies resulting from the optimization of the return perform better than the others. On the other hand, for the `MiniGrid-Empty-16x16-v0` and the `MiniGrid-SimpleCrossingS11N5-v0` environments, a high performing policy can only be found when optimizing the learning objective $L^s(\\theta)$. We illustrate that these results can be justified by the efficiency and attraction criteria in Figure [25](#fig:minigrid_sparse_proba){reference-type=\"ref\" reference=\"fig:minigrid_sparse_proba\"}. For each parameters obtained during the stochastic ascent steps on the return, we first estimate the probability of improving both objective functions by stochastic gradient ascent. These probabilities are used to compare learning objectives in terms of the efficiency criterion; the larger, the better. Then, in order to illustrate the attraction criterion, we estimate the probability of improving the return and the learning objective, both by gradient ascent steps on the learning objective, for each parameters obtained during the stochastic ascent steps on the objective $L^a(\\theta)$ and $L^s(\\theta)$. As far as $L^a(\\theta)$ is concerned, all probabilities remain small as result that the optimization procedure converges fast towards a stationary point where the target goal is observed with negligible probability. The efficiency and attraction criteria are respected for negligible probabilities $\\delta$, which is also a justification for the failure of converging towards good policies in the dense setting. For $L^s(\\theta)$, on the contrary, and in both environments, the probability of improving the learning objective remains large for each parameter encountered when optimizing the return. The efficiency of the learning objective is much higher than that of the return in that part of the parameter space. Furthermore, the probability of improving the return when optimizing the learning objective, is small at the beginning and increases after some iterations. This indicates that once the policy has a sufficiently large intrinsic return, the attraction criterion is respected for a high value $\\delta$.\n\n<figure id=\"fig:minigrid_sparse_return\">\n<figure>\n<embed src=\"figures/minigrid/without_action_cost/MiniGrid-Empty-8x8-v0_return.pdf\" />\n</figure>\n<figure>\n<embed src=\"figures/minigrid/without_action_cost/MiniGrid-Empty-16x16-v0_return.pdf\" />\n</figure>\n<figure>\n<embed src=\"figures/minigrid/without_action_cost/MiniGrid-SimpleCrossingS9N1-v0_return.pdf\" />\n</figure>\n<figure>\n<embed src=\"figures/minigrid/without_action_cost/MiniGrid-SimpleCrossingS9N2-v0_return.pdf\" />\n</figure>\n<figure>\n<embed src=\"figures/minigrid/without_action_cost/MiniGrid-SimpleCrossingS9N3-v0_return.pdf\" />\n</figure>\n<figure>\n<embed src=\"figures/minigrid/without_action_cost/MiniGrid-SimpleCrossingS11N5-v0_return.pdf\" />\n</figure>\n<figure>\n<embed src=\"figures/minigrid/without_action_cost/MiniGrid-FourRooms-v0_return.pdf\" />\n</figure>\n<figcaption>Evolution of the return of policies during optimization in the sparse minigrid environments. In blue, the return <span class=\"math inline\"><em>J</em>(<em>π</em><sub><em>θ</em></sub>)</span> is optimized; in orange, the learning objective <span class=\"math inline\"><em>L</em><sup><em>a</em></sup>(<em>θ</em>)</span> is optimized; and in green, the learning objective <span class=\"math inline\"><em>L</em><sup><em>s</em></sup>(<em>θ</em>)</span> is optimized performing Adam steps in REINFORCE directions. Note that the median, worst, and best cases over five runs are represented for the different curves. In most environments, a high-performing policy can be found by optimizing the return <span class=\"math inline\"><em>J</em>(<em>π</em><sub><em>θ</em></sub>)</span>. This results from its quasiconcavity. The <span class=\"math inline\"><em>ϵ</em></span>-coherence criterion can also be observed. However, for the <code>MiniGrid-Empty-16x16-v0</code> and the <code>MiniGrid-SimpleCrossingS11N5-v0</code> environments, a high-performing policy can only be found when optimizing the learning objective <span class=\"math inline\"><em>L</em><sup><em>s</em></sup>(<em>θ</em>)</span>.</figcaption>\n</figure>\n\n<figure id=\"fig:minigrid_sparse_proba\">\n<figure>\n<embed src=\"figures/minigrid/without_action_cost/MiniGrid-Empty-16x16-v0_exp1_improvement_proba_return_plot.pdf\" />\n</figure>\n<figure>\n<embed src=\"figures/minigrid/without_action_cost/MiniGrid-Empty-16x16-v0_exp1_improvement_proba_learning_plot.pdf\" />\n</figure>\n<figure>\n<embed src=\"figures/minigrid/without_action_cost/MiniGrid-Empty-16x16-v0_exp2_improvement_proba_return_plot.pdf\" />\n</figure>\n<figure>\n<embed src=\"figures/minigrid/without_action_cost/MiniGrid-Empty-16x16-v0_exp2_improvement_proba_learning_plot.pdf\" />\n</figure>\n<figure>\n<embed src=\"figures/minigrid/without_action_cost/MiniGrid-SimpleCrossingS11N5-v0_exp1_improvement_proba_return_plot.pdf\" />\n</figure>\n<figure>\n<embed src=\"figures/minigrid/without_action_cost/MiniGrid-SimpleCrossingS11N5-v0_exp1_improvement_proba_learning_plot.pdf\" />\n</figure>\n<figure>\n<embed src=\"figures/minigrid/without_action_cost/MiniGrid-SimpleCrossingS11N5-v0_exp2_improvement_proba_return_plot.pdf\" />\n</figure>\n<figure>\n<embed src=\"figures/minigrid/without_action_cost/MiniGrid-SimpleCrossingS11N5-v0_exp2_improvement_proba_learning_plot.pdf\" />\n</figure>\n<figcaption>For the <code>MiniGrid-Empty-16x16-v0</code> and the <code>MiniGrid-SimpleCrossingS11N5-v0</code> environments, and for both learning objective functions <span class=\"math inline\"><em>L</em><sup><em>a</em></sup>(<em>θ</em>)</span> and <span class=\"math inline\"><em>L</em><sup><em>s</em></sup>(<em>θ</em>)</span>, we first represent the estimated probability of improving the return <span class=\"math inline\"><em>J</em>(<em>π</em><sub><em>θ</em></sub>)</span> and the corresponding learning objective when following their REINFORCE gradient estimate. This value is estimated at each run of the optimization of the policy with learning objective <span class=\"math inline\"><em>J</em>(<em>π</em><sub><em>θ</em></sub>)</span>. Second, we represent the estimated probability of improving the corresponding learning objective and the return <span class=\"math inline\"><em>J</em>(<em>π</em><sub><em>θ</em></sub>)</span> when following the REINFORCE gradient estimate of the learning objectives. These values are estimated at each run of the optimization of the policy with the learning objectives. The probabilities were estimated based on the frequencies of improving the objective functions by more than <span class=\"math inline\">0.2</span> when following <span class=\"math inline\">5</span> Adam ascent steps using REINFORCE update directions.</figcaption>\n</figure>\n\n[^1]: Experimental details and implementations can be found at\\\n    <https://github.com/adrienBolland/micro-rl-lib>.\n\n[^2]: For the sake of keeping discussions simple, the definition of pseudoconcavity is simplified [@mangasarian1975pseudo], and additional assumptions on the stochastic gradient estimates are neglected.",
    "rationale": "Summary: The authors use two pathological environments to explore aspects of exploration bonuses in objectives.  For example, they show that these bonuses can potentially eliminate local optima and make the loss landscape more amenable to SGD.\n\nStrengths: This work offers a compelling perspective and analysis of the topic.  While some of the ideas they explore might be \"common knowledge\" in the community, I believe that this might be the first time that some of these ideas have been explored thoroughly and scientifically.\n\nWeaknesses: Typos and minor suggestions:\n- Discount factor backwards bracket in 2.1\n- Is (1) missing the sum?\n- “The valley is composed of two floors”: “floors” is a strange way to phrase it, and the intended meaning was initially unclear to me.  Consider rephrasing to something like \"The valley contains two separate low points”.\n- If there’s room, you could point to an appendix to briefly remind the reader why the middle and RHS of (7) are the same as alpha -> 0.  I know it’s fundamental SGD theory, but it confused me for a few minutes in this context (especially since, at first glance, the equation is not true, since it is just a first-order approximation and is only true as alpha approaches 0).\n- In 4.2, the thorough description of the environment is nice, but I think it’s missing the time limit used.  Also, were the curves given in Figure 3 computed analytically or empirically?  Some more info might improve the paper.\nIn 4.2, you could remind the reader what (4) is so that they do not have to hunt for it.\n\nWeaknesses:\n1) Some of the questions below are clarity weaknesses or other possible weaknesses.\n2) The contribution is entirely empirical (there are some nice theory concepts, but no proofs or useful properties are shown except empirically), and the empirical results are based entirely on extremely simple toy problems.  In my opinion, this does not invalidate their interesting perspectives on the issues that they illustrate with these results, but it does make the contribution less substantial.\n3) The paper overall is a bit challenging to read.  For an example of a well-written bit that does *not* have this issue, the conclusion says “First, it modifies the learning objective in order to remove local extrema. Second, it modifies the gradient estimates and increases the likelihood that the update steps lead to optimal policies.”  This is great; more “high-level summary” passages like this in sections 3-4 would’ve made it much easier to read.  For an example of a hard-to-understand part, see the last question below.\n\nNote: My primary concerns and slightly negative score come primarily from #2 (contribution), and the second question below (\"reward-engineering\").  The clarity weaknesses are not severe enough to have a large impact on my score.\n\nQuestions: What is the meaning of the and symbol in (6)?  How is the gradient equal to “0 and L(theta)”?  Or is the and symbol meant to separate this line into two separate equalities?  If so, this is confusing, consider representing this in a more standard way, or using parentheses to disambiguate.  **Update:** upon reading more of the paper, and seeing this used more, I know the latter interpretation is correct, so no need to answer this question in your response, but I’ll leave this here to illustrate the potential confusion to the reader.\n\nIn 4.1, J^d seems less like an exploration term, and more like a “reward-engineering” term that simply makes the problem easier.  Am I missing a perspective on this?  This leads to a larger concern, in that much of the contribution of 4.1 hinges on this term, and I have doubts about whether this term can be legitimately thought of as an exploration bonus that is superior to the entropy bonus.\n\nCan you please sum up the core take-away from 4.1?  I’ve reread the paragraph “We have empirically shown that exploration in policy gradients…” several times, but I’m struggling to understand exactly what I was supposed to take away from this section.  Is the point that the two criteria were good criteria in practice for choosing a good exploration bonus term?  If so, I am not convinced that this result will generalize beyond this specific toy setting (and the issue raised in the question above becomes even more of a concern in this case).  If not (or even if so), I think the intended take-away of this section needs to be spelled out more clearly.  **Update:** I understand better now upon a reread, so no need to address this question directly in your response.  However, the Section 4.1 paragraph noted is a perfect example of the last weakness noted above, so I’ll leave this question in the review.",
    "rating": 2,
    "label": "not novel",
    "rationale_edited": " The authors use two pathological environments to explore aspects of exploration bonuses in objectives.  For example, they show that these bonuses can potentially eliminate local optima and make the loss landscape more amenable to SGD.\n\nThis work offers a compelling perspective and analysis of the topic.  While some of the ideas they explore might be \"common knowledge\" in the community, I believe that this might be the first time that some of these ideas have been explored thoroughly and scientifically.\n\nThe contribution is entirely empirical (there are some nice theory concepts, but no proofs or useful properties are shown except empirically), and the empirical results are based entirely on extremely simple toy problems.  In my opinion, this does not invalidate their interesting perspectives on the issues that they illustrate with these results, but it does make the contribution less substantial.",
    "chosen": true
  },
  {
    "title": "Does Deep Active Learning Work in the Wild?",
    "abstract": "Deep active learning (DAL) methods have shown significant improvements in sample efficiency compared to simple random sampling. While these studies are valuable, they nearly always assume that optimal DAL hyperparameter (HP) settings are known in advance, or optimize the HPs through repeating DAL several times with different HP settings. Here, we argue that in real-world settings, or _in the wild_, there is significant uncertainty regarding good HPs, and their optimization contradicts the premise of using DAL (i.e., we require labeling efficiency).  In this study, we evaluate the performance of eleven modern DAL methods on eight benchmark problems as we vary a key HP shared by all methods: the pool ratio.  Despite adjusting only one HP, our results indicate that eight of the eleven DAL methods sometimes underperform relative to simple random sampling and some frequently perform worse. Only three methods always outperform random sampling (albeit narrowly), and we find that these methods all utilize diversity to select samples - a relatively simple criterion.  Our findings reveal the limitations of existing DAL methods when deployed _in the wild_, and present this as an important new open problem in the field.",
    "text": "# Introduction {#sec:introduction}\n\nIn this work, we focus on the application of active learning to deep neural networks (DNNs), sometimes referred to as Deep Active Learning (DAL) [@roy2018deep]. Broadly speaking, the premise of DAL is that some training instances will yield superior performance compared to others. Therefore, we can improve the training sample efficiency of DNNs by selecting the best training instances. A large number of methods have been investigated in recent years for DAL [@settles2009active; @ren2021survey; @holzmuller2023framework], often reporting significant improvements in sample efficiency compared to simpler strategies, such as random sampling [@tsymbalov2018dropout; @kading2018active; @kee2018query]. While these studies provide valuable insights, they nearly always assume good DAL hyperparameter (HP) settings are known in advance, or alternatively, they optimize the HPs (e.g., by repeating DAL several times with different HP settings). To our knowledge however, there is little evidence that one can assume good hyperparameters are known in advance for novel problems (see [3](#sec:ProblemSetting){reference-type=\"ref+label\" reference=\"sec:ProblemSetting\"}, where we find HP settings in the literature vary widely across problems). Moreover, running a DAL method multiple times in search of good HP settings may result in significant label inefficiency, even when compared to random sampling. Therefore, in real-world settings where DAL is applied to a novel problem, or *in the wild* as we term it here, the best DAL HPs are not generally known in advance, and it is unclear whether DAL still offers advantages (e.g., compared to random sampling) when accounting for HP uncertainty. If DAL models do not reliably outperform simple random sampling in the presence of HP uncertainty, it greatly undermines their value, and the likelihood that they will be adopted. Despite the significance of this problem, it has received little attention in the literature.\n\n<figure id=\"img:pool_based\">\n<div class=\"center\">\n<img src=\"imgs/Pool_based_AL_subsample_from_big_pool_0124_two_row.png\" />\n</div>\n<figcaption>Schematic diagram for pool-based DAL procedure. In the input space X, the triangles represent labeled data (<span class=\"math inline\"><em>L</em></span>), and the circles represent unlabeled data (<span class=\"math inline\"><em>D</em></span> for the full set of unlabeled data, and<span class=\"math inline\"><em>U</em></span> for subsampled unlabeled pool). At each step, after the model is trained using the existing training set <span class=\"math inline\"><em>L</em></span>, a subset of unlabeled data <span class=\"math inline\"><em>U</em></span> is sampled and evaluated by the AL criteria q(x). Then, the top-k points according to q(x) are labeled by the oracle function.</figcaption>\n</figure>\n\n#### Contributions {#subsec:contributions_of_this_work}\n\nIn this work, we perform the first systematic evaluation of DAL *in the wild*. We focus our investigation on DAL for regression, where to our knowledge, most applicable DAL methods are pool-based, and therefore they share an important HP: the pool ratio, $\\gamma$ (see [3](#sec:ProblemSetting){reference-type=\"ref+label\" reference=\"sec:ProblemSetting\"}). Using this property of regression problems, we evaluate a large number of DAL models as we vary a single HP, their $\\gamma$ setting, thereby providing a *distribution* of performance that one can expect in real-world settings (i.e., in the wild), where the best setting for $\\gamma$ is uncertain. We note that most DAL models have several (often unique) HPs that exhibit uncertainty, and each can contribute to performance variability of DAL methods in the wild. However, examining variability with respect to all of these HPs would require a lengthy exposition, and would be computationally costly. Therefore we focus on $\\gamma$, which mitigates the aforementioned challenges, while still providing sufficient empirical evidence to support our main conclusions.\n\nTo support our investigation, we assembled eight scientific computing regression problems to examine the performance of DAL methods in this setting; to our knowledge, this is the first such benchmark of its kind. We then identified past and recent DAL methods that are suitable for regression, totaling eleven methods\n\nTo support our study, we identified eleven DAL methods that are suitable for regression. We then examined the performance of these DAL methods on each of eight benchmark problems, compared to simple random sampling, as we vary their $\\gamma$ settings. Our results indicate that their performance varies significantly with respect to $\\gamma$, and that the best HP varies for different DAL/dataset with no single $\\gamma$ value working best across all settings, confirming our hypothesis that there is significant uncertainty regarding the best HP setting for novel problems. We also find that most of the DAL methods sometimes underperform simple random sampling and some frequently perform much worse:\n\n-   We compile a large benchmark of eleven state-of-the-art DAL methods across eight datasets. For some of our DAL methods, we are the first to adapt them to regression. Upon publication, we will publish the datasets and code to facilitate reproducibility.\n\n-   Using our benchmark, we perform the first analysis of DAL performance *in the wild*. Using $\\gamma$ as an example, we systematically demonstrate the rarely-discussed problem that most DAL models are often outperformed by simple random sampling when we account for HP uncertainty.\n\n-   We analyze the factors that contribute to the robustness of DAL in the wild, with respect to $\\gamma$.\n\n# Related works {#sec:related_work}\n\n#### Active learning benchmarks\n\nThe majority of existing AL benchmarks are for classification tasks, rather than regression [@jose2024regression], and many AL methods for classification cannot be applied to regression. Some existing studies include [@zhan2021comparative], which benchmarked AL using a Support Vector Machine (SVM) with 17 AL methods on 35 datasets. [@yang2018benchmark] benchmarked logistic regression with 12 AL methods and 44 datasets. [@meduri2020comprehensive] benchmarked specific entity matching application (classification) of AL with 3 AL methods on 12 datasets, with 3 different types of classifiers (DNN, SVM, and Tree-based). [@trittenbach2021overview] benchmarked an AL application in outlier detection on 20 datasets and discussed the limitation of simple metrics extensively. [@hu2021towards] benchmarked 5 classification tasks (including both image and text) using DNN. [@beck2021effective] benchmarked multiple facets of DAL on 5 image classification tasks. For the regression AL benchmark, [@o2017model] benchmarked 5 AL methods and 7 UCI [^1] datasets, but they only employed linear models. [@wu2019active] compared 5 AL methods on 12 UCI regression datasets, also using linear regression models. Our work is fundamentally different from both, as we use DNNs as our regressors, and we employ several recently-published problems that also involved DNN regressors, making them especially relevant for DAL study. The recent study by [@holzmuller2023framework] is the only work that is similar to ours, in which the authors benchmarked 8 pool-based DAL methods for regression on 15 datasets. The primary focus of their work was to propose a novel DAL regression framework, termed LCMD; meanwhile the focus of our work is to investigate DAL in the wild. Consequently, [@holzmuller2023framework] presents different performance metrics and conclusions compared to our study.\n\n#### Active learning for regression problems\n\nRegression problems have received (relatively) little attention compared to classification [@ren2021survey; @guyon2011results]. For the limited AL literature dedicated to regression tasks, Expected Model Change (EMC) [@settles2008curious; @cai2013maximizing] was explored, where an ensemble of models was used to estimate the true label of a new query point using both linear regression and tree-based regressors. Gaussian processes were also used with a natural variance estimate on unlabeled points in a similar paradigm [@kading2018active]. [@smith2018less] used Query By Committee (QBC), which trains multiple networks and finds the most disagreeing unlabeled points of the committee of models trained. [@tsymbalov2018dropout] used the Monte Carlo drop-out under a Bayesian setting, also aiming for maximally disagreed points. [@yu2010passive] found $x$-space-only methods outperforming y-space methods in robustness. [@yoo2019learning] proposed an uncertainty-based mechanism that learns to predict the loss using an auxiliary model that can be used on regression tasks. [@ranganathan2020deep] and [@kading2016active] used Expected Model Output Change (EMOC) with Convolutional Neural Network (CNN) on image regression tasks with different assumptions. We included all these methods that used deep learning in our benchmark.\n\n#### DAL in the wild\n\nTo our knowledge, all empirical studies of pool-based DAL methods assume that an effective pool ratio hyperparameter, $\\gamma$, is known apriori. While the majority of works assumed the original training set as the fixed, unlabeled pool, [@yoo2019learning] limited their method to a subset of 10k instances instead of the full unlabeled set and [@beluch2018power] used subsampling to create the pool $U$ (and hence $\\gamma$). In real-world settings - in the wild - we are not aware of any method to set $\\gamma$ a priori, and there has been no study of DAL methods under this setting. Therefore, we believe ours is the first such study.\n\n# Problem Setting {#sec:ProblemSetting}\n\nIn this work, we focus on DAL for regression problems, which comprise a significant portion of DAL problems involving DNNs [@jose2024regression]. As discussed in [1](#sec:introduction){reference-type=\"ref+label\" reference=\"sec:introduction\"}, nearly all DAL methods for regression are pool-based, which is one of the three major paradigms of AL, along with stream-based and query synthesis. [@settles2009active]\n\n#### Formal description\n\nLet $L^i = (X^{i}, Y^{i})$ be the dataset used to train a regression model at the $i^{th}$ iteration of active learning. We assume access to some oracle, denoted $f : \\mathcal{X} \\rightarrow \\mathcal{Y}$, that can accurately produce the target values, $y \\in \\mathcal{Y}$ associated with input values $x \\in \\mathcal{X}$. Since we focus on DAL, we assume a DNN as our regression model, denoted $\\hat{f}$. We assume that some relatively small number of $N_{0}$ labeled training instances are available to initially train $\\hat{f}$, denoted $L^0$. In each iteration of DAL, we must choose $k$ query instances to be labeled by the oracle, yielding a set of labeled instances, denoted $Q$, that is added to the training dataset. Our goal is then to choose $Q$ that maximizes the performance of the DNN-based regression models over unseen test data at each iteration of active learning.\n\n#### Pool-based Deep Active Learning\n\nGeneral pool-based DAL methods assume that we have some pool $U$ of $N_{U}$ unlabeled instances from which we can choose the $k$ instances to label. The set $U$ is sampled from a larger and potentially-infinite set, denoted $D$, and $N_{U}$ is a HP chosen by the DAL user. We note that in some DAL applications, such as computer vision, it is conventional to utilize all available unlabeled data for $U$, and the pool size is not often explicitly varied or discussed. However, this convention is equivalent to setting $U=D$, and thereby implicitly setting the $N_{U}$ HP. Most pool-based methods rely upon some acquisition function $q: \\mathcal{X} \\rightarrow \\mathbb{R}$ to assign some scalar value to each $x \\in U$ indicating its \\\"informativeness\\\", or utility for training $\\hat{f}$. In each iteration of active learning, $q$ is used to evaluate all instances in $U$, and the top $k$ are chosen to be labeled and included in $L$.\n\n<figure id=\"img:pool_ratio_schematic\">\n<div class=\"center\">\n<img src=\"imgs/QBC_2by1.png\" />\n</div>\n<figcaption>Pool-based DAL for uncertainty-based mechanism. <span class=\"math inline\"><em>q</em>(<em>x</em>)</span> is the acquisition metric. (a, b) are two scenarios of the pool ratio (<span class=\"math inline\"><em>γ</em></span>) being too small (4 in a) or too large (32 in b) in <span class=\"math inline\"><em>k</em></span> (step size) of 2. </figcaption>\n</figure>\n\n#### The pool ratio hyperparameter, $\\gamma$\n\nWe define the *pool ratio* as $\\gamma = N_{U}/k$. By definition, $N_{U}$ and $k$ are hyperparameters of pool-based problems, and therefore $\\gamma$ also is. While one could, in principle, vary $N_{U}$ and $k$ independently, this is not often done in practice. Typically $k$ is set as small as possible, limited by computational resources. This leaves $N_{U}$ as the major free hyperparameter; however, prior research has found that its impact depends strongly on its size relative to $k$ [@kee2018query; @tsymbalov2018dropout; @kading2018active], encoded in $\\gamma$. Given a fixed value of $k$, increasing $N_{U}$ can lead to the discovery of points with higher values of $q(x)$ due to denser sampling of the input space. However, a larger $N_{U}$ also increases the similarity of the points, which provides redundant information to the model - a problem referred to as mode collapse [@burbidge2007active; @ren2021survey; @kee2018query]. In the limit as $N_{U} \\rightarrow \\infty$, all of the $k$ selected query points will be located near the same $x \\in \\mathcal{X}$ that has the highest value of $q(x)$. This tradeoff is illustrated in [2](#img:pool_ratio_schematic){reference-type=\"ref+label\" reference=\"img:pool_ratio_schematic\"} for a simple problem, and has also been noted in [@cacciarelli2024active].\n\nIn most real-world settings, there is a substantial quantity of unlabeled data (often infinite), and the user has the freedom (or burden) of choosing a suitable $\\gamma$ setting for their problem by varying the size of $U$. Crucially, and as we show in our experiments, choosing a sub-optimal $\\gamma$ value can result in poorer performance than naive random sampling. This is not necessarily a problem if either (i) one $\\gamma$ setting works across most problems or, alternatively, (ii) $\\gamma$ can be optimized on new problems without using labels. To the best of our knowledge, there is no method for optimizing $\\gamma$ on a new problem without running multiple trials of AL to find the best one (i.e., collecting labels), defeating the purpose of AL in real-world settings. Furthermore, the value of $\\gamma$ varies widely across the literature, suggesting that suitable settings for $\\gamma$ indeed vary across problems (see supplement for a list).\n\n# Benchmark Regression Problems {#sec:benchmark_problems}\n\n:::::: center\n::::: small\n:::: sc\n::: {#tbl:benchmark_dataset}\n  Dataset         Sine      Robo          Stack          ADM        Foil        Hydr       Bess       Damp\n  ----------- ------------ ------ --------------------- ------ --------------- ------ -------------- ------\n  $Dim_{x}$        1         4              5             14          5          6          2          3\n  $Dim_{y}$        1         2             201           2000         1          1          1         100\n  Oracle       Analytical          Numerical simulator   DNN    Random Forest          ODE solution  \n\n  : Benchmark datasets dimensionality and oracle functions. $Dim_{x, y}$ are the dimensionality of $x$ and y. Note that ODE solutions are implemented in the form of analytical functions as well.\n:::\n::::\n:::::\n::::::\n\nTo compose our benchmarks, we focused primarily upon problems in scientific computing, which is an important emerging problem setting [@subramanian2024towards; @takamoto2022pdebench; @majid2024mixture]. We propose eight regression problems to include in our benchmark set: two simple toy problems (SINE, ROBO), four contemporary problems from publications in diverse fields of science and engineering (STACK, ADM, FOIL, HYDR) and two problems solving ordinary differential equations (also prevalent in engineering). Summary details of our benchmark problems can be found in [1](#tbl:benchmark_dataset){reference-type=\"ref+label\" reference=\"tbl:benchmark_dataset\"} and [\\[tbl:oracle_details\\]](#tbl:oracle_details){reference-type=\"ref+label\" reference=\"tbl:oracle_details\"}.\n\nWe utilized four major selection criteria, beyond choosing scientific computing problems: (i) diversity: we sought to include a set of problems that span different disciplines (aero and fluid-dynamics, materials science), and problems that require physical experiments (e.g., FOIL, HYDRO) versus simulators (e.g., ADM); (ii) availability of labeled data: the problems we chose (unlike many high dimension ones) all had sufficiently large amount of labeled data, allowing us to easily study the impact of different pool ratios; (iii) dimensionality: we sought problems with relatively low dimensionality because they mitigate computational costs allowing for more extensive experimentation, while still being representative of many contemporary scientific computing problems (e.g., labeling can be highly expensive, severely limiting total labeled data, and making even low-dimensional problems challenging); (iv) difficulty: the problems in our dataset are also \"difficult\" in the sense that the accuracy of the learners (i.e., the DNN regressors) can vary significantly depending upon which data are labeled, making it possible to distinguish between more/less effective AL approaches. Although this is not the only notion of \"difficulty\" that may be relevant for selecting benchmark problems, we believe this is the most important one, and has been used in recent DAL studies [@holzmuller2023framework]. We now describe our benchmark problems:\n\n::::: center\n:::: small\n::: sc\n:::\n::::\n:::::\n\n**1D sine wave (SINE)** A noiseless 1-dimensional sinusoid with smoothly-varying frequency. **2D robotic arm (ROBO)** [@ren2020benchmarking] The goal is to predict the 2-D spatial location of the endpoint of a robotic arm based on its three joint angles. **Stacked material (STACK)** [@Chen2019] The goal is to predict the 201-D reflection spectrum of a material based on the thickness of its five layers. **Artificial Dielectric Material (ADM)** [@deng2021neural] The goal is to predict the 2000-D reflection spectrum of a material based on its 14-D geometric structure. Full wave electromagnetic simulations were utilized in [@deng2021benchmarking] to label data in the original work, requiring 1-2 minutes per input point. **NASA Airfoil (FOIL)** [@Dua:2019] The goal is to predict the sound pressure of an airfoil based on the structural properties of the foil, such as its angle of attack and chord length. This problem was published by NASA [@brooks1989airfoil] and the instance labels were obtained from a series of real-world aerodynamic tests in an anechoic wind tunnel. It has been used in other AL literature [@wu2018pool; @liu2020unsupervised; @jose2024regression]. **Hydrodynamics (HYDR)** [@Dua:2019] The goal is to predict the residual resistance of a yacht hull in water based on its shape. This problem was published by the Technical University of Delft, and the instance labels were obtained by real-world experiments using a model yacht hull in the water. It is also referred to as the \\\"Yacht\\\" dataset in some AL literature [@wu2019active; @cai2013maximizing; @jose2024regression]. **Bessel function (BESS)** The goal is to predict the value of the solution to Bessel's differential equation, a second-order ordinary differential equation that is common in many engineering problems. The inputs are the function order $\\alpha$ and input position $x$. The order $\\alpha$ is limited to non-negative integers below 10. **Damping Oscillator (DAMP)** The goal is to predict the full-swing trajectory of a damped oscillator in the first 100 time steps, of the solution to a second-order ordinary differential equation. The input is the magnitude, damping coefficient, and frequency of the oscillation.\n\n:::::: table*\n::::: center\n:::: small\n::: sc\n                Method                                                                  Acquisition function (q)\n  ---------------------------------- ------------------------------------------------------------------------------------------------------------------------------\n            Core-set (GSx)                                       $\\displaystyle \\min_{x\\in \\mathcal{L} \\cup \\mathcal{Q}} dist(x^*, x)$\n          [@sener2017active]         \n      Greedy sampling in y (GSy)                            $\\displaystyle  \\min_{y\\in \\mathcal{L} \\cup \\mathcal{Q}} dist(\\hat{f}(x^*), y)$\n           [@wu2019active]           \n   Improved greedy sampling (GSxy)                  $\\displaystyle  \\min_{(x,y)\\in \\mathcal{L} \\cup \\mathcal{Q}} dist(x^*, x)*dist(\\hat{f}(x^*), y)$\n           [@wu2019active]           \n       Query by committee (QBC)                                   $\\displaystyle  \\frac{1}{N}\\sum^N_{n=1}(\\hat{f}_n(x^*)-\\mu(x^*))^2$\n           [@kee2018query]           \n                                     \n           QBC w/ diversity                                                   $\\displaystyle  q_{QBC}(x^*) + q_{div}(x^*)$\n       (QBCDiv) [@kee2018query]                                             $\\displaystyle (q_{div}(x^*) =  q_{GSx}(x^*) )$\n      QBC w/ diversity & density                                      $\\displaystyle  q_{QBC}(x^*) + q_{div}(x^*) + q_{den}(x^*)$\n     (QBCDivDen) [@kee2018query]                            $\\displaystyle  (q_{den}(x^*) = \\dfrac{1}{k} \\sum_{x\\in N_k(x^*)} sim(x^*, x) )$\n   bayesian by disagreement (BALD)                                             $\\displaystyle  q_{QBC}(x^*)$ with dropout\n       [@tsymbalov2018dropout]       \n     Expected model output change                  $\\displaystyle \\mathbb{E}_{y'|x'} \\mathbb{E}_{x} || \\hat{f}(x^*; \\phi') - \\hat{f}(x^*; \\phi)||_1$\n    (EMOC) [@ranganathan2020deep]     $\\displaystyle \\approx \\mathbb{E}_{x} || \\nabla_{\\phi} \\hat{f}(x; \\phi) * \\nabla_{\\phi} \\mathcal{L}(\\phi; (x^{*'}, y'))||_1$\n   Learning Loss [@yoo2019learning]                                                  $\\displaystyle f_{loss}(x^*)$\n           Cluster-Variance                                                            $q_{QBC}(x)^*$ in clusters\n         [@citovsky2021batch]        \n        Density-Aware Core-Set                                                       $q_{GSx}(x^*) + q_{den}(x^*)$\n       (DACS)[@kim2022defense]       \n:::\n::::\n:::::\n\nin\n::::::\n\nFrom the literature, we found eleven AL methods that are (i) applicable to regression problems, (ii) with DNN-based regressors, making them suitable for benchmark regression problems. Due to space constraints, we list each method in [\\[tbl:benchmark_method\\]](#tbl:benchmark_method){reference-type=\"ref+label\" reference=\"tbl:benchmark_method\"} along with key details, and refer readers to the supplement for full details. Some of the methods have unique HPs that must be set by the user. In these cases, we adopt HP settings suggested by the methods' authors, shown in [\\[tbl:benchmark_method\\]](#tbl:benchmark_method){reference-type=\"ref+label\" reference=\"tbl:benchmark_method\"}. Upon publication, we will publish code for all of these methods to support future benchmarking.\n\n# Benchmark Experiment Design {#sec:exp_design}\n\nIn our experiments, we compare eleven state-of-the-art DAL methods on eight scientific computing problems. We evaluate the performance of our DAL methods as a function of $\\gamma$ on each of our benchmark problems, with $\\gamma \\in [2,4,8,16,32,64]$ (i.e., at each step we sample our $U$ with $k*\\gamma$ points). Following convention [@kee2018query; @tsymbalov2018dropout], we assume a small training dataset is available at the outset of active learning, $T^{0}$, which has $N_{0} = 80$ randomly sampled training instances. We then run each DAL model to $T^{50}$ AL steps, each step identifying $k=40$ points to be labeled from a fresh, randomly generated pool of size $k*\\gamma$. For each benchmark problem, we assume an appropriate neural network architecture is known apriori. Each experiment (i.e., the combination of dataset, DAL model, and $\\gamma$ value) is run 5 times to account for randomness. The MSE is calculated over a set of 4000 test points that are uniformly sampled within the $x$-space boundary. To reduce unnecessary noise related to our core hypothesis, we use the same (randomly sampled) unlabeled pools across different DAL methods.\n\nWe must train a regression model for each combination of problem and DAL method. Because some DAL methods require an ensemble model (e.g., QBC), we use an ensemble of 10 DNNs as the regressor for all of our DAL algorithms (except for the ADM problem, which is set to 5 due to the GPU RAM limit). More details on the models used and training procedures can be found in the supplement. Following convention [@kading2018active; @wu2018pool; @o2017model], we summarize our DAL performance by the area under curve (AUC) of the error plot. We report the full MSE vs \\# labeled point plots in the supplement. For the AUC calculation, we use 'sklearn.metrics.auc' [@scikit-learn] then further normalize by such AUC of random sampling method for easier visualization. All reported results are given in the unit of normalized AUC of MSE ($nAUC_{MSE})$.\n\n# Experimental Results {#sec:result}\n\n<figure id=\"img:main_perf\">\n<div class=\"center\">\n<img src=\"imgs/agg_mid_bar_plot_touched.png\" />\n</div>\n<figcaption>Performance of each DAL method (x-axis) in terms of <span class=\"math inline\"><em>n</em><em>A</em><em>U</em><em>C</em><sub><em>M</em><em>S</em><em>E</em></sub></span> (y-axis). For each DAL method, we report a bar indicating the <em>range</em> of <span class=\"math inline\"><em>n</em><em>A</em><em>U</em><em>C</em><sub><em>M</em><em>S</em><em>E</em></sub></span> values obtained as we vary the pool ratio, <span class=\"math inline\"><em>γ</em> ∈ [2, 4, ..., 64]</span>; for a given DAL method, we report one bar for each of the eight benchmark problems, indicated by a unique color in the legend. Each bar is bisected by a solid black and magenta line, respectively. The black line represents the average <span class=\"math inline\"><em>n</em><em>A</em><em>U</em><em>C</em><sub><em>M</em><em>S</em><em>E</em></sub></span> value across all settings of <span class=\"math inline\"><em>γ</em></span>. The magenta line represents the performance using <span class=\"math inline\"><em>γ</em><sub><em>p</em><em>r</em><em>i</em><em>o</em><em>r</em></sub></span> (see <a href=\"#sec:result\" data-reference-type=\"ref+label\" data-reference=\"sec:result\">6</a> for details). The dashed red line at <span class=\"math inline\"><em>n</em><em>A</em><em>U</em><em>C</em><sub><em>M</em><em>S</em><em>E</em></sub> = 1</span> corresponds to the performance obtained using random sampling. Note that some vertical bars are intentionally clipped at the top to improve the visualization overall.</figcaption>\n</figure>\n\nThe performance of all eleven DAL methods on all eight benchmark datasets is summarized in [3](#img:main_perf){reference-type=\"ref+label\" reference=\"img:main_perf\"}. The y-axis is the normalized $nAUC_{MSE}$, the x-axis is the DAL methods of interest, and the color code represents the different benchmark datasets. The horizontal red dashed line represents the performance of random sampling, which by definition is equal to one (see [5](#sec:exp_design){reference-type=\"ref+label\" reference=\"sec:exp_design\"}). Further details about [3](#img:main_perf){reference-type=\"ref+label\" reference=\"img:main_perf\"} are provided in its caption. We next discuss the results, with a focus on findings that are most relevant to DAL in the wild.\n\nThe results in [3](#img:main_perf){reference-type=\"ref+label\" reference=\"img:main_perf\"} indicate that *all* of our benchmark DAL methods are sensitive to their setting of $\\gamma$ - a central hypothesis of this work. As indicated by the vertical bars in [3](#img:main_perf){reference-type=\"ref+label\" reference=\"img:main_perf\"}, the $nAUC_{MSE}$ obtained by each DAL method varies substantially with respect to $\\gamma$. For most of the DAL methods, there exist settings of $\\gamma$ (often many) that cause them to perform worse than random sampling. This has significant implications for DAL in the wild since, to our knowledge, there is no general method for estimating a good $\\gamma$ setting prior to collecting large quantities of labeled data (e.g., to run trials of DAL with different $\\gamma$ settings), and DAL methods may perform worse, and unreliably, when accounting for the uncertainty of $\\gamma$.\n\n<figure id=\"img:best_pr_hist\">\n<div class=\"center\">\n<img src=\"imgs/best_pr_hist.png\" style=\"width:65.0%\" />\n</div>\n<figcaption>Frequency histogram of the best pool ratio values found in each DAL. For a given DAL method, this figure shows the frequency (% out of 8) that a particular pool ratio (x-axis) performs the best in terms of average <span class=\"math inline\"><em>n</em><em>A</em><em>U</em><em>C</em><sub><em>M</em><em>S</em><em>E</em></sub></span> metric. </figcaption>\n</figure>\n\n## DALs are sensitive to their pool ratio, $\\gamma$ {#sec:results_dals_are_sensitive_to_pool_ratio}\n\nThe sensitivity of DAL regression models to $\\gamma$ may be less significant if there exist $\\gamma$ settings that tend to perform well across most problems (for a given DAL method). [4](#img:best_pr_hist){reference-type=\"ref+label\" reference=\"img:best_pr_hist\"} presents a histogram of the best-performing $\\gamma$ settings for each DAL method. The results indicate that for each method there is no setting of $\\gamma$ that performs best across all problems. This corroborates our observations from the literature where we found a wide range of $\\gamma$ settings used across studies. However, we do see that some methods tend to have similar $\\gamma$ settings across all problems. For example, DACS has its best performance near $\\gamma=2$, although DACS performs poorly overall. GSxy, however, is one of the best-performing methods overall, and its best-performing settings cluster around $\\gamma = 16$. Given this observation, we investigate how well we can perform if we use historical results for a given method to choose a $\\gamma$ value for future problems. We emulate this scenario by evaluating the performance of each DAL method when adopting the best single $\\gamma$ setting from [4](#img:best_pr_hist){reference-type=\"ref+label\" reference=\"img:best_pr_hist\"} (i.e., the setting that wins across the most benchmarks), which we term $\\gamma_{prior}$, and then apply it across all benchmarks. The result of this strategy is given by the magenta line in [3](#img:main_perf){reference-type=\"ref+label\" reference=\"img:main_perf\"}. In most (but not all) cases, $\\gamma_{prior}$ yields lower MSE than the average MSE of all $\\gamma$ settings (the black lines). In some cases, $\\gamma_{prior}$ yields substantial overall performance improvements, such as for GSx and GSxy, suggesting that this is a reasonable $\\gamma$ selection strategy, although the benefits seem to vary across DAL models. However, even when using $\\gamma_{prior}$, the performance of DAL models still varies greatly, and many models still perform worse than random sampling. Therefore, while $\\gamma_{prior}$ may often be beneficial, it does not completely mitigate $\\gamma$-uncertainty.\n\n## Do any DAL methods outperform random sampling in the wild? {#sec:results_do_dal_outperform_random_sampling}\n\nThe results indicate that several DAL methods *tend* to obtain much lower $nAUC_{MSE}$ (i.e., they are better) than random sampling. This includes methods such as GSx, GSxy, GSy, QBC-x (variations of QBC) and ClusterVar. The results therefore suggest that these methods are beneficial more often than not, compared to random sampling - an important property. However, as discussed in [6.1](#sec:results_dals_are_sensitive_to_pool_ratio){reference-type=\"ref+label\" reference=\"sec:results_dals_are_sensitive_to_pool_ratio\"}, all DAL methods exhibit significant performance variance with respect to $\\gamma$, and some of the aforementioned methods still sometimes perform worse than random sampling. For example, this is the case of QBC, GSy, and QBCDivDen on the SINE problem. In settings where DAL is useful, the cost of collecting labels tends to be high, and therefore the risk of poor DAL performance (e.g., relative to simple random sampling) may strongly deter its use. Therefore, another important criteria is performance robustness: do any DAL methods consistently perform better than random sampling, in the wild? Our results indicate that GSx, GSxy, and QBCDiv always perform at least as well as random sampling, and often substantially better, regardless of the problem or $\\gamma$ setting. Note that all three robust DALs (GSx, GSxy, QBCDiv) employ x-space diversity in their loss function, which we discuss further in [6.3](#sec:results_why_some_methods_are_better){reference-type=\"ref+label\" reference=\"sec:results_why_some_methods_are_better\"}.\n\n<figure id=\"img:mode_collapse\">\n<div class=\"center\">\n<img src=\"imgs/negative_correlation_robo_touched-01.png\" />\n</div>\n<figcaption>A representative, combined plot with <span class=\"math inline\"><em>n</em><em>A</em><em>U</em><em>C</em><sub><em>M</em><em>S</em><em>E</em></sub></span> performance (bottom, y-axis at left, solid) and collapse metric, nDiv (upper, y-axis at right, more transparent) for each of the eleven DAL models at all pool ratios (color coded) for <em>robotic arm</em> dataset (ROBO). Dashed horizontal red lines starting from both <span class=\"math inline\"><em>y</em></span> axes represent the random sampling’s average <span class=\"math inline\"><em>n</em><em>A</em><em>U</em><em>C</em><sub><em>M</em><em>S</em><em>E</em></sub></span> and nDiv at 1. </figcaption>\n</figure>\n\n## Sample diversity is important for DAL in the wild {#sec:results_why_some_methods_are_better}\n\nOur results indicate that the best-performing DAL methods are GSx, GSxy, and QBCDiv. We say these methods are \\\"best\\\" because they are both robust (see [6.2](#sec:results_do_dal_outperform_random_sampling){reference-type=\"ref+label\" reference=\"sec:results_do_dal_outperform_random_sampling\"}), and they also usually yield lower MSEs, than other DAL methods. These methods share the common property that they encourage training data diversity, as measured by $x$-space distance between points. Interestingly, GSx *only* relies on x-space diversity. These results suggest that $x$-space diversity is a highly effective DAL acquisition criterion. Furthermore, and in contrast to other criteria, seeking points that maximize $x$-space diversity does not (by definition) increase the risk of mode collapse. Consequently, increasing $\\gamma$ results in greater diversity but without any increased risk of mode collapse (more details in [5](#img:mode_collapse){reference-type=\"ref+label\" reference=\"img:mode_collapse\"}). This may be a major reason why GSx, GSxy, and QBCDiv are less sensitive to $\\gamma$, and provide much more robust performance in the wild than other DAL methods. While sampling methods that use diversity have been found to be promising [@jose2024regression], our work provides evidence, for the first time, that sampling based upon diversity *may* be robust to hyperparameter uncertainty (we only examine uncertainty of $\\gamma$) while other popular sampling criteria (e.g., estimated model error) seem to be much less reliable in the wild.\n\nTo corroborate these findings, we evaluated the $x$-space diversity of each DAL method as a function of $\\gamma$. In particular, we calculated the diversity metric as the average nearest neighbor distance $$Div = \\dfrac{1}{|T|} \\sum_t^T \\dfrac{1}{K} \\sum_{i}^K \\min_{x^* \\in \\mathcal{Q^T}} dist(x^*, x^i)$$ where $\\mathcal{Q}^t$ represents the queried batch at active learning step $t$ and $|T|=50$ is the total number of active learning steps. Note that this metric is similar to, but not a simple average of $q_{GSx}(x)$ as $Div$ only focuses on per batch diversity and does not take the labeled set into consideration. It is also further normalized ($nDiv$) by the value of random sampling for each dataset separately. The lower this metric's value, the more severe the mode collapse issue would be.\n\nThe $nDiv$ is plotted in the top half of [5](#img:mode_collapse){reference-type=\"ref+label\" reference=\"img:mode_collapse\"} using the inverted right y-axis. For the obvious failure cases (BALD, EMOC and Learning Loss) in this particular dataset (their $nAUC_{MSE}$ exceeds 1), a clear trend of mode collapse can be observed in the upper half of the plot (nDiv much lower than 1). Meanwhile, a strong correlation between the pool ratio and the diversity metric can be observed: (i) For GSx and GSxy methods, which seek to maximize diversity, their diversity increases monotonically with larger pool ratio. (ii) For uncertainty-based methods (BALD, EMOC, LearningLoss, QBC, MSE), which seek to maximize query uncertainty, their diversity decreases monotonically with larger pool ratios. (iii) For combined methods like QBCDiv and QBCDivDen, the relationship between pool ratio and diversity shows a weak correlation, consistent with the benefits of having diversity as a selection criterion. (iv) Lastly, we observe that all top-performing methods have high diversity, regardless of $\\gamma$, suggesting it is an important condition for effective DAL.\n\n# Conclusions {#sec:conclusions}\n\nFor the first time, we evaluated eleven state-of-the-art DAL methods on eight benchmark datasets for regression *in the wild*, where we assume that the best pool ratio hyperparameter, $\\gamma$, is uncertain. We summarize our findings as follows:\n\n-   *DAL methods for regression often perform worse than simple random sampling, when evaluated in the wild*. Using $\\gamma$ as an example, we systematically demonstrate the rarely-discussed problem that most DAL models are often outperformed by simple random sampling when we account for HP uncertainty.\n\n-   *Some DAL methods were relatively robust, and outperformed random sampling robustly in the wild (e.g., GSx, GSxy, QBCDiv)*.\n\n-   *Insofar as robustness to pool ratio is concerned, our results suggest that DAL approaches utilizing sample diversity tend to be much more robust in the wild than other popular selection criteria.*\n\n## Limitations\n\nOne limitation of this work is that we focused on scientific computing benchmark problems, and problems with relatively low dimensionality. Including higher dimensional problems is an especially important opportunity for future work due to the importance of vision problems in the DAL community, and also because sensitivity to pool ratio has been noted in that setting as well [@yoo2019learning; @sener2017active], but not studied systematically. Another important limitation is that we constrained our evaluation of DAL methods to uncertainty in their pool ratio. Future studies would benefit from evaluating each DAL approach with respect to uncertainty in all of its relevant DAL HPs (i.e., those that require labeled data to be optimized), providing a more comprehensive assessment of modern DAL methods in the wild.\n\n### Acknowledgments {#acknowledgments .unnumbered}\n\nSimiao Ren thanks the support of the Duke aiM trainee program, by the NSF grant DGE-2022040.\n\n# Details of the benchmarking methods\n\n**Core-set (GSx: Greedy sampling in $x$ space)** [@sener2017active]. This approach only relies upon the diversity of points in the input space, $\\mathcal{X}$, when selecting new query locations. A greedy selection criterion is used, given by $$q_{GSx}(x^*) = \\min_{x\\in \\mathcal{L} \\cup \\mathcal{Q}} dist(x^*, x)$$ where $\\mathcal{L}$ is the labeled set, $\\mathcal{Q}$ is the already selected query points and $dist$ being L2 distance.\n\n**Greedy sampling in $y$ space (GSy)** [@wu2019active]. Similar to GSx which maximizes diversity in the $x$ space in a greedy fashion, GSy maximizes the diversity in the $y$ space in a greedy fashion: $$q_{GSy}(x^*) = \\min_{y\\in \\mathcal{L} \\cup \\mathcal{Q}} dist(f(x^*), y)$$ where $f(x)$ is the current model prediction of the $x$ and $y$ is the labels in the already labeled training set plus the predicted labels for the points (to be labeled) selected in the current step.\n\n**Greedy sampling in xy space (GSxy)** [@wu2019active]. Named as 'Improved greedy sampling (iGS)' in the original paper [@wu2019active], this approach combines GSx and GSy and uses multiplication of the distance of both $x$ and $y$ space in its acquisition function: $$q_{GSxy}(x^*) = \\min_{(x,y)\\in \\mathcal{L} \\cup \\mathcal{Q}} dist(x^*, x)*dist(f(x^*), y)$$\n\n**Query-by-committee (QBC)** [@seung1992query] The QBC approach is pure uncertainty sampling if we set $q(x) = q_{QBC}(x)$: $$q_{QBC}(x) = \\frac{1}{N}\\sum^N_{n=1}(\\hat{f}_n(x)-\\mu(x))^2$$ Here $\\hat{f}_{n}$ denotes the $n^{th}$ model in an ensemble of $N_{ens}$ models (DNNs in our case), and $\\mu(x)$ is the mean of the ensemble predictions at $x$. In each iteration of AL these models are trained on all available training data at that iteration.\n\n**QBC with diversity (Div-QBC)** [@kee2018query]. This method improves upon QBC by adding a term to $q$ that also encourages the selected query points to be diverse from one another. This method introduces a hyperparameter for the relative weight of the diversity and QBC criteria and we use an equal weighting ($\\alpha = 0.5$ [@kee2018query]). $$\\begin{aligned}\n    q_{QBCDiv}(x) &= (1-\\alpha)* q_{QBC}(x) + \\alpha * q_{div}(x)\\\\\n    q_{div}(x^*) &=  q_{GSx}(x^*) \n\\end{aligned}$$\n\n**QBC with diversity and density (DenDiv-QBC)** [@kee2018query]. This method builds upon Div-QBC by adding a term to $q(x)$ that encourages query points to have uniform density. This method introduces two new hyperparameters for the relative weight ($\\alpha = \\beta = \\dfrac{1}{3}$) of the density, diversity, and QBC criteria, and we use an equal weighting as done in the original paper [@kee2018query]. $$\\begin{aligned}\n    q_{QBCDivDen}(x) &= (1-\\alpha - \\beta)* q_{QBC}(x) \\\\\n    & + \\alpha * q_{div}(x) + \\beta * q_{den}(x)\\\\\n    q_{den}(x^*) &= \\dfrac{1}{k} \\sum_{x\\in N_k(x^*)} sim(x^*, x)\n\\end{aligned}$$ where $N_k(x^*)$ is the k nearest neighbors of an unlabeled point, $sim(x^*, x)$ is the cosine similarity between points.\n\n**Bayesian active learning by disagreement (BALD)** [@tsymbalov2018dropout]. BALD uses the Monte Carlo dropout technique to produce multiple probabilistic model output to estimate the uncertainty of model output and uses that as the criteria of selection (same as $q_{QBC}(x)$). We used 25 forward passes to estimate the disagreement.\n\n**Expected model output change (EMOC)** [@kading2018active; @ranganathan2020deep]. EMOC is a well-studied AL method for the classification task that strives to maximize the change in the model (output) by labeling points that have the largest gradient. However, as the true label is unknown, some label distribution assumptions must be made. Simple approximations like uniform probability across all labels exist can made for classification but not for regression tasks. [@ranganathan2020deep] made an assumption that the label is simply the average of all predicted output in the unlabeled set ($y'(x') = \\mathbb{E}_{x \\in \\mathcal{U}} f(x)$) and we use this implementation for our benchmark of EMOC. $$\\begin{aligned}\n    q_{EMOC}(x') &= \\mathbb{E}_{y'|x'} \\mathbb{E}_{x} || f(x; \\phi') - f(x; \\phi)||_1\\\\\n    &\\approx \\mathbb{E}_{x} || \\nabla_{\\phi} f(x; \\phi) * \\nabla_{\\phi} \\mathcal{L}(\\phi; (x', y'))||_1\n\\end{aligned}$$ where $f(x; \\phi)$ is the current model output for point $x$ with model parameter $\\phi$, $\\phi'$ is the updated parameter after training on labeled point x' with label y' and $\\mathcal{L}(\\phi; (x', y')$ is the loss of the model with current model parameter $\\phi$ on new labeled data $(x', y')$.\n\n**Learning Loss** [@yoo2019learning]. Learning Loss is another uncertainty-based AL method that instead of using proxies calculated (like variance), learns the uncertainty directly by adding an auxiliary model to predict the loss of the current point that the regression model would make. The training of the auxiliary model concurs with the main regressor training and it uses a soft, pair-wise ranking loss instead of Mean Squared Error (MSE) loss to account for the fluctuations of the actual loss during the training. $$q_{LL}(x) = f_{loss}(x)$$ where $f_{loss}(x)$ is the output of the loss prediction auxiliary model. In this AL method, there are multiple hyper-parameters (co-training epoch, total auxiliary model size, auxiliary model connections, etc.) added to the AL process, all of which we used the same values in the paper if specified [@yoo2019learning].\n\n**Density Aware Core-set (DACS)** [@kim2022defense] A diversity-based AL method that not only considers core-set metric but also considers the density and strives to sample low-density regions. The original DACS also encodes the image space into feature space and uses locality-sensitive hashing techniques to accelerate the nearest neighbor calculation and prevent computational bottlenecks. As our scientific computing tasks does not involve high dimensional image as well as having much lower dataset size in general, instead of encoded feature space distance, we used input space distance and locality-sensitive hashing was dropped as we don't face such computational bottleneck for the nearest neighbor calculation with our smaller pool compared to theirs.\n\n**Cluster Margin adapted to regression problem: Cluster Variance (ClusterVar)** [@citovsky2021batch] To alleviate the robustness issue arising in larger batch AL scenarios, Cluster-Margin [@citovsky2021batch] method is proposed to add necessary diversity to the uncertainty sampling. The original method used margin as its uncertainty metric as it was demonstrated on image classification tasks, and we adapted it into a variance metric in a regression setting. During Cluster Margin, Hierarchical Agglomerative Clustering (HAC) is run once on the unlabeled pool before the AL process and during each round a round-robin selection is carried out from the smallest cluster to the largest cluster, each time selecting the unlabeled sample with the highest uncertainty metric.\n\n# Details of benchmark datasets used\n\n**1D sine wave (Wave).** A noiseless 1-dimensional sinusoid with varying frequency over $x$, illustrated in [6](#img:sup_data_toy){reference-type=\"ref+label\" reference=\"img:sup_data_toy\"}. $$y = x * sin(a_1 * sin(a_2 * x)),$$ where $a_1 = 3$ and $a_2 = 30$ is chosen to make a relative complicated loss surface for the neural network to learn while also having a difference in sensitivity in the domain of x.\n\n**2D robotic arm (Arm) [@ren2020benchmarking]** In this problem we aim to predict the 2D spatial location of the endpoint of a robotic arm based on its joint angles. Illustrated in [6](#img:sup_data_toy){reference-type=\"ref+label\" reference=\"img:sup_data_toy\"}. The Oracle function is given by $$y_0 = \\sum_{i=1}^3 \\cos(\\dfrac{pi}{2}x_i)*l_i, \n    y_1 = x_0 + \\sum_{i=1}^3 \\sin(\\dfrac{pi}{2}x_i)*l_i$$ where $y$ is the position in the 2D plane, $x_0$ is the adjustable starting horizontal position, $x_{i=1,2,3}$ are the angles of the arm relative to horizontal reference and $l_{i=0,1,2} = [0.5, 0.5, 1]$ represents the i-th length of the robotic arm component. The dataset is available under the MIT license.\n\n<figure id=\"img:sup_data_toy\">\n<div class=\"center\">\n<img src=\"imgs/sup_figs/dataset_toy.png\" />\n</div>\n<figcaption>Schematic illustration of sine wave (a) and robotic arm (b) datasets</figcaption>\n</figure>\n\n**Stacked material (Stack) [@Chen2019].** In this problem, we aim to predict the reflection spectrum of a material, sampled at 201 wavelength points, based upon the thickness of each of the 5 layers of the material, illustrated in [7](#img:sup_data_mm){reference-type=\"ref+label\" reference=\"img:sup_data_mm\"}. It was also benchmarked in [@ren2022inverse]. An analytic Oracle function is available based upon physics[@Chen2019].\n\n**Artificial Dielectric Material (ADM) [@deng2021neural]** This problem takes the geometric structure of a material as input, and the reflection spectrum of the material, as a function of frequency, illustrated in [7](#img:sup_data_mm){reference-type=\"ref+label\" reference=\"img:sup_data_mm\"}. It was also benchmarked in [@deng2021benchmarking]. This dataset -released under CC BY 4.0 License - consists of input space of 3D geometric shape parameterized into 14 dimension space and the output is the spectral response of the material. The oracle function is a DNN [@deng2021benchmarking].\n\n<figure id=\"img:sup_data_mm\">\n<div class=\"center\">\n<img src=\"imgs/sup_figs/dataset_material.png\" />\n</div>\n<figcaption>(a, c) are schematic illustration of two material design datasets (Stack &amp; ADM). (b, d) are example spectra of their material property after simulations from their geometric parameterization (typically from Maxwell equation solvers that are slow and hence can benefit from active learning)</figcaption>\n</figure>\n\n**NASA Airfoil (Foil) [@Dua:2019]** NASA dataset published on <https://archive.ics.uci.edu/dataset/291/airfoil+self+noise> UCI ML repository under CC BY 4.0 License [@Dua:2019] obtained from a series of aerodynamic and acoustic tests of 2D/3D airfoil blade sections conducted in an anechoic wind tunnel, illustrated in [8](#img:sup_UCI){reference-type=\"ref+label\" reference=\"img:sup_UCI\"}. The input is the physical properties of the airfoil, like the angle of attack and chord length and the regression target is the sound pressure in decibels. We use a well-fitted random forest fit to the original dataset as our oracle function following prior work[@trabucco2022design]. The fitted random forest architecture and its weights are also shared in our code repo to ensure future work makes full use of such benchmark datasets as we did.\n\n**Hydrodynamics (Hydro) [@Dua:2019]** Experiment conducted by the Technical University of Delft, illustrated in [8](#img:sup_UCI){reference-type=\"ref+label\" reference=\"img:sup_UCI\"}, (hosted on <https://archive.ics.uci.edu/ml/datasets/Yacht+Hydrodynamics> UCI ML repository under CC BY 4.0 License [@Dua:2019]), this dataset contains basic hull dimensions and boat velocity and their corresponding residuary resistance. Input is 6 dimensions and output is the 1 dimension. We use a well-fitted random forest fit to the original dataset as our oracle function. The fitted random forest architecture and its weights are also shared in our code repository to ensure future work makes full use of such benchmark dataset as we did.\n\n<figure id=\"img:sup_UCI\">\n<div class=\"center\">\n<img src=\"imgs/sup_figs/Appendix_UCI_dataset.png\" />\n</div>\n<figcaption>Schematic illustration of Airfoil and Hydro experiments. Reproduced from the original source of experiment reports from NASA and Delft University of technology. (a) The Hydro experiment with an actual yacht being built and resistance was measured in a water flow experiment as the regression target y. (b) Airfoil experiment where input is the parameters of the airfoil and the sound pressure level is measured as target <span class=\"math inline\"><em>y</em></span> of the regression task.</figcaption>\n</figure>\n\n**Bessel equation** The solution to the below single dimension second-order differential equation: $$x^2\\dfrac{d^2y}{dx^2} + x\\dfrac{dy}{dx} + (x^2 - \\alpha^2)y=0$$ where input is $\\alpha$ and $x$ position given. $\\alpha$ is limited to non-negative integers smaller than 10 and $x \\in [0, 10]$. The solution examples can be visualized in [9](#img:sup_ode){reference-type=\"ref+label\" reference=\"img:sup_ode\"}. Our choice of $\\alpha$ values makes the Bessel functions cylinder harmonics and they frequently appear in solutions to Laplace's equation (in cylindrical systems). The implementation we used is the python package 'scipy.special.jv(v,z)' [@2020SciPy-NMeth].\n\n**Damping oscillator equation** The solution to the below ordinary second-order differential equation: $$m\\dfrac{dx^2}{d^2t} + b\\dfrac{dx}{dt} + \\dfrac{mg}{l}x = 0$$ where m is the mass of the oscillator, b is the air drag, g is the gravity coefficient, l is the length of the oscillator's string and it has analytical solution of form $$x = a e^{-bt} cos(\\alpha - \\psi)$$ where a is the amplitude, b is the damping coefficient, $\\alpha$ is the frequency and $\\psi$ is the phase shift. We assume $\\psi$ to be 0 and let a,b,$\\alpha$ be the input parameters. The output, unlike our previous ODE dataset, is taken as the first 100 time step trajectory of the oscillator, making it a high dimensional manifold (nominal dimension of 100 with true dimension of 3). The trajectory is illustrated in [9](#img:sup_ode){reference-type=\"ref+label\" reference=\"img:sup_ode\"}. We implement the above solution by basic python math operations.\n\n<figure id=\"img:sup_ode\">\n<div class=\"center\">\n<img src=\"imgs/sup_figs/ODE.png\" />\n</div>\n<figcaption>Schematic illustration of Bessel function solution and the damping oscillator solutions.</figcaption>\n</figure>\n\n# List of pool ratio used in existing literature\n\n17000 [@mccallumzy1998employing], 20 to 2000 [@kee2018query], 300 to 375[@santos2020modeling], 11-20 [@roy2018deep], 1000 [@burbidge2007active], and 1 to 11 [@tan2019batch].\n\n# Details of models training and architecture\n\nIn the below [2](#tbl:model_architectures){reference-type=\"ref+label\" reference=\"tbl:model_architectures\"}, we present the model architecture for each of our benchmarked datasets. Unless otherwise noted, all of them are fully connected neural networks.\n\n:::::: center\n::::: small\n:::: sc\n::: {#tbl:model_architectures}\n  Feat     Sine   Robo   Stack     ADM     Foil   Hydr   BESS   DAMP\n  ------- ------ ------ ------- --------- ------ ------ ------ ------\n  NODE      20    500     700     1500     200     50     50    500\n  LAYER     9      4       9     $4^{*}$    4      6      6      6\n\n  : Regression model, $\\hat{f}$ architecture details for each problem. \\*: for ADM, there are 3 layers of convolutions after the linear layer)\n:::\n::::\n:::::\n::::::\n\nWe implemented our models in PyTorch [@NEURIPS2019_9015]. Beyond the above architectural differences, the rest of the model training settings are the same across the models: Starting labeled set of size 80, in each step DAL finds 40 points to be labeled for 50 active learning steps. Each regression model is an ensemble network of 10 models of size illustrated in [2](#tbl:model_architectures){reference-type=\"ref+label\" reference=\"tbl:model_architectures\"} except the ADM dataset (5 instead of 10 due to RAM issue). The test dataset is kept at 4000 points uniformly sampled across the $x$-space and they are fixed the same across all experiments for the same dataset. No bootstrapping is used to train the ensemble network and the only source of difference between networks in the ensemble (committee) is the random initialization of weights.\n\nThe batch size is set to be 5000 (larger than the largest training set) so that the incomplete last batch would not affect the training result (as we sample more and more data, we can't throw away the last incomplete batch but having largely incomplete batch de-stabilizes training and introduce noise into the experiment. Adam optimizer is used with 500 training epochs and the model always retrains from scratch. (We observe that the training loss is much higher if we inherit from the last training episode and do not start from scratch, which is consistent with other literature [@beck2021effective]). The learning rate is generally 1e-3 (some datasets might differ), and the decay rate of 0.8 with the decay at the plateau training schedule. The regularization weight is usually 1e-4 (dataset-dependent as well). The hyper-parameters only change with respect to the dataset but never with respect to DAL used.\n\nThe hyperparameters are tuned in the same way as the model architecture: Assume we have a relatively large dataset (2000 randomly sampling points) and tune our hyperparameter on this set. This raises another robustness problem of deep active learning, which is how to determine the model architecture before we have enough labels. This is currently out of the scope of this work as we focused on how different DALs behave with the assumption that the model architectures are chosen smartly and would be happy to investigate this issue in future work.\n\nFor the BALD method, we used a dropout rate of 0.5 as advised by previous work. As BALD requires a different architecture than other base methods (a dropout structure, that is capable of getting a good estimate even with 50% of the neurons being dropped), the model architecture for the active learning is different in that it enlarges each layer by a constant factor that can make it the relatively same amount of total neurons like other DAL methods. Initially, the final trained version of the dropout model is used as the regression model to be evaluated. However, we found that an oversized dropout model hardly fits as well as our ensembled counterpart like other DAL methods. Therefore, to ensure the fairness of comparison, we trained another separate, ensembled regression model same as the other DALs and reported our performance on that.\n\nFor the LearningLoss method, we used the same hyper-parameter that we found in the cited work in the main text: relative weight of learning loss of 0.001 and a training schedule of 60% of joint model training and the rest epoch we cut the gradient flow to the main model from the auxiliary model. For the design of the auxiliary model, we employed a concatenation of the output of the last three hidden layers of our network, each followed by a fully connected network of 10 neurons, before being directed to the final fully connected layer of the auxiliary network that produces a single loss estimate.\n\nFor the EMOC method, due to RAM limit and time constraint, we can not consider all the model parameters during the gradient calculation step (For time constraint, [3](#tbl:time_performance){reference-type=\"ref+label\" reference=\"tbl:time_performance\"} gives a good reference of how much longer EMOC cost, even in this reduced form). Therefore, we implemented two approximations: (i) For the training set points where the current model gradients are evaluated, instead of taking the ever-growing set that is more and more biased towards the DAL selection, we fixed it to be the 80 original, uniformly sampled points. (ii) We limit the number of model parameters to evaluate the EMOC criteria to 50k. We believe taking the effect of 50 thousand parameters gives a good representation of the model's response (output change) for new points. We acknowledge that these approximations might be posing constraints to EMOC, however, these are practical, solid challenges for DAL practitioners as well and these are likely the compromise to be made during application.\n\n## Computational resources\n\nHere we report the computational resources we used for this work: AMD CPU with 64 cores; NVIDIA 3090 GPU x4 (for each of the experiment we used a single GPU to train); 256GB RAM.\n\n# Additional performance plots\n\nAs the benchmark conducts a huge set of experiments that are hard to fit in the main text, here we present all the resulting figures for those who are interested to dig more takeaways.\n\n## Time performance of the benchmarked DAL methods\n\nWe also list the time performance of each DAL method, using the ROBO dataset as an example in the below [3](#tbl:time_performance){reference-type=\"ref+label\" reference=\"tbl:time_performance\"}. Note that this is only the sampling time cost, not including the model training time, which is usually significantly larger than the active learning sampling time at each step. The only DAL method that potentially has a time performance issue is the EMOC method, which requires the calculation of each of the gradients with respect to all parameters and therefore takes a much longer time than other DAL methods. However, as it is shown in the main text that it is not a robust method in our setting, there is no dilemma of performance/time tradeoff presented here.\n\n:::::: center\n::::: small\n:::: sc\n::: {#tbl:time_performance}\n  Dataset    Random   GSx     GSxy       GSy      BALD       EMOC              \n  --------- -------- ------ -------- ----------- ------- ------------ -- -- -- --\n  Time        2.15    4.96   10.27      6.85      9.06    **756.6**            \n  Dataset      LL     QBC    QBCDiv   QBCDivDen   DACS    ClusterVar           \n  Time        6.53    4.29    9.04      10.38     21.70      4.31              \n\n  : Time performance for average time spent during the sampling process for ROBO dataset per active learning step (40 points) in ms for pool ratio of 2. LL: LearningLoss\n:::\n::::\n:::::\n::::::\n\n## Combined plot with $nAUC_{MSE}$ and nDiv\n\n<figure>\n<div class=\"center\">\n<img src=\"imgs/sup_figs/combined_plot/negative_correlation_sine.png\" />\n</div>\n<figcaption><span class=\"math inline\"><em>n</em><em>A</em><em>U</em><em>C</em><sub><em>M</em><em>S</em><em>E</em></sub></span> and nDiv plot for SINE</figcaption>\n</figure>\n\n<figure>\n<div class=\"center\">\n<img src=\"imgs/sup_figs/combined_plot/negative_correlation_Stack.png\" />\n</div>\n<figcaption><span class=\"math inline\"><em>n</em><em>A</em><em>U</em><em>C</em><sub><em>M</em><em>S</em><em>E</em></sub></span> and nDiv plot for STACK</figcaption>\n</figure>\n\n<figure>\n<div class=\"center\">\n<img src=\"imgs/sup_figs/combined_plot/negative_correlation_ADM.png\" />\n</div>\n<figcaption><span class=\"math inline\"><em>n</em><em>A</em><em>U</em><em>C</em><sub><em>M</em><em>S</em><em>E</em></sub></span> and nDiv plot for ADM</figcaption>\n</figure>\n\n<figure>\n<div class=\"center\">\n<img src=\"imgs/sup_figs/combined_plot/negative_correlation_airfoil.png\" />\n</div>\n<figcaption><span class=\"math inline\"><em>n</em><em>A</em><em>U</em><em>C</em><sub><em>M</em><em>S</em><em>E</em></sub></span> and nDiv plot for FOIL</figcaption>\n</figure>\n\n<figure>\n<div class=\"center\">\n<img src=\"imgs/sup_figs/combined_plot/negative_correlation_hydro.png\" />\n</div>\n<figcaption><span class=\"math inline\"><em>n</em><em>A</em><em>U</em><em>C</em><sub><em>M</em><em>S</em><em>E</em></sub></span> and nDiv plot for HYDR</figcaption>\n</figure>\n\n<figure>\n<div class=\"center\">\n<img src=\"imgs/sup_figs/combined_plot/negative_correlation_bessel.png\" />\n</div>\n<figcaption><span class=\"math inline\"><em>n</em><em>A</em><em>U</em><em>C</em><sub><em>M</em><em>S</em><em>E</em></sub></span> and nDiv plot for BESS</figcaption>\n</figure>\n\n<figure>\n<div class=\"center\">\n<img src=\"imgs/sup_figs/combined_plot/negative_correlation_damp.png\" />\n</div>\n<figcaption><span class=\"math inline\"><em>n</em><em>A</em><em>U</em><em>C</em><sub><em>M</em><em>S</em><em>E</em></sub></span> and nDiv plot for DAMP</figcaption>\n</figure>\n\n## MSE vs active learning step plot\n\nWe also present the traditional plot of the MSE vs active learning step for reference. For each of the plots below, the MSE are smoothed with a smoothing parameter of 0.5 using the tensorboard smoothing visualizing function [@tensorflow2015-whitepaper]. The $x$ labels are from 0 - 49, where 0 measures the end of the first active learning step.\n\n<figure>\n<div class=\"center\">\n<img src=\"imgs/sup_figs/MSE_plots/sine_all_method_combined_agg_mode_None.png\" />\n</div>\n<figcaption>MSE plot for SINE</figcaption>\n</figure>\n\n<figure>\n<div class=\"center\">\n<img src=\"imgs/sup_figs/MSE_plots/robo_all_method_combined_agg_mode_None.png\" />\n</div>\n<figcaption>MSE plot for ROBO</figcaption>\n</figure>\n\n<figure>\n<div class=\"center\">\n<img src=\"imgs/sup_figs/MSE_plots/Stack_all_method_combined_agg_mode_None.png\" />\n</div>\n<figcaption>MSE plot for STACK</figcaption>\n</figure>\n\n<figure>\n<div class=\"center\">\n<img src=\"imgs/sup_figs/MSE_plots/ADM_all_method_combined_agg_mode_None.png\" />\n</div>\n<figcaption>MSE plot for ADM</figcaption>\n</figure>\n\n<figure>\n<div class=\"center\">\n<img src=\"imgs/sup_figs/MSE_plots/hydro_all_method_combined_agg_mode_None.png\" />\n</div>\n<figcaption>MSE plot for HYDR</figcaption>\n</figure>\n\n<figure>\n<div class=\"center\">\n<img src=\"imgs/sup_figs/MSE_plots/airfoil_all_method_combined_agg_mode_None.png\" />\n</div>\n<figcaption>MSE plot for FOIL</figcaption>\n</figure>\n\n<figure>\n<div class=\"center\">\n<img src=\"imgs/sup_figs/MSE_plots/bessel_all_method_combined_agg_mode_None.png\" />\n</div>\n<figcaption>MSE plot for BESS</figcaption>\n</figure>\n\n<figure>\n<div class=\"center\">\n<img src=\"imgs/sup_figs/MSE_plots/damp_all_method_combined_agg_mode_None.png\" />\n</div>\n<figcaption>MSE plot for DAMP</figcaption>\n</figure>\n\n[^1]: University of California Irvine Machine Learning Repository",
    "rationale": "Summary: This is a benchmark paper, the author evaluates eleven state-of-the-art DAL methods across eight datasets in the wild.\n\nStrengths: 1. It tests eleven different DAL methods across various problems.\n\n2. It highlights that methods using sample diversity are more reliable.\n\n3. The findings might be useful for people who want to use DAL in practical settings.\n\nWeaknesses: 1. Setting the pool ratio itself is not meaningful, since we can first use diversity-based measures to determine the subset and then use uncertainty-based measures to get better AL performance.\n\n2. Datasets are too simple.\n\n3. The conclusions of this paper rely on empirical evidence to support their findings. For example, it lacks a formal theoretical framework or mathematical proof to explain why diversity-based methods are inherently more robust.\n\n4. As the author mentioned \"The recent study by Holzmüller et al. (2023) is the only work that is similar to ours\", however, the comparisons described in this paper are not strong enough to explain the novelty and necessity of this paper.\n\nQuestions: 1. These figures are really hard to read, like Figure 3, and MSE plots in supplementary materials.",
    "rating": 2,
    "label": "not novel",
    "chosen": true,
    "rationale_edited": "This is a benchmark paper, the author evaluates eleven state-of-the-art DAL methods across eight datasets in the wild.\n\nDatasets are too simple.\n\nThe conclusions of this paper rely on empirical evidence to support their findings. For example, it lacks a formal theoretical framework or mathematical proof to explain why diversity-based methods are inherently more robust.\n\nAs the author mentioned \"The recent study by Holzmüller et al. (2023) is the only work that is similar to ours\", however, the comparisons described in this paper are not strong enough to explain the novelty and necessity of this paper."
  }
]
