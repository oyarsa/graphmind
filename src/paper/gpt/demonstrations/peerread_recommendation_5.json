[
  {
    "title": "Mixing Dirichlet Topic Models and Word Embeddings to Make lda2vec",
    "abstract": "Distributed dense word vectors have been shown to be effective at capturing tokenlevel semantic and syntactic regularities in language, while topic models can form interpretable representations over documents. In this work, we describe lda2vec, a model that learns dense word vectors jointly with Dirichlet-distributed latent document-level mixtures of topic vectors. In contrast to continuous dense document representations, this formulation produces sparse, interpretable document mixtures through a non-negative simplex constraint. Our method is simple to incorporate into existing automatic differentiation frameworks and allows for unsupervised document representations geared for use by scientists while simultaneously learning word vectors and the linear relationships between them.",
    "text": "1 Introduction\nTopic models are popular for their ability to organize document collections into a smaller set of prominent themes. In contrast to dense distributed representations, these document and topic representations are generally accessible to humans and more easily lend themselves to being interpreted. This interpretability provides additional options to highlight the patterns and structures within our systems of documents. For example, using Latent Dirichlet Allocation (LDA) topic models can reveal cluster of words within documents (Blei et al., 2003), highlight temporal trends (Charlin et al., 2015), and infer networks of complementary products (McAuley et al., 2015). See Blei et al. (2010) for an overview of topic modelling in domains as diverse as computer vision, genetic markers, survey data, and social network data.\nDense vector approaches to building document representations also exist: Le and Mikolov (2014) propose paragraph vectors that are predictive of bags of words within paragraphs, Kiros et al. (2015) build vectors that reconstruct the sentence sequences before and after a given sentence, and Ghosh et al. (2016) construct contextual LSTMs that predict proceeding sentence features. Probabilistic topic models tend to form documents as a sparse mixed-membership of topics while neural network models tend to model documents as dense vectors. By virtue of both their sparsity and low-dimensionality, representations from the former are simpler to inspect.\n\n2 Model\nThis section describes the model for lda2vec. We are interested in modifying the Skipgram Negative-Sampling (SGNS) objective in (Mikolov et al., 2013) to utilize document-wide feature vectors while simultaneously learning continuous document weights loading onto topic vectors.",
    "rating": 1,
    "rationale": "Summary: This paper proposes lda2vec, combining word embeddings with Dirichlet topic models to learn interpretable document representations alongside word vectors.\n\nQuality Assessment:\n- The technical presentation is unclear and difficult to follow, making it hard to understand how the model actually works.\n- The combination of LDA and word2vec is potentially interesting but the paper fails to clearly explain the integration.\n- Experimental evaluation is limited and does not convincingly demonstrate the claimed benefits.\n- The writing quality needs significant improvement.\n\nConclusion: While the idea of combining topic models with word embeddings is reasonable, the paper's poor presentation and insufficient experimental validation make it unsuitable for acceptance. Major revisions are needed."
  },
  {
    "title": "Determining Gains Acquired from Word Embedding Quantitatively using Discrete Distribution Clustering",
    "abstract": "Word embeddings have become widelyused in document analysis. A large number of models have been proposed, but the net gain these models can achieve expectably beyond the traditional bag-ofwords based approaches remains undetermined. Our empirical studies, conducted from a nonparametric unsupervised perspective, reveal where and how word embeddings can contribute to document analysis. Our approach is based on a recent algorithmic advance in nonparametric clustering for empirical measures, which neither invents nor relies on any document vector representations. The new document clustering approach proposed in this work is easy to use and stably outperforms other existing methodologies on a variety of document-clustering tasks.",
    "text": "1 Introduction\nWord embeddings, or word vectors, have been broadly adopted for document analysis (Mikolov et al., 2013b,a). A key appeal of word embedding methods is that they can be obtained from external large-scale corpus and then be easily utilized for different data. Before choosing word embeddings for data analysis, researchers must first consider how much extra gain can be brought from the \"embedded\" knowledge of words in comparison with that achieved by existing bag-of-words based approaches. Moreover, they must also consider how to quantify that gain. Such a preliminary evaluation is often necessary before any further decisions can be made about the data.\nAnswering such questions is important: almost every model used in practice exploits some basic representations — bag-of-words and word embeddings — for the sake of its computational tractability. Based on word embeddings, high-level models are designed for various tasks.\n\n2 Related Work\nIn the original D2-clustering framework (Li and Wang, 2008), calculating Wasserstein barycenter involves solving a large-scale LP problem at each inner iteration, severely limiting the scalability and robustness of the framework. Such high magnitude of computations had prohibited it from many real-world applications until recently.",
    "rating": 2,
    "rationale": "Summary: This paper investigates the gains from using word embeddings over bag-of-words approaches through a nonparametric clustering framework based on Wasserstein distances.\n\nQuality Assessment:\n- The research question is relevant and practical - understanding when word embeddings help is valuable.\n- The paper has numerous grammatical errors that affect readability.\n- The technical approach using D2-clustering is reasonable but the presentation could be clearer.\n- Experimental results show some improvements but the analysis lacks depth.\n- The contribution is incremental rather than substantial.\n\nConclusion: The paper addresses a useful question but the presentation quality issues and limited novelty make it a borderline submission. It needs revision to improve clarity and strengthen the analysis."
  },
  {
    "title": "THIRD-PERSON IMITATION LEARNING",
    "abstract": "Reinforcement learning (RL) makes it possible to train agents capable of achieving sophisticated goals in complex and uncertain environments. A key difficulty in reinforcement learning is specifying a reward function for the agent to optimize. Traditionally, imitation learning in RL has been used to overcome this problem. Unfortunately, hitherto imitation learning methods tend to require that demonstrations are supplied in the first-person: the agent is provided with a sequence of states and a specification of the actions that it should have taken. While powerful, this kind of imitation learning is limited by the relatively hard problem of collecting first-person demonstrations. Humans address this problem by learning from third-person demonstrations: they observe other humans perform tasks, infer the task, and accomplish the same task themselves. In this paper, we present a method for unsupervised third-person imitation learning.",
    "text": "1 INTRODUCTION\nReinforcement learning (RL) is a framework for training agents to maximize rewards in large, unknown, stochastic environments. In recent years, combining techniques from deep learning with reinforcement learning has yielded a string of successful applications in game playing and robotics Mnih et al. (2015; 2016); Schulman et al. (2015a); Levine et al. (2016). These successful applications, and the speed at which the abilities of RL algorithms have been increasing, makes it an exciting area of research with significant potential for future applications.\nOne of the major weaknesses of RL is the need to manually specify a reward function. For each task we wish our agent to accomplish, we must provide it with a reward function whose maximizer will precisely recover the desired behavior. This weakness is addressed by the field of Inverse Reinforcement Learning (IRL).\n\n2 RELATED WORK\nImitation learning (also learning from demonstrations or programming by demonstration) considers the problem of acquiring skills from observing demonstrations. Imitation learning has a long history, with several good survey articles, including (Schaal, 1999; Calinon, 2009; Argall et al., 2009). Two main lines of work within imitation learning are: 1) behavioral cloning, where the demonstrations are used to directly learn a mapping from observations to actions using supervised learning.",
    "rating": 3,
    "rationale": "Summary: This paper extends imitation learning to third-person demonstrations where the learner observes from a different viewpoint than the demonstrator, using domain confusion techniques to extract viewpoint-agnostic features.\n\nQuality Assessment:\n- The problem formulation is novel and addresses a practical limitation of traditional imitation learning.\n- The use of adversarial training for domain adaptation is well-motivated.\n- The paper is clearly written and the approach is explained well.\n- Experiments are limited to relatively simple domains (pointmass, reacher, inverted pendulum).\n- Would benefit from more complex environments to demonstrate scalability.\n\nConclusion: A solid contribution that introduces an interesting new problem setting. The approach is reasonable and results are promising, though more extensive evaluation would strengthen the paper. Acceptable for publication with minor improvements."
  },
  {
    "title": "IMPROVING NEURAL LANGUAGE MODELS WITH A CONTINUOUS CACHE",
    "abstract": "We propose an extension to neural network language models to adapt their prediction to the recent history. Our model is a simplified version of memory augmented networks, which stores past hidden activations as memory and accesses them through a dot product with the current hidden activation. This mechanism is very efficient and scales to very large memory sizes. We also draw a link between the use of external memory in neural network and cache models used with count based language models. We demonstrate on several language model datasets that our approach performs significantly better than recent memory augmented networks.",
    "text": "1 INTRODUCTION\nLanguage models, which are probability distributions over sequences of words, have many applications such as machine translation (Brown et al., 1993), speech recognition (Bahl et al., 1983) or dialogue agents (Stolcke et al., 2000). While traditional neural networks language models have obtained state-of-the-art performance in this domain (Jozefowicz et al., 2016; Mikolov et al., 2010), they lack the capacity to adapt to their recent history, limiting their application to dynamic environments (Dodge et al., 2015). A recent approach to solve this problem is to augment these networks with an external memory (Graves et al., 2014; Grefenstette et al., 2015; Joulin & Mikolov, 2015; Sukhbaatar et al., 2015).\nWhile these networks have obtained promising results on language modeling datasets (Sukhbaatar et al., 2015), they are quite computationally expensive. Typically, they have to learn a parametrizable mechanism to read or write to memory cells. This may limit both the size of their usable memory as well as the quantity of data they can be trained on.\n\n2 LANGUAGE MODELING\nA language model is a probability distribution over sequences of words. Let V be the size of the vocabulary; each word is represented by a one-hot encoding vector.",
    "rating": 4,
    "rationale": "Summary: This paper proposes a simple and efficient cache mechanism for neural language models that stores past hidden states and retrieves them via dot product attention, enabling adaptation to recent context.\n\nQuality Assessment:\n- The approach is elegant in its simplicity - no additional parameters to train for the cache mechanism.\n- Strong connection drawn between neural memory and traditional cache language models.\n- Comprehensive experiments on multiple datasets showing consistent improvements.\n- The method scales efficiently to large memory sizes, addressing a key limitation of previous memory-augmented networks.\n- Clear writing and well-organized presentation.\n\nConclusion: A strong paper that proposes a simple yet effective solution to an important problem. The theoretical grounding, practical efficiency, and empirical results make this a valuable contribution to the field. Recommended for acceptance."
  },
  {
    "title": "LEARNING PHYSICAL DYNAMICS",
    "abstract": "We present the Neural Physics Engine (NPE), a framework for learning simulators of intuitive physics that naturally generalize across variable object count and different scene configurations. We propose a factorization of a physical scene into composable object-based representations and a neural network architecture whose compositional structure factorizes object dynamics into pairwise interactions. Like a symbolic physics engine, the NPE is endowed with generic notions of objects and their interactions; realized as a neural network, it can be trained via stochastic gradient descent to adapt to specific object properties and dynamics of different worlds. We evaluate the efficacy of our approach on simple rigid body dynamics in two-dimensional worlds.",
    "text": "1 INTRODUCTION\nEndowing an agent with a program for physical reasoning constrains the agent's representation of the environment by establishing a prior on the environment's physics. The agent can leverage these constraints to rapidly learn new tasks, to flexibly adapt to changes in inputs and goals, and to naturally generalize reasoning to novel scenes (Lake et al., 2016).\nFor example, a foundational sense of intuitive physics is a prior that guides humans to decompose a scene into objects and carry expectations of object boundaries and motion across different scenarios (Spelke, 1990). Humans perceive balls on a billiard table not as meaningless patches of color but rather as impermeable objects. They expect balls moving toward each other to bounce a certain way after a collision rather than pass through each other, crumble into pieces, or disperse into smoke.\n\n2 APPROACH\n\n2.1 NEURAL PHYSICS ENGINE\nWe consider in detail a specific instantiation of the NPE that uses a neighborhood mask to select context objects. This section discusses each of the four ingredients of the NPE framework, that, when combined, comprise a neural network-based physics simulator that learns from observation.\nObject-Based Representations We make two observations in our factorization of the scene.",
    "rating": 5,
    "rationale": "Summary: This paper presents the Neural Physics Engine (NPE), a compositional neural network architecture that learns physical dynamics by factorizing scenes into objects and modeling pairwise interactions between them.\n\nQuality Assessment:\n- Addresses a fundamental problem in AI: learning intuitive physics that generalizes across scenes.\n- The compositional architecture elegantly combines the flexibility of neural networks with the structure of symbolic physics engines.\n- Key insight: factorizing dynamics into pairwise interactions enables generalization to variable numbers of objects.\n- Strong experimental results demonstrating generalization to unseen configurations and object counts.\n- Ability to infer latent properties (mass) shows the model learns meaningful physical representations.\n- Clear presentation with excellent motivation connecting to cognitive science.\n\nConclusion: An excellent paper that makes a significant contribution to learning physical reasoning. The compositional approach is principled and the results convincingly demonstrate its advantages. Highly recommended for acceptance."
  }
]
