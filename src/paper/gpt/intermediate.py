"""Intermediate result file handling for resumable processing."""

from __future__ import annotations

import asyncio
import logging
from collections.abc import Sequence
from dataclasses import dataclass
from pathlib import Path

from pydantic import BaseModel

from paper.gpt.model import PromptResult
from paper.types import Identifiable
from paper.util import log_memory_usage
from paper.util.serde import Compress, load_data_jsonl, save_data_jsonl

logger = logging.getLogger(__name__)


async def append_intermediate_result_async[T: BaseModel](
    path: Path, result: PromptResult[T]
) -> None:
    """Async wrapper for append_intermediate_result.

    Save result to intermediate file by appending object to JSON Lines file.
    Runs the sync function in a thread to avoid blocking the event loop.

    All `Exception`s generated by file IO and JSON validation are caught and logged.

    Args:
        path:
            Path to intermediate output file. Used to first read the existing items,
            then to save the new one. An empty one will be created if it doesn't exist.
        result:
            Item to be saved.
    """
    await asyncio.to_thread(append_intermediate_result, path, result)


def append_intermediate_result[T: BaseModel](
    path: Path, result: PromptResult[T]
) -> None:
    """Save result to intermediate file by appending object to JSON Lines file.

    All `Exception`s generated by file IO and JSON validation are caught and logged.

    Args:
        path:
            Path to intermediate output file. Used to first read the existing items,
            then to save the new one. An empty one will be created if it doesn't exist.
        result:
            Item to be saved.
    """
    try:
        save_data_jsonl(path, result, compress=Compress.ZSTD)
        log_memory_usage(path.parent / "memory.txt")
    except Exception:
        logger.exception("Error writing intermediate results to: %s", path)


@dataclass(frozen=True, kw_only=True)
class RemainingItems[T, U]:
    """Contains `done` items loaded from intermediate files and those `remaining`."""

    remaining: Sequence[U]
    done: Sequence[T]


def init_remaining_items[T: Identifiable, U: Identifiable](
    continue_type_: type[T],
    output_dir: Path,
    continue_papers_file: Path | None,
    input_data: Sequence[U],
    continue_: bool = False,
) -> tuple[Path, RemainingItems[PromptResult[T], U]]:
    """Initialise paper processing by handling already processed files.

    Creates output directory and checks intermediate results file to determine which
    papers still need processing. Returns files and remaining items needed for further
    processing.

    See also `get_remaining_items`.

    Args:
        continue_type_: Type of the contents of the remaining data.
        output_dir: Directory where results will be stored.
        continue_papers_file: File containing list of previously items papers.
        input_data: Items to process.
        continue_: If True, skips previously processed items.

    Returns:
        Tuple containing:
            - Path to intermediate results file.
            - RemainingItems containing processed and unprocessed papers.
    """
    output_dir.mkdir(parents=True, exist_ok=True)
    output_intermediate_file = output_dir / "results.tmp.jsonl.zst"

    papers_remaining = get_remaining_items(
        continue_type_,
        output_intermediate_file,
        continue_papers_file,
        input_data,
        continue_,
    )

    if not papers_remaining.remaining:
        logger.info(
            "No items left to process. They're all on the `continues` file. Exiting."
        )
        return output_intermediate_file, papers_remaining

    if continue_:
        logger.info(
            "Skipping %d items from the `continue` file.", len(papers_remaining.done)
        )

    return output_intermediate_file, papers_remaining


def get_remaining_items[T: Identifiable, U: Identifiable](
    continue_type_: type[T],
    output_intermediate_file: Path,
    continue_papers_file: Path | None,
    original: Sequence[U],
    continue_: bool,
) -> RemainingItems[PromptResult[T], U]:
    """Split items that were previously processed from this run's input list.

    Loads data from the intermediate file, then removes the items from the input list
    that appear there. The check is done by the record `id`. The existing items are
    returned as `done`, and the ones left to process as `remaining`. The `done` list
    only contains values in the input data, not all values in the intermediate file.

    Args:
        continue_type_: Pydantic type for the output items that will be read.
        output_intermediate_file: File that stores the processed output.
        continue_papers_file: File with the previously processed items. If this is None
            and `output_intermediate_file` exists, it will be set to that.
        original: Items read for the original dataset file.
        continue_: If true, uses the previous results.

    Returns:
        Done and remaining items as separated lists.
    """
    if continue_papers_file is None and output_intermediate_file.is_file():
        continue_papers_file = output_intermediate_file

    if not continue_:
        if continue_papers_file and continue_papers_file.exists():
            rotate_path(continue_papers_file)
            logger.info(
                "Rotating existing intermediate result file and creating new one."
            )
        return RemainingItems(remaining=list(original), done=[])

    continue_papers: Sequence[PromptResult[T]] = []
    if continue_papers_file:
        logger.info("Continuing items from: %s", continue_papers_file)
        try:
            continue_papers = load_data_jsonl(
                continue_papers_file, PromptResult[continue_type_]
            )
        except Exception:
            logger.exception("Error reading previous files")

    continue_paper_ids = {paper.item.id for paper in continue_papers}
    # Split into papers that _are_ in the continue file.
    done = [
        next(c for c in continue_papers if c.item.id == paper.id)
        for paper in original
        if paper.id in continue_paper_ids
    ]
    # And those that _are not_.
    remaining = [paper for paper in original if paper.id not in continue_paper_ids]

    return RemainingItems(remaining=remaining, done=done)


def rotate_path(path: Path) -> None:
    """Rotate a path by finding the next available numeric suffix.

    If path exists, rename it to path.N where N is the next available number.

    Args:
        path: Path to the file or directory to rotate.
        The original path (not the rotated path).
    """
    if not path.exists():
        # Path doesn't exist, so no rotation needed
        return

    # Find the next available rotation number
    rotation = 0
    while path.with_name(f"{path.name}.{rotation}").exists():
        rotation += 1

    # Rename the original file to use the next available number
    path.rename(path.with_name(f"{path.name}.{rotation}"))
