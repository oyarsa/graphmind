"""Run Elo tournaments to compare rationales from different evaluation approaches.

This file implements a pairwise tournament system using Elo ratings to compare
rationales generated by different models or approaches.

The input type is one of:

- `raw`: original dataset type, `peerread.Paper`
- `graph`: output of `gpt.evaluate_paper_graph`, `PromptResult[GraphResult]`
- `paper`: output of `gpt.evaluate_paper_scimon`, `PromptResult[PaperResult]`
- `summ`: output of `gpt.summarise_related_peter.py`, `PromptResult[PaperWithRelatedSummary]`
"""

from __future__ import annotations

import asyncio
import itertools
import logging
import random
import statistics
from collections import defaultdict
from collections.abc import Iterable, Sequence
from dataclasses import dataclass, field
from enum import StrEnum
from functools import partial
from pathlib import Path
from typing import Annotated, Any

import dotenv
import typer
from pydantic import BaseModel, ConfigDict, computed_field
from rich.table import Table
from tqdm import tqdm

from paper import peerread as pr
from paper.gpt.evaluate_paper import EvaluationInput as EvaluationInput
from paper.gpt.evaluate_paper import PaperResult
from paper.gpt.extract_graph import GraphResult
from paper.gpt.model import PaperWithRelatedSummary, PromptResult
from paper.gpt.prompts import PromptTemplate, load_prompts
from paper.gpt.run_gpt import GPTResult, LLMClient, OpenAIClient
from paper.util import (
    Timer,
    cli,
    ensure_envvar,
    render_rich,
    sample,
    setup_logging,
    shuffled,
)
from paper.util.serde import load_data, load_data_single, save_data

logger = logging.getLogger(__name__)

PAIRWISE_COMPARISON_PROMPTS = load_prompts("pairwise_comparison")
TOURNAMENT_METRICS = {
    "clarity": (
        "How well-written the text is. How easy it is to understand and to follow its"
        " ideas."
    ),
    "faithfulness": (
        "Whether the rationale justifies the novelty label. For example, if the text is"
        " mostly positive, so should the label."
    ),
    "factuality": (
        "Is the rationale grounded correctly in scientific facts from the main and"
        " related papers?"
    ),
    "specificity": (
        "Does the rationale cover information specific to the paper, or does it make"
        " overly generic statements?"
    ),
    "contributions": (
        "Does the rationale effectively compare the main paper with the prior work?"
    ),
}
INPUT_TYPES_ALLOWED = ("raw", "paper", "graph", "summ")


# Elo rating constants
DEFAULT_ELO = 1200  # Starting rating for all players
K_FACTOR = 32  # How much ratings can change in a single match
EXPECTED_SCORE_DIVISOR = 400  # For expected score calculation
MELO_DEFAULT_TRIALS = 10  # How many different Elo trials to run


class MatchWinner(StrEnum):
    """Who won a tournament match."""

    A = "A"
    B = "B"
    TIE = "tie"


class GPTPairwiseComparison(BaseModel):
    """Result from pairwise comparison of two rationales by LLM."""

    model_config = ConfigDict(frozen=True)

    winner: MatchWinner
    """Who wins the match. The model can only reply A or B, but we use TIE for errors."""
    explanation: str
    """Explanation for the winner evaluation."""
    metric: str
    """Which metric this comparison is for."""


@dataclass(frozen=True, kw_only=True)
class PlayerMatch:
    """Match in player match history."""

    player: str
    opponent: str
    score: float
    explanation: str


@dataclass
class EloPlayer:
    """Represents a player in the Elo rating system."""

    name: str
    rating: float = DEFAULT_ELO
    wins: int = 0
    losses: int = 0
    ties: int = 0
    match_history: list[PlayerMatch] = field(default_factory=list)

    def add_match_result(self, opponent: str, score: float, explanation: str) -> None:
        """Add a match result to the player's history.

        Mutates the object.

        Args:
            opponent: Name of the opponent.
            score: 1.0 for win, 0.5 for tie, 0.0 for loss.
            explanation: Explanation for the match result.
        """
        self.match_history.append(
            PlayerMatch(
                player=self.name,
                opponent=opponent,
                score=score,
                explanation=explanation,
            )
        )

        if score == 1.0:
            self.wins += 1
        elif score == 0.5:
            self.ties += 1
        else:
            self.losses += 1

    def update_rating(self, expected_score: float, actual_score: float) -> None:
        """Update the Elo rating based on match outcome.

        Mutates the object.

        Args:
            expected_score: Expected probability of winning (0.0-1.0).
            actual_score: Actual outcome (1.0=win, 0.5=tie, 0.0=loss).
        """
        self.rating += K_FACTOR * (actual_score - expected_score)


@dataclass(frozen=True, kw_only=True)
class TournamentMatch:
    """Entry in tournament match history."""

    player_a: str
    player_b: str
    result: GPTPairwiseComparison


class TournamentSystem:
    """Manages an Elo tournament for rationale comparisons."""

    def __init__(self, model_names: Sequence[str], metric: str) -> None:
        """Initialize the tournament system.

        Args:
            model_names: Names of the different models being compared.
            metric: The metric this tournament is evaluating.
        """
        self.metric = metric
        self.players: dict[str, EloPlayer] = {
            name: EloPlayer(name=name) for name in model_names
        }
        self.matches: list[TournamentMatch] = []

    def calculate_expected_score(
        self, player_a: EloPlayer, player_b: EloPlayer
    ) -> float:
        """Calculate expected score for player A against player B.

        Args:
            player_a: First player.
            player_b: Second player.

        Returns:
            Expected probability of player A winning (0.0-1.0).
        """
        return 1.0 / (
            1.0 + 10.0 ** ((player_b.rating - player_a.rating) / EXPECTED_SCORE_DIVISOR)
        )

    def record_match(
        self,
        player_a_name: str,
        player_b_name: str,
        result: GPTPairwiseComparison,
    ) -> None:
        """Record the outcome of a match and update player ratings.

        Mutates the object.

        Args:
            player_a_name: Name of the first player.
            player_b_name: Name of the second player.
            result: The result of the comparison.
        """
        # Ensure both players exist
        player_a = self.players[player_a_name]
        player_b = self.players[player_b_name]

        # Calculate expected scores
        expected_a = self.calculate_expected_score(player_a, player_b)
        expected_b = 1.0 - expected_a

        # Determine actual scores from the result
        match result.winner:
            case MatchWinner.A:
                actual_a, actual_b = 1.0, 0.0
            case MatchWinner.B:
                actual_a, actual_b = 0.0, 1.0
            case MatchWinner.TIE:
                actual_a, actual_b = 0.5, 0.5

        # Update ratings
        player_a.update_rating(expected_a, actual_a)
        player_b.update_rating(expected_b, actual_b)

        # Record match results for each player
        player_a.add_match_result(player_b_name, actual_a, result.explanation)
        player_b.add_match_result(player_a_name, actual_b, result.explanation)

        # Save match in tournament history
        self.matches.append(
            TournamentMatch(
                player_a=player_a_name, player_b=player_b_name, result=result
            )
        )

    def get_rankings(self) -> list[PlayerRank]:
        """Get rankings for all players."""
        rankings = sorted(
            (
                (p.name, p.rating, p.wins, p.losses, p.ties)
                for p in self.players.values()
            ),
            key=lambda x: x[1],
            reverse=True,
        )

        return [
            PlayerRank(
                rank=i,
                name=name,
                rating=rating,
                wins=wins,
                losses=losses,
                ties=ties,
            )
            for i, (name, rating, wins, losses, ties) in enumerate(rankings, 1)
        ]


class PlayerRank(BaseModel):
    """Player entry in ranking for a given tournament."""

    model_config = ConfigDict(frozen=True)

    rank: int
    name: str
    rating: float
    wins: int
    losses: int
    ties: int


class TournamentManager:
    """Manage multiple tournaments for different metrics."""

    def __init__(self, model_names: Sequence[str], metrics: Sequence[str]) -> None:
        """Initialize tournament manager.

        Args:
            model_names: Names of the different models being compared.
            metrics: Metrics to run tournaments for.
        """
        self.tournaments = {
            metric: TournamentSystem(model_names, metric) for metric in metrics
        }
        self.model_names = model_names
        self.metrics = metrics

    def record_match(
        self, player_a: str, player_b: str, result: GPTPairwiseComparison
    ) -> None:
        """Record a match result in the appropriate tournament.

        Mutates object.

        Args:
            player_a: Name of the first player.
            player_b: Name of the second player.
            result: Comparison result from the LLM.
        """
        self.tournaments[result.metric].record_match(player_a, player_b, result)

    def get_overall_ranks(self) -> dict[str, ModelRankStats]:
        """Calculate overall rankings based on average ranks across metrics.

        Returns:
            Dictionary with model names as keys and rank statistics as values.
        """
        # For each model, collect its rank in each tournament
        model_ranks: dict[str, list[int]] = defaultdict(list)
        for metric in self.metrics:
            for player in self.tournaments[metric].get_rankings():
                model_ranks[player.name].append(player.rank)

        return {
            model: ModelRankStats(
                ranks=ranks,
                mean_rank=statistics.mean(ranks),
                median_rank=statistics.median(ranks),
                best_rank=min(ranks),
                worst_rank=max(ranks),
                metric_ranks={
                    metric: next(
                        player.rank
                        for player in self.tournaments[metric].get_rankings()
                        if player.name == model
                    )
                    for metric in self.metrics
                },
            )
            for model, ranks in model_ranks.items()
        }


class ModelRankStats(BaseModel):
    """Statistics for model ranks across metrics."""

    model_config = ConfigDict(frozen=True)

    ranks: list[int]
    mean_rank: float
    median_rank: float
    best_rank: int
    worst_rank: int
    metric_ranks: dict[str, int]


class PaperMetadata(BaseModel):
    """Metadata for paper being compared."""

    model_config = ConfigDict(frozen=True)

    id: str
    title: str
    abstract: str
    label: int
    rationale: str


def extract_metadata(paper: EvaluationInput) -> PaperMetadata:
    """Extract title and abstract and other metadata needed for prompts."""
    match paper:
        case pr.Paper() | PaperResult():
            return PaperMetadata(
                id=paper.id,
                title=paper.title,
                abstract=paper.abstract,
                label=paper.label,
                rationale=paper.rationale,
            )
        case GraphResult():
            return PaperMetadata(
                id=paper.id,
                title=paper.paper.title,
                abstract=paper.paper.abstract,
                label=paper.paper.label,
                rationale=paper.paper.rationale,
            )
        case PaperWithRelatedSummary():
            return PaperMetadata(
                id=paper.id,
                title=paper.paper.title,
                abstract=paper.paper.abstract,
                label=paper.label,
                rationale=paper.paper.paper.rationale,
            )


def _find_common_papers(
    paper_collections: Sequence[Sequence[EvaluationInput]],
) -> dict[str, list[EvaluationInput]]:
    """Find papers that exist in all collections based on ID.

    Args:
        paper_collections: List of lists of paper objects.

    Returns:
        Mapping of paper IDs to list of paper objects from each collection.
    """
    # Extract IDs from each collection
    id_sets = [{extract_metadata(p).id for p in papers} for papers in paper_collections]
    # Find IDs common to all collections
    common_ids = set[str].intersection(*id_sets)

    # Group papers by ID
    result: dict[str, list[EvaluationInput]] = {}
    for paper_id in common_ids:
        paper_group: list[EvaluationInput] = []

        for papers in paper_collections:
            for paper in papers:
                if extract_metadata(paper).id == paper_id:
                    paper_group.append(paper)
                    break

        result[paper_id] = paper_group

    return result


def format_evaluation_prompt(
    metric: str,
    paper_metadata: PaperMetadata,
    rationale_a: str,
    rationale_b: str,
    prompt: PromptTemplate,
) -> str:
    """Format user prompt from model data.

    Args:
        metric: The metric to focus on in the comparison.
        paper_metadata: Paper metadata (title, abstract, etc.).
        rationale_a: First rationale to compare.
        rationale_b: Second rationale to compare.
        prompt: Prompt template for the comparison.

    Returns:
        Comparison result wrapped in a GPTResult.
    """
    return prompt.template.format(
        title=paper_metadata.title,
        abstract=paper_metadata.abstract,
        rationale_a=rationale_a,
        rationale_b=rationale_b,
        metric=metric,
        definition=TOURNAMENT_METRICS[metric],
    )


async def _compare_rationales(
    client: LLMClient,
    paper_metadata: PaperMetadata,
    rationale_a: str,
    rationale_b: str,
    metric: str,
    prompt: PromptTemplate,
) -> GPTResult[GPTPairwiseComparison]:
    """Compare two rationales for the same paper using LLM.

    Args:
        client: LLM client.
        paper_metadata: Paper metadata (title, abstract, etc.).
        rationale_a: First rationale to compare.
        rationale_b: Second rationale to compare.
        metric: The metric to focus on in the comparison.
        prompt: Prompt template for the comparison.

    Returns:
        Comparison result wrapped in a GPTResult.
    """
    user_prompt_text = format_evaluation_prompt(
        metric, paper_metadata, rationale_a, rationale_b, prompt
    )

    result = await client.run(GPTPairwiseComparison, prompt.system, user_prompt_text)
    return result.map(
        lambda r: r
        if r is not None
        # Default to TIE when LLM returns an error.
        else GPTPairwiseComparison(
            winner=MatchWinner.TIE,
            explanation="Comparison error. Defaulting to tie.",
            metric=metric,
        )
    )


class ComparisonResult(BaseModel):
    """Result of comparing two model outputs by LLM."""

    model_config = ConfigDict(frozen=True)

    model_a: str
    """First model name."""
    model_b: str
    """Second model name."""
    paper_id: str
    """ID of the paper being compared."""
    metric: str
    """Metric being evaluated."""
    result: GPTPairwiseComparison
    """LLM's comparison result."""
    cost: float
    """API cost for this comparison."""


async def _run_all_comparisons(
    client: LLMClient,
    common_papers: dict[str, list[EvaluationInput]],
    metrics: list[str],
    model_names: list[str],
    model_indices_pairs: list[tuple[int, int]],
    paper_ids: list[str],
    prompt: PromptTemplate,
) -> list[ComparisonResult]:
    """Run all pairwise comparisons between models without updating Elo ratings.

    Args:
        client: LLM client.
        common_papers: Papers from each model, grouped by paper ID.
        metrics: Metrics to evaluate.
        model_names: Names of the models being compared.
        model_indices_pairs: Pairs of model indices to compare.
        paper_ids: IDs of papers to use in comparisons.
        prompt: Prompt template for the comparison.

    Returns:
        List of comparison results.
    """
    all_comparisons: list[ComparisonResult] = []
    total_comparisons = len(paper_ids) * len(model_indices_pairs) * len(metrics)

    with tqdm(total=total_comparisons, desc="Running pairwise comparisons") as pbar:
        for paper_id in paper_ids:
            papers = common_papers[paper_id]
            paper_metadata = extract_metadata(papers[0])

            for metric in metrics:
                for i, j in model_indices_pairs:
                    model_a = model_names[i]
                    model_b = model_names[j]

                    rationale_a = extract_metadata(papers[i]).rationale
                    rationale_b = extract_metadata(papers[j]).rationale

                    comparison_result = await _compare_rationales(
                        client, paper_metadata, rationale_a, rationale_b, metric, prompt
                    )

                    all_comparisons.append(
                        ComparisonResult(
                            model_a=model_a,
                            model_b=model_b,
                            paper_id=paper_id,
                            metric=metric,
                            result=comparison_result.result,
                            cost=comparison_result.cost,
                        )
                    )

                    pbar.update(1)

    return all_comparisons


def _calculate_elo_rankings(
    comparison_results: list[ComparisonResult],
    model_names: list[str],
    metrics: list[str],
) -> TournamentResult:
    """Calculate Elo rankings from comparison results.

    Args:
        comparison_results: Results of all pairwise comparisons.
        model_names: Names of the models being compared.
        metrics: Metrics that were evaluated.

    Returns:
        Tournament results with Elo rankings.
    """
    manager = TournamentManager(model_names, metrics)
    total_cost = sum(result.cost for result in comparison_results)

    # Process all comparisons and update Elo ratings
    logger.info("Calculating Elo rankings from %d comparisons", len(comparison_results))
    for comparison in comparison_results:
        manager.record_match(comparison.model_a, comparison.model_b, comparison.result)

    return TournamentResult(
        overall_ranks=manager.get_overall_ranks(),
        total_comparisons=len(comparison_results),
        total_cost=total_cost,
        tournaments=manager.tournaments,
    )


def _calculate_melo_rankings(
    comparison_results: list[ComparisonResult],
    model_names: list[str],
    metrics: list[str],
    num_trials: int = 10,
    seed: int = 42,
) -> TournamentResult:
    """Calculate Multi-Elo rankings from comparison results.

    Runs multiple Elo tournaments with different random orderings and averages
    the final ratings.

    Args:
        comparison_results: Results of all pairwise comparisons.
        model_names: Names of the models being compared.
        metrics: Metrics that were evaluated.
        num_trials: Number of tournaments to run with different orderings.
        seed: Random seed for reproducibility.

    Returns:
        Tournament results with Multi-Elo rankings.
    """
    # Group comparisons by metric to simplify the multi-tournament process
    comparisons_by_metric: dict[str, list[ComparisonResult]] = defaultdict(list)
    for comp in comparison_results:
        comparisons_by_metric[comp.metric].append(comp)

    total_cost = sum(result.cost for result in comparison_results)

    # Initialize structures to average results across trials
    all_ratings: dict[str, dict[str, list[float]]] = defaultdict(
        lambda: defaultdict(list)
    )
    all_ranks: dict[str, dict[str, list[int]]] = defaultdict(lambda: defaultdict(list))
    all_wins: dict[str, dict[str, list[int]]] = defaultdict(lambda: defaultdict(list))
    all_losses: dict[str, dict[str, list[int]]] = defaultdict(lambda: defaultdict(list))
    all_ties: dict[str, dict[str, list[int]]] = defaultdict(lambda: defaultdict(list))

    random_gen = random.Random(seed)

    # Run multiple tournaments with different random orderings
    logger.info(f"Running {num_trials} tournaments with different orderings")
    for _ in range(num_trials):
        trial_seed = random_gen.randint(0, 10000)
        trial_random = random.Random(trial_seed)

        # Create a new manager for this trial
        manager = TournamentManager(model_names, metrics)

        # For each metric, shuffle the comparisons differently
        for metric in metrics:
            metric_comparisons: list[ComparisonResult] = comparisons_by_metric[
                metric
            ].copy()
            trial_random.shuffle(metric_comparisons)

            # Process the shuffled comparisons
            for comparison in metric_comparisons:
                manager.record_match(
                    comparison.model_a, comparison.model_b, comparison.result
                )

            # Record each player's result for this metric in this trial
            for player in manager.tournaments[metric].get_rankings():
                all_ratings[metric][player.name].append(player.rating)
                all_ranks[metric][player.name].append(player.rank)
                all_wins[metric][player.name].append(player.wins)
                all_losses[metric][player.name].append(player.losses)
                all_ties[metric][player.name].append(player.ties)

    # Create a final manager with averaged results
    final_manager = TournamentManager(model_names, metrics)

    # Replace the players in each tournament with the averaged results
    for metric in metrics:
        for name in model_names:
            player = final_manager.tournaments[metric].players[name]

            # Average the ratings and statistics
            avg_rating = statistics.mean(all_ratings[metric][name])
            avg_wins = int(statistics.mean(all_wins[metric][name]))
            avg_losses = int(statistics.mean(all_losses[metric][name]))
            avg_ties = int(statistics.mean(all_ties[metric][name]))

            # Update the player with averaged values
            player.rating = avg_rating
            player.wins = avg_wins
            player.losses = avg_losses
            player.ties = avg_ties

    return TournamentResult(
        overall_ranks=final_manager.get_overall_ranks(),
        total_comparisons=len(comparison_results),
        total_cost=total_cost,
        tournaments=final_manager.tournaments,
    )


class OverallRankingEntry(BaseModel):
    """Overall ranking entry for a model."""

    model_config = ConfigDict(frozen=True)

    name: str
    mean_rank: float
    median_rank: float
    best_rank: int
    worst_rank: int
    metric_ranks: dict[str, int]


class TournamentSummary(BaseModel):
    """Summary of tournament results."""

    model_config = ConfigDict(frozen=True)

    model_names: list[str]
    metrics: list[str]
    total_comparisons: int
    total_cost: float
    metric_rankings: dict[str, list[PlayerRank]]
    overall_rankings: list[OverallRankingEntry]


class TournamentResult(BaseModel):
    """Full result of all tournaments run."""

    model_config = ConfigDict(frozen=True)

    overall_ranks: dict[str, ModelRankStats]
    """Model ranks across all tournaments."""
    total_comparisons: int
    """Number of comparisons across models."""
    total_cost: float
    """Total LLM cost for all comparisons."""
    tournaments: dict[str, TournamentSystem]
    """Tournament data for each metric."""


def _tournament_summary(
    result: TournamentResult, model_names: list[str], metrics: list[str]
) -> TournamentSummary:
    """Convert internal result to a serializable summary.

    Args:
        result: Tournament result.
        model_names: Names of all models in the tournament.
        metrics: Names of metrics evaluated.

    Returns:
        Serializable tournament summary.
    """
    return TournamentSummary(
        model_names=model_names,
        metrics=metrics,
        total_comparisons=result.total_comparisons,
        total_cost=result.total_cost,
        metric_rankings={
            metric: [
                PlayerRank(
                    rank=player.rank,
                    name=player.name,
                    rating=player.rating,
                    wins=player.wins,
                    losses=player.losses,
                    ties=player.ties,
                )
                for player in tournament.get_rankings()
            ]
            for metric, tournament in result.tournaments.items()
        },
        overall_rankings=[
            OverallRankingEntry(
                name=name,
                mean_rank=stats.mean_rank,
                median_rank=stats.median_rank,
                best_rank=stats.best_rank,
                worst_rank=stats.worst_rank,
                metric_ranks=stats.metric_ranks,
            )
            for name, stats in sorted(
                result.overall_ranks.items(), key=lambda x: x[1].mean_rank
            )
        ],
    )


def _all_pairings[T](xs: Iterable[T]) -> list[tuple[T, T]]:
    """Create possible pairings of elements (AxB, AxC, BxC, BxA etc.). Order-sensitive."""
    return list(itertools.permutations(xs, 2))


def _display_tournament_results(results: TournamentSummary) -> str:
    """Format tournament results for display."""
    table = Table(title="Elo Tournament Rankings")

    table.add_column("Rank", style="cyan", justify="right")
    table.add_column("Model", style="green")
    table.add_column("Mean Rank", justify="right")
    table.add_column("Median Rank", justify="right")

    metrics = list(results.metric_rankings.keys())
    for metric in metrics:
        table.add_column(metric.capitalize(), justify="right")

    # Add rows for each model's overall ranking
    for i, model in enumerate(results.overall_rankings, 1):
        name = model.name
        mean_rank = f"{model.mean_rank:.2f}"
        median_rank = f"{model.median_rank:.1f}"

        # Get metric-specific ranks
        metric_ranks = [str(model.metric_ranks[m]) for m in metrics]

        table.add_row(str(i), name, mean_rank, median_rank, *metric_ranks)

    return render_rich(table)


app = typer.Typer(
    context_settings={"help_option_names": ["-h", "--help"]},
    add_completion=False,
    rich_markup_mode="rich",
    pretty_exceptions_show_locals=False,
    no_args_is_help=True,
)


class RankingAlgorithm(StrEnum):
    """Available ranking algorithms."""

    ELO = "elo"
    MELO = "melo"


@app.command(no_args_is_help=True)
def tournament(
    inputs: Annotated[
        list[str],
        typer.Argument(
            help="Input files to process. Each file is in the format path:type:name."
        ),
    ],
    output_dir: Annotated[
        Path,
        typer.Option(
            "--output",
            help="The path to the output directory where results will be saved.",
        ),
    ],
    model: Annotated[
        str,
        typer.Option("--model", "-m", help="Model to use for evaluation"),
    ] = "gpt-4o-mini",
    tournament_prompt: Annotated[
        str,
        typer.Option(
            help="The prompts to use for pairwise comparison.",
            click_type=cli.Choice(PAIRWISE_COMPARISON_PROMPTS),
        ),
    ] = "standard",
    metrics: Annotated[
        list[str] | None,
        typer.Option(
            "--metric",
            help="Metrics to evaluate in tournament",
            click_type=cli.Choice(TOURNAMENT_METRICS),
        ),
    ] = None,
    limit: Annotated[
        int,
        typer.Option("--limit", "-n", help="Number of papers to process per model"),
    ] = 10,
    seed: Annotated[
        int,
        typer.Option(help="Random seed for the tournament to ensure reproducibility"),
    ] = 0,
    algorithm: Annotated[
        RankingAlgorithm,
        typer.Option(
            "--algo",
            help="Ranking algorithm to use: 'elo' (single tournament) or 'melo' (10"
            " tournaments with different order)",
        ),
    ] = RankingAlgorithm.ELO,
    reuse_comparisons: Annotated[
        Path | None,
        typer.Option(
            "--reuse",
            help="Path to raw_comparisons.json file from a previous run to reuse. If"
            " provided, ignores input data and parameters.",
        ),
    ] = None,
    melo_trials: Annotated[
        int, typer.Option(help="If the algorithm is 'melo', how many trials to run.")
    ] = MELO_DEFAULT_TRIALS,
) -> None:
    """Run a pairwise tournament between multiple models.

    The tournament can use different ranking algorithms:
    - elo: Standard Elo rating system with a single ordering
    - melo: Multiple Elo tournaments with different random orderings. Set the number of
      trials with '--melo-trials'.

    If you provide --reuse with a path to a raw_comparisons.json file, the system will
    skip the LLM comparison phase and just calculate rankings using the existing
    comparison data.
    """
    tournament_metrics = metrics or list(TOURNAMENT_METRICS)

    dotenv.load_dotenv()
    output_dir.mkdir(parents=True, exist_ok=True)

    # Parse input files, types, and model names
    parsed_inputs: list[tuple[Path, str]] = []
    model_names: list[str] = []

    for input_str in inputs:
        parts = input_str.split(":", maxsplit=2)
        if len(parts) == 3:
            file_path, file_type, model_name = parts
        elif len(parts) == 2:
            file_path, file_type = parts
            model_name = Path(file_path).parent.name
        else:
            file_path = parts[0]
            file_type = "graph"
            model_name = Path(file_path).parent.name

        file_type = file_type.strip()
        if file_type not in INPUT_TYPES_ALLOWED:
            raise ValueError(
                f"File type for {file_path} not allowed: '{file_type}'. Must be one"
                f" of {INPUT_TYPES_ALLOWED}."
            )
        parsed_inputs.append((Path(file_path), file_type))
        model_names.append(model_name)

    asyncio.run(
        run_tournaments(
            parsed_inputs,
            model_names,
            output_dir,
            model,
            tournament_prompt,
            tournament_metrics,
            limit,
            seed,
            algorithm,
            reuse_comparisons,
            melo_trials,
        )
    )


def _load_evaluation_input(
    file_path: Path, file_type: str, limit: int
) -> Sequence[EvaluationInput]:
    match file_type.lower():
        case "graph":
            return sample(
                PromptResult.unwrap(load_data(file_path, PromptResult[GraphResult])),
                limit,
            )
        case "paper":
            return sample(
                PromptResult.unwrap(load_data(file_path, PromptResult[PaperResult])),
                limit,
            )
        case "raw":
            return sample(load_data(file_path, pr.Paper), limit)
        case "summ":
            return sample(
                PromptResult.unwrap(
                    load_data(file_path, PromptResult[PaperWithRelatedSummary])
                ),
                limit,
            )
        case _:
            raise ValueError(
                f"Invalid file_type: {file_type}. Must be one of {INPUT_TYPES_ALLOWED}."
            )


class RawComparisonOutput(BaseModel):
    """Raw comparisons output for serialization."""

    model_config = ConfigDict(frozen=True)

    model_names: list[str]
    metrics: list[str]
    seed: int
    comparisons: list[ComparisonResult]
    metadata: dict[str, Any]

    @computed_field
    @property
    def cost(self) -> float:
        """Total cost of all comparisons."""
        return sum(cmp.cost for cmp in self.comparisons)


async def _load_reused_comparisons(path: Path) -> RawComparisonOutput:
    """Load comparison data from a previous run."""
    logger.info(f"Reusing comparison data from {path}")
    data = load_data_single(path, RawComparisonOutput)

    logger.info(
        f"Loaded {len(data.comparisons)} comparisons for {len(data.model_names)} models"
    )
    return data


async def _generate_new_comparisons(
    client: LLMClient,
    inputs: list[tuple[Path, str]],
    model_names: list[str],
    metrics: list[str],
    limit: int,
    model: str,
    tournament_prompt_key: str,
    seed: int,
    algorithm: RankingAlgorithm,
) -> RawComparisonOutput:
    """Generate new comparisons by running the LLM.

    Args:
        client: LLM client used to perform comparisons.
        inputs: List of (file_path, file_type) tuples.
        model_names: Names of the models.
        metrics: Metrics to evaluate.
        limit: Maximum number of papers to use.
        model: GPT model to use.
        tournament_prompt_key: Key for the comparison prompt.
        seed: Random seed.
        algorithm: Ranking algorithm to use.

    Returns:
        The full result from the comparisons.

    Raises:
        ValueError if there are no common papers to compare.
    """
    paper_collections = [
        _load_evaluation_input(file_path, file_type, limit)
        for file_path, file_type in inputs
    ]
    common_papers = _find_common_papers(paper_collections)

    if not common_papers:
        raise ValueError(
            "No common papers found across all models. Tournament cannot proceed."
        )

    logger.info(
        f"Found {len(common_papers)} papers common to all {len(model_names)} models"
    )

    # Step 1: Run all pairwise comparisons
    prompt = PAIRWISE_COMPARISON_PROMPTS[tournament_prompt_key]
    paper_ids = shuffled(common_papers)
    model_indices_pairs = _all_pairings(range(len(model_names)))

    with Timer() as comparison_timer:
        comparisons = await _run_all_comparisons(
            client,
            common_papers,
            metrics,
            model_names,
            model_indices_pairs,
            paper_ids,
            prompt,
        )

    logger.info(f"Comparisons time: {comparison_timer.human}")

    return RawComparisonOutput(
        model_names=model_names,
        metrics=metrics,
        seed=seed,
        comparisons=comparisons,
        metadata={
            "model": model,
            "prompt": tournament_prompt_key,
            "paper_count": len(paper_ids),
            "algorithm": algorithm,
        },
    )


async def run_tournaments(
    inputs: list[tuple[Path, str]],
    model_names: list[str],
    output_dir: Path,
    model: str,
    tournament_prompt_key: str,
    metrics: list[str],
    limit: int,
    seed: int,
    algorithm: RankingAlgorithm,
    reuse_comparisons_path: Path | None,
    melo_trials: int,
) -> None:
    """Run the tournament on the given inputs.

    Args:
        inputs: List of (file_path, file_type) tuples.
        model_names: Names of the models.
        output_dir: Directory to save results.
        model: GPT model to use.
        tournament_prompt_key: Key for the comparison prompt.
        metrics: Metrics to evaluate.
        limit: Maximum number of papers to use.
        seed: Random seed.
        algorithm: Ranking algorithm to use (elo or melo).
        reuse_comparisons_path: Optional path to comparisons JSON file to reuse. If
            provided, other information (e.g. input data, model names, GPT model) is
            ignored.
        melo_trials: How many MElo trials to run.
    """
    random.seed(seed)
    client = OpenAIClient(
        api_key=ensure_envvar("OPENAI_API_KEY"), model=model, seed=seed
    )

    # Step 1: Either load existing comparisons or generate new ones
    if reuse_comparisons_path is not None:
        raw_comparisons = await _load_reused_comparisons(reuse_comparisons_path)
    else:
        raw_comparisons = await _generate_new_comparisons(
            client,
            inputs,
            model_names,
            metrics,
            limit,
            model,
            tournament_prompt_key,
            seed,
            algorithm,
        )

    # Step 2: Calculate rankings using the select algorithm, report and save results
    with Timer() as ranking_timer:
        match algorithm:
            case RankingAlgorithm.ELO:
                ranker = _calculate_elo_rankings
            case RankingAlgorithm.MELO:
                ranker = partial(
                    _calculate_melo_rankings, seed=seed, num_trials=melo_trials
                )

        tournament_result = ranker(raw_comparisons.comparisons, model_names, metrics)
        summary = _tournament_summary(tournament_result, model_names, metrics)

    logger.info(f"Rankings calculation time: {ranking_timer.human}")
    logger.info(f"Total cost: ${raw_comparisons.cost:.10f}")

    logger.info("\n%s", _display_tournament_results(summary))
    save_data(output_dir / "raw_comparisons.json", raw_comparisons)
    save_data(output_dir / f"tournament_results_{algorithm}.json", summary)


@app.callback()
def main() -> None:
    """Set up logging."""
    setup_logging()


if __name__ == "__main__":
    app()
