"""Run Elo tournaments to compare rationales from different evaluation approaSequence.

This file implements a pairwise tournament system using Elo ratings to compare
rationales generated by different models or approaches.

The input type is one of:

- `raw`: original dataset type, `peerread.Paper`
- `graph`: output of `gpt.evaluate_paper_graph`, `PromptResult[GraphResult]`
- `paper`: output of `gpt.evaluate_paper_scimon`, `PromptResult[PaperResult]`
- `summ`: output of `gpt.summarise_related_peter.py`, `PromptResult[PaperWithRelatedSummary]`
"""

from __future__ import annotations

import asyncio
import itertools
import logging
import random
import statistics
from collections import defaultdict
from collections.abc import Collection, Iterable, Mapping, Sequence
from enum import StrEnum
from functools import partial
from pathlib import Path
from typing import Annotated, Any, Self

import dotenv
import typer
from pydantic import BaseModel, ConfigDict, computed_field
from rich.table import Table
from tqdm import tqdm

from paper import peerread as pr
from paper.gpt.evaluate_paper import PaperResult
from paper.gpt.extract_graph import GraphResult
from paper.gpt.model import PaperWithRelatedSummary, PromptResult
from paper.gpt.prompts import PromptTemplate, load_prompts, print_prompts
from paper.gpt.run_gpt import GPTResult, LLMClient, OpenAIClient
from paper.util import (
    Timer,
    cli,
    ensure_envvar,
    render_rich,
    sample,
    setup_logging,
    shuffled,
)
from paper.util.serde import load_data, load_data_single, save_data

logger = logging.getLogger(__name__)

PAIRWISE_COMPARISON_PROMPTS = load_prompts("pairwise_comparison")
TOURNAMENT_METRICS: Mapping[str, str] = {
    "clarity": (
        "How well-written the text is. How easy it is to understand and to follow its"
        " ideas."
    ),
    "faithfulness": (
        "Whether the rationale justifies the novelty label. For example, if the text is"
        " mostly positive, so should the label."
    ),
    "factuality": (
        "Is the rationale grounded correctly in scientific facts from the main and"
        " related papers?"
    ),
    "specificity": (
        "Does the rationale cover information specific to the paper, or does it make"
        " overly generic statements?"
    ),
    "contributions": (
        "Does the rationale effectively compare the main paper with the prior work?"
    ),
}
INPUT_TYPES_ALLOWED = ("raw", "paper", "graph", "summ")


# Elo rating constants
DEFAULT_ELO = 1200  # Starting rating for all players
K_FACTOR = 32  # How much ratings can change in a single match
EXPECTED_SCORE_DIVISOR = 400  # For expected score calculation
MELO_DEFAULT_TRIALS = 10  # How many different Elo trials to run

type EvaluationInput = GraphResult | PaperResult | pr.Paper | PaperWithRelatedSummary
"""Type alias for rationale evaluation."""


class MatchWinner(StrEnum):
    """Who won a tournament match."""

    A = "A"
    B = "B"
    TIE = "tie"


class GPTPairwiseComparison(BaseModel):
    """Result from pairwise comparison of two rationales by LLM."""

    model_config = ConfigDict(frozen=True)

    winner: MatchWinner
    """Who wins the match. The model can only reply A or B, but we use TIE for errors."""
    explanation: str
    """Explanation for the winner evaluation."""


class PlayerMatch(BaseModel):
    """Match in player match history."""

    model_config = ConfigDict(frozen=True)

    player: str
    opponent: str
    score: float
    explanation: str


class EloPlayer(BaseModel):
    """Represents a player in the Elo rating system."""

    model_config = ConfigDict(frozen=True)

    name: str
    rating: float
    wins: int
    losses: int
    ties: int
    match_history: Sequence[PlayerMatch]

    @classmethod
    def fresh(cls, name: str) -> EloPlayer:
        """Create fresh player without rating or matches."""
        return EloPlayer(
            name=name, rating=DEFAULT_ELO, wins=0, losses=0, ties=0, match_history=()
        )

    def play_match(
        self,
        opponent: str,
        expected_score: float,
        actual_score: float,
        explanation: str,
    ) -> EloPlayer:
        """Play match against `opponent`, updating the match record and rating."""
        new_match = PlayerMatch(
            player=self.name,
            opponent=opponent,
            score=actual_score,
            explanation=explanation,
        )

        return EloPlayer(
            wins=self.wins + (1 if actual_score == 1.0 else 0),
            ties=self.ties + (1 if actual_score == 0.5 else 0),
            losses=self.losses + (1 if actual_score == 0.0 else 0),
            match_history=(*self.match_history, new_match),
            rating=_update_elo_rating(self.rating, actual_score, expected_score),
            name=self.name,
        )


def _update_elo_rating(
    current_rating: float, actual_score: float, expected_score: float
) -> float:
    """Update Elo rating based on match result.

    Args:
        current_rating: Current rating of the player.
        actual_score: Actual score of the player (0.0-1.0).
        expected_score: Expected score of the player (0.0-1.0).

    Returns:
        Updated Elo rating.
    """
    return current_rating + K_FACTOR * (actual_score - expected_score)


class TournamentMatch(BaseModel):
    """Entry in tournament match history."""

    model_config = ConfigDict(frozen=True)

    player_a: str
    player_b: str
    result: GPTPairwiseComparison


def _elo_expected_probabilities(
    player_a: EloPlayer, player_b: EloPlayer
) -> tuple[float, float]:
    """Calculate expected score for player A against player B.

    Args:
        player_a: First player.
        player_b: Second player.

    Returns:
        Tuple with expected probabilities of (player A, player B) winning in [0, 1].
    """
    expected_a = 1.0 / (
        1.0 + 10.0 ** ((player_b.rating - player_a.rating) / EXPECTED_SCORE_DIVISOR)
    )
    expected_b = 1 - expected_a
    return expected_a, expected_b


class TournamentSystem(BaseModel):
    """Manages an Elo tournament for rationale comparisons."""

    model_config = ConfigDict(frozen=True)

    metric: str
    players: Mapping[str, EloPlayer]
    matches: Sequence[TournamentMatch]

    @classmethod
    def create(cls, model_names: Collection[str], metric: str) -> TournamentSystem:
        """Create a new tournament system from the players (models) and metrics.

        Args:
            model_names: Names of the different models being compared.
            metric: The metric this tournament is evaluating.

        Returns:
            A new TournamentSystem instance.
        """
        return cls(
            metric=metric,
            players={name: EloPlayer.fresh(name=name) for name in model_names},
            matches=(),
        )

    def record_match(
        self,
        player_a_name: str,
        player_b_name: str,
        result: GPTPairwiseComparison,
    ) -> TournamentSystem:
        """Record the outcome of a match and update player ratings.

        Args:
            player_a_name: Name of the first player.
            player_b_name: Name of the second player.
            result: The result of the comparison.

        Returns:
            A new TournamentSystem with updated state.
        """
        player_a = self.players[player_a_name]
        player_b = self.players[player_b_name]

        expected_a, expected_b = _elo_expected_probabilities(player_a, player_b)

        # Determine actual scores from the result
        match result.winner:
            case MatchWinner.A:
                actual_a, actual_b = 1.0, 0.0
            case MatchWinner.B:
                actual_a, actual_b = 0.0, 1.0
            case MatchWinner.TIE:
                actual_a, actual_b = 0.5, 0.5

        updated_players = {
            **self.players,
            player_a_name: player_a.play_match(
                player_b_name, expected_a, actual_a, result.explanation
            ),
            player_b_name: player_b.play_match(
                player_a_name, expected_b, actual_b, result.explanation
            ),
        }

        new_match = TournamentMatch(
            player_a=player_a_name, player_b=player_b_name, result=result
        )
        return TournamentSystem(
            metric=self.metric,
            players=updated_players,
            matches=(*self.matches, new_match),
        )

    def get_rankings(self) -> list[PlayerRank]:
        """Get rankings for all players."""
        players_sorted_by_rating = sorted(
            self.players.values(), key=lambda p: p.rating, reverse=True
        )

        return [
            PlayerRank(
                rank=i,
                name=player.name,
                rating=player.rating,
                wins=player.wins,
                losses=player.losses,
                ties=player.ties,
            )
            for i, player in enumerate(players_sorted_by_rating, 1)
        ]


class PlayerRank(BaseModel):
    """Player entry in ranking for a given tournament."""

    model_config = ConfigDict(frozen=True)

    rank: int
    name: str
    rating: float
    wins: int
    losses: int
    ties: int


class TournamentManager(BaseModel):
    """Manage multiple tournaments for different metrics."""

    model_config = ConfigDict(frozen=True)

    tournaments: Mapping[str, TournamentSystem]
    modell_names: Sequence[str]
    metrics: Sequence[str]

    @classmethod
    def create(
        cls, model_names: Sequence[str], metrics: Sequence[str]
    ) -> TournamentManager:
        """Create a new tournament manager with different tournaments be metric.

        Args:
            model_names: Names of the different models being compared.
            metrics: Metrics to run tournaments for.

        Returns:
            A new TournamentManager instance.
        """
        return cls(
            tournaments={
                metric: TournamentSystem.create(model_names, metric)
                for metric in metrics
            },
            modell_names=model_names,
            metrics=metrics,
        )

    def record_match(
        self, player_a: str, player_b: str, result: GPTPairwiseComparison, metric: str
    ) -> TournamentManager:
        """Record a match result in the appropriate tournament for the metric.

        Args:
            player_a: Name of the first player.
            player_b: Name of the second player.
            result: Comparison result from the LLM.
            metric: Metric used for the comparison.

        Returns:
            A new TournamentManager with updated state.
        """
        tournament = self.tournaments[metric]
        updated_tournaments = {
            **self.tournaments,
            metric: tournament.record_match(player_a, player_b, result),
        }

        return TournamentManager(
            tournaments=updated_tournaments,
            # Unchanged
            modell_names=self.modell_names,
            metrics=self.metrics,
        )

    def get_overall_ranks(self) -> dict[str, ModelRankStats]:
        """Calculate overall rankings based on average ranks across metrics.

        Returns:
            Dictionary with model names as keys and rank statistics as values.
        """
        # For each model, collect its rank in each tournament
        model_ranks: dict[str, list[int]] = defaultdict(list)
        for metric in self.metrics:
            for player in self.tournaments[metric].get_rankings():
                model_ranks[player.name].append(player.rank)

        return {
            model: ModelRankStats(
                ranks=ranks,
                mean_rank=statistics.mean(ranks),
                median_rank=statistics.median(ranks),
                best_rank=min(ranks),
                worst_rank=max(ranks),
                metric_ranks={
                    metric: next(
                        player.rank
                        for player in self.tournaments[metric].get_rankings()
                        if player.name == model
                    )
                    for metric in self.metrics
                },
            )
            for model, ranks in model_ranks.items()
        }


class ModelRankStats(BaseModel):
    """Statistics for model ranks across metrics."""

    model_config = ConfigDict(frozen=True)

    ranks: Sequence[int]
    mean_rank: float
    median_rank: float
    best_rank: int
    worst_rank: int
    metric_ranks: Mapping[str, int]


class PaperCore(BaseModel):
    """Core information from the paper being compared."""

    model_config = ConfigDict(frozen=True)

    id: str
    title: str
    abstract: str
    label: int
    rationale: str


def extract_metadata(paper: EvaluationInput) -> PaperCore:
    """Extract title and abstract and other metadata needed for prompts."""
    match paper:
        case pr.Paper() | PaperResult():
            return PaperCore(
                id=paper.id,
                title=paper.title,
                abstract=paper.abstract,
                label=paper.label,
                rationale=paper.rationale,
            )
        case GraphResult():
            return PaperCore(
                id=paper.id,
                title=paper.paper.title,
                abstract=paper.paper.abstract,
                label=paper.paper.label,
                rationale=paper.paper.rationale,
            )
        case PaperWithRelatedSummary():
            return PaperCore(
                id=paper.id,
                title=paper.paper.title,
                abstract=paper.paper.abstract,
                label=paper.label,
                rationale=paper.paper.paper.rationale,
            )


def _find_common_papers(
    paper_collections: Collection[Collection[EvaluationInput]],
) -> dict[str, list[EvaluationInput]]:
    """Find papers that exist in all collections based on ID.

    Args:
        paper_collections: List of lists of paper objects.

    Returns:
        Mapping of paper IDs to list of paper objects from each collection.
    """
    # Extract IDs from each collection
    id_sets = [{extract_metadata(p).id for p in papers} for papers in paper_collections]
    # Find IDs common to all collections
    common_ids = set[str].intersection(*id_sets)

    # Group papers by ID
    result: dict[str, list[EvaluationInput]] = {}
    for paper_id in common_ids:
        paper_group: list[EvaluationInput] = []

        for papers in paper_collections:
            for paper in papers:
                if extract_metadata(paper).id == paper_id:
                    paper_group.append(paper)
                    break

        result[paper_id] = paper_group

    return result


def format_evaluation_prompt(
    metric: str,
    paper_metadata: PaperCore,
    rationale_a: str,
    rationale_b: str,
    prompt: PromptTemplate,
) -> str:
    """Format user prompt from model data.

    Args:
        metric: The metric to focus on in the comparison.
        paper_metadata: Paper metadata (title, abstract, etc.).
        rationale_a: First rationale to compare.
        rationale_b: Second rationale to compare.
        prompt: Prompt template for the comparison.

    Returns:
        Comparison result wrapped in a GPTResult.
    """
    return prompt.template.format(
        title=paper_metadata.title,
        abstract=paper_metadata.abstract,
        rationale_a=rationale_a,
        rationale_b=rationale_b,
        metric=metric,
        definition=TOURNAMENT_METRICS[metric],
    )


async def _compare_rationales(
    client: LLMClient,
    paper_metadata: PaperCore,
    rationale_a: str,
    rationale_b: str,
    metric: str,
    prompt: PromptTemplate,
) -> GPTResult[GPTPairwiseComparison]:
    """Compare two rationales for the same paper using LLM.

    Args:
        client: LLM client.
        paper_metadata: Paper metadata (title, abstract, etc.).
        rationale_a: First rationale to compare.
        rationale_b: Second rationale to compare.
        metric: The metric to focus on in the comparison.
        prompt: Prompt template for the comparison.

    Returns:
        Comparison result wrapped in a GPTResult.
    """
    user_prompt_text = format_evaluation_prompt(
        metric, paper_metadata, rationale_a, rationale_b, prompt
    )

    result = await client.run(GPTPairwiseComparison, prompt.system, user_prompt_text)
    return result.map(
        lambda r: r
        if r is not None
        # Default to TIE when LLM returns an error.
        else GPTPairwiseComparison(
            winner=MatchWinner.TIE, explanation="Comparison error. Defaulting to tie."
        )
    )


class ComparisonResult(BaseModel):
    """Result of comparing two model outputs by LLM."""

    model_config = ConfigDict(frozen=True)

    modell_a: str
    """First model name."""
    modell_b: str
    """Second model name."""
    paper_id: str
    """ID of the paper being compared."""
    metric: str
    """Metric being evaluated."""
    result: GPTPairwiseComparison
    """LLM's comparison result."""
    cost: float
    """API cost for this comparison."""


async def _run_all_comparisons(
    client: LLMClient,
    common_papers: Mapping[str, list[EvaluationInput]],
    metrics: Collection[str],
    model_names: Sequence[str],
    model_indices_pairs: Collection[tuple[int, int]],
    paper_ids: Collection[str],
    prompt: PromptTemplate,
) -> list[ComparisonResult]:
    """Run all pairwise comparisons between models without updating Elo ratings.

    Args:
        client: LLM client.
        common_papers: Papers from each model, grouped by paper ID.
        metrics: Metrics to evaluate.
        model_names: Names of the models being compared.
        model_indices_pairs: Pairs of model indices to compare.
        paper_ids: IDs of papers to use in comparisons.
        prompt: Prompt template for the comparison.

    Returns:
        List of comparison results.
    """
    all_comparisons: list[ComparisonResult] = []
    total_comparisons = len(paper_ids) * len(model_indices_pairs) * len(metrics)

    with tqdm(total=total_comparisons, desc="Running pairwise comparisons") as pbar:
        for paper_id in paper_ids:
            papers = common_papers[paper_id]
            paper_metadata = extract_metadata(papers[0])

            for metric in metrics:
                for i, j in model_indices_pairs:
                    model_a = model_names[i]
                    model_b = model_names[j]

                    rationale_a = extract_metadata(papers[i]).rationale
                    rationale_b = extract_metadata(papers[j]).rationale

                    comparison_result = await _compare_rationales(
                        client, paper_metadata, rationale_a, rationale_b, metric, prompt
                    )

                    all_comparisons.append(
                        ComparisonResult(
                            modell_a=model_a,
                            modell_b=model_b,
                            paper_id=paper_id,
                            metric=metric,
                            result=comparison_result.result,
                            cost=comparison_result.cost,
                        )
                    )

                    pbar.update(1)

    return all_comparisons


def _calculate_elo_rankings(
    comparison_results: Collection[ComparisonResult],
    model_names: Sequence[str],
    metrics: Sequence[str],
) -> TournamentResult:
    """Calculate Elo rankings from comparison results.

    Args:
        comparison_results: Results of all pairwise comparisons.
        model_names: Names of the models being compared.
        metrics: Metrics that were evaluated.

    Returns:
        Tournament results with Elo rankings.
    """
    manager = TournamentManager.create(model_names, metrics)

    logger.info("Calculating Elo rankings from %d comparisons", len(comparison_results))
    for comparison in comparison_results:
        manager = manager.record_match(
            comparison.modell_a,
            comparison.modell_b,
            comparison.result,
            comparison.metric,
        )

    return TournamentResult(
        overall_ranks=manager.get_overall_ranks(),
        tournaments=manager.tournaments,
        total_comparisons=len(comparison_results),
        total_cost=sum(result.cost for result in comparison_results),
    )


def _calculate_melo_rankings(
    comparison_results: Collection[ComparisonResult],
    model_names: Sequence[str],
    metrics: Sequence[str],
    num_trials: int,
    seed: int,
) -> TournamentResult:
    """Calculate Multi-Elo rankings from comparison results.

    Runs multiple Elo tournaments with different random orderings and averages
    the final ratings.

    Args:
        comparison_results: Results of all pairwise comparisons.
        model_names: Names of the models being compared.
        metrics: Metrics that were evaluated.
        num_trials: Number of tournaments to run with different orderings.
        seed: Random seed for reproducibility.

    Returns:
        Tournament results with Multi-Elo rankings.
    """
    # Group comparisons by metric to simplify the multi-tournament process
    comparisons_by_metric: dict[str, list[ComparisonResult]] = defaultdict(list)
    for comp in comparison_results:
        comparisons_by_metric[comp.metric].append(comp)

    total_cost = sum(result.cost for result in comparison_results)

    # Initialize structures to average results across trials
    all_ratings: dict[str, dict[str, list[float]]] = defaultdict(
        lambda: defaultdict(list)
    )
    all_ranks: dict[str, dict[str, list[int]]] = defaultdict(lambda: defaultdict(list))
    all_wins: dict[str, dict[str, list[int]]] = defaultdict(lambda: defaultdict(list))
    all_losses: dict[str, dict[str, list[int]]] = defaultdict(lambda: defaultdict(list))
    all_ties: dict[str, dict[str, list[int]]] = defaultdict(lambda: defaultdict(list))

    random_gen = random.Random(seed)

    logger.info(f"Running {num_trials} tournaments with different orderings")
    for _ in range(num_trials):
        trial_seed = random_gen.randint(0, 10000)
        trial_random = random.Random(trial_seed)

        manager = TournamentManager.create(model_names, metrics)

        # For each metric, shuffle the comparisons differently
        for metric in metrics:
            metric_comparisons = comparisons_by_metric[metric].copy()
            trial_random.shuffle(metric_comparisons)

            # Process the shuffled comparisons
            for cmp in metric_comparisons:
                manager = manager.record_match(
                    cmp.modell_a, cmp.modell_b, cmp.result, cmp.metric
                )

            # Record each player's result for this metric in this trial
            for player in manager.tournaments[metric].get_rankings():
                all_ratings[metric][player.name].append(player.rating)
                all_ranks[metric][player.name].append(player.rank)
                all_wins[metric][player.name].append(player.wins)
                all_losses[metric][player.name].append(player.losses)
                all_ties[metric][player.name].append(player.ties)

    # Create a final manager with averaged results
    tournaments: dict[str, TournamentSystem] = {}
    for metric in metrics:
        # For each model, create a new player with averaged statistics
        updated_players: dict[str, EloPlayer] = {}
        for name in model_names:
            # Create a new player with averaged values
            updated_players[name] = EloPlayer(
                name=name,
                rating=statistics.mean(all_ratings[metric][name]),
                wins=int(statistics.mean(all_wins[metric][name])),
                losses=int(statistics.mean(all_losses[metric][name])),
                ties=int(statistics.mean(all_ties[metric][name])),
                match_history=(),  # Not a real player.
            )

        # Create a tournament with averaged values
        tournaments[metric] = TournamentSystem(
            metric=metric,
            players=updated_players,
            matches=(),  # Not a real tournament.
        )

    final_manager = TournamentManager(
        tournaments=tournaments,
        modell_names=model_names,
        metrics=metrics,
    )

    return TournamentResult(
        overall_ranks=final_manager.get_overall_ranks(),
        total_comparisons=len(comparison_results) * num_trials,
        total_cost=total_cost,
        tournaments=final_manager.tournaments,
    )


class OverallRankingEntry(BaseModel):
    """Overall ranking entry for a model."""

    model_config = ConfigDict(frozen=True)

    name: str
    mean_rank: float
    median_rank: float
    best_rank: int
    worst_rank: int
    metric_ranks: Mapping[str, int]


class TournamentSummary(BaseModel):
    """Summary of tournament results."""

    model_config = ConfigDict(frozen=True)

    modell_names: Sequence[str]
    metrics: Sequence[str]
    total_comparisons: int
    total_cost: float
    metric_rankings: Mapping[str, Sequence[PlayerRank]]
    overall_rankings: Sequence[OverallRankingEntry]


class TournamentResult(BaseModel):
    """Full result of all tournaments run."""

    model_config = ConfigDict(frozen=True)

    overall_ranks: Mapping[str, ModelRankStats]
    """Model ranks across all tournaments."""
    total_comparisons: int
    """Number of comparisons across models."""
    total_cost: float
    """Total LLM cost for all comparisons."""
    tournaments: Mapping[str, TournamentSystem]
    """Tournament data for each metric."""


def _tournament_summary(
    result: TournamentResult, model_names: Sequence[str], metrics: Sequence[str]
) -> TournamentSummary:
    """Convert internal result to a serializable summary.

    Args:
        result: Tournament result.
        model_names: Names of all models in the tournament.
        metrics: Names of metrics evaluated.

    Returns:
        Serializable tournament summary.
    """
    return TournamentSummary(
        modell_names=model_names,
        metrics=metrics,
        total_comparisons=result.total_comparisons,
        total_cost=result.total_cost,
        metric_rankings={
            metric: [
                PlayerRank(
                    rank=player.rank,
                    name=player.name,
                    rating=player.rating,
                    wins=player.wins,
                    losses=player.losses,
                    ties=player.ties,
                )
                for player in tournament.get_rankings()
            ]
            for metric, tournament in result.tournaments.items()
        },
        overall_rankings=[
            OverallRankingEntry(
                name=name,
                mean_rank=stats.mean_rank,
                median_rank=stats.median_rank,
                best_rank=stats.best_rank,
                worst_rank=stats.worst_rank,
                metric_ranks=stats.metric_ranks,
            )
            for name, stats in sorted(
                result.overall_ranks.items(), key=lambda x: x[1].mean_rank
            )
        ],
    )


def _all_pairings[T](xs: Iterable[T]) -> list[tuple[T, T]]:
    """Create possible pairings of elements (A-B, A-C, B-C, B-A etc.). Order-sensitive."""
    return list(itertools.permutations(xs, 2))


def _display_tournament_results(results: TournamentSummary) -> str:
    """Format tournament results for display."""
    table = Table(title="Elo Tournament Rankings")

    table.add_column("Rank", style="cyan", justify="right")
    table.add_column("Model", style="green")
    table.add_column("Mean Rank", justify="right")
    table.add_column("Median Rank", justify="right")

    metrics = list(results.metric_rankings.keys())
    for metric in metrics:
        table.add_column(metric.capitalize(), justify="right")

    # Add rows for each model's overall ranking
    for i, model in enumerate(results.overall_rankings, 1):
        name = model.name
        mean_rank = f"{model.mean_rank:.2f}"
        median_rank = f"{model.median_rank:.1f}"

        # Get metric-specific ranks
        metric_ranks = [str(model.metric_ranks[m]) for m in metrics]

        table.add_row(str(i), name, mean_rank, median_rank, *metric_ranks)

    return render_rich(table)


app = typer.Typer(
    context_settings={"help_option_names": ["-h", "--help"]},
    add_completion=False,
    rich_markup_mode="rich",
    pretty_exceptions_show_locals=False,
    no_args_is_help=True,
)


class RankingAlgorithm(StrEnum):
    """Available ranking algorithms."""

    ELO = "elo"
    MELO = "melo"


class InputFileType(StrEnum):
    """Types of input data formats."""

    RAW = "raw"
    """Original dataset paper: `peerread.Paper`"""
    GRAPH = "graph"
    """Output of `gpt.evaluate_paper_graph`: `PromptResult[GraphResult]`"""
    PAPER = "paper"
    """Output of `gpt.evaluate_paper_scimon`: `PromptResult[PaperResult]`"""
    SUMM = "summ"
    """Output of `gpt.summarise_related_peter`: `PromptResult[PaperWithRelatedSummary]`"""

    @classmethod
    def from_dirty(cls, type_: str) -> Self:
        """Create instance by cleaning up `type_` by stripping and lowercasing.

        Use when `type_` comes from a potentially dirty source, such as a CLI argument.

        Raises:
            ValueError if the type is invalid.
        """
        return cls(type_.strip().lower())


@app.command(no_args_is_help=True)
def run(
    inputs: Annotated[
        list[str],
        typer.Argument(
            help="Input files to process. Each file is in the format path:type:name."
        ),
    ],
    output_dir: Annotated[
        Path,
        typer.Option(
            "--output",
            help="The path to the output directory where results will be saved.",
        ),
    ],
    model: Annotated[
        str,
        typer.Option("--model", "-m", help="Model to use for evaluation"),
    ] = "gpt-4o-mini",
    tournament_prompt: Annotated[
        str,
        typer.Option(
            help="The prompts to use for pairwise comparison.",
            click_type=cli.Choice(PAIRWISE_COMPARISON_PROMPTS),
        ),
    ] = "standard",
    metrics: Annotated[
        list[str] | None,
        typer.Option(
            "--metric",
            help="Metrics to evaluate in tournament",
            click_type=cli.Choice(TOURNAMENT_METRICS),
        ),
    ] = None,
    limit: Annotated[
        int,
        typer.Option("--limit", "-n", help="Number of papers to process per model"),
    ] = 10,
    seed: Annotated[
        int,
        typer.Option(help="Random seed for the tournament to ensure reproducibility"),
    ] = 0,
    algorithm: Annotated[
        RankingAlgorithm,
        typer.Option(
            "--algo",
            help="Ranking algorithm to use: 'elo' (single tournament) or 'melo' (10"
            " tournaments with different order)",
        ),
    ] = RankingAlgorithm.ELO,
    reuse_comparisons: Annotated[
        Path | None,
        typer.Option(
            "--reuse",
            help="Path to raw_comparisons.json file from a previous run to reuse. If"
            " provided, ignores input data and parameters.",
        ),
    ] = None,
    melo_trials: Annotated[
        int, typer.Option(help="If the algorithm is 'melo', how many trials to run.")
    ] = MELO_DEFAULT_TRIALS,
) -> None:
    """Run a pairwise tournament between multiple models.

    The tournament can use different ranking algorithms:
    - elo: Standard Elo rating system with a single ordering
    - melo: Multiple Elo tournaments with different random orderings. Set the number of
      trials with '--melo-trials'.

    If you provide --reuse with a path to a raw_comparisons.json file, the system will
    skip the LLM comparison phase and just calculate rankings using the existing
    comparison data.
    """
    tournament_metrics = metrics or list(TOURNAMENT_METRICS)

    dotenv.load_dotenv()
    output_dir.mkdir(parents=True, exist_ok=True)

    # Parse input files, types, and model names
    parsed_inputs: list[tuple[Path, InputFileType]] = []
    model_names: list[str] = []

    for input_str in inputs:
        match input_str.split(":", maxsplit=2):
            case [file_path_, file_type_, model_name]:
                file_path = Path(file_path_)
                file_type = InputFileType.from_dirty(file_type_)
            case [file_path_, file_type_]:
                file_path = Path(file_path_)
                file_type = InputFileType.from_dirty(file_type_)
                model_name = file_path.parent.name
            case [file_path_]:
                file_path = Path(file_path_)
                file_type = InputFileType.GRAPH
                model_name = file_path.parent.name
            case _:
                raise ValueError("Invalid input string format.")

        parsed_inputs.append((file_path, file_type))
        model_names.append(model_name)

    asyncio.run(
        run_tournaments(
            parsed_inputs,
            model_names,
            output_dir,
            model,
            tournament_prompt,
            tournament_metrics,
            limit,
            seed,
            algorithm,
            reuse_comparisons,
            melo_trials,
        )
    )


def _load_evaluation_input(
    file_path: Path, file_type: InputFileType, limit: int
) -> Sequence[EvaluationInput]:
    match file_type:
        case InputFileType.GRAPH:
            data = PromptResult.unwrap(load_data(file_path, PromptResult[GraphResult]))
        case InputFileType.PAPER:
            data = PromptResult.unwrap(load_data(file_path, PromptResult[PaperResult]))
        case InputFileType.SUMM:
            data = PromptResult.unwrap(
                load_data(file_path, PromptResult[PaperWithRelatedSummary])
            )
        case InputFileType.RAW:
            data = load_data(file_path, pr.Paper)

    return sample(data, limit)


class RawComparisonOutput(BaseModel):
    """Raw comparisons output for serialization."""

    model_config = ConfigDict(frozen=True)

    modell_names: Sequence[str]
    metrics: Sequence[str]
    seed: int
    comparisons: Sequence[ComparisonResult]
    metadata: Mapping[str, Any]

    @computed_field
    @property
    def cost(self) -> float:
        """Total cost of all comparisons."""
        return sum(cmp.cost for cmp in self.comparisons)


async def _load_reused_comparisons(path: Path) -> RawComparisonOutput:
    """Load comparison data from a previous run."""
    logger.info(f"Reusing comparison data from {path}")
    data = load_data_single(path, RawComparisonOutput)

    logger.info(
        f"Loaded {len(data.comparisons)} comparisons for {len(data.modell_names)} models"
    )
    return data


async def _generate_new_comparisons(
    client: LLMClient,
    inputs: Collection[tuple[Path, InputFileType]],
    model_names: Sequence[str],
    metrics: Sequence[str],
    limit: int,
    model: str,
    tournament_prompt_key: str,
    seed: int,
    algorithm: RankingAlgorithm,
) -> RawComparisonOutput:
    """Generate new comparisons by running the LLM.

    Args:
        client: LLM client used to perform comparisons.
        inputs: List of (file_path, file_type) tuples.
        model_names: Names of the models.
        metrics: Metrics to evaluate.
        limit: Maximum number of papers to use.
        model: GPT model to use.
        tournament_prompt_key: Key for the comparison prompt.
        seed: Random seed.
        algorithm: Ranking algorithm to use.

    Returns:
        The full result from the comparisons.

    Raises:
        ValueError if there are no common papers to compare.
    """
    paper_collections = [
        _load_evaluation_input(file_path, file_type, limit)
        for file_path, file_type in inputs
    ]
    common_papers = _find_common_papers(paper_collections)

    if not common_papers:
        raise ValueError(
            "No common papers found across all models. Tournament cannot proceed."
        )

    logger.info(
        f"Found {len(common_papers)} papers common to all {len(model_names)} models"
    )

    # Step 1: Run all pairwise comparisons
    prompt = PAIRWISE_COMPARISON_PROMPTS[tournament_prompt_key]
    paper_ids = shuffled(common_papers)
    model_indices_pairs = _all_pairings(range(len(model_names)))

    with Timer() as comparison_timer:
        comparisons = await _run_all_comparisons(
            client,
            common_papers,
            metrics,
            model_names,
            model_indices_pairs,
            paper_ids,
            prompt,
        )

    logger.info(f"Comparisons time: {comparison_timer.human}")

    return RawComparisonOutput(
        modell_names=model_names,
        metrics=metrics,
        seed=seed,
        comparisons=comparisons,
        metadata={
            "model": model,
            "prompt": tournament_prompt_key,
            "paper_count": len(paper_ids),
            "algorithm": algorithm,
        },
    )


async def run_tournaments(
    inputs: Collection[tuple[Path, InputFileType]],
    model_names: Sequence[str],
    output_dir: Path,
    model: str,
    tournament_prompt_key: str,
    metrics: Sequence[str],
    limit: int,
    seed: int,
    algorithm: RankingAlgorithm,
    reuse_comparisons_path: Path | None,
    melo_trials: int,
) -> None:
    """Run the tournament on the given inputs.

    Args:
        inputs: List of (file_path, file_type) tuples.
        model_names: Names of the models.
        output_dir: Directory to save results.
        model: GPT model to use.
        tournament_prompt_key: Key for the comparison prompt.
        metrics: Metrics to evaluate.
        limit: Maximum number of papers to use.
        seed: Random seed.
        algorithm: Ranking algorithm to use (elo or melo).
        reuse_comparisons_path: Optional path to comparisons JSON file to reuse. If
            provided, other information (e.g. input data, model names, GPT model) is
            ignored.
        melo_trials: How many MElo trials to run.
    """
    random.seed(seed)
    client = OpenAIClient(
        api_key=ensure_envvar("OPENAI_API_KEY"), model=model, seed=seed
    )

    # Step 1: Either load existing comparisons or generate new ones
    if reuse_comparisons_path is not None:
        raw_comparisons = await _load_reused_comparisons(reuse_comparisons_path)
    else:
        raw_comparisons = await _generate_new_comparisons(
            client,
            inputs,
            model_names,
            metrics,
            limit,
            model,
            tournament_prompt_key,
            seed,
            algorithm,
        )

    # Step 2: Calculate rankings using the select algorithm, report and save results
    with Timer() as ranking_timer:
        match algorithm:
            case RankingAlgorithm.ELO:
                ranker = _calculate_elo_rankings
            case RankingAlgorithm.MELO:
                ranker = partial(
                    _calculate_melo_rankings, seed=seed, num_trials=melo_trials
                )

        tournament_result = ranker(raw_comparisons.comparisons, model_names, metrics)
        summary = _tournament_summary(tournament_result, model_names, metrics)

    logger.info(f"Rankings calculation time: {ranking_timer.human}")
    logger.info(f"Total cost: ${raw_comparisons.cost:.10f}")

    logger.info("\n%s", _display_tournament_results(summary))
    save_data(output_dir / "raw_comparisons.json", raw_comparisons)
    save_data(output_dir / f"tournament_results_{algorithm}.json", summary)


@app.callback()
def main() -> None:
    """Set up logging."""
    setup_logging()


@app.command(help="List available prompts.")
def prompts(
    detail: Annotated[
        bool, typer.Option(help="Show full description of the prompts.")
    ] = False,
) -> None:
    """Print the available prompt names, and optionally, the full prompt text."""
    print_prompts("RATIONALE TOURNAMENT", PAIRWISE_COMPARISON_PROMPTS, detail=detail)


if __name__ == "__main__":
    app()
