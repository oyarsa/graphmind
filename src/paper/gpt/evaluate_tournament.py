"""Run Elo tournaments to compare rationales from different evaluation approaches.

This file implements a pairwise tournament system using Elo ratings to compare
rationales generated by different models or approaches.

The input type is one of:

- `raw`: original dataset type, `peerread.Paper`
- `graph`: output of `gpt.evaluate_paper_graph`, `PromptResult[GraphResult]`
- `paper`: output of `gpt.evaluate_paper_scimon`, `PromptResult[PaperResult]`
- `summ`: output of `gpt.summarise_related_peter.py`, `PromptResult[PaperWithRelatedSummary]`
"""

from __future__ import annotations

import asyncio
import itertools
import logging
import random
import statistics
from collections import defaultdict
from collections.abc import Iterable, Sequence
from dataclasses import dataclass, field
from enum import StrEnum
from pathlib import Path
from typing import Annotated

import dotenv
import typer
from pydantic import BaseModel, ConfigDict
from rich.table import Table
from tqdm import tqdm

from paper import peerread as pr
from paper.gpt.evaluate_paper import EvaluationInput as EvaluationInput
from paper.gpt.evaluate_paper import PaperResult
from paper.gpt.extract_graph import GraphResult
from paper.gpt.model import PaperWithRelatedSummary, PromptResult
from paper.gpt.prompts import PromptTemplate, load_prompts
from paper.gpt.run_gpt import GPTResult, LLMClient, OpenAIClient
from paper.util import Timer, cli, ensure_envvar, render_rich, sample, setup_logging
from paper.util.serde import load_data, save_data

logger = logging.getLogger(__name__)

PAIRWISE_COMPARISON_PROMPTS = load_prompts("pairwise_comparison")
TOURNAMENT_METRICS = {
    "clarity": (
        "How well-written the text is. How easy it is to understand and to follow its"
        " ideas."
    ),
    "faithfulness": (
        "Whether the rationale justifies the novelty label. For example, if the text is"
        " mostly positive, so should the label."
    ),
    "factuality": (
        "Is the rationale grounded correctly in scientific facts from the main and"
        " related papers?"
    ),
    "specificity": (
        "Does the rationale cover information specific to the paper, or does it make"
        " overly generic statements?"
    ),
    "contributions": (
        "Does the rationale effectively compare the main paper with the prior work?"
    ),
}
INPUT_TYPES_ALLOWED = ("raw", "paper", "graph", "summ")


# Elo rating constants
DEFAULT_ELO = 1200  # Starting rating for all players
K_FACTOR = 32  # How much ratings can change in a single match
EXPECTED_SCORE_DIVISOR = 400  # For expected score calculation


class MatchWinner(StrEnum):
    """Who won a tournament match."""

    A = "A"
    B = "B"
    TIE = "tie"


class GPTPairwiseComparison(BaseModel):
    """Result from pairwise comparison of two rationales by LLM."""

    model_config = ConfigDict(frozen=True)

    winner: MatchWinner
    """Who wins the match. The model can only reply A or B, but we use TIE for errors."""
    explanation: str
    """Explanation for the winner evaluation."""
    metric: str
    """Which metric this comparison is for."""


@dataclass(frozen=True, kw_only=True)
class PlayerMatch:
    """Match in player match history."""

    player: str
    opponent: str
    score: float
    explanation: str


@dataclass
class EloPlayer:
    """Represents a player in the Elo rating system."""

    name: str
    rating: float = DEFAULT_ELO
    wins: int = 0
    losses: int = 0
    ties: int = 0
    match_history: list[PlayerMatch] = field(default_factory=list)

    def add_match_result(self, opponent: str, score: float, explanation: str) -> None:
        """Add a match result to the player's history.

        Mutates the object.

        Args:
            opponent: Name of the opponent.
            score: 1.0 for win, 0.5 for tie, 0.0 for loss.
            explanation: Explanation for the match result.
        """
        self.match_history.append(
            PlayerMatch(
                player=self.name,
                opponent=opponent,
                score=score,
                explanation=explanation,
            )
        )

        if score == 1.0:
            self.wins += 1
        elif score == 0.5:
            self.ties += 1
        else:
            self.losses += 1

    def update_rating(self, expected_score: float, actual_score: float) -> None:
        """Update the Elo rating based on match outcome.

        Mutates the object.

        Args:
            expected_score: Expected probability of winning (0.0-1.0).
            actual_score: Actual outcome (1.0=win, 0.5=tie, 0.0=loss).
        """
        self.rating += K_FACTOR * (actual_score - expected_score)


@dataclass(frozen=True, kw_only=True)
class TournamentMatch:
    """Entry in tournament match history."""

    player_a: str
    player_b: str
    result: GPTPairwiseComparison


class TournamentSystem:
    """Manages an Elo tournament for rationale comparisons."""

    def __init__(self, model_names: Sequence[str], metric: str) -> None:
        """Initialize the tournament system.

        Args:
            model_names: Names of the different models being compared.
            metric: The metric this tournament is evaluating.
        """
        self.metric = metric
        self.players: dict[str, EloPlayer] = {
            name: EloPlayer(name=name) for name in model_names
        }
        self.matches: list[TournamentMatch] = []

    def calculate_expected_score(
        self, player_a: EloPlayer, player_b: EloPlayer
    ) -> float:
        """Calculate expected score for player A against player B.

        Args:
            player_a: First player.
            player_b: Second player.

        Returns:
            Expected probability of player A winning (0.0-1.0).
        """
        return 1.0 / (
            1.0 + 10.0 ** ((player_b.rating - player_a.rating) / EXPECTED_SCORE_DIVISOR)
        )

    def record_match(
        self,
        player_a_name: str,
        player_b_name: str,
        result: GPTPairwiseComparison,
    ) -> None:
        """Record the outcome of a match and update player ratings.

        Mutates the object.

        Args:
            player_a_name: Name of the first player.
            player_b_name: Name of the second player.
            result: The result of the comparison.
        """
        # Ensure both players exist
        player_a = self.players[player_a_name]
        player_b = self.players[player_b_name]

        # Calculate expected scores
        expected_a = self.calculate_expected_score(player_a, player_b)
        expected_b = 1.0 - expected_a

        # Determine actual scores from the result
        match result.winner:
            case MatchWinner.A:
                actual_a, actual_b = 1.0, 0.0
            case MatchWinner.B:
                actual_a, actual_b = 0.0, 1.0
            case MatchWinner.TIE:
                actual_a, actual_b = 0.5, 0.5

        # Update ratings
        player_a.update_rating(expected_a, actual_a)
        player_b.update_rating(expected_b, actual_b)

        # Record match results for each player
        player_a.add_match_result(player_b_name, actual_a, result.explanation)
        player_b.add_match_result(player_a_name, actual_b, result.explanation)

        # Save match in tournament history
        self.matches.append(
            TournamentMatch(
                player_a=player_a_name, player_b=player_b_name, result=result
            )
        )

    def get_rankings(self) -> list[PlayerRank]:
        """Get rankings for all players."""
        rankings = sorted(
            (
                (p.name, p.rating, p.wins, p.losses, p.ties)
                for p in self.players.values()
            ),
            key=lambda x: x[1],
            reverse=True,
        )

        return [
            PlayerRank(
                rank=i,
                name=name,
                rating=rating,
                wins=wins,
                losses=losses,
                ties=ties,
            )
            for i, (name, rating, wins, losses, ties) in enumerate(rankings, 1)
        ]


class PlayerRank(BaseModel):
    """Player entry in ranking for a given tournament."""

    model_config = ConfigDict(frozen=True)

    rank: int
    name: str
    rating: float
    wins: int
    losses: int
    ties: int


class TournamentManager:
    """Manage multiple tournaments for different metrics."""

    def __init__(self, model_names: Sequence[str], metrics: Sequence[str]) -> None:
        """Initialize tournament manager.

        Args:
            model_names: Names of the different models being compared.
            metrics: Metrics to run tournaments for.
        """
        self.tournaments = {
            metric: TournamentSystem(model_names, metric) for metric in metrics
        }
        self.model_names = model_names
        self.metrics = metrics

    def record_match(
        self, player_a: str, player_b: str, result: GPTPairwiseComparison
    ) -> None:
        """Record a match result in the appropriate tournament.

        Mutates object.

        Args:
            player_a: Name of the first player.
            player_b: Name of the second player.
            result: Comparison result from the LLM.
        """
        self.tournaments[result.metric].record_match(player_a, player_b, result)

    def get_overall_ranks(self) -> dict[str, ModelRankStats]:
        """Calculate overall rankings based on average ranks across metrics.

        Returns:
            Dictionary with model names as keys and rank statistics as values.
        """
        # For each model, collect its rank in each tournament
        model_ranks: dict[str, list[int]] = defaultdict(list)
        for metric in self.metrics:
            for player in self.tournaments[metric].get_rankings():
                model_ranks[player.name].append(player.rank)

        return {
            model: ModelRankStats(
                ranks=ranks,
                mean_rank=statistics.mean(ranks),
                median_rank=statistics.median(ranks),
                best_rank=min(ranks),
                worst_rank=max(ranks),
                metric_ranks={
                    metric: next(
                        player.rank
                        for player in self.tournaments[metric].get_rankings()
                        if player.name == model
                    )
                    for metric in self.metrics
                },
            )
            for model, ranks in model_ranks.items()
        }


class ModelRankStats(BaseModel):
    """Statistics for model ranks across metrics."""

    model_config = ConfigDict(frozen=True)

    ranks: list[int]
    mean_rank: float
    median_rank: float
    best_rank: int
    worst_rank: int
    metric_ranks: dict[str, int]


class PaperMetadata(BaseModel):
    """Metadata for paper being compared."""

    model_config = ConfigDict(frozen=True)

    id: str
    title: str
    abstract: str
    label: int
    rationale: str


def extract_metadata(paper: EvaluationInput) -> PaperMetadata:
    """Extract title and abstract and other metadata needed for prompts."""
    match paper:
        case pr.Paper() | PaperResult():
            return PaperMetadata(
                id=paper.id,
                title=paper.title,
                abstract=paper.abstract,
                label=paper.label,
                rationale=paper.rationale,
            )
        case GraphResult():
            return PaperMetadata(
                id=paper.id,
                title=paper.paper.title,
                abstract=paper.paper.abstract,
                label=paper.paper.label,
                rationale=paper.paper.rationale,
            )
        case PaperWithRelatedSummary():
            return PaperMetadata(
                id=paper.id,
                title=paper.paper.title,
                abstract=paper.paper.abstract,
                label=paper.label,
                rationale=paper.paper.paper.rationale,
            )


def _find_common_papers(
    paper_collections: Sequence[Sequence[EvaluationInput]],
) -> dict[str, list[EvaluationInput]]:
    """Find papers that exist in all collections based on ID.

    Args:
        paper_collections: List of lists of paper objects.

    Returns:
        Mapping of paper IDs to list of paper objects from each collection.
    """
    # Extract IDs from each collection
    id_sets = [{extract_metadata(p).id for p in papers} for papers in paper_collections]
    # Find IDs common to all collections
    common_ids = set[str].intersection(*id_sets)

    # Group papers by ID
    result: dict[str, list[EvaluationInput]] = {}
    for paper_id in common_ids:
        paper_group: list[EvaluationInput] = []

        for papers in paper_collections:
            for paper in papers:
                if extract_metadata(paper).id == paper_id:
                    paper_group.append(paper)
                    break

        result[paper_id] = paper_group

    return result


def format_evaluation_prompt(
    metric: str,
    paper_metadata: PaperMetadata,
    rationale_a: str,
    rationale_b: str,
    prompt: PromptTemplate,
) -> str:
    """Format user prompt from model data.

    Args:
        metric: The metric to focus on in the comparison.
        paper_metadata: Paper metadata (title, abstract, etc.).
        rationale_a: First rationale to compare.
        rationale_b: Second rationale to compare.
        prompt: Prompt template for the comparison.

    Returns:
        Comparison result wrapped in a GPTResult.
    """
    return prompt.template.format(
        title=paper_metadata.title,
        abstract=paper_metadata.abstract,
        rationale_a=rationale_a,
        rationale_b=rationale_b,
        metric=metric,
        definition=TOURNAMENT_METRICS[metric],
    )


async def _compare_rationales(
    client: LLMClient,
    paper_metadata: PaperMetadata,
    rationale_a: str,
    rationale_b: str,
    metric: str,
    prompt: PromptTemplate,
) -> GPTResult[GPTPairwiseComparison]:
    """Compare two rationales for the same paper using LLM.

    Args:
        client: LLM client.
        paper_metadata: Paper metadata (title, abstract, etc.).
        rationale_a: First rationale to compare.
        rationale_b: Second rationale to compare.
        metric: The metric to focus on in the comparison.
        prompt: Prompt template for the comparison.

    Returns:
        Comparison result wrapped in a GPTResult.
    """
    user_prompt_text = format_evaluation_prompt(
        metric, paper_metadata, rationale_a, rationale_b, prompt
    )

    result = await client.run(GPTPairwiseComparison, prompt.system, user_prompt_text)
    return result.map(
        lambda r: r
        if r is not None
        # Default to TIE when LLM returns an error.
        else GPTPairwiseComparison(
            winner=MatchWinner.TIE,
            explanation="Comparison error. Defaulting to tie.",
            metric=metric,
        )
    )


async def _run_tournaments(
    client: LLMClient,
    common_papers: dict[str, list[EvaluationInput]],
    metrics: list[str],
    model_names: list[str],
    model_indices_pairs: list[tuple[int, int]],
    paper_ids: list[str],
    prompt: PromptTemplate,
) -> TournamentResult:
    total_cost = 0.0
    total_comparisons = len(paper_ids) * len(model_indices_pairs) * len(metrics)
    manager = TournamentManager(model_names, metrics)

    with tqdm(total=total_comparisons, desc="Running Elo tournament") as pbar:
        for paper_id in paper_ids:
            papers = common_papers[paper_id]
            paper_metadata = extract_metadata(papers[0])

            for metric in metrics:
                for i, j in model_indices_pairs:
                    model_a = model_names[i]
                    model_b = model_names[j]

                    rationale_a = extract_metadata(papers[i]).rationale
                    rationale_b = extract_metadata(papers[j]).rationale

                    comparison_result = await _compare_rationales(
                        client, paper_metadata, rationale_a, rationale_b, metric, prompt
                    )

                    manager.record_match(model_a, model_b, comparison_result.result)

                    total_cost += comparison_result.cost
                    pbar.update(1)

    return TournamentResult(
        overall_ranks=manager.get_overall_ranks(),
        total_comparisons=total_comparisons,
        total_cost=total_cost,
        tournaments=manager.tournaments,
    )


class OverallRankingEntry(BaseModel):
    """Overall ranking entry for a model."""

    model_config = ConfigDict(frozen=True)

    name: str
    mean_rank: float
    median_rank: float
    best_rank: int
    worst_rank: int
    metric_ranks: dict[str, int]


class TournamentSummary(BaseModel):
    """Summary of tournament results."""

    model_config = ConfigDict(frozen=True)

    model_names: list[str]
    metrics: list[str]
    paper_count: int
    total_comparisons: int
    total_cost: float
    metric_rankings: dict[str, list[PlayerRank]]
    overall_rankings: list[OverallRankingEntry]


class TournamentResult(BaseModel):
    """Full result of all tournaments run."""

    model_config = ConfigDict(frozen=True)

    overall_ranks: dict[str, ModelRankStats]
    """Model ranks across all tournaments."""
    total_comparisons: int
    """Number of comparisons across models."""
    total_cost: float
    """Total LLM cost for all comparisons."""
    tournaments: dict[str, TournamentSystem]
    """Tournament data for each metric."""


def _tournament_summary(
    result: TournamentResult,
    model_names: list[str],
    metrics: list[str],
    paper_count: int,
) -> TournamentSummary:
    """Convert internal result to a serializable summary.

    Args:
        result: Tournament result.
        model_names: Names of all models in the tournament.
        metrics: Names of metrics evaluated.
        paper_count: Number of papers compared.

    Returns:
        Serializable tournament summary.
    """
    return TournamentSummary(
        model_names=model_names,
        metrics=metrics,
        paper_count=paper_count,
        total_comparisons=result.total_comparisons,
        total_cost=result.total_cost,
        metric_rankings={
            metric: [
                PlayerRank(
                    rank=player.rank,
                    name=player.name,
                    rating=player.rating,
                    wins=player.wins,
                    losses=player.losses,
                    ties=player.ties,
                )
                for player in tournament.get_rankings()
            ]
            for metric, tournament in result.tournaments.items()
        },
        overall_rankings=[
            OverallRankingEntry(
                name=name,
                mean_rank=stats.mean_rank,
                median_rank=stats.median_rank,
                best_rank=stats.best_rank,
                worst_rank=stats.worst_rank,
                metric_ranks=stats.metric_ranks,
            )
            for name, stats in sorted(
                result.overall_ranks.items(), key=lambda x: x[1].mean_rank
            )
        ],
    )


async def _run_tournament(
    client: LLMClient,
    common_papers: dict[str, list[EvaluationInput]],
    model_names: list[str],
    tournament_prompt_key: str,
    metrics: list[str],
    seed: int,
) -> TournamentSummary:
    """Run a pairwise Elo tournament between multiple models.

    Args:
        client: LLM client.
        common_papers: Dictionary mapping paper IDs to papers from each model.
        model_names: Names of the models being compared.
        tournament_prompt_key: Key for the comparison prompt to use.
        metrics: Metrics to evaluate.
        seed: Random seed for reproducibility.

    Returns:
        Tournament results summary.
    """
    random.seed(seed)

    prompt = PAIRWISE_COMPARISON_PROMPTS[tournament_prompt_key]

    paper_ids = list(common_papers.keys())
    random.shuffle(paper_ids)

    model_indices_pairs = _all_pairings(range(len(model_names)))
    result = await _run_tournaments(
        client,
        common_papers,
        metrics,
        model_names,
        model_indices_pairs,
        paper_ids,
        prompt,
    )
    return _tournament_summary(result, model_names, metrics, len(paper_ids))


def _all_pairings[T](xs: Iterable[T]) -> list[tuple[T, T]]:
    """Create possible pairings of elements (AxB, AxC, BxC, BxA etc.). Order-sensitive."""
    return list(itertools.permutations(xs, 2))


def _display_tournament_results(results: TournamentSummary) -> str:
    """Format tournament results for display."""
    table = Table(title="Elo Tournament Rankings")

    table.add_column("Rank", style="cyan", justify="right")
    table.add_column("Model", style="green")
    table.add_column("Mean Rank", justify="right")
    table.add_column("Median Rank", justify="right")

    metrics = list(results.metric_rankings.keys())
    for metric in metrics:
        table.add_column(metric.capitalize(), justify="right")

    # Add rows for each model's overall ranking
    for i, model in enumerate(results.overall_rankings, 1):
        name = model.name
        mean_rank = f"{model.mean_rank:.2f}"
        median_rank = f"{model.median_rank:.1f}"

        # Get metric-specific ranks
        metric_ranks = [str(model.metric_ranks[m]) for m in metrics]

        table.add_row(str(i), name, mean_rank, median_rank, *metric_ranks)

    return render_rich(table)


app = typer.Typer(
    context_settings={"help_option_names": ["-h", "--help"]},
    add_completion=False,
    rich_markup_mode="rich",
    pretty_exceptions_show_locals=False,
    no_args_is_help=True,
)


@app.command(no_args_is_help=True)
def tournament(
    inputs: Annotated[
        list[str],
        typer.Argument(
            help="Input files to process. Each file is in the format path:type:name."
        ),
    ],
    output_dir: Annotated[
        Path,
        typer.Option(
            "--output",
            help="The path to the output directory where results will be saved.",
        ),
    ],
    model: Annotated[
        str,
        typer.Option("--model", "-m", help="Model to use for evaluation"),
    ] = "gpt-4o-mini",
    tournament_prompt: Annotated[
        str,
        typer.Option(
            help="The prompts to use for pairwise comparison.",
            click_type=cli.Choice(PAIRWISE_COMPARISON_PROMPTS),
        ),
    ] = "standard",
    metrics: Annotated[
        list[str] | None,
        typer.Option(
            "--metric",
            help="Metrics to evaluate in tournament",
            click_type=cli.Choice(TOURNAMENT_METRICS),
        ),
    ] = None,
    limit: Annotated[
        int,
        typer.Option("--limit", "-n", help="Number of papers to process per model"),
    ] = 10,
    seed: Annotated[
        int,
        typer.Option(help="Random seed for the tournament to ensure reproducibility"),
    ] = 0,
) -> None:
    """Run a pairwise Elo tournament between multiple models."""
    tournament_metrics = metrics or list(TOURNAMENT_METRICS)

    dotenv.load_dotenv()
    output_dir.mkdir(parents=True, exist_ok=True)

    # Parse input files, types, and model names
    parsed_inputs: list[tuple[Path, str]] = []
    model_names: list[str] = []

    for input_str in inputs:
        parts = input_str.split(":", maxsplit=2)
        if len(parts) == 3:
            file_path, file_type, model_name = parts
        elif len(parts) == 2:
            file_path, file_type = parts
            model_name = Path(file_path).parent.name
        else:
            file_path = parts[0]
            file_type = "graph"
            model_name = Path(file_path).parent.name

        file_type = file_type.strip()
        if file_type not in INPUT_TYPES_ALLOWED:
            raise ValueError(
                f"File type for {file_path} not allowed: '{file_type}'. Must be one"
                f" of {INPUT_TYPES_ALLOWED}."
            )
        parsed_inputs.append((Path(file_path), file_type))
        model_names.append(model_name)

    asyncio.run(
        run_tournaments(
            parsed_inputs,
            model_names,
            output_dir,
            model,
            tournament_prompt,
            tournament_metrics,
            limit,
            seed,
        )
    )


def _load_evaluation_input(
    file_path: Path, file_type: str, limit: int
) -> Sequence[EvaluationInput]:
    match file_type.lower():
        case "graph":
            return sample(
                PromptResult.unwrap(load_data(file_path, PromptResult[GraphResult])),
                limit,
            )
        case "paper":
            return sample(
                PromptResult.unwrap(load_data(file_path, PromptResult[PaperResult])),
                limit,
            )
        case "raw":
            return sample(load_data(file_path, pr.Paper), limit)
        case "summ":
            return sample(
                PromptResult.unwrap(
                    load_data(file_path, PromptResult[PaperWithRelatedSummary])
                ),
                limit,
            )
        case _:
            raise ValueError(
                f"Invalid file_type: {file_type}. Must be one of {INPUT_TYPES_ALLOWED}."
            )


async def run_tournaments(
    inputs: list[tuple[Path, str]],
    model_names: list[str],
    output_dir: Path,
    model: str,
    tournament_prompt_key: str,
    metrics: list[str],
    limit: int,
    seed: int,
) -> None:
    """Run the tournament on the given inputs.

    Args:
        inputs: List of (file_path, file_type) tuples.
        model_names: Names of the models.
        output_dir: Directory to save results.
        model: GPT model to use.
        tournament_prompt_key: Key for the comparison prompt.
        metrics: Metrics to evaluate.
        limit: Maximum number of papers to use.
        seed: Random seed.
    """
    random.seed(seed)

    client = OpenAIClient(
        api_key=ensure_envvar("OPENAI_API_KEY"), model=model, seed=seed
    )

    paper_collections = [
        _load_evaluation_input(file_path, file_type, limit)
        for file_path, file_type in inputs
    ]
    common_papers = _find_common_papers(paper_collections)

    if common_papers:
        logger.info(
            f"Found {len(common_papers)} papers common to all {len(model_names)} models"
        )
    else:
        logger.error(
            "No common papers found across all models. Tournament cannot proceed."
        )
        return

    with Timer() as timer:
        results = await _run_tournament(
            client, common_papers, model_names, tournament_prompt_key, metrics, seed
        )

    logger.info(f"Time elapsed: {timer.human}")
    logger.info(f"Total cost: ${results.total_cost:.10f}")

    logger.info("\n%s", _display_tournament_results(results))

    save_data(output_dir / "tournament_results.json", results.model_dump())


@app.callback()
def main() -> None:
    """Set up logging."""
    setup_logging()


if __name__ == "__main__":
    app()
