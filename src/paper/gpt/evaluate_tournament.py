"""Run Elo tournaments to compare rationales from different evaluation approaches.

This file implements a pairwise tournament system using Elo ratings to compare
rationales generated by different models or approaches.
"""

from __future__ import annotations

import asyncio
import itertools
import logging
import random
import statistics
from collections.abc import Sequence
from dataclasses import dataclass, field
from enum import StrEnum
from io import StringIO
from pathlib import Path
from typing import Annotated, Any

import dotenv
import typer
from pydantic import BaseModel, ConfigDict
from rich.console import Console
from rich.table import Table
from tqdm import tqdm

from paper import peerread as pr
from paper.gpt.evaluate_paper import EvaluationInput as EvaluationInput
from paper.gpt.evaluate_paper import PaperResult
from paper.gpt.extract_graph import GraphResult
from paper.gpt.model import PaperWithRelatedSummary, PromptResult
from paper.gpt.prompts import PromptTemplate, load_prompts
from paper.gpt.run_gpt import GPTResult, LLMClient, OpenAIClient
from paper.util import Timer, cli, ensure_envvar, sample, setup_logging
from paper.util.serde import load_data, save_data

logger = logging.getLogger(__name__)

PAIRWISE_COMPARISON_PROMPTS = load_prompts("pairwise_comparison")

# Elo rating constants
DEFAULT_ELO = 1200  # Starting rating for all players
K_FACTOR = 32  # How much ratings can change in a single match
EXPECTED_SCORE_DIVISOR = 400  # For expected score calculation


class MatchWinner(StrEnum):
    """Who won a tournament match."""

    A = "A"
    B = "B"
    TIE = "tie"


class PairwiseComparisonResult(BaseModel):
    """Result from pairwise comparison of two rationales by LLM."""

    model_config = ConfigDict(frozen=True)

    winner: MatchWinner
    score: float
    """1.0 for clear winner, 0.5 for tie."""
    explanation: str
    metric: str
    """Which metric this comparison is for."""


@dataclass(frozen=True, kw_only=True)
class PlayerMatch:
    """Match in player match history."""

    player: str
    opponent: str
    score: float
    explanation: str


@dataclass
class EloPlayer:
    """Represents a player in the Elo rating system."""

    name: str
    rating: float = DEFAULT_ELO
    wins: int = 0
    losses: int = 0
    ties: int = 0
    matches_played: int = 0

    # Track all match results for detailed analysis
    match_history: list[PlayerMatch] = field(default_factory=list)

    def add_match_result(self, opponent: str, score: float, explanation: str) -> None:
        """Add a match result to the player's history.

        Args:
            opponent: Name of the opponent.
            score: 1.0 for win, 0.5 for tie, 0.0 for loss.
            explanation: Explanation for the match result.
        """
        self.match_history.append(
            PlayerMatch(
                player=self.name,
                opponent=opponent,
                score=score,
                explanation=explanation,
            )
        )
        self.matches_played += 1

        if score == 1.0:
            self.wins += 1
        elif score == 0.5:
            self.ties += 1
        else:
            self.losses += 1

    def update_rating(self, expected_score: float, actual_score: float) -> None:
        """Update the Elo rating based on match outcome.

        Args:
            expected_score: Expected probability of winning (0.0-1.0).
            actual_score: Actual outcome (1.0=win, 0.5=tie, 0.0=loss).
        """
        self.rating += K_FACTOR * (actual_score - expected_score)

    @property
    def win_percentage(self) -> float:
        """Calculate win percentage of the player."""
        if self.matches_played == 0:
            return 0.0
        return (self.wins + 0.5 * self.ties) / self.matches_played


@dataclass(frozen=True, kw_only=True)
class TournamentMatch:
    """Entry in tournament match history."""

    player_a: str
    player_b: str
    result: PairwiseComparisonResult


class TournamentSystem:
    """Manages an Elo tournament for rationale comparisons."""

    def __init__(self, model_names: Sequence[str], metric: str) -> None:
        """Initialize the tournament system.

        Args:
            model_names: Names of the different models being compared.
            metric: The metric this tournament is evaluating.
        """
        self.metric = metric
        self.players: dict[str, EloPlayer] = {
            name: EloPlayer(name=name) for name in model_names
        }
        self.matches: list[TournamentMatch] = []

    def calculate_expected_score(
        self, player_a: EloPlayer, player_b: EloPlayer
    ) -> float:
        """Calculate expected score for player A against player B.

        Args:
            player_a: First player.
            player_b: Second player.

        Returns:
            Expected probability of player A winning (0.0-1.0).
        """
        return 1.0 / (
            1.0 + 10.0 ** ((player_b.rating - player_a.rating) / EXPECTED_SCORE_DIVISOR)
        )

    def record_match(
        self, player_a_name: str, player_b_name: str, result: PairwiseComparisonResult
    ) -> None:
        """Record the outcome of a match and update player ratings.

        Args:
            player_a_name: Name of the first player.
            player_b_name: Name of the second player.
            result: The result of the comparison.
        """
        # Ensure both players exist
        player_a = self.players[player_a_name]
        player_b = self.players[player_b_name]

        # Calculate expected scores
        expected_a = self.calculate_expected_score(player_a, player_b)
        expected_b = 1.0 - expected_a

        # Determine actual scores from the result
        match result.winner:
            case MatchWinner.A:
                actual_a, actual_b = 1.0, 0.0
            case MatchWinner.B:
                actual_a, actual_b = 0.0, 1.0
            case MatchWinner.TIE:
                actual_a, actual_b = 0.5, 0.5

        # Update ratings
        player_a.update_rating(expected_a, actual_a)
        player_b.update_rating(expected_b, actual_b)

        # Record match results for each player
        player_a.add_match_result(player_b_name, actual_a, result.explanation)
        player_b.add_match_result(player_a_name, actual_b, result.explanation)

        # Save match in tournament history
        self.matches.append(
            TournamentMatch(
                player_a=player_a_name, player_b=player_b_name, result=result
            )
        )

    def get_rankings_with_rank(self) -> list[PlayerRank]:
        """Get rankings with rank position.

        Returns:
            List of PlayerRank with rank data for all players.
        """
        rankings = sorted(
            (
                (p.name, p.rating, p.wins, p.losses, p.ties)
                for p in self.players.values()
            ),
            key=lambda x: x[1],
            reverse=True,
        )

        return [
            PlayerRank(
                rank=i,
                name=name,
                rating=rating,
                wins=wins,
                losses=losses,
                ties=ties,
            )
            for i, (name, rating, wins, losses, ties) in enumerate(rankings, 1)
        ]


@dataclass(frozen=True, kw_only=True)
class PlayerRank:
    """Player entry in ranking."""

    rank: int
    name: str
    rating: float
    wins: int
    losses: int
    ties: int


class TournamentManager:
    """Manages multiple tournaments for different metrics."""

    def __init__(self, model_names: Sequence[str], metrics: Sequence[str]) -> None:
        """Initialize tournament manager.

        Args:
            model_names: Names of the different models being compared.
            metrics: Metrics to run tournaments for.
        """
        self.tournaments = {
            metric: TournamentSystem(model_names, metric) for metric in metrics
        }
        self.model_names = model_names
        self.metrics = metrics

    def record_match(
        self, player_a: str, player_b: str, result: PairwiseComparisonResult
    ) -> None:
        """Record a match result in the appropriate tournament.

        Args:
            player_a: Name of the first player.
            player_b: Name of the second player.
            result: Comparison result from the LLM.
        """
        self.tournaments[result.metric].record_match(player_a, player_b, result)

    def get_overall_ranks(self) -> dict[str, dict[str, Any]]:
        """Calculate overall rankings based on average ranks across metrics.

        Returns:
            Dictionary with model names as keys and rank statistics as values.
        """
        # For each model, collect its rank in each tournament
        model_ranks: dict[str, list[int]] = {name: [] for name in self.model_names}

        for metric in self.metrics:
            tournament = self.tournaments[metric]
            rankings = tournament.get_rankings_with_rank()
            for player in rankings:
                model_ranks[player.name].append(player.rank)

        # Calculate mean and median ranks
        return {
            model: {
                "ranks": ranks,
                "mean_rank": statistics.mean(ranks),
                "median_rank": statistics.median(ranks),
                "best_rank": min(ranks),
                "worst_rank": max(ranks),
                "metric_ranks": {
                    metric: next(
                        player.rank
                        for player in self.tournaments[metric].get_rankings_with_rank()
                        if player.name == model
                    )
                    for metric in self.metrics
                },
            }
            for model, ranks in model_ranks.items()
        }


class PaperMetadata(BaseModel):
    """Metadata for paper being compared."""

    model_config = ConfigDict(frozen=True)

    id: str
    title: str
    abstract: str
    label: int
    rationale: str


def extract_metadata(paper: EvaluationInput) -> PaperMetadata:
    """Extract title and abstract and other metadata needed for prompts.

    Args:
        paper: Paper object

    Returns:
        PaperMetadata object with title, abstract, etc.
    """
    match paper:
        case GraphResult():
            return PaperMetadata(
                id=paper.id,
                title=paper.paper.title,
                abstract=paper.paper.abstract,
                label=paper.paper.label,
                rationale=paper.paper.rationale,
            )
        case PaperResult():
            return PaperMetadata(
                id=paper.id,
                title=paper.title,
                abstract=paper.abstract,
                label=paper.label,
                rationale=paper.rationale,
            )
        case PaperWithRelatedSummary():
            return PaperMetadata(
                id=paper.id,
                title=paper.paper.title,
                abstract=paper.paper.abstract,
                label=paper.label,
                rationale=paper.paper.paper.rationale,
            )
        case pr.Paper():
            # Must be pr.Paper
            return PaperMetadata(
                id=paper.id,
                title=paper.title,
                abstract=paper.abstract,
                label=paper.label,
                rationale=paper.rationale,
            )


def find_common_papers(
    paper_collections: Sequence[Sequence[EvaluationInput]],
) -> dict[str, list[EvaluationInput]]:
    """Find papers that exist in all collections based on ID.

    Args:
        paper_collections: List of lists of paper objects.

    Returns:
        Dictionary mapping paper IDs to list of paper objects from each collection.
    """
    # Extract IDs from each collection
    id_sets = [{extract_metadata(p).id for p in papers} for papers in paper_collections]
    # Find IDs common to all collections
    common_ids = set[str].intersection(*id_sets) if id_sets else set[str]()

    # Group papers by ID
    result: dict[str, list[EvaluationInput]] = {}
    for paper_id in common_ids:
        paper_group: list[EvaluationInput] = []

        for papers in paper_collections:
            for paper in papers:
                if extract_metadata(paper).id == paper_id:
                    paper_group.append(paper)
                    break

        result[paper_id] = paper_group

    return result


async def compare_rationales(
    client: LLMClient,
    paper_metadata: PaperMetadata,
    rationale_a: str,
    rationale_b: str,
    model_a: str,
    model_b: str,
    metric: str,
    prompt: PromptTemplate,
) -> GPTResult[PairwiseComparisonResult]:
    """Compare two rationales for the same paper using LLM.

    Args:
        client: LLM client.
        paper_metadata: Paper metadata (title, abstract, etc.).
        rationale_a: First rationale to compare.
        rationale_b: Second rationale to compare.
        model_a: Name of the first model.
        model_b: Name of the second model.
        metric: The metric to focus on in the comparison.
        prompt: Prompt template for the comparison.

    Returns:
        Comparison result wrapped in a GPTResult.
    """
    user_prompt_text = prompt.template.format(
        title=paper_metadata.title,
        abstract=paper_metadata.abstract,
        label=paper_metadata.label,
        rationale_a=rationale_a,
        rationale_b=rationale_b,
        model_a=model_a,
        model_b=model_b,
        metric=metric,
    )

    result = await client.run(PairwiseComparisonResult, prompt.system, user_prompt_text)
    return result.map(
        lambda r: r
        if r is not None
        # Default to TIE when LLM returns an error.
        else PairwiseComparisonResult(
            winner=MatchWinner.TIE,
            score=0.5,
            explanation="Evaluation error. Setting to tie.",
            metric=metric,
        )
    )


async def run_tournament(
    client: LLMClient,
    common_papers: dict[str, list[EvaluationInput]],
    model_names: list[str],
    output_dir: Path,
    tournament_prompt_key: str,
    metrics: list[str],
    seed: int,
) -> dict[str, Any]:
    """Run a pairwise Elo tournament between multiple models.

    Args:
        client: LLM client.
        common_papers: Dictionary mapping paper IDs to papers from each model.
        model_names: Names of the models being compared.
        output_dir: Directory to save results.
        model: GPT model to use.
        tournament_prompt_key: Key for the comparison prompt to use.
        metrics: Metrics to evaluate.
        seed: Random seed for reproducibility.

    Returns:
        Tournament results.
    """
    random.seed(seed)

    prompt = PAIRWISE_COMPARISON_PROMPTS[tournament_prompt_key]

    # Create possible pairings of models (A vs B, A vs C, B vs C, etc.), order-sensitive
    model_pairs = list(itertools.permutations(range(len(model_names)), 2))

    paper_ids = list(common_papers.keys())
    random.shuffle(paper_ids)

    total_cost = 0.0
    total_comparisons = len(paper_ids) * len(model_pairs) * len(metrics)

    manager = TournamentManager(model_names, metrics)
    with tqdm(total=total_comparisons, desc="Running Elo tournament") as pbar:
        for paper_id in paper_ids:
            papers = common_papers[paper_id]
            paper_metadata = extract_metadata(papers[0])

            for metric in metrics:
                for i, j in model_pairs:
                    model_a = model_names[i]
                    model_b = model_names[j]

                    rationale_a = extract_metadata(papers[i]).rationale
                    rationale_b = extract_metadata(papers[j]).rationale

                    comparison_result = await compare_rationales(
                        client,
                        paper_metadata,
                        rationale_a,
                        rationale_b,
                        model_a,
                        model_b,
                        metric,
                        prompt,
                    )

                    manager.record_match(model_a, model_b, comparison_result.result)

                    total_cost += comparison_result.cost
                    pbar.update(1)

    overall_ranks = manager.get_overall_ranks()

    # Save tournament results
    tournament_results = {
        "model_names": model_names,
        "metrics": metrics,
        "paper_count": len(paper_ids),
        "total_comparisons": total_comparisons,
        "total_cost": total_cost,
        "metric_rankings": {
            metric: [
                {
                    "rank": player.rank,
                    "name": player.name,
                    "rating": player.rating,
                    "wins": player.wins,
                    "losses": player.losses,
                    "ties": player.ties,
                }
                for player in tournament.get_rankings_with_rank()
            ]
            for metric, tournament in manager.tournaments.items()
        },
        "overall_rankings": [
            {
                "name": name,
                "mean_rank": stats["mean_rank"],
                "median_rank": stats["median_rank"],
                "best_rank": stats["best_rank"],
                "worst_rank": stats["worst_rank"],
                "metric_ranks": stats["metric_ranks"],
            }
            for name, stats in sorted(
                overall_ranks.items(), key=lambda x: x[1]["mean_rank"]
            )
        ],
    }

    save_data(output_dir / "tournament_results.json", tournament_results)

    return tournament_results


def _console_to_str(*objects: Any) -> str:
    buf = StringIO()
    console = Console(file=buf, force_jupyter=False)
    console.print(*objects)
    return buf.getvalue()


def _display_tournament_results(results: dict[str, Any]) -> str:
    """Format tournament results for display.

    Args:
        results: Tournament results dictionary.

    Returns:
        Formatted string for display.
    """
    table = Table(title="Elo Tournament Rankings")

    table.add_column("Rank", style="cyan", justify="right")
    table.add_column("Model", style="green")
    table.add_column("Mean Rank", justify="right")
    table.add_column("Median Rank", justify="right")

    metrics = list(results["metric_rankings"].keys())
    for metric in metrics:
        table.add_column(metric.capitalize(), justify="right")

    # Add rows for each model's overall ranking
    for i, model in enumerate(results["overall_rankings"], 1):
        name = model["name"]
        mean_rank = f"{model['mean_rank']:.2f}"
        median_rank = f"{model['median_rank']:.1f}"

        # Get metric-specific ranks
        metric_ranks = [str(model["metric_ranks"][m]) for m in metrics]

        table.add_row(str(i), name, mean_rank, median_rank, *metric_ranks)

    return _console_to_str(table)


TOURNAMENT_ALL_METRICS = [
    "clarity",
    "faithfulness",
    "factuality",
    "specificity",
    "contributions",
]

# Set up Typer CLI
app = typer.Typer(
    context_settings={"help_option_names": ["-h", "--help"]},
    add_completion=False,
    rich_markup_mode="rich",
    pretty_exceptions_show_locals=False,
    no_args_is_help=True,
)


@app.command(no_args_is_help=True)
def tournament(
    inputs: Annotated[
        list[str],
        typer.Argument(
            help="Input files to process. Each file is in the format path:type:name."
        ),
    ],
    output_dir: Annotated[
        Path,
        typer.Option(
            "--output",
            help="The path to the output directory where results will be saved.",
        ),
    ],
    model: Annotated[
        str,
        typer.Option("--model", "-m", help="Model to use for evaluation"),
    ] = "gpt-4o-mini",
    tournament_prompt: Annotated[
        str,
        typer.Option(
            help="The prompts to use for pairwise comparison.",
            click_type=cli.Choice(PAIRWISE_COMPARISON_PROMPTS),
        ),
    ] = "standard",
    metrics: Annotated[
        list[str] | None,
        typer.Option(
            "--metric",
            help="Metrics to evaluate in tournament",
            click_type=cli.Choice(TOURNAMENT_ALL_METRICS),
        ),
    ] = None,
    limit: Annotated[
        int,
        typer.Option("--limit", "-n", help="Number of papers to process per model"),
    ] = 10,
    seed: Annotated[
        int,
        typer.Option(help="Random seed for the tournament to ensure reproducibility"),
    ] = 0,
) -> None:
    """Run a pairwise Elo tournament between multiple models."""
    tournament_metrics = metrics or TOURNAMENT_ALL_METRICS

    dotenv.load_dotenv()
    output_dir.mkdir(parents=True, exist_ok=True)

    # Parse input files, types, and model names
    parsed_inputs: list[tuple[Path, str]] = []
    model_names: list[str] = []

    for input_str in inputs:
        parts = input_str.split(":", maxsplit=2)
        if len(parts) == 3:
            file_path, file_type, model_name = parts
        elif len(parts) == 2:
            file_path, file_type = parts
            model_name = Path(file_path).parent.name
        else:
            file_path = parts[0]
            file_type = "graph"
            model_name = Path(file_path).parent.name

        parsed_inputs.append((Path(file_path), file_type))
        model_names.append(model_name)

    asyncio.run(
        _run_tournament(
            parsed_inputs,
            model_names,
            output_dir,
            model,
            tournament_prompt,
            tournament_metrics,
            limit,
            seed,
        )
    )


async def _run_tournament(
    inputs: list[tuple[Path, str]],
    model_names: list[str],
    output_dir: Path,
    model: str,
    tournament_prompt_key: str,
    metrics: list[str],
    limit: int,
    seed: int,
) -> None:
    """Run the tournament on the given inputs.

    Args:
        inputs: List of (file_path, file_type) tuples.
        model_names: Names of the models.
        output_dir: Directory to save results.
        model: GPT model to use.
        tournament_prompt_key: Key for the comparison prompt.
        metrics: Metrics to evaluate.
        limit: Maximum number of papers to use.
        seed: Random seed.
    """
    random.seed(seed)

    client = OpenAIClient(
        api_key=ensure_envvar("OPENAI_API_KEY"), model=model, seed=seed
    )

    # Load papers from each input file
    paper_collections: list[Sequence[EvaluationInput]] = []

    for file_path, file_type in inputs:
        match file_type.lower():
            case "graph":
                papers = sample(
                    PromptResult.unwrap(
                        load_data(file_path, PromptResult[GraphResult])
                    ),
                    limit,
                )
            case "paper":
                papers = sample(
                    PromptResult.unwrap(
                        load_data(file_path, PromptResult[PaperResult])
                    ),
                    limit,
                )
            case "raw":
                papers = sample(load_data(file_path, pr.Paper), limit)
            case "summ":
                papers = sample(
                    PromptResult.unwrap(
                        load_data(file_path, PromptResult[PaperWithRelatedSummary])
                    ),
                    limit,
                )
            case _:
                raise ValueError(
                    f"Invalid file_type: {file_type}. Must be 'graph', 'paper', 'summ',"
                    " or 'raw'."
                )

        paper_collections.append(papers)

    # Find common papers across all models
    common_papers = find_common_papers(paper_collections)
    logger.info(
        f"Found {len(common_papers)} papers common to all {len(model_names)} models"
    )

    if not common_papers:
        logger.error(
            "No common papers found across all models. Tournament cannot proceed."
        )
        return

    with Timer() as timer:
        results = await run_tournament(
            client,
            common_papers,
            model_names,
            output_dir,
            tournament_prompt_key,
            metrics,
            seed,
        )

    logger.info(f"Time elapsed: {timer.human}")
    logger.info(f"Total cost: ${results['total_cost']:.10f}")

    # Display results
    logger.info("\n%s", _display_tournament_results(results))


@app.callback()
def main() -> None:
    """Set up logging."""
    setup_logging()


if __name__ == "__main__":
    app()
