"""Run Elo tournaments to compare rationales from different evaluation models.

This file implements a pairwise tournament system using Elo ratings to compare
rationales generated by different models.

The input type is one of:

- `raw`: original dataset type, `peerread.Paper`
- `graph`: output of `gpt.evaluate_paper_graph`, `PromptResult[GraphResult]`
- `paper`: output of `gpt.evaluate_paper_scimon`, `PromptResult[PaperResult]`
- `summ`: output of `gpt.summarise_related_peter.py`, `PromptResult[PaperWithRelatedSummary]`
"""

from __future__ import annotations

import asyncio
import itertools
import logging
import random
import statistics
from collections import defaultdict
from collections.abc import Collection, Iterable, Mapping, Sequence
from dataclasses import dataclass
from enum import StrEnum
from functools import partial
from pathlib import Path
from typing import Annotated, Any, Self

import dotenv
import typer
from pydantic import BaseModel, ConfigDict
from rich.table import Table
from tqdm import tqdm

from paper import peerread as pr
from paper.gpt.evaluate_paper import PaperResult
from paper.gpt.extract_graph import GraphResult
from paper.gpt.model import PaperWithRelatedSummary, Prompt, PromptResult
from paper.gpt.prompts import PromptTemplate, load_prompts, print_prompts
from paper.gpt.run_gpt import (
    GPTResult,
    LLMClient,
    OpenAIClient,
    gpr_map,
    gpr_traverse,
)
from paper.util import (
    Timer,
    cli,
    ensure_envvar,
    get_params,
    progress,
    render_params,
    render_rich,
    sample,
    setup_logging,
)
from paper.util.serde import load_data, load_data_single, save_data

logger = logging.getLogger(__name__)

PAIRWISE_COMPARISON_PROMPTS = load_prompts("pairwise_comparison")
TOURNAMENT_METRICS: Mapping[str, str] = {
    "clarity": (
        "How well-written the text is. How easy it is to understand and to follow its"
        " ideas."
    ),
    "faithfulness": (
        "Whether the rationale justifies the novelty label. For example, if the text is"
        " mostly positive, so should the label."
    ),
    "factuality": (
        "Is the rationale grounded correctly in scientific facts from the main and"
        " related papers?"
    ),
    "specificity": (
        "Does the rationale cover information specific to the paper, or does it make"
        " overly generic statements?"
    ),
    "contributions": (
        "Does the rationale effectively compare the main paper with the prior work?"
    ),
}
REQUEST_BATCH_SIZE = 100


# Elo rating constants
DEFAULT_ELO = 1200  # Starting rating for all players
K_FACTOR = 32  # How much ratings can change in a single match
EXPECTED_SCORE_DIVISOR = 400  # For expected score calculation
MELO_DEFAULT_TRIALS = 10  # How many different Elo trials to run

# Bradley-Terry model constants
DEFAULT_BT_STRENGTH = 1.0  # Initial strength parameter for all players
BT_CONVERGENCE_THRESHOLD = 1e-6  # Threshold for convergence in MLE
BT_MAX_ITERATIONS = 100  # Maximum iterations for MLE algorithm

type EvaluationInput = GraphResult | PaperResult | pr.Paper | PaperWithRelatedSummary
"""Type alias for rationale evaluation."""


class MatchWinner(StrEnum):
    """Who won a tournament match."""

    A = "A"
    B = "B"
    TIE = "tie"


class MatchResult(BaseModel):
    """Result from pairwise comparison of two rationales by LLM."""

    model_config = ConfigDict(frozen=True)

    winner: MatchWinner
    """Who wins the match. The model can only reply A or B, but we use TIE for errors."""
    explanation: str
    """Explanation for the winner evaluation."""


class PlayerMatch(BaseModel):
    """Match in player match history."""

    model_config = ConfigDict(frozen=True)

    player: str
    opponent: str
    score: float
    explanation: str


class EloPlayer(BaseModel):
    """Represents a player in the Elo rating system."""

    model_config = ConfigDict(frozen=True)

    name: str
    rating: float
    wins: int
    losses: int
    ties: int
    match_history: Sequence[PlayerMatch]

    @classmethod
    def fresh(cls, name: str) -> EloPlayer:
        """Create fresh player without rating or matches."""
        return EloPlayer(
            name=name, rating=DEFAULT_ELO, wins=0, losses=0, ties=0, match_history=()
        )

    def play_match(
        self,
        opponent: str,
        expected_score: float,
        actual_score: float,
        explanation: str,
    ) -> EloPlayer:
        """Play match against `opponent`, updating the match record and rating."""
        new_match = PlayerMatch(
            player=self.name,
            opponent=opponent,
            score=actual_score,
            explanation=explanation,
        )

        return EloPlayer(
            wins=self.wins + (1 if actual_score == 1.0 else 0),
            ties=self.ties + (1 if actual_score == 0.5 else 0),
            losses=self.losses + (1 if actual_score == 0.0 else 0),
            match_history=(*self.match_history, new_match),
            rating=_update_elo_rating(self.rating, actual_score, expected_score),
            name=self.name,
        )


class BradleyTerryPlayer(BaseModel):
    """Represents a player in the Bradley-Terry rating system."""

    model_config = ConfigDict(frozen=True)

    name: str
    strength: float  # Strength parameter θᵢ
    wins: int
    losses: int
    ties: int
    match_history: Sequence[PlayerMatch]

    @classmethod
    def fresh(cls, name: str) -> BradleyTerryPlayer:
        """Create fresh player with initial strength and no matches."""
        return BradleyTerryPlayer(
            name=name,
            strength=DEFAULT_BT_STRENGTH,
            wins=0,
            losses=0,
            ties=0,
            match_history=(),
        )

    def play_match(
        self,
        opponent: str,
        actual_score: float,
        explanation: str,
    ) -> BradleyTerryPlayer:
        """Record a match against an opponent.

        Unlike Elo, Bradley-Terry doesn't update strengths immediately; they are
        recomputed globally after all matches.

        Args:
            opponent: The name of the opponent.
            actual_score: The actual score (1.0=win, 0.5=tie, 0.0=loss).
            explanation: The explanation for the match result.

        Returns:
            Updated player with new match history.
        """
        new_match = PlayerMatch(
            player=self.name,
            opponent=opponent,
            score=actual_score,
            explanation=explanation,
        )

        return BradleyTerryPlayer(
            name=self.name,
            strength=self.strength,  # Unchanged until global recomputation
            wins=self.wins + (1 if actual_score == 1.0 else 0),
            ties=self.ties + (1 if actual_score == 0.5 else 0),
            losses=self.losses + (1 if actual_score == 0.0 else 0),
            match_history=(*self.match_history, new_match),
        )

    def set_strength(self, new_strength: float) -> BradleyTerryPlayer:
        """Create a new player with updated strength parameter but same history.

        Args:
            new_strength: The new strength parameter.

        Returns:
            Updated player with new strength value.
        """
        return BradleyTerryPlayer(
            name=self.name,
            strength=new_strength,
            wins=self.wins,
            losses=self.losses,
            ties=self.ties,
            match_history=self.match_history,
        )


def _update_elo_rating(
    current_rating: float, actual_score: float, expected_score: float
) -> float:
    """Update Elo rating based on match result.

    Args:
        current_rating: Current rating of the player.
        actual_score: Actual score of the player (0.0-1.0).
        expected_score: Expected score of the player (0.0-1.0).

    Returns:
        Updated Elo rating.
    """
    return current_rating + K_FACTOR * (actual_score - expected_score)


class TournamentMatch(BaseModel):
    """Entry in tournament match history."""

    model_config = ConfigDict(frozen=True)

    player_a: str
    player_b: str
    result: MatchResult


def _elo_expected_probabilities(
    player_a: EloPlayer, player_b: EloPlayer
) -> tuple[float, float]:
    """Calculate expected score for player A against player B.

    Args:
        player_a: First player.
        player_b: Second player.

    Returns:
        Tuple with expected probabilities of (player A, player B) winning in [0, 1].
    """
    expected_a = 1.0 / (
        1.0 + 10.0 ** ((player_b.rating - player_a.rating) / EXPECTED_SCORE_DIVISOR)
    )
    expected_b = 1 - expected_a
    return expected_a, expected_b


def _bt_update_strengths(
    players: Mapping[str, BradleyTerryPlayer],
    win_counts: Mapping[tuple[str, str], float],
    match_counts: Mapping[tuple[str, str], float],
) -> dict[str, BradleyTerryPlayer]:
    """Update player strengths using the Bradley-Terry model's maximum likelihood estimation.

    This is an iterative algorithm that converges to the maximum likelihood estimates
    of the player strengths given the observed match outcomes.

    Args:
        players: Current mapping of player names to player objects.
        win_counts: Mapping of (player, opponent) pairs to count of wins (1.0 for win, 0.5 for tie).
        match_counts: Mapping of (player, opponent) pairs to count of matches played.

    Returns:
        Updated mapping of player names to player objects with new strength values.
    """
    player_names = list(players.keys())
    n_players = len(player_names)

    # Current strength parameters
    current_strengths = {name: players[name].strength for name in player_names}

    # Iterative MLE algorithm
    for _ in range(BT_MAX_ITERATIONS):
        new_strengths: dict[str, float] = {}
        max_diff = 0.0

        for name in player_names:
            # Number of matches player has played
            total_matches = sum(
                match_counts.get((name, opponent), 0)
                for opponent in player_names
                if opponent != name
            )

            if total_matches == 0:
                # If player hasn't played any matches, keep strength unchanged
                new_strengths[name] = current_strengths[name]
                continue

            # Total wins for player i
            total_wins = sum(
                win_counts.get((name, opponent), 0)
                for opponent in player_names
                if opponent != name
            )

            # Expected wins based on current parameters
            expected_wins = 0.0
            for opponent in player_names:
                if opponent == name:
                    continue

                matches = match_counts.get((name, opponent), 0)
                if matches > 0:
                    # Probability of winning against this opponent
                    opponent_strength = current_strengths[opponent]
                    # Prevent division by zero by ensuring strengths are positive
                    player_strength = max(current_strengths[name], 1e-10)
                    opp_strength = max(opponent_strength, 1e-10)
                    p_win = player_strength / (player_strength + opp_strength)
                    expected_wins += matches * p_win

            # Avoid division by zero
            if expected_wins < 1e-10:
                new_strengths[name] = current_strengths[name]
                continue

            # Update strength
            new_strength = current_strengths[name] * total_wins / expected_wins
            new_strengths[name] = new_strength

            # Track largest change
            diff = abs(new_strength - current_strengths[name])
            max_diff = max(max_diff, diff)

        # Normalize to avoid numerical issues
        strength_values = list(new_strengths.values())
        sum_strengths = sum(strength_values)
        if sum_strengths > 0:
            scale_factor = n_players / sum_strengths
            for name_str in list(new_strengths.keys()):
                new_strengths[name_str] = new_strengths[name_str] * scale_factor

        # Check for convergence
        if max_diff < BT_CONVERGENCE_THRESHOLD:
            break

        current_strengths = new_strengths

    # Create new player objects with updated strengths
    return {
        name: players[name].set_strength(current_strengths[name])
        for name in player_names
    }


class TournamentSystem(BaseModel):
    """Manages a tournament for rationale comparisons.

    This is the base implementation that works with Elo ratings.
    """

    model_config = ConfigDict(frozen=True)

    metric: str
    players: Mapping[str, EloPlayer]
    matches: Sequence[TournamentMatch]

    @classmethod
    def create(cls, item_names: Collection[str], metric: str) -> TournamentSystem:
        """Create a new tournament system from the players (items) names and metrics.

        Args:
            item_names: Names of the different items being compared.
            metric: The metric this tournament is evaluating.

        Returns:
            A new TournamentSystem instance.
        """
        return cls(
            metric=metric,
            players={name: EloPlayer.fresh(name=name) for name in item_names},
            matches=(),
        )

    def record_match(
        self,
        player_a_name: str,
        player_b_name: str,
        result: MatchResult,
    ) -> TournamentSystem:
        """Record the outcome of a match and update player ratings.

        Args:
            player_a_name: Name of the first player.
            player_b_name: Name of the second player.
            result: The result of the comparison.

        Returns:
            A new TournamentSystem with updated state.
        """
        player_a = self.players[player_a_name]
        player_b = self.players[player_b_name]

        expected_a, expected_b = _elo_expected_probabilities(player_a, player_b)

        # Determine actual scores from the result
        match result.winner:
            case MatchWinner.A:
                actual_a, actual_b = 1.0, 0.0
            case MatchWinner.B:
                actual_a, actual_b = 0.0, 1.0
            case MatchWinner.TIE:
                actual_a, actual_b = 0.5, 0.5

        updated_players = {
            **self.players,
            player_a_name: player_a.play_match(
                player_b_name, expected_a, actual_a, result.explanation
            ),
            player_b_name: player_b.play_match(
                player_a_name, expected_b, actual_b, result.explanation
            ),
        }

        new_match = TournamentMatch(
            player_a=player_a_name, player_b=player_b_name, result=result
        )
        return TournamentSystem(
            metric=self.metric,
            players=updated_players,
            matches=(*self.matches, new_match),
        )

    def get_rankings(self) -> list[PlayerRank]:
        """Get rankings for all players."""
        players_sorted_by_rating = sorted(
            self.players.values(), key=lambda p: p.rating, reverse=True
        )

        return [
            PlayerRank(
                rank=i,
                name=player.name,
                rating=player.rating,
                wins=player.wins,
                losses=player.losses,
                ties=player.ties,
            )
            for i, player in enumerate(players_sorted_by_rating, 1)
        ]


class BradleyTerryTournamentSystem(BaseModel):
    """Manages a tournament for rationale comparisons using the Bradley-Terry model."""

    model_config = ConfigDict(frozen=True)

    metric: str
    players: Mapping[str, BradleyTerryPlayer]
    matches: Sequence[TournamentMatch]

    @classmethod
    def create(
        cls, item_names: Collection[str], metric: str
    ) -> BradleyTerryTournamentSystem:
        """Create a new tournament system from the players (items) names and metrics.

        Args:
            item_names: Names of the different items being compared.
            metric: The metric this tournament is evaluating.

        Returns:
            A new BradleyTerryTournamentSystem instance.
        """
        return cls(
            metric=metric,
            players={name: BradleyTerryPlayer.fresh(name=name) for name in item_names},
            matches=(),
        )

    def record_match(
        self,
        player_a_name: str,
        player_b_name: str,
        result: MatchResult,
    ) -> BradleyTerryTournamentSystem:
        """Record the outcome of a match and add it to match history.

        Unlike Elo, Bradley-Terry doesn't update ratings immediately after each match.
        Instead, all matches are recorded and then strength parameters are computed
        globally after all match outcomes are known.

        Args:
            player_a_name: Name of the first player.
            player_b_name: Name of the second player.
            result: The result of the comparison.

        Returns:
            A new BradleyTerryTournamentSystem with the match recorded.
        """
        player_a = self.players[player_a_name]
        player_b = self.players[player_b_name]

        # Determine actual scores from the result
        match result.winner:
            case MatchWinner.A:
                actual_a, actual_b = 1.0, 0.0
            case MatchWinner.B:
                actual_a, actual_b = 0.0, 1.0
            case MatchWinner.TIE:
                actual_a, actual_b = 0.5, 0.5

        updated_players = {
            **self.players,
            player_a_name: player_a.play_match(
                player_b_name, actual_a, result.explanation
            ),
            player_b_name: player_b.play_match(
                player_a_name, actual_b, result.explanation
            ),
        }

        new_match = TournamentMatch(
            player_a=player_a_name, player_b=player_b_name, result=result
        )

        system = BradleyTerryTournamentSystem(
            metric=self.metric,
            players=updated_players,
            matches=(*self.matches, new_match),
        )

        # After recording all matches, update all player strengths at once
        return system._update_all_strengths()

    def _update_all_strengths(self) -> BradleyTerryTournamentSystem:
        """Update the strength parameters of all players using the Bradley-Terry model.

        This is called after recording matches to globally update all player strengths.

        Returns:
            A new BradleyTerryTournamentSystem with updated player strengths.
        """
        # Compute win counts and match counts for all player pairs
        win_counts: dict[tuple[str, str], float] = {}
        match_counts: dict[tuple[str, str], float] = {}

        for match in self.matches:
            player_a = match.player_a
            player_b = match.player_b

            match_counts[player_a, player_b] = (
                match_counts.get((player_a, player_b), 0) + 1
            )
            match_counts[player_b, player_a] = (
                match_counts.get((player_b, player_a), 0) + 1
            )

            # Record win counts based on match result
            match match.result.winner:
                case MatchWinner.A:
                    win_counts[player_a, player_b] = (
                        win_counts.get((player_a, player_b), 0) + 1
                    )
                case MatchWinner.B:
                    win_counts[player_b, player_a] = (
                        win_counts.get((player_b, player_a), 0) + 1
                    )
                case MatchWinner.TIE:
                    win_counts[player_a, player_b] = (
                        win_counts.get((player_a, player_b), 0) + 0.5
                    )
                    win_counts[player_b, player_a] = (
                        win_counts.get((player_b, player_a), 0) + 0.5
                    )

        # Update player strengths using maximum likelihood estimation
        updated_players = _bt_update_strengths(self.players, win_counts, match_counts)

        return BradleyTerryTournamentSystem(
            metric=self.metric,
            players=updated_players,
            matches=self.matches,
        )

    def get_rankings(self) -> list[PlayerRank]:
        """Get rankings for all players based on their strength parameters."""
        players_sorted_by_strength = sorted(
            self.players.values(), key=lambda p: p.strength, reverse=True
        )

        return [
            PlayerRank(
                rank=i,
                name=player.name,
                rating=player.strength,  # Use strength as rating for output
                wins=player.wins,
                losses=player.losses,
                ties=player.ties,
            )
            for i, player in enumerate(players_sorted_by_strength, 1)
        ]


class PlayerRank(BaseModel):
    """Player entry in ranking for a given tournament."""

    model_config = ConfigDict(frozen=True)

    rank: int
    name: str
    rating: float
    wins: int
    losses: int
    ties: int


class TournamentManager(BaseModel):
    """Manage multiple tournaments for different metrics."""

    model_config = ConfigDict(frozen=True)

    tournaments: Mapping[str, TournamentSystem]
    item_names: Sequence[str]
    metrics: Sequence[str]

    @classmethod
    def create(
        cls, item_names: Sequence[str], metrics: Sequence[str]
    ) -> TournamentManager:
        """Create a new tournament manager with different tournaments be metric.

        Args:
            item_names: Names of the items being compared.
            metrics: Metrics to run tournaments for.

        Returns:
            A new TournamentManager instance.
        """
        return cls(
            tournaments={
                metric: TournamentSystem.create(item_names, metric)
                for metric in metrics
            },
            item_names=item_names,
            metrics=metrics,
        )

    def record_match(
        self, player_a: str, player_b: str, result: MatchResult, metric: str
    ) -> TournamentManager:
        """Record a match result in the appropriate tournament for the metric.

        Args:
            player_a: Name of the first player.
            player_b: Name of the second player.
            result: Comparison result from the LLM.
            metric: Metric used for the comparison.

        Returns:
            A new TournamentManager with updated state.
        """
        tournament = self.tournaments[metric]
        updated_tournaments = {
            **self.tournaments,
            metric: tournament.record_match(player_a, player_b, result),
        }

        return TournamentManager(
            tournaments=updated_tournaments,
            # Unchanged
            item_names=self.item_names,
            metrics=self.metrics,
        )

    def get_overall_ranks(self) -> dict[str, ItemRankStats]:
        """Calculate overall rankings based on average ranks across metrics.

        Returns:
            Dictionary with item names as keys and rank statistics as values.
        """
        # For each item, collect its rank in each tournament
        item_ranks: dict[str, list[int]] = defaultdict(list)
        for metric in self.metrics:
            for player in self.tournaments[metric].get_rankings():
                item_ranks[player.name].append(player.rank)

        return {
            item: ItemRankStats(
                ranks=ranks,
                mean_rank=statistics.mean(ranks),
                median_rank=statistics.median(ranks),
                best_rank=min(ranks),
                worst_rank=max(ranks),
                metric_ranks={
                    metric: next(
                        player.rank
                        for player in self.tournaments[metric].get_rankings()
                        if player.name == item
                    )
                    for metric in self.metrics
                },
            )
            for item, ranks in item_ranks.items()
        }


class ItemRankStats(BaseModel):
    """Statistics for model ranks across metrics."""

    model_config = ConfigDict(frozen=True)

    ranks: Sequence[int]
    mean_rank: float
    median_rank: float
    best_rank: int
    worst_rank: int
    metric_ranks: Mapping[str, int]


class PaperCore(BaseModel):
    """Core information from the paper being compared."""

    model_config = ConfigDict(frozen=True)

    id: str
    title: str
    abstract: str
    label: int
    rationale: str
    approval: bool | None
    conference: str
    year: int | None
    sections: Sequence[pr.PaperSection]

    def main_text(self) -> str:
        """Join all paper sections to form the main text."""
        return pr.clean_maintext("\n".join(s.text for s in self.sections))


def extract_core_data(paper: EvaluationInput) -> PaperCore:
    """Extract title and abstract and other data needed for prompts."""
    match paper:
        case pr.Paper() | PaperResult():
            return PaperCore(
                id=paper.id,
                title=paper.title,
                abstract=paper.abstract,
                label=paper.label,
                rationale=paper.rationale,
                approval=paper.approval,
                conference=paper.conference,
                year=paper.year,
                sections=paper.sections,
            )
        case GraphResult():
            return PaperCore(
                id=paper.id,
                title=paper.paper.title,
                abstract=paper.paper.abstract,
                label=paper.paper.label,
                rationale=paper.paper.rationale,
                approval=paper.paper.approval,
                conference=paper.paper.conference,
                year=paper.paper.year,
                sections=paper.paper.sections,
            )
        case PaperWithRelatedSummary():
            return PaperCore(
                id=paper.id,
                title=paper.paper.title,
                abstract=paper.paper.abstract,
                label=paper.label,
                rationale=paper.paper.paper.rationale,
                approval=paper.paper.paper.approval,
                conference=paper.paper.paper.conference,
                year=paper.paper.paper.year,
                sections=paper.paper.paper.sections,
            )


def _find_common_papers(
    paper_collections: Collection[Collection[EvaluationInput]],
) -> dict[str, list[EvaluationInput]]:
    """Find papers that exist in all collections based on ID.

    Args:
        paper_collections: List of lists of paper objects.

    Returns:
        Mapping of paper IDs to list of paper objects from each collection.
    """
    # Extract IDs from each collection
    id_sets = [
        {extract_core_data(p).id for p in papers} for papers in paper_collections
    ]
    # Find IDs common to all collections
    common_ids = set[str].intersection(*id_sets)

    # Group papers by ID
    return {
        paper_id: [
            next(p for p in papers_col if extract_core_data(p).id == paper_id)
            for papers_col in paper_collections
        ]
        for paper_id in common_ids
    }


def format_evaluation_prompt(
    metric: str,
    paper: PaperCore,
    rationale_a: str,
    rationale_b: str,
    prompt: PromptTemplate,
) -> str:
    """Format user prompt from paper data.

    Args:
        metric: The metric to focus on in the comparison.
        paper: Paper data (title, abstract, etc.).
        rationale_a: First rationale to compare.
        rationale_b: Second rationale to compare.
        prompt: Prompt template for the comparison.

    Returns:
        Comparison result wrapped in a GPTResult.
    """
    return prompt.template.format(
        title=paper.title,
        abstract=paper.abstract,
        rationale_a=rationale_a,
        rationale_b=rationale_b,
        metric=metric,
        definition=TOURNAMENT_METRICS[metric],
    )


async def _compare_rationales(
    client: LLMClient,
    paper: PaperCore,
    rationale_a: str,
    rationale_b: str,
    metric: str,
    prompt: PromptTemplate,
) -> GPTResult[PromptResult[MatchResult]]:
    """Compare two rationales for the same paper using LLM.

    Args:
        client: LLM client.
        paper: Paper data (title, abstract, etc.).
        rationale_a: First rationale to compare.
        rationale_b: Second rationale to compare.
        metric: The metric to focus on in the comparison.
        prompt: Prompt template for the comparison.

    Returns:
        Comparison result and prompt wrapped in a GPTResult.
    """
    user_prompt_text = format_evaluation_prompt(
        metric, paper, rationale_a, rationale_b, prompt
    )

    result = await client.run(MatchResult, prompt.system, user_prompt_text)
    return result.map(
        lambda r: PromptResult(
            item=r
            if r is not None
            # Default to TIE when LLM returns an error.
            else MatchResult(
                winner=MatchWinner.TIE,
                explanation="Comparison error. Defaulting to tie.",
            ),
            prompt=Prompt(system=prompt.system, user=user_prompt_text),
        )
    )


class ComparisonResult(BaseModel):
    """Result of comparing two model outputs by LLM."""

    model_config = ConfigDict(frozen=True)

    paper: PaperCore
    """Full paper data used for the comparison."""
    item_a: str
    """First item name."""
    item_b: str
    """Second item name."""
    rationale_a: str
    """Rationale from item A."""
    rationale_b: str
    """Rationale from item B."""
    metric: str
    """Metric being evaluated."""
    result: MatchResult
    """LLM's comparison result."""


async def _run_all_comparisons(
    client: LLMClient,
    common_papers: Mapping[str, list[EvaluationInput]],
    metrics: Collection[str],
    item_names: Sequence[str],
    item_indices_pairs: Collection[tuple[int, int]],
    paper_ids: Collection[str],
    prompt: PromptTemplate,
) -> GPTResult[Sequence[PromptResult[ComparisonResult]]]:
    """Run all pairwise comparisons between items.

    Args:
        client: LLM client.
        common_papers: Papers from each item, grouped by paper ID.
        metrics: Metrics to evaluate.
        item_names: Names of the items being compared.
        item_indices_pairs: Pairs of item indices to compare.
        paper_ids: IDs of papers to use in comparisons.
        prompt: Prompt template for the comparison.

    Returns:
        List of comparison results.
    """
    total_comparisons = len(paper_ids) * len(item_indices_pairs) * len(metrics)
    logger.info(
        "Comparisons: Papers=%d * ItemPairs=%d * Metrics=%d = %d",
        len(paper_ids),
        len(item_indices_pairs),
        len(metrics),
        total_comparisons,
    )

    @dataclass(frozen=True, kw_only=True)
    class ComparisonSpec:
        """Specification for comparison task."""

        item_a: str
        item_b: str
        paper_id: str
        metric: str
        paper: PaperCore
        rationale_a: str
        rationale_b: str
        prompt: PromptTemplate

    comparison_specs: list[ComparisonSpec] = []

    for paper_id in paper_ids:
        papers = common_papers[paper_id]
        paper = extract_core_data(papers[0])

        for metric in metrics:
            for i, j in item_indices_pairs:
                comparison_specs.append(
                    ComparisonSpec(
                        item_a=item_names[i],
                        item_b=item_names[j],
                        paper_id=paper_id,
                        metric=metric,
                        paper=paper,
                        rationale_a=extract_core_data(papers[i]).rationale,
                        rationale_b=extract_core_data(papers[j]).rationale,
                        prompt=prompt,
                    )
                )

    comparison_results: list[
        GPTResult[PromptResult[tuple[ComparisonSpec, MatchResult]]]
    ] = []

    with tqdm(
        total=len(comparison_specs),
        desc="Running pairwise comparisons",
        position=0,
        leave=True,
    ) as pbar_cmp:
        for batch_specs in itertools.batched(comparison_specs, REQUEST_BATCH_SIZE):
            tasks = [
                _compare_rationales(
                    client,
                    spec.paper,
                    spec.rationale_a,
                    spec.rationale_b,
                    spec.metric,
                    spec.prompt,
                )
                for spec in batch_specs
            ]
            batch_results = await progress.gather(
                tasks,
                desc="Running pairwise comparisons batch",
                position=1,
                leave=False,
            )
            comparison_results.extend(
                gpr_map(result, lambda r, spec=spec: (spec, r))
                for spec, result in zip(batch_specs, batch_results)
            )
            pbar_cmp.update(len(batch_specs))

    def transform(
        item: tuple[ComparisonSpec, MatchResult],
    ) -> ComparisonResult:
        spec, cmp = item
        return ComparisonResult(
            item_a=spec.item_a,
            item_b=spec.item_b,
            metric=spec.metric,
            paper=spec.paper,
            rationale_a=spec.rationale_a,
            rationale_b=spec.rationale_b,
            result=cmp,
        )

    return gpr_traverse(comparison_results, transform)


def _calculate_elo_rankings(
    comparison_results: Collection[ComparisonResult],
    item_names: Sequence[str],
    metrics: Sequence[str],
) -> TournamentResult:
    """Calculate Elo rankings from comparison results.

    Args:
        comparison_results: Results of all pairwise comparisons.
        item_names: Names of the items being compared.
        metrics: Metrics that were evaluated.

    Returns:
        Tournament results with Elo rankings.
    """
    manager = TournamentManager.create(item_names, metrics)

    logger.info("Calculating Elo rankings from %d comparisons", len(comparison_results))
    for comparison in comparison_results:
        manager = manager.record_match(
            comparison.item_a,
            comparison.item_b,
            comparison.result,
            comparison.metric,
        )

    return TournamentResult(
        overall_ranks=manager.get_overall_ranks(),
        tournaments=manager.tournaments,
        total_comparisons=len(comparison_results),
    )


def _calculate_bradley_terry_rankings(
    comparison_results: Collection[ComparisonResult],
    item_names: Sequence[str],
    metrics: Sequence[str],
) -> TournamentResult:
    """Calculate rankings using the Bradley-Terry model from comparison results.

    The Bradley-Terry model computes strength parameters for each player
    based on the outcomes of all pairwise comparisons. It estimates the
    probability of player i beating player j as θᵢ/(θᵢ + θⱼ) where θ are
    the strength parameters.

    Args:
        comparison_results: Results of all pairwise comparisons.
        item_names: Names of the items being compared.
        metrics: Metrics that were evaluated.

    Returns:
        Tournament results with Bradley-Terry rankings.
    """
    logger.info(
        "Calculating Bradley-Terry rankings from %d comparisons",
        len(comparison_results),
    )

    # Group comparisons by metric
    comparisons_by_metric: dict[str, list[ComparisonResult]] = defaultdict(list)
    for comp in comparison_results:
        comparisons_by_metric[comp.metric].append(comp)

    # Create a tournament system for each metric
    bt_tournaments: dict[str, BradleyTerryTournamentSystem] = {}
    for metric in metrics:
        # Initialize a new Bradley-Terry tournament
        tournament = BradleyTerryTournamentSystem.create(item_names, metric)

        # Process all comparisons for this metric
        for comparison in comparisons_by_metric[metric]:
            tournament = tournament.record_match(
                comparison.item_a,
                comparison.item_b,
                comparison.result,
            )

        bt_tournaments[metric] = tournament

    # Convert Bradley-Terry results to standard Tournament objects for the TournamentManager
    tournaments: dict[str, TournamentSystem] = {}
    for metric, bt_tournament in bt_tournaments.items():
        # Create equivalent EloPlayer objects for each BradleyTerryPlayer
        elo_players = {
            name: EloPlayer(
                name=name,
                # Map strength to rating by keeping the same relative scale
                rating=bt_player.strength * 400,  # Scale to be similar to Elo ratings
                wins=bt_player.wins,
                losses=bt_player.losses,
                ties=bt_player.ties,
                match_history=bt_player.match_history,
            )
            for name, bt_player in bt_tournament.players.items()
        }

        # Create standard tournament with equivalent Elo players
        tournaments[metric] = TournamentSystem(
            metric=metric,
            players=elo_players,
            matches=bt_tournament.matches,
        )

    # Create a TournamentManager with standard tournament objects
    manager = TournamentManager(
        tournaments=tournaments,
        item_names=item_names,
        metrics=metrics,
    )

    return TournamentResult(
        overall_ranks=manager.get_overall_ranks(),
        total_comparisons=len(comparison_results),
        tournaments=tournaments,
    )


def _calculate_melo_rankings(
    comparison_results: Collection[ComparisonResult],
    item_names: Sequence[str],
    metrics: Sequence[str],
    num_trials: int,
    seed: int,
) -> TournamentResult:
    """Calculate Multi-Elo rankings from comparison results.

    Runs multiple Elo tournaments with different random orderings and averages
    the final ratings.

    Args:
        comparison_results: Results of all pairwise comparisons.
        item_names: Names of the items being compared.
        metrics: Metrics that were evaluated.
        num_trials: Number of tournaments to run with different orderings.
        seed: Random seed for reproducibility.

    Returns:
        Tournament results with Multi-Elo rankings.
    """
    # Group comparisons by metric to simplify the multi-tournament process
    comparisons_by_metric: dict[str, list[ComparisonResult]] = defaultdict(list)
    for comp in comparison_results:
        comparisons_by_metric[comp.metric].append(comp)

    # Initialize structures to average results across trials
    all_ratings: dict[str, dict[str, list[float]]] = defaultdict(
        lambda: defaultdict(list)
    )
    all_ranks: dict[str, dict[str, list[int]]] = defaultdict(lambda: defaultdict(list))
    all_wins: dict[str, dict[str, list[int]]] = defaultdict(lambda: defaultdict(list))
    all_losses: dict[str, dict[str, list[int]]] = defaultdict(lambda: defaultdict(list))
    all_ties: dict[str, dict[str, list[int]]] = defaultdict(lambda: defaultdict(list))

    random_gen = random.Random(seed)

    logger.info(f"Running {num_trials} tournaments with different orderings")
    for _ in range(num_trials):
        trial_seed = random_gen.randint(0, 10000)
        trial_random = random.Random(trial_seed)

        manager = TournamentManager.create(item_names, metrics)

        # For each metric, shuffle the comparisons differently
        for metric in metrics:
            metric_comparisons = comparisons_by_metric[metric].copy()
            trial_random.shuffle(metric_comparisons)

            # Process the shuffled comparisons
            for cmp in metric_comparisons:
                manager = manager.record_match(
                    cmp.item_a, cmp.item_b, cmp.result, cmp.metric
                )

            # Record each player's result for this metric in this trial
            for player in manager.tournaments[metric].get_rankings():
                all_ratings[metric][player.name].append(player.rating)
                all_ranks[metric][player.name].append(player.rank)
                all_wins[metric][player.name].append(player.wins)
                all_losses[metric][player.name].append(player.losses)
                all_ties[metric][player.name].append(player.ties)

    # Create a final manager with averaged results
    tournaments: dict[str, TournamentSystem] = {}
    for metric in metrics:
        # For each item, create a new player with averaged statistics
        updated_players: dict[str, EloPlayer] = {}
        for name in item_names:
            # Create a new player with averaged values
            updated_players[name] = EloPlayer(
                name=name,
                rating=statistics.mean(all_ratings[metric][name]),
                wins=int(statistics.mean(all_wins[metric][name])),
                losses=int(statistics.mean(all_losses[metric][name])),
                ties=int(statistics.mean(all_ties[metric][name])),
                match_history=(),  # Not a real player.
            )

        # Create a tournament with averaged values
        tournaments[metric] = TournamentSystem(
            metric=metric,
            players=updated_players,
            matches=(),  # Not a real tournament.
        )

    final_manager = TournamentManager(
        tournaments=tournaments,
        item_names=item_names,
        metrics=metrics,
    )

    return TournamentResult(
        overall_ranks=final_manager.get_overall_ranks(),
        total_comparisons=len(comparison_results) * num_trials,
        tournaments=final_manager.tournaments,
    )


class OverallRankingEntry(BaseModel):
    """Overall ranking entry for a model."""

    model_config = ConfigDict(frozen=True)

    name: str
    mean_rank: float
    median_rank: float
    best_rank: int
    worst_rank: int
    metric_ranks: Mapping[str, int]


class TournamentSummary(BaseModel):
    """Summary of tournament results."""

    model_config = ConfigDict(frozen=True)

    item_names: Sequence[str]
    metrics: Sequence[str]
    total_comparisons: int
    metric_rankings: Mapping[str, Sequence[PlayerRank]]
    overall_rankings: Sequence[OverallRankingEntry]


class TournamentResult(BaseModel):
    """Full result of all tournaments run."""

    model_config = ConfigDict(frozen=True)

    overall_ranks: Mapping[str, ItemRankStats]
    """Item ranks across all tournaments."""
    total_comparisons: int
    """Number of comparisons across items."""
    tournaments: Mapping[str, TournamentSystem]
    """Tournament data for each metric."""


def _tournament_summary(
    result: TournamentResult, item_names: Sequence[str], metrics: Sequence[str]
) -> TournamentSummary:
    """Convert internal result to a serializable summary.

    Args:
        result: Tournament result.
        item_names: Names of all items in the tournament.
        metrics: Names of metrics evaluated.

    Returns:
        Serializable tournament summary.
    """
    return TournamentSummary(
        item_names=item_names,
        metrics=metrics,
        total_comparisons=result.total_comparisons,
        metric_rankings={
            metric: [
                PlayerRank(
                    rank=player.rank,
                    name=player.name,
                    rating=player.rating,
                    wins=player.wins,
                    losses=player.losses,
                    ties=player.ties,
                )
                for player in tournament.get_rankings()
            ]
            for metric, tournament in result.tournaments.items()
        },
        overall_rankings=[
            OverallRankingEntry(
                name=name,
                mean_rank=stats.mean_rank,
                median_rank=stats.median_rank,
                best_rank=stats.best_rank,
                worst_rank=stats.worst_rank,
                metric_ranks=stats.metric_ranks,
            )
            for name, stats in sorted(
                result.overall_ranks.items(), key=lambda x: x[1].mean_rank
            )
        ],
    )


def _all_pairings[T](xs: Iterable[T]) -> list[tuple[T, T]]:
    """Create possible pairings of elements (A-B, A-C, B-C, B-A etc.). Order-sensitive."""
    return list(itertools.permutations(xs, 2))


def _display_head_to_head(
    comparison_results: Collection[ComparisonResult],
    item_names: Sequence[str],
    metrics: Sequence[str],
) -> str:
    """Display head-to-head comparison results as a table.

    Args:
        comparison_results: All comparison results.
        item_names: Names of all items being compared.
        metrics: Metrics used for evaluation.

    Returns:
        String representation of the head-to-head table.
    """
    match_results_by_metric = {
        comp.metric: {(comp.paper.id, comp.item_a, comp.item_b): comp.result}
        for comp in comparison_results
    }

    # Format {metric: {(player_a, player_b): (wins, ties, losses)}}
    h2h_by_metric = {
        metric: {(a, b): (0, 0, 0) for a in item_names for b in item_names if a != b}
        for metric in metrics
    }

    # Count wins/losses/ties from direct matchups properly
    for metric, results in match_results_by_metric.items():
        for (_, player_a, player_b), result in results.items():
            # Handle for player A perspective (row player)
            wins_a, ties_a, losses_a = h2h_by_metric[metric][player_a, player_b]

            # Handle for player B perspective (row player in different row)
            wins_b, ties_b, losses_b = h2h_by_metric[metric][player_b, player_a]

            match result.winner:
                case MatchWinner.A:
                    wins_a += 1
                    losses_b += 1
                case MatchWinner.B:
                    losses_a += 1
                    wins_b += 1
                case MatchWinner.TIE:
                    ties_a += 1
                    ties_b += 1

            # Update both perspectives
            h2h_by_metric[metric][player_a, player_b] = (wins_a, ties_a, losses_a)
            h2h_by_metric[metric][player_b, player_a] = (wins_b, ties_b, losses_b)

    # Create tables for each metric
    tables: list[str] = []

    for metric in metrics:
        table = Table(title=f"Head-to-Head Results: {metric.capitalize()}")

        table.add_column("Player", style="cyan")
        for name in item_names:
            table.add_column(name, justify="center")

        for player_a in item_names:
            row = [player_a]
            for player_b in item_names:
                if player_a == player_b:
                    row.append("—")  # Diagonal cells
                else:
                    wins, ties, losses = h2h_by_metric[metric][player_a, player_b]
                    row.append(f"W:{wins} T:{ties} L:{losses}")

            table.add_row(*row)

        tables.append(render_rich(table))

    return "\n\n".join(tables)


def _display_tournament_results(results: TournamentSummary) -> str:
    """Format tournament results for display."""
    table = Table(title="Tournament Rankings")

    table.add_column("Rank", style="cyan", justify="right")
    table.add_column("Item", style="green")
    table.add_column("Mean Rank", justify="right")
    table.add_column("Median Rank", justify="right")

    metrics = list(results.metric_rankings.keys())
    for metric in metrics:
        table.add_column(metric.capitalize(), justify="right")

    # Add rows for each item's overall ranking
    for i, item in enumerate(results.overall_rankings, 1):
        name = item.name
        mean_rank = f"{item.mean_rank:.2f}"
        median_rank = f"{item.median_rank:.1f}"

        # Get metric-specific ranks
        metric_ranks = [str(item.metric_ranks[m]) for m in metrics]

        table.add_row(str(i), name, mean_rank, median_rank, *metric_ranks)

    return render_rich(table)


app = typer.Typer(
    context_settings={"help_option_names": ["-h", "--help"]},
    add_completion=False,
    rich_markup_mode="rich",
    pretty_exceptions_show_locals=False,
    no_args_is_help=True,
)


class RankingAlgorithm(StrEnum):
    """Available ranking algorithms."""

    ELO = "elo"
    MELO = "melo"
    BRADLEY_TERRY = "bradley-terry"


class InputFileType(StrEnum):
    """Types of input data formats."""

    RAW = "raw"
    """Original dataset paper: `peerread.Paper`"""
    GRAPH = "graph"
    """Output of `gpt.evaluate_paper_graph`: `PromptResult[GraphResult]`"""
    PAPER = "paper"
    """Output of `gpt.evaluate_paper_scimon`: `PromptResult[PaperResult]`"""
    SUMM = "summ"
    """Output of `gpt.summarise_related_peter`: `PromptResult[PaperWithRelatedSummary]`"""

    @classmethod
    def from_dirty(cls, type_: str) -> Self:
        """Create instance by cleaning up `type_` by stripping and lowercasing.

        Use when `type_` comes from a potentially dirty source, such as a CLI argument.

        Raises:
            ValueError if the type is invalid.
        """
        return cls(type_.strip().lower())


@app.command(no_args_is_help=True)
def run(
    inputs: Annotated[
        list[str],
        typer.Argument(
            help="Input files to process. Each file is in the format path:type:name."
        ),
    ],
    output_dir: Annotated[
        Path,
        typer.Option(
            "--output",
            help="The path to the output directory where results will be saved.",
        ),
    ],
    model: Annotated[
        str,
        typer.Option("--model", "-m", help="Model to use for evaluation"),
    ] = "gpt-4o-mini",
    tournament_prompt: Annotated[
        str,
        typer.Option(
            help="The prompts to use for pairwise comparison.",
            click_type=cli.Choice(PAIRWISE_COMPARISON_PROMPTS),
        ),
    ] = "standard",
    metrics: Annotated[
        list[str] | None,
        typer.Option(
            "--metric",
            help="Metrics to evaluate in tournament",
            click_type=cli.Choice(TOURNAMENT_METRICS),
        ),
    ] = None,
    limit: Annotated[
        int,
        typer.Option("--limit", "-n", help="Number of papers to process per model"),
    ] = 10,
    seed: Annotated[
        int,
        typer.Option(help="Random seed for the tournament to ensure reproducibility"),
    ] = 0,
    algorithm: Annotated[
        RankingAlgorithm,
        typer.Option(
            "--algo",
            help="Ranking algorithm to use: 'elo' (single tournament) or 'melo' (10"
            " tournaments with different order)",
        ),
    ] = RankingAlgorithm.ELO,
    reuse_comparisons: Annotated[
        Path | None,
        typer.Option(
            "--reuse",
            help="Path to raw_comparisons.json file from a previous run to reuse. If"
            " provided, ignores input data and parameters.",
        ),
    ] = None,
    melo_trials: Annotated[
        int, typer.Option(help="If the algorithm is 'melo', how many trials to run.")
    ] = MELO_DEFAULT_TRIALS,
    show_head_to_head: Annotated[
        bool,
        typer.Option(
            "--head-to-head", help="Show head to head scores for all metrics."
        ),
    ] = False,
) -> None:
    """Run a pairwise tournament between multiple models.

    The tournament can use different ranking algorithms:
    - elo: Standard Elo rating system with a single ordering
    - melo: Multiple Elo tournaments with different random orderings. Set the number of
      trials with '--melo-trials'.
    - bradley_terry: Bradley-Terry model using maximum likelihood estimation to compute
      player strengths based on all match outcomes. Unlike Elo, this is not order-dependent
      and uses a global optimization approach.

    If you provide --reuse with a path to a raw_comparisons.json file, the system will
    skip the LLM comparison phase and just calculate rankings using the existing
    comparison data.
    """
    params = get_params()
    logger.info(render_params(params))

    tournament_metrics = metrics or list(TOURNAMENT_METRICS)

    dotenv.load_dotenv()
    output_dir.mkdir(parents=True, exist_ok=True)

    # Parse input files, types, and model names
    parsed_inputs: list[tuple[Path, InputFileType]] = []
    model_names: list[str] = []

    for input_str in inputs:
        match input_str.split(":", maxsplit=2):
            case [file_path_, file_type_, model_name]:
                file_path = Path(file_path_)
                file_type = InputFileType.from_dirty(file_type_)
            case [file_path_, file_type_]:
                file_path = Path(file_path_)
                file_type = InputFileType.from_dirty(file_type_)
                model_name = file_path.parent.name
            case [file_path_]:
                file_path = Path(file_path_)
                file_type = InputFileType.GRAPH
                model_name = file_path.parent.name
            case _:
                raise ValueError("Invalid input string format.")

        parsed_inputs.append((file_path, file_type))
        model_names.append(model_name)

    asyncio.run(
        run_tournaments(
            parsed_inputs,
            model_names,
            output_dir,
            model,
            tournament_prompt,
            tournament_metrics,
            limit,
            seed,
            algorithm,
            reuse_comparisons,
            melo_trials,
            show_head_to_head,
        )
    )


def _load_evaluation_input(
    file_path: Path, file_type: InputFileType
) -> Sequence[EvaluationInput]:
    match file_type:
        case InputFileType.GRAPH:
            data = PromptResult.unwrap(load_data(file_path, PromptResult[GraphResult]))
        case InputFileType.PAPER:
            data = PromptResult.unwrap(load_data(file_path, PromptResult[PaperResult]))
        case InputFileType.SUMM:
            data = PromptResult.unwrap(
                load_data(file_path, PromptResult[PaperWithRelatedSummary])
            )
        case InputFileType.RAW:
            data = load_data(file_path, pr.Paper)

    return data


class RawComparisonOutput(BaseModel):
    """Raw comparisons output for serialization."""

    model_config = ConfigDict(frozen=True)

    item_names: Sequence[str]
    metrics: Sequence[str]
    seed: int
    comparisons: Sequence[PromptResult[ComparisonResult]]
    metadata: Mapping[str, Any]


@dataclass(frozen=True, kw_only=True)
class CachedResult[T]:
    """Result of a GPT request and its full API cost."""

    result: T

    @property
    def cost(self) -> float:
        """The cost of a cached result is always -1 to signal it didn't cost anything."""
        return -1


async def _load_reused_comparisons(path: Path) -> CachedResult[RawComparisonOutput]:
    """Load comparison data from a previous run."""
    logger.info(f"Reusing comparison data from {path}")
    data = load_data_single(path, RawComparisonOutput)

    logger.info(
        f"Loaded {len(data.comparisons)} comparisons for {len(data.item_names)} models"
    )
    return CachedResult(result=data)


async def _generate_new_comparisons(
    client: LLMClient,
    inputs: Collection[tuple[Path, InputFileType]],
    model_names: Sequence[str],
    metrics: Sequence[str],
    limit: int,
    model: str,
    tournament_prompt_key: str,
    seed: int,
    algorithm: RankingAlgorithm,
) -> GPTResult[RawComparisonOutput]:
    """Generate new comparisons by running the LLM.

    Args:
        client: LLM client used to perform comparisons.
        inputs: List of (file_path, file_type) tuples.
        model_names: Names of the models.
        metrics: Metrics to evaluate.
        limit: Maximum number of papers to use.
        model: GPT model to use.
        tournament_prompt_key: Key for the comparison prompt.
        seed: Random seed.
        algorithm: Ranking algorithm to use.

    Returns:
        The full result from the comparisons.

    Raises:
        ValueError if there are no common papers to compare.
    """
    paper_collections = [
        _load_evaluation_input(file_path, file_type) for file_path, file_type in inputs
    ]
    common_papers = _find_common_papers(paper_collections)

    if not common_papers:
        raise ValueError(
            "No common papers found across all models. Tournament cannot proceed."
        )

    logger.info(
        f"Found {len(common_papers)} papers common to all {len(model_names)} models"
    )

    # Step 1: Run all pairwise comparisons
    prompt = PAIRWISE_COMPARISON_PROMPTS[tournament_prompt_key]
    paper_ids = sample(list(common_papers), limit)
    model_indices_pairs = _all_pairings(range(len(model_names)))

    with Timer() as comparison_timer:
        comparisons_result = await _run_all_comparisons(
            client,
            common_papers,
            metrics,
            model_names,
            model_indices_pairs,
            paper_ids,
            prompt,
        )

    logger.info(f"Comparisons time: {comparison_timer.human}")

    return comparisons_result.map(
        lambda cmp: RawComparisonOutput(
            comparisons=cmp,
            item_names=model_names,
            metrics=metrics,
            seed=seed,
            metadata={
                "model": model,
                "prompt": tournament_prompt_key,
                "paper_count": len(paper_ids),
                "algorithm": algorithm,
            },
        )
    )


async def run_tournaments(
    inputs: Collection[tuple[Path, InputFileType]],
    model_names: Sequence[str],
    output_dir: Path,
    model: str,
    tournament_prompt_key: str,
    metrics: Sequence[str],
    limit: int,
    seed: int,
    algorithm: RankingAlgorithm,
    reuse_comparisons_path: Path | None,
    melo_trials: int,
    show_head_to_head: bool,
) -> None:
    """Run the tournament on the given inputs.

    Args:
        inputs: List of (file_path, file_type) tuples.
        model_names: Names of the models.
        output_dir: Directory to save results.
        model: GPT model to use.
        tournament_prompt_key: Key for the comparison prompt.
        metrics: Metrics to evaluate.
        limit: Maximum number of papers to use.
        seed: Random seed.
        algorithm: Ranking algorithm to use (elo or melo).
        reuse_comparisons_path: Optional path to comparisons JSON file to reuse. If
            provided, other information (e.g. input data, model names, GPT model) is
            ignored.
        melo_trials: How many MElo trials to run.
        show_head_to_head: Show head to head scores for all metrics.
    """
    random.seed(seed)
    client = OpenAIClient(
        api_key=ensure_envvar("OPENAI_API_KEY"), model=model, seed=seed
    )

    # Step 1: Either load existing comparisons or generate new ones
    if reuse_comparisons_path is not None:
        raw_comparisons = await _load_reused_comparisons(reuse_comparisons_path)
    else:
        raw_comparisons = await _generate_new_comparisons(
            client,
            inputs,
            model_names,
            metrics,
            limit,
            model,
            tournament_prompt_key,
            seed,
            algorithm,
        )

    # Step 2: Display head-to-head results, then calculate rankings
    comparisons = PromptResult.unwrap(raw_comparisons.result.comparisons)

    if show_head_to_head:
        logger.info("\n%s", _display_head_to_head(comparisons, model_names, metrics))

    # Calculate rankings using the selected algorithm and report results
    with Timer() as ranking_timer:
        match algorithm:
            case RankingAlgorithm.ELO:
                ranker = _calculate_elo_rankings
            case RankingAlgorithm.MELO:
                ranker = partial(
                    _calculate_melo_rankings, seed=seed, num_trials=melo_trials
                )
            case RankingAlgorithm.BRADLEY_TERRY:
                ranker = _calculate_bradley_terry_rankings

        tournament_result = ranker(comparisons, model_names, metrics)
        summary = _tournament_summary(tournament_result, model_names, metrics)

    logger.info(f"Rankings calculation time: {ranking_timer.human}")
    logger.info(f"Total comparison cost: ${raw_comparisons.cost:.10f}")

    logger.info("\n%s", _display_tournament_results(summary))
    if isinstance(raw_comparisons, GPTResult):
        save_data(output_dir / "raw_comparisons.json", raw_comparisons.result)
    save_data(output_dir / f"tournament_results_{algorithm}.json", summary)


@app.callback()
def main() -> None:
    """Set up logging."""
    setup_logging()


@app.command(help="List available prompts.")
def prompts(
    detail: Annotated[
        bool, typer.Option(help="Show full description of the prompts.")
    ] = False,
) -> None:
    """Print the available prompt names, and optionally, the full prompt text."""
    print_prompts("RATIONALE TOURNAMENT", PAIRWISE_COMPARISON_PROMPTS, detail=detail)


if __name__ == "__main__":
    app()
