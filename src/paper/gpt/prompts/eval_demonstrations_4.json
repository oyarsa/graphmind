[
  {
    "title": "Automatic Annotation and Evaluation of Error Types for Grammatical Error Correction",
    "abstract": "Until now, error type performance for Grammatical Error Correction (GEC) systems could only be measured in terms of recall because system output is not annotated. In this paper, we overcome this problem by using a linguisticallyenhanced alignment to automatically extract the edits between parallel original and corrected sentences and then classify them using a new dataset-independent rule-based classifier. As human experts rated the predicted error types as “Good” or “Acceptable” in at least 95% of cases, we applied our approach to the system output produced in the CoNLL-2014 shared task to carry out a detailed analysis of system error type performance for the first time.",
    "text": "1 Introduction\nThe Conference on Natural Language Learning (CoNLL) shared task of 2014 (Ng et al., 2014) required teams to build systems that were capable of correcting all types of grammatical errors in learner text. While the submitted systems were evaluated against text that had been explicitly annotated with error type information, the teams themselves were not required to annotate their output in a similar way. This mismatch ultimately meant that a detailed error type analysis of each system was impossible and that error type performance could only be measured in terms of recall.\nThe main aim of this paper is to rectify this situation and provide a method by which unannotated error correction data can be automatically annotated with error type information. This is important because some systems may be more effective at correcting certain error types than oth-\ners, yet this information is otherwise concealed in an overall score. Although several new metrics and methodologies for Grammatical Error Correction (GEC) have been proposed since the end of the CoNLL-2014 shared task (Felice and Briscoe, 2015; Bryant and Ng, 2015; Napoles et al., 2015; Grundkiewicz et al., 2015), none of these are currently capable of producing individual error type scores.\nOur approach consists of two main steps. First, we automatically extract the edits between parallel original and corrected sentences by means of a linguistically-enhanced alignment algorithm (Felice et al., 2016), and second, we classify them according to a new rule-based framework specifically designed with error type evaluation in mind. This enables us to automatically annotate system hypothesis corrections with the same alignment and error type information as the reference and hence carry out a more detailed evaluation. The tool we use to do this will be released with this paper.\n\n2 Edit Extraction\nThe first stage of automatic annotation is edit extraction. Specifically, given an original and corrected sentence pair, we need to determine the start and end boundaries of any edits. This is fundamentally an alignment problem:\nThe first attempt at automatic edit extraction was made by Swanson and Yamangil (2012), who simply used the Levenshtein distance to align original and corrected sentence pairs. As the Levenshtein distance only aligns individual tokens how-\n2\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n112\n113\n114\n115\n116\n117\n118\n119\n120\n121\n122\n123\n124\n125\n126\n127\n128\n129\n130\n131\n132\n133\n134\n135\n136\n137\n138\n139\n140\n141\n142\n143\n144\n145\n146\n147\n148\n149\n150\n151\n152\n153\n154\n155\n156\n157\n158\n159\n160\n161\n162\n163\n164\n165\n166\n167\n168\n169\n170\n171\n172\n173\n174\n175\n176\n177\n178\n179\n180\n181\n182\n183\n184\n185\n186\n187\n188\n189\n190\n191\n192\n193\n194\n195\n196\n197\n198\n199\never, they also merged all adjacent non-matches in an effort to capture multi-token edits. Xue and Hwa (2014) subsequently improved on Swanson and Yamangil’s work by training a maximum entropy classifier to predict whether edits should be merged or not.\nMost recently, Felice et al. (2016) proposed a new method of edit extraction using a linguistically-enhanced alignment supported by a set of merging rules. In particular, they incorporated various linguistic information, such as partof-speech and lemma, into the cost function of the Damerau-Levenshtein1 algorithm to make it more likely that tokens with similar linguistic properties align. This approach ultimately proved most effective at approximating human edits in several datasets (80-85 F1), and so we use it in the present study.\n\n3 Automatic Error Typing\nHaving extracted the edits, the next step is to assign them error types. While Swanson and Yamangil (2012) did this by means of maximum entropy classifiers, one disadvantage of this approach is that such classifiers are biased towards their particular training corpora. In particular, the fact that different datasets are annotated according to different standards means that it is inappropriate to evaluate the predicted error types of an in-domain corpus against the predicted error types of an out-of-domain corpus (c.f. Xue and Hwa (2014)). Instead, a dataset-agnostic error type evaluation is much more desirable.\nTo solve this problem, we took inspiration from Swanson and Yamangil’s (2012) observation that most error types are based on part-of-speech (POS) categories, and wrote a rule to classify an edit based only on its automatic POS tags. We then added another rule to differentiate Missing, Unnecessary and Replacement errors depending on whether tokens were inserted, deleted or substituted. From there, we extended our approach to classify errors that are not well-characterised by POS alone (such as Spelling or Word Order) and ensured that all types are assigned based only on automatically obtained properties of the data.\nOne of the key strengths of this approach is that by being dependent only on automatic mark-up information, our classifier is entirely dataset in-\n1Damerau-Levenshtein is an extension of Levenshtein that also handles transpositions; e.g. AB→BA\ndependent and does not require labelled training data. This is in contrast with machine learning approaches which require different classifiers for different datasets and which ultimately may not be entirely compatible with each other. Instead, our approach is analogous to automating the annotation guidelines given to human annotators.\nA second significant advantage of our approach is that it is also always possible to determine precisely why an edit was assigned to a particular error category. In contrast, human and machine learning classification decisions are often less transparent and may furthermore be subject to annotator bias. Moreover, by being fully deterministic, our approach bypasses bias effects altogether and should hence be more consistent.\n\n3.1 Automatic Markup\nThe prerequisites for our rule-based classifier are that each token in both the original and corrected sentence is POS tagged, lemmatized, stemmed and dependency parsed. We use spaCy2 v1.6 for all but the stemming, which is performed by the Lancaster Stemmer in NLTK.3 Since fine-grained POS tags are often too detailed for the purposes of error evaluation, we also map spaCy’s Penn Treebank style tags to the coarser set of Universal Dependency tags.4 We use the latest Hunspell GB-large dictionary5 to help classify non-word errors. The marked-up tokens in an edit span are then input to our classifier and an error type is returned.\n\n3.2 Error Categories\nThe complete list of 25 error types in our new framework is shown in Table 2. Note that most of them can be prefixed with ‘M:’, ‘R:’ or ‘U:’, depending on whether they describe a Missing, Replacement, or Unnecessary edit, to enable evaluation at different levels of granularity (See Appendix A for all valid combinations). This means we can choose to evaluate, for example, only replacement errors (anything prefixed by ‘R:’), only noun errors (anything suffixed with ‘NOUN’) or only replacement noun errors (‘R:NOUN’). This flexibility allows us to make more detailed observations about different aspects of system perfor-\n2https://spacy.io/ 3http://www.nltk.org/ 4http://universaldependencies.org/tagset-conversion/\nen-penn-uposf.html 5https://sourceforge.net/projects/wordlist/files/speller/ 2016.11.20/\n3\n201\n202\n203\n204\n205\n206\n207\n208\n209\n210\n211\n212\n213\n214\n215\n216\n217\n218\n219\n220\n221\n222\n223\n224\n225\n226\n227\n228\n229\n230\n231\n232\n233\n234\n235\n236\n237\n238\n239\n240\n241\n242\n243\n244\n245\n246\n247\n248\n249\n278\n279\n280\n281\n282\n283\n284\n285\n286\n287\n288\n289\n290\n291\n292\n293\n294\n295\n296\n297\n298\n299\nCode Meaning Description / Example ADJ Adjective big → wide ADJ:FORM Adjective Form Comparative or superlative adjective errors.goodest → best, bigger → biggest, more easy → easier ADV Adverb speedily → quickly CONJ Conjunction and → but CONTR Contraction n’t → not DET Determiner the → a MORPH Morphology Tokens have the same lemma, but nothing else in common.quick (adj) → quickly (adv) NOUN Noun person → people NOUN:INFL Noun Inflection Count-mass noun errors.informations → information NOUN:NUM Noun Number cat → cats NOUN:POSS Noun Possessive friends → friend’s ORTH Orthography Case and/or whitespace errors.Bestfriend → best friend OTHER Other Errors that do not fall into any other category (e.g. paraphrasing).at his best → well, job → professional PART Particle (look) in → (look) at PREP Preposition of → at PRON Pronoun ours → ourselves PUNCT Punctuation ! → . SPELL Spelling genectic → genetic, color → colour UNK Unknown The annotator detected an error but was unable to correct it. VERB Verb ambulate → walk VERB:FORM Verb Form Infinitives (with or without “to”), gerunds (-ing) and participles.to eat → eating, dancing → danced VERB:INFL Verb Inflection Misapplication of tense morphology.getted → got, fliped → flipped VERB:SVA Subject-Verb Agreement (He) have → (He) has VERB:TENSE Verb Tense Includes inflectional and periphrastic tense, modal verbs and passivization.eats → ate, eats → has eaten, eats → can eat, eats → was eaten WO Word Order only can → can only\nmance. One caveat concerning error scheme design is that it is always possible to add new categories for increasingly detailed error types; for instance, we currently label [could → should] a tense error, when it might otherwise be considered a modal error. The reason we do not call it a modal error, however, is because it would then become less clear how to handle other cases such as [can → should] and [has eaten → should eat], which might be considered a more complex combination of a modal and tense error. As it is impractical to create new categories and rules to differentiate between such narrow distinctions however, our final framework aims to be a compromise between informativeness and practicality.\n\n3.3 Classifier Evaluation\nAs our new error scheme is based only on automatically obtained properties of the data, there are no gold standard labels against which to evaluate classifier performance. For this reason, we instead carried out a small-scale manual evaluation, where\nwe simply asked 5 GEC researchers to rate the appropriateness of the predicted error categories for 200 randomly chosen edits in context (100 from FCE-test (Yannakoudakis et al., 2011) and 100 from CoNLL-2014) as “Good”, “Acceptable” or “Bad”. “Good’ meant the chosen category was the most appropriate for the given edit, “Acceptable” meant the chosen category was appropriate, but probably not optimum, while “Bad” meant the chosen category was not appropriate for the edit. Raters were warned that edit boundaries had been determined automatically, and hence might be unusual, but that they should focus on the appropriateness of the error category regardless of whether they agreed with the boundary or not.\nThe result of this evaluation is shown in Table 3. Significantly, all 5 raters individually considered at least 95% of our rule-based error types to be either “Good” or “Acceptable”, despite the degree of noise introduced by automatic edit extraction. Furthermore, whenever raters judged an edit as “Bad”, this could usually be traced back to a mistake made by the POS tagger; e.g. [ring\n4\n301\n302\n303\n304\n305\n306\n307\n308\n309\n310\n311\n312\n313\n314\n315\n316\n317\n318\n319\n320\n321\n322\n323\n324\n325\n326\n327\n328\n329\n330\n331\n332\n333\n334\n335\n336\n337\n338\n339\n340\n341\n342\n343\n344\n345\n346\n347\n348\n349\n350\n351\n352\n353\n354\n355\n356\n357\n358\n359\n360\n361\n362\n363\n364\n365\n366\n367\n368\n369\n370\n371\n372\n373\n374\n375\n376\n377\n378\n379\n380\n381\n382\n383\n384\n385\n386\n387\n388\n389\n390\n391\n392\n393\n394\n395\n396\n397\n398\n399\nRater Good Acceptable Bad 1 92.0% 4.0% 4.0% 2 89.5% 6.5% 4.0% 3 83.0% 13.0% 4.0% 4 84.5% 11.0% 4.5% 5 82.5% 15.5% 2.0% OVERALL 86.3% 10.0% 3.7%\nTable 3: The percent distribution for how each expert rated the appropriateness of the predicted error types. E.g. Rater 3 considered 83% of all predicted types to be “Good”.\n→ rings] might be considered a NOUN:NUM or VERB:SVA error depending on whether the tagger considered both sides of the edit nouns or verbs. Inter-annotator agreement was good at 0.724 κfree (Randolph, 2005).\nIn contrast, the best results using a classifier were between 50-70 F1 (Felice et al., 2016). Although these results are incomparable with previous approaches which were evaluated using a different metric and error scheme, we nevertheless believe that the high scores awarded by the raters validate the efficacy of our rule-based approach.\n\n4 CoNLL-2014 Shared Task Analysis\nTo demonstrate the value of our approach, we applied our automatic annotation tool to the data produced in the CoNLL-2014 shared task (Ng et al., 2014). In particular, we used our tool to generate annotated versions of the system output files produced by each participating team.6 Although our approach can be applied to any dataset, we chose CoNLL-2014 because it constitutes the largest collection of publicly available GEC system output.\nOne benefit of explicitly annotating the hypothesis files is that it makes evaluation much more straightforward. Specifically, if both the hypothesis and reference files are annotated in the same format, we need only compare the edits in each file to produce an F-score. In particular, for a given sentence, any edit with the same span and correction in both files is a true positive (TP), while the remaining edits in the hypothesis are false positives (FP) and the remaining edits in the reference are false negatives (FN). This is in contrast with all other metrics in GEC, which typically incorporate some sort of edit extraction or alignment component directly into their evalua-\n6http://www.comp.nus.edu.sg/∼nlp/conll14st.html\nM2 Scorer Our Approach Team Gold Auto Gold Auto AMU 35.01 35.06 32.67 32.22 CAMB 37.33 37.32 34.92 33.99 CUUI 36.79 37.64 34.15 34.68 IITB 5.90 5.97 5.77 5.74 IPN 7.09 7.69 6.12 6.15 NTHU 29.92 29.85 26.74 25.74 PKU 25.32 25.40 23.95 23.62 POST 30.88 31.02 28.43 28.00 RAC 26.68 26.89 23.39 23.21 SJTU 15.19 15.24 15.15 14.90 UFC 7.84 7.90 7.97 7.90 UMC 25.37 25.46 23.77 23.53\nTable 4: Overall scores for each team in CoNLL2014 using gold and auto references with both the M2 scorer and our simpler edit comparison approach. All scores are in terms of F0.5.\ntion algorithms (Dahlmeier and Ng, 2012; Felice and Briscoe, 2015; Napoles et al., 2015). Our approach, on the other hand, treats edit extraction and evaluation as separate tasks.\n\n4.1 Gold Reference vs. Auto Reference\nBefore evaluating the newly annotated hypothesis files against the reference, we must also address another mismatch: namely that the hypothesis edits were aligned and classified automatically, while the reference edits were aligned and classified manually using a different framework. Since evaluation is now a straightforward comparison between two files however, it is especially important that both files are processed in the same way. For instance, a hypothesis edit [have eating → has eaten] will not match the reference edits [have → has] and [eating → eaten] because the former is one edit while the latter is two edits, even though they equate to the same thing.\nWe can solve this problem by reprocessing the reference file in the same way as the hypothesis file. This means all the reference edits are subject to the same alignment and classification criteria as the hypothesis edits. While it may seem unorthodox to discard gold reference information in favour of automatic reference information, Table 4 shows that this has no significant impact on the results when using either the M2 scorer, the de facto standard of GEC evaluation (Dahlmeier and Ng, 2012), or our own approach.\n5\n401\n402\n403\n404\n405\n406\n407\n408\n409\n410\n411\n412\n413\n414\n415\n416\n417\n418\n419\n420\n421\n422\n423\n424\n425\n426\n427\n428\n429\n430\n431\n432\n433\n434\n435\n436\n437\n438\n439\n440\n441\n442\n443\n444\n445\n446\n447\n448\n449\n450\n451\n452\n453\n454\n455\n456\n457\n458\n459\n460\n461\n462\n463\n464\n465\n466\n467\n468\n469\n470\n471\n472\n473\n474\n475\n476\n477\n478\n479\n480\n481\n482\n483\n484\n485\n486\n487\n488\n489\n490\n491\n492\n493\n494\n495\n496\n497\n498\n499\nAMU CAMB CUUI IITB Type P R F0.5 P R F0.5 P R F0.5 P R F0.5\nMissing 43.61 14.36 30.98 46.32 30.00 41.77 26.71 18.62 24.57 15.38 0.59 2.56 Replacement 37.19 26.90 34.54 37.37 28.07 35.05 45.78 22.89 38.15 29.85 1.49 6.22 Unnecessary - - - 25.51 27.59 25.90 34.20 33.91 34.14 46.15 1.55 6.83\nIPN NTHU PKU POST Type P R F0.5 P R F0.5 P R F0.5 P R F0.5\nMissing 2.86 0.29 1.04 35.34 11.60 25.08 33.33 4.37 14.34 31.14 13.13 24.44 Replacement 9.87 3.86 7.53 27.61 19.15 25.37 29.62 18.32 26.37 33.16 19.32 29.00 Unnecessary 0.00 0.00 0.00 34.57 16.17 28.16 0.00 0.00 0.00 26.32 33.12 27.44\nRAC SJTU UFC UMC Type P R F0.5 P R F0.5 P R F0.5 P R F0.5\nMissing 1.49 0.27 0.78 62.50 4.42 17.24 - - - 40.08 23.57 35.16 Replacement 29.50 20.87 27.25 50.54 3.43 13.47 72.00 2.64 11.52 34.62 9.69 22.87 Unnecessary 0.00 0.00 0.00 17.65 11.51 15.95 - - - 16.89 17.33 16.98\nTable 5: Precision, recall and F0.5 for Missing, Unnecessary, and Replacement errors for each team. A dash indicates the team’s system did not attempt to correct the given error type (TP+FP = 0).\nWe validated this hypothesis for each team by means of bootstrap significance testing (Efron and Tibshirani, 1993) and found no statistically significant difference between auto and gold references (1,000 iterations, p > .05). This leads us to conclude that our auto annotations are qualitatively as good as human annotations.\nTable 4 also shows that M2 scores tend to be higher than our own, which initially led us to believe that our approach was underestimating performance. We subsequently found, however, that the M2 scorer in fact tends to overestimate performance (c.f. Felice and Briscoe (2015) and Napoles et al. (2015)).\nIn particular, given a choice between matching [have eating → has eaten] from Annotator 1 or [have → has] and [eating → eaten] from Annotator 2, the M2 scorer will always choose Annotator 2 because two true positives (TP) are worth more than one. Similarly, whenever the scorer encounters two false positives (FP) within a certain distance of each other,7 it merges them and treats them as one false positive; e.g. [is a cat → are a cats] is selected over [is → are] and [cat → cats] even though these edits are best handled separately. Ultimately, it can be said that the M2 scorer exploits its dynamic edit boundary prediction in order to maximise true positives and minimise false positives and hence produces slightly inflated scores.\n7The distance is controlled by the max unchanged words parameter which is set to 2 by default.\n\n4.2 Operation Tier\nIn our first category experiment, we simply investigated the performance of each system in terms of Unnecessary, Missing or Replacement edits. The results are shown in Table 5.\nThe most surprising result is that 5 teams failed to correctly resolve any unnecessary token errors at all (AMU, IPN, PKU, RAC, UFC). This is especially surprising given that we would expect unnecessary token errors to be easier to correct than others; a system need only detect and delete without having to propose any alternative. There is also no obvious explanation as to why these teams had difficulty with this error type because each of them employed different combinations of correction strategies including machine translation (MT), language modelling, classifiers and rules.\nIn contrast, CUUI’s classifier approach (Rozovskaya et al., 2014) was the most successful at correcting not only unnecessary token errors, but also replacement token errors, while CAMB’s hybrid MT approach (Felice et al., 2014) significantly outperformed all others in terms of missing token errors. It would hence make sense to combine these two approaches, and indeed recent research has shown this improves overall performance (Rozovskaya and Roth, 2016).\n\n4.3 General Error Types\nTable 6 shows precision, recall and F0.5 for each of the error types in our proposed framework for each team in CoNLL-2014. We refer the reader to the shared task paper for more information about\n6\n501\n502\n503\n504\n505\n506\n507\n508\n509\n510\n511\n512\n513\n514\n515\n516\n517\n518\n519\n520\n521\n522\n523\n524\n525\n526\n527\n528\n529\n530\n531\n532\n533\n534\n535\n536\n537\n538\n539\n540\n541\n542\n543\n544\n545\n546\n547\n548\n549\n550\n551\n552\n553\n554\n555\n556\n557\n558\n559\n560\n561\n562\n563\n564\n565\n566\n567\n568\n569\n570\n571\n572\n573\n574\n575\n576\n577\n578\n579\n580\n581\n582\n583\n584\n585\n586\n587\n588\n589\n590\n591\n592\n593\n594\n595\n596\n597\n598\n599\nAMU CAMB CUUI IITB IPN NTHU PKU POST RAC SJTU UFC UMC\nADJ P 7.14 9.09 - 0.00 0.00 0.00 50.00 0.00 11.11 0.00 - 4.35 R 9.38 12.82 - 0.00 0.00 0.00 6.67 0.00 3.33 0.00 - 3.57\nF0.5 7.50 9.65 - 0.00 0.00 0.00 21.74 0.00 7.58 0.00 - 4.17\nADJ:FORM P 60.00 75.00 100.00 100.00 0.00 33.33 100.00 50.00 11.54 - - 80.00 R 60.00 50.00 27.27 33.33 0.00 33.33 33.33 11.11 42.86 - - 57.14\nF0.5 60.00 68.18 65.22 71.43 0.00 33.33 71.43 29.41 13.51 - - 74.07\nADV P 11.76 12.66 0.00 0.00 0.00 0.00 0.00 - 0.00 4.76 - 7.27 R 5.88 23.26 0.00 0.00 0.00 0.00 0.00 - 0.00 3.03 - 10.00\nF0.5 9.80 13.93 0.00 0.00 0.00 0.00 0.00 - 0.00 4.27 - 7.69\nCONJ P 6.25 0.00 - - 0.00 0.00 - 0.00 - 0.00 - 0.00 R 7.69 0.00 - - 0.00 0.00 - 0.00 - 0.00 - 0.00\nF0.5 6.49 0.00 - - 0.00 0.00 - 0.00 - 0.00 - 0.00\nCONTR P 29.17 40.00 46.15 - 0.00 - - 33.33 0.00 66.67 - 28.57 R 87.50 28.57 75.00 - 0.00 - - 50.00 0.00 33.33 - 28.57\nF0.5 33.65 37.04 50.00 - 0.00 - - 35.71 0.00 55.56 - 28.57\nDET P 33.55 36.44 30.92 21.43 0.00 35.91 29.35 26.12 0.00 43.88 - 36.36 R 14.13 43.17 52.03 0.92 0.00 28.53 7.87 49.41 0.00 12.57 - 23.72\nF0.5 26.32 37.61 33.65 3.93 0.00 34.14 18.99 28.84 0.00 29.29 - 32.86\nMORPH P 54.67 57.35 52.94 15.38 2.25 25.00 21.43 30.30 27.27 50.00 36.36 34.48 R 45.56 45.35 20.00 2.70 2.78 20.25 32.14 12.82 15.19 2.74 5.06 11.63\nF0.5 52.56 54.47 39.82 7.94 2.34 23.88 22.96 23.81 23.53 11.24 16.26 24.75\nNOUN P 25.35 28.42 0.00 25.00 8.33 0.00 3.45 10.00 30.43 0.00 - 28.57 R 15.52 22.13 0.00 2.22 4.30 0.00 0.96 1.90 6.54 0.00 - 10.00\nF0.5 22.50 26.89 0.00 8.20 7.02 0.00 2.27 5.41 17.59 0.00 - 20.83\nNOUN:INFL P 55.56 60.00 50.00 - 0.00 100.00 57.14 80.00 60.00 0.00 - - R 83.33 75.00 66.67 - 0.00 40.00 57.14 66.67 60.00 0.00 - -\nF0.5 59.52 62.50 52.63 - 0.00 76.92 57.14 76.92 60.00 0.00 - -\nNOUN:NUM P 48.24 42.59 43.57 43.75 12.84 44.05 28.92 30.52 27.72 54.29 - 44.93 R 55.66 53.00 59.91 3.95 10.05 48.54 42.34 56.54 35.92 10.50 - 17.32\nF0.5 49.56 44.33 46.09 14.52 12.16 44.88 30.88 33.62 29.04 29.60 - 34.07\nNOUN:POSS P 20.00 66.67 - - - - 20.00 0.00 0.00 25.00 - 50.00 R 14.29 10.53 - - - - 5.26 0.00 0.00 4.55 - 5.00\nF0.5 18.52 32.26 - - - - 12.82 0.00 0.00 13.16 - 17.86\nORTH P 60.00 66.67 73.81 - 3.45 0.00 28.57 49.32 16.67 - - 50.00 R 11.11 40.00 59.62 - 4.55 0.00 6.90 64.29 49.12 - - 17.24\nF0.5 31.91 58.82 70.45 - 3.62 0.00 17.54 51.72 19.20 - - 36.23\nOTHER P 19.33 23.57 16.13 12.50 2.38 1.40 11.11 13.95 0.00 0.00 - 13.54 R 6.65 9.87 1.37 0.30 0.31 0.58 0.58 1.69 0.00 0.00 - 3.74\nF0.5 13.99 18.44 5.12 1.39 1.02 1.09 2.40 5.70 0.00 0.00 - 8.88\nPART P 71.43 26.67 0.00 - - 12.90 - - - 40.00 - 18.18 R 20.00 16.00 0.00 - - 16.00 - - - 9.09 - 10.00\nF0.5 47.17 23.53 0.00 - - 13.42 - - - 23.81 - 15.63\nPREP P 47.62 41.70 32.69 75.00 0.00 10.95 - 21.74 0.00 37.50 - 20.53 R 16.53 35.91 13.65 1.44 0.00 12.81 - 2.18 0.00 7.18 - 13.42\nF0.5 34.60 40.40 25.56 6.67 0.00 11.28 - 7.79 0.00 20.33 - 18.56\nPRON P 43.75 20.00 0.00 0.00 11.11 50.00 100.00 27.27 4.76 0.00 - 22.45 R 9.72 13.25 0.00 0.00 1.72 2.86 1.56 4.76 1.54 0.00 - 14.10\nF0.5 25.74 18.15 0.00 0.00 5.32 11.63 7.35 14.02 3.36 0.00 - 20.07\nPUNCT P 25.00 60.47 39.53 100.00 0.00 48.28 - 27.27 0.00 5.00 - 43.02 R 3.57 15.66 11.33 1.85 0.00 9.72 - 6.34 0.00 0.97 - 23.13\nF0.5 11.36 38.46 26.40 8.62 0.00 26.92 - 16.42 0.00 2.73 - 36.71\nSPELL P 77.78 78.43 50.00 0.00 30.77 0.00 44.58 68.27 74.60 - - 100.00 R 64.95 42.55 2.60 0.00 5.41 0.00 71.15 70.30 86.24 - - 1.32\nF0.5 74.82 67.11 10.75 0.00 15.87 0.00 48.18 68.67 76.67 - - 6.25\nVERB P 18.84 16.09 - 0.00 7.69 0.00 14.29 0.00 0.00 0.00 - 18.87 R 8.07 8.86 - 0.00 0.71 0.00 0.68 0.00 0.00 0.00 - 6.58\nF0.5 14.87 13.83 - 0.00 2.60 0.00 2.87 0.00 0.00 0.00 - 13.74\nVERB:FORM P 34.85 38.24 70.59 50.00 8.77 36.84 30.77 20.00 35.42 30.77 100.00 34.04 R 23.71 26.26 26.37 1.15 5.75 36.84 35.16 3.45 34.69 4.65 1.22 18.18\nF0.5 31.86 35.04 52.86 5.26 7.94 36.84 31.56 10.20 35.27 14.49 5.81 28.99\nVERB:INFL P 100.00 100.00 - - 100.00 100.00 50.00 100.00 100.00 - 0.00 - R 100.00 100.00 - - 50.00 50.00 50.00 50.00 100.00 - 0.00 -\nF0.5 100.00 100.00 - - 83.33 83.33 50.00 83.33 100.00 - 0.00 -\nVERB:SVA P 49.06 42.68 54.71 50.00 24.53 50.58 57.14 33.33 34.83 59.09 82.86 58.33 R 27.37 31.82 70.45 1.15 13.98 66.92 17.20 16.67 31.00 14.13 28.16 14.29\nF0.5 42.35 39.95 57.27 5.26 21.31 53.18 39.02 27.78 33.99 36.11 59.67 36.08\nVERB:TENSE P 20.55 26.05 75.00 66.67 7.14 38.89 10.61 20.00 23.27 15.38 100.00 30.51 R 8.82 17.92 5.33 1.27 1.24 4.22 4.35 2.34 21.26 2.52 1.26 10.98\nF0.5 16.23 23.88 20.74 5.92 3.66 14.71 8.24 7.97 22.84 7.60 5.99 22.50\nWO P - 38.89 0.00 66.67 - - - 0.00 0.00 - - 41.18 R - 33.33 0.00 14.29 - - - 0.00 0.00 - - 35.00\nF0.5 - 37.63 0.00 38.46 - - - 0.00 0.00 - - 39.77\nTable 6: Precision, recall and F0.5 for each team and error type. A dash indicates the team’s system did not attempt to correct the given error type (TP+FP = 0). The highest F-score for each type is highlighted.\n7\n601\n602\n603\n604\n605\n606\n607\n608\n609\n610\n611\n612\n613\n614\n615\n616\n617\n618\n619\n620\n621\n622\n623\n624\n625\n626\n627\n628\n629\n630\n631\n632\n633\n634\n635\n636\n637\n638\n639\n640\n641\n642\n643\n644\n645\n646\n647\n648\n649\n663\n664\n665\n666\n667\n668\n669\n670\n671\n672\n673\n674\n675\n676\n677\n678\n679\n680\n681\n682\n683\n684\n685\n686\n687\n688\n689\n690\n691\n692\n693\n694\n695\n696\n697\n698\n699\neach team’s system (Ng et al., 2014).\nOverall, CAMB was the most successful team in terms of error types, achieving the highest Fscore in 10 (out of 24) error categories, followed by AMU (Junczys-Dowmunt and Grundkiewicz, 2014), who scored highest in 6 categories. All but 2 teams (IITB and IPN) achieved the best score in at least 1 category, which suggests that different approaches to GEC complement different error types. Only CAMB attempted to correct at least 1 error from every category.\nRegarding individual error categories: PKU’s language model approach significantly outperformed all others in handling ADJ errors (21.74 F0.5). ADV and CONJ errors proved extremely difficult for all teams, with the best results at 13.93 F0.5 (CAMB) and 6.49 F0.5 (AMU) respectively. Although several teams built specialist classifiers for DET errors, CAMB’s hybrid MT system still slightly outperformed them (37.61 F0.5). MT approaches were most effective at correcting NOUN errors (AMU, CAMB, UMC), while fairly high scores for NOUN:NUM errors showed that this category could be successfully handled by MT (AMU, CAMB), classifiers (CUUI) or language model approaches (NTHU). Few teams attempted to correct NOUN:POSS errors, but CAMB’s system handled them the best (32.26 F0.5). CUUI’s classifier for ORTH errors significantly outperformed all other teams at 70.45 F0.5. As with DET errors, several teams employed specialist classifiers to tackle PREP errors, but CAMB’s hybrid system still worked best overall (40.40 F0.5). AMU’s MT system was most successful at correcting PRON errors (25.74 F0.5), while CAMB was most successful at correcting PUNCT errors (38.46 F0.5). Although spell checkers are widespread nowadays, many teams did not seem to employ them; this would have been an easy way to boost overall performance. CUUI’s classifier approach to VERB:FORM errors significantly outperformed other approaches (52.86 F0.5), suggesting a classifier is well-suited to this category. While UFC’s rule-based approach achieved the highest score for VERB:SVA errors (59.67 F0.5), it is worth noting that CUUI’s classifier approach was not far behind (57.27 F0.5). Finally, only 3 teams were successful at handling WO errors (CAMB, IITB and UMC), all of whom achieved similar scores of just under 40 F0.5 using MT.\n\n4.4 Detailed Error Types\nIn addition to analysing general error types, the modular design of our framework also allows us to evaluate error type performance at an even greater level of detail. For example, Table 7 shows the breakdown of Determiner errors for two teams using different approaches in terms of edit operation. Note that this is a representative example of detailed error type performance as an analysis of all error type combinations for all teams would take up too much space.\nWhile CAMB’s hybrid MT approach achieved a higher score than CUUI’s classifier approach overall (37.61 F0.5 vs. 33.65 F0.5), our more detailed evaluation reveals that actually CUUI’s approach performed better at Replacement Determiner errors than CAMB (26.53 F0.5 vs. 21.39 F0.5). This shows that even though one approach might be better than another overall, other approaches may still have complementary strengths. In fact the main weakness of CUUI’s classifier seems to be that a high recall for missing and unnecessary determiners is counterbalanced by a low precision, which suggests that minimising false positives in these categories is the most obvious avenue for improvement.\n\n4.5 Multi Token Errors\nAnother benefit of explicitly annotating all hypothesis edits is that edit spans become fixed; this means we can evaluate system performance in terms of edit size. Table 8 hence shows the overall performance for each team at correcting multitoken edits, where a multi-token edit is an edit that affects at least two tokens on either the source or\n8\n701\n702\n703\n704\n705\n706\n707\n708\n709\n710\n711\n712\n713\n714\n715\n716\n717\n718\n719\n720\n721\n722\n723\n724\n725\n726\n727\n728\n729\n730\n731\n732\n733\n734\n735\n736\n737\n738\n739\n740\n741\n742\n743\n744\n745\n746\n747\n748\n749\n767\n768\n769\n770\n771\n772\n773\n774\n775\n776\n777\n778\n779\n780\n781\n782\n783\n784\n785\n786\n787\n788\n789\n790\n791\n792\n793\n794\n795\n796\n797\n798\n799\nTeam P R F0.5 AMU 17.14 5.33 11.88 CAMB 27.22 17.06 24.32 CUUI 15.69 3.67 9.48 IITB 28.57 0.94 4.15 IPN 3.33 0.47 1.51 NTHU 0.00 0.00 0.00 PKU 25.00 1.40 5.73 POST 12.77 2.82 7.48 RAC 2.96 2.82 2.93 SJTU 10.00 0.47 1.99 UFC - - - UMC 19.82 9.82 16.47\nTable 8: Each team’s performance at correcting multi-token edits; i.e. there are at least 2 tokens on one side of the edit.\ntarget side. In the CoNLL-2014 test set, there are roughly 220 such edits (about 10% of all edits).\nIn general, teams did not do well at multi-token edits. In fact only three teams achieved scores greater than 10 F0.5 and all of them used MT (AMU, CAMB, UMC). This is significant because recent work has suggested that the main goal of GEC should be to produce fluent-sounding, rather than just grammatical, sentences, even though this often requires complex multi-token edits (Sakaguchi et al., 2016). If no system is particularly adept at correcting multi-token errors however, robust fluency correction will likely require more sophisticated methods than are currently available.\n\n4.6 Detection vs. Correction\nAnother important aspect of GEC that is less frequently reported in the literature is that of error detection; i.e. the extent to which a system can identify erroneous tokens in text. This can be calculated by comparing the edit overlap between the hypothesis and reference files regardless of the proposed correction in a manner similar to Recognition evaluation in the HOO shared tasks for GEC (Dale and Kilgarriff, 2011).\nFigure 1 hence shows how each team’s score for detection differed in relation to their score for correction. While CAMB still scored highest for detection overall, it is interesting to note that the difference between the 2nd and 3rd place contenders (CUUI and AMU) is a lot narrower. This suggests that even though AMU detected roughly as many errors as CUUI, it was less successful at correcting them. IPN and PKU are also notable for detecting\nmany more errors than they were able to correct. Although we do not do so here, our scorer is also capable of providing a detailed error type breakdown for detection.\n\n5 Conclusion\nIn this paper, we have described a method to automatically annotate parallel error correction data with explicit edit spans and error type information. This can be used to standardise existing error correction corpora or facilitate a detailed error type evaluation. The tool we use to do this will be released with this paper.\nOur approach makes use of previous work to align sentences based on linguistic intuition and then introduces a new rule-based framework to classify edits. This framework is entirely dataset independent, and relies only on automatically obtained information such as POS tags and lemmas. A small-scale evaluation of our classifier found that each rater considered >95% of the predicted error types as either “Good” (85%) or “Acceptable” (10%).\nWe demonstrated the value of our approach by carrying out a detailed evaluation of system error type performance for the first time for all teams in the CoNLL-2014 shared task on Grammatical Error Correction. We found that different systems had different strengths and weaknesses which we hope researchers can exploit to further improve general performance.\n9\n801\n802\n803\n804\n805\n806\n807\n808\n809\n810\n811\n812\n813\n814\n815\n816\n817\n818\n819\n820\n821\n822\n823\n824\n825\n826\n827\n828\n829\n830\n831\n832\n833\n834\n835\n836\n837\n838\n839\n840\n841\n842\n843\n844\n845\n846\n847\n848\n849\n850\n851\n852\n853\n854\n855\n856\n857\n858\n859\n860\n861\n862\n863\n864\n865\n866\n867\n868\n869\n870\n871\n872\n873\n874\n875\n876\n877\n878\n879\n880\n881\n882\n883\n884\n885\n886\n887\n888\n889\n890\n891\n892\n893\n894\n895\n896\n897\n898\n899\n",
    "rationale": "- Strengths: Useful application for teachers and learners; supports\nfine-grained comparison of GEC systems.\n\n- Weaknesses: Highly superficial description of the system; evaluation not\nsatisfying.\n\n- General Discussion:\n\nThe paper presents an approach of automatically enriching the output of GEC\nsystems with error types. This is a very useful application because both\nteachers and learners can benefit from this information (and many GEC systems\nonly output a corrected version, without making the type of error explicit). It\nalso allows for finer-grained comparison of GEC systems, in terms of precision\nin general, and error type-specific figures for recall and precision.\n\nUnfortunately, the description of the system remains highly superficial. The\ncore of the system consists of a set of (manually?) created rules but the paper\ndoes not provide any details about these rules. The authors should, e.g., show\nsome examples of such rules, specify the number of rules, tell us how complex\nthey are, how they are ordered (could some early rule block the application of\na later rule?), etc. -- Instead of presenting relevant details of the system,\nseveral pages of the paper are devoted to an evaluation of the systems that\nparticipated in CoNLL-2014. Table 6 (which takes one entire page) list results\nfor all systems, and the text repeats many facts and figures that can be read\noff the table. \n\nThe evaluation of the proposed system is not satisfying in several aspects. \nFirst, the annotators should have independently annotated a gold standard for\nthe 200 test sentences instead of simply rating the output of the system. Given\na fixed set of tags, it should be possible to produce a gold standard for the\nrather small set of test sentences. It is highly probable that the approach\ntaken in the paper yields considerably better ratings for the annotations than\ncomparison with a real gold standard (see, e.g., Marcus et al. (1993) for a\ncomparison of agreement when reviewing pre-annotated data vs. annotating from\nscratch). \nSecond, it is said that \"all 5 raters individually considered at least 95% of\nour rule-based error types to be either “Good” or “Acceptable”\".\nMultiple rates should not be considered individually and their ratings averaged\nthis way, this is not common practice. If each of the \"bad\" scores were\nassigned to different edits (we don't learn about their distribution from the\npaper), 18.5% of the edits were considered \"bad\" by some annotator -- this\nsounds much worse than the average 3.7%, as calculated in the paper.\nThird, no information about the test data is provided, e.g. how many error\ncategories they contain, or which error categories are covered (according to\nthe cateogories rated as \"good\" by the annotators).\nForth, what does it mean that \"edit boundaries might be unusual\"? A more\nprecise description plus examples are at need here. Could this be problematic\nfor the application of the system?\n\nThe authors state that their system is less domain dependent as compared to\nsystems that need training data. I'm not sure that this is true. E.g., I\nsuppose that Hunspell's vocabulary probably doesn't cover all domains in the\nsame detail, and manually-created rules can be domain-dependent as well -- and\nare completely language dependent, a clear drawback as compared to machine\nlearning approaches. Moreover, the test data used here (FCE-test, CoNLL-2014)\nare from one domain only: student essays.\n\nIt remains unclear why a new set of error categories was designed. One reason\nfor the tags is given: to be able to search easily for underspecified\ncategories (like \"NOUN\" in general). It seems to me that the tagset presented\nin Nicholls (2003) supports such searches as well. Or why not using the\nCoNLL-2014 tagset? Then the CoNLL gold standard could have been used for\nevaluation.\n\nTo sum up, the main motivation of the paper remains somewhat unclear. Is it\nabout a new system? But the most important details of it are left out. Is it\nabout a new set of error categories? But hardly any motivation or discussion of\nit is provided. Is it about evaluating the CoNLL-2014 systems? But the\npresentation of the results remains superficial.\n\nTypos:\n- l129 (and others): c.f. -> cf.\n- l366 (and others): M2 -> M^2 (= superscribed 2)\n- l319: 50-70 F1: what does this mean? 50-70%?\n\nCheck references for incorrect case\n- e.g. l908: esl -> ESL\n- e.g. l878/79: fleiss, kappa",
    "rating": 3
  },
  {
    "title": "Learning Cognitive Features from Gaze Data for Sentiment and Sarcasm Classification using Convolutional Neural Network",
    "abstract": "Cognitive NLP systemsi.e., NLP systems that make use of behavioral data augment traditional text based features with cognitive features extracted from eye-movement patterns, EEG signals, brain-imaging etc.. Such extraction of features is typically manual. We contend that manual extraction of features is not good enough to tackle text subtleties that characteristically prevail in complex classification tasks like sentiment analysis and sarcasm detection, and that even the extraction and choice of features should be delegated to the learning system. We introduce a framework to automatically extract cognitive features from the eye-movement / gaze data of human readers reading the text and use them as features along with textual features for the tasks of sentiment polarity and sarcasm detection. Our proposed framework is based on Convolutional Neural Network (CNN). The CNN learns features from both gaze and text and uses them to classify the input text. We test our technique on published sentiment and sarcasm labeled datasets, enriched with gaze information, to show that using a combination of automatically learned text and gaze features yields better classification performance over (i) CNN based systems that rely on text input alone and (ii) existing systems that rely on handcrafted gaze and textual features.",
    "text": "1 Introduction\nDetection of sentiment and sarcasm in usergenerated short reviews is of primary importance for social media analysis, recommendation and dialog systems. Traditional sentiment analyzers and\nsarcasm detectors face challenges that arise at lexical, syntactic, semantic and pragmatic levels (Liu and Zhang, 2012; Mishra et al., 2016c). Featurebased systems (Akkaya et al., 2009; Sharma and Bhattacharyya, 2013; Poria et al., 2014) can aptly handle lexical and syntactic challenges (e.g. learning that the word deadly conveys a strong positive sentiment in opinions such as Shane Warne is a deadly bowler, as opposed to The high altitude Himalayan roads have deadly turns). It is, however, extremely difficult to tackle subtleties at semantic and pragmatic levels. For example, the sentence “I really love my job. I work 40 hours a week to be this poor.” requires an NLP system to be able to understand that the opinion holder has not expressed a positive sentiment towards her / his job. In the absence of explicit clues in the text, it is difficult for automatic systems to arrive at a correct classification decision, as they often lack external knowledge about various aspects of the text being classified.\nMishra et al. (2016b) and Mishra et al. (2016c) show that NLP systems based on cognitive data (or simply, Cognitive NLP systems) , that of leverage eye-movement information obtained from human readers, can tackle the semantic and pragmatic challenges better. The hypothesis here is that human gaze activities are related to the cognitive processes in the brain, that combines the “external knowledge” that the a reader possesses with textual clues that she / he perceives. While incorporating behavioral information obtained from gaze-data in NLP systems is intriguing and quite plausible, especially due to the availability of low cost eye-tracking machinery (Wood and Bulling, 2014; Yamamoto et al., 2013), few methods exist for text classification and they rely on handcrafted features extracted from gaze data (Mishra et al., 2016b,c). These systems have limited capabilities due to two reasons: (a) Manually designed gaze based features may not adequately capture all\n2\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n112\n113\n114\n115\n116\n117\n118\n119\n120\n121\n122\n123\n124\n125\n126\n127\n128\n129\n130\n131\n132\n133\n134\n135\n136\n137\n138\n139\n140\n141\n142\n143\n144\n145\n146\n147\n148\n149\n162\n163\n164\n165\n179\n180\n181\n182\n183\n184\n185\n186\n187\n188\n189\n190\n191\n192\n193\n194\n195\n196\n197\n198\n199\nforms of textual subtleties (b) Eye-movement data is not as intuitive to analyze as text which makes the task of designing manual features more difficult. So, in this work, instead of handcrafting the gaze based and textual features, we try to learn feature representations from both gaze and textual data using Convolutional Neural Networks (CNNs). We test our technique on two publicly available datasets enriched with eye-movement information, used for binary classification tasks of sentiment polarity and sarcasm detection. Our experiments show that the automatically extracted features help to achieve significant classification performance improvement over (a) existing systems that rely on handcrafted gaze and textual features and (b) CNN based systems that rely on text input alone.\nThe rest of the paper is organized as follows. Section 2 discusses the motivation behind using readers’ eye-movement data in a text classification setting. Section 3 discusses on why CNNs is preferred over other available alternatives for feature extraction. The CNN architecture is proposed and discussed in Section 4. Section 5 describes our experimental setup and results are discussed in Section 6. We provide a detailed analysis of the results along with some insightful observations in Section 7. Section 8 points to relevant literature followed by Section 9 that concludes the paper. Terminology: A fixation is a relatively long stay of gaze on a visual object (such as words in text) where as a sacccade corresponds to quick shifting of gaze between two positions of rest. Forward and backward saccades are called progressions and regressions respectively. A scanpath is a line graph that contains fixations as nodes and saccades as edges.\n\n2 Eye-movement and Linguistic Subtleties\nPresence of linguistic subtleties often induces (a) surprisal (Kutas and Hillyard, 1980; Malsburg et al., 2015), due to the underlying disparity /context incongruity or (b) higher cognitive load (Rayner and Duffy, 1986), due to the presence of lexically and syntactically complex structures. While surprisal accounts for irregular saccades (Malsburg et al., 2015), higher cognitive load results in longer fixation duration (Kliegl et al., 2004).\nMishra et al. (2016b) find that presence of sarcasm in text triggers either irregular saccadic patterns or unusually high duration fixations than non-sarcastic texts (illustrated through example scanpath representations in Figure 1). For sentiment bearing texts, highly subtle eye-movement patterns are observed for semantically / pragmatically complex negative opinions (expressing irony, sarcasm, thwarted expectations etc.) than the simple ones (Mishra et al., 2016b). The association between linguistic subtleties and eye-movement patterns could be captured through sophisticated feature engineering that considers both gaze and text inputs. In our work, CNNs take the onus of feature engineering.\n3\n201\n202\n203\n204\n205\n206\n207\n208\n209\n210\n211\n212\n213\n214\n215\n216\n217\n218\n219\n220\n221\n222\n223\n224\n225\n226\n227\n228\n229\n230\n231\n232\n233\n234\n235\n236\n237\n238\n239\n240\n241\n242\n243\n244\n245\n246\n247\n248\n249\n250\n251\n252\n253\n254\n255\n256\n257\n258\n259\n260\n261\n262\n263\n264\n265\n266\n267\n268\n269\n270\n271\n272\n273\n274\n275\n276\n277\n278\n279\n280\n281\n282\n283\n284\n285\n286\n287\n288\n289\n290\n291\n292\n293\n294\n295\n296\n297\n298\n299\nT ex\nt C\nom p\non en\nt\nNon-static\nStatic\nSaccade\nFixation\nG az\ne C\nom p\non en t P1 P2 P3 P4 P5 P6 P7 P8\nN×K representation of sentences with static and non static channels\nP×G representation of sentences\nwith fixation and saccade channels\n1-D convolution operation with multiple filter width\nand feature maps\n2-D convolution operation with multiple filter row and\nColumn widths\nMax-pooling for each filter width\nMax-pooling over\nmultiple dimensions for\nmultiple filter widths\nFully connected with dropouts and softmax\noutput\nMerged pooled values\nFigure 3: Deep convolutional model for feature extraction from both text and gaze inputs\n\n3 Why Convolutional Neural Network?\nCNNs have been quite effective in learning filters for image processing tasks, filters being used to transform the input image into more informative feature space (Krizhevsky et al., 2012). Filters learned at various CNN layers are quite similar to handcrafted filters used for detection of edges, contours and removal of redundant backgrounds. We believe, a similar technique can also be applied to eye-movement data, where the learned filters will, hopefully, extract informative cognitive features. For instance, for sarcasm, we expect the network to learn filters that detect long distance saccades (refer to Figure 2 for an analogical illustration). With more number of convolution filters of different dimensions, the network may extract multiple features related to different gaze attributes (such as fixations, progressions, regressions and skips) and will be free from any form of human bias that manually extracted features are susceptible to.\n\n4 Learning Feature Representations: The CNN architecture\nFigure 3 shows the CNN architecture with two components for processing and extracting features from text and gaze inputs. The components are explained below.\n\n4.1 Text Component\nThe text component is quite similar to the one proposed by Kim (2014) for sentence classification.\nWords (in the form of one-hot representation) in the input text are first replaced by their embeddings of dimension K (ith word in the sentence represented by an embedding vector xi ∈ RK). As per Kim (2014), a multi-channel variant of CNN (referred to as MULTICHANNELTEXT) can be implemented by using two channels of embeddingsone that remains static through out training (referred to as STATICTEXT), and the other one that gets updated during training (referred to as NONSTATICTEXT). We separately experiment with static, non-static and multi-channel variants.\nFor each possible input channel of the text component, a given text is transformed into a tensor of fixed length N (padded with zero-tensors wherever necessary to tackle length variations) by concatenating the word embeddings.\nx1:N = x1 ⊕ x2 ⊕ x3 ⊕ ...⊕ xN (1)\nwhere ⊕ is the concatenation operator. To extract local features1, convolution operation is applied. Convolution operation involves a filter, W ∈ RHK , which is convolved with a window of H embeddings to produce a local feature for the H words. A local feature, ci is generated from a window of embeddings xi:i+H−1 by applying a non linear function (such as a hyperbolic tangent) over the convoluted output. Mathematically,\nci = f(W.xi:i+H−1 + b) (2)\n1features specific to a region in case of images or window of words in case of text\n4\n301\n302\n303\n304\n305\n306\n307\n308\n309\n310\n311\n312\n313\n314\n315\n316\n317\n318\n319\n320\n321\n322\n323\n324\n325\n326\n327\n328\n329\n330\n331\n332\n333\n334\n335\n336\n337\n338\n339\n340\n341\n342\n343\n344\n345\n346\n347\n348\n349\n350\n351\n352\n353\n354\n355\n356\n357\n358\n359\n360\n361\n362\n363\n364\n365\n366\n367\n368\n369\n370\n371\n372\n373\n374\n375\n376\n377\n378\n379\n380\n381\n382\n383\n384\n385\n386\n387\n388\n389\n390\n391\n392\n393\n394\n395\n396\n397\n398\n399\nwhere b ∈ R is the bias and f is the non-linear function. This operation is applied to each possible window of H words to produce a feature map (c) for the window size H .\nc = [c1, c2, c3, ..., cN−H+1] (3)\nA global feature is then obtained by applying max pooling operation2 (Collobert et al., 2011) over the feature map. The idea behind max-pooling is to capture the most important feature - one with the highest value - for each feature map.\nWe have described the process by which one feature is extracted from one filter (for illustration, red bordered portions in Figure 3 depict the case of H = 2). The model uses multiple filters (with varying window sizes) to obtain multiple features representing the text. In the MULTICHANNELTEXT variant, for a window of H words, convolution operation is separately applied on both the embedding channels. Local features learned from both the channels are concatenated before applying max-pooling.\n\n4.2 Gaze Component\nThe gaze component deals with scanpaths of multiple participants annotating the same text. Scanpaths can be pre-processed to extract two sequences of gaze data to form separate channels of input: (1) A sequence of normalized3 durations of fixations (in milliseconds) in the order in which they appear in the scanpath and (2) A sequence of position of fixations (in terms of word id) in the order in which they appear in the scanpath. These channels are related to two fundamental gaze attributes such as fixation and saccade respectively. With two channels, we thus have three possible configurations of the gaze component such as (i) FIXATION, where the input is normalized fixation duration sequence, (ii) SACCADE, where the input is fixation position sequence, and (iii) MULTICHANNELGAZE, where both the inputs channels are considered.\nFor each possible input channel, the input is in the form of a P × G matrix (with P → number of participants and G → length of the input sequence). Each element of the matrix gij ∈ R, with i ∈ P and j ∈ G, corresponds to the jth gaze attribute (either fixation duration or word id, depending on the channel) of the input sequence of\n2mean pooling does not perform well. 3scaled across participants using min-max normalization\nto reduce subjectivity\nthe ith participant. Now, unlike the text component, here we apply convolution operation across two dimensions i.e. choosing a two dimensional convolution filter W ∈ RJK (for simplicity, we have kept J = K, thus , making the dimension of W , J2). For the dimension size of J2, a local feature cij is computed from the window of gaze elements gij:(i+J−1)(j+J−1) by,\ncij = f(W.gij:(i+J−1)(j+J−1) + b) (4)\nwhere b ∈ R is the bias and f is a non-linear function. This operation is applied to each possible window of size J2 to produce a feature map (c),\nc =[c11, c12, c13, ..., c1(G−J+1),\nc21, c22, c23, ..., c2(G−J+1), ...,\nc(P−J+1)1, c(P−J+1)2, ..., c(P−J+1)(G−J+1)]\n(5)\nA global feature is then obtained by applying max pooling operation. Unlike the text component, max-pooling operator is applied to a 2D window of local features size M × N (for simplicity, we set M = N , denoted henceforth as M2). For the window of size M2, the pooling operation on c will result in as set of global features ĉJ = max{cij:(i+M−1)(j+M−1)} for each possible i, j.\nWe have described the process by which one feature is extracted from one filter (of 2D window size J2 and max-pooling window size of M2). In Figure 3, red and blue bordered portions illustrate the cases of J2 = [3, 3] and M2 = [2, 2] respectively. Like the text component, the gaze component uses multiple filters (also with varying window size) to obtain multiple features representing the gaze input. In the MULTICHANNELGAZE variant, for a 2D window of J2, convolution operation is separately applied on both fixation duration and saccade channels and local features learned from both the channels are concatenated before max-pooling is applied.\nOnce the global features are learned from both the text and gaze components, they are merged and passed to a fully connected feed forward layer (with number of units set to 150) followed by a SoftMax layer that outputs the the probabilistic distribution over the class labels.\nThe gaze component of our network is not invariant of the order in which the scanpath data is given as input- i.e., the P rows in the P × G can not be shuffled, even if each row is independent from others. The only way we can think of for\n5\n401\n402\n403\n404\n405\n406\n407\n408\n409\n410\n411\n412\n413\n414\n415\n416\n417\n418\n419\n420\n421\n422\n423\n424\n425\n426\n427\n428\n429\n430\n431\n432\n433\n434\n435\n436\n437\n438\n439\n440\n441\n442\n443\n444\n445\n446\n447\n448\n449\n450\n451\n452\n453\n454\n455\n456\n457\n458\n459\n460\n461\n462\n463\n464\n465\n466\n467\n468\n469\n470\n471\n472\n473\n474\n475\n476\n477\n478\n479\n480\n481\n482\n483\n484\n485\n486\n487\n488\n489\n490\n491\n492\n493\n494\n495\n496\n497\n498\n499\naddressing this issue is by applying convolution operations to all P × G matrices formed with all the permutations of P , capturing every possible ordering. Unfortunately, this makes the training process significantly less scalable, as the number of model parameters to be learned becomes huge. As of now, training and testing are carried out by keeping the order of the input constant.\n\n5 Experiment Setup\nWe now share several details regarding our experiments below. 1. Dataset: We experiment on sentiment and sarcasm tasks using two publicly available datasets enriched with eye-movement information. Dataset 1 has been released by Mishra et al. (2016a). It contains 994 text snippets with 383 positive and 611 negative examples. Out of the 994 snippets, 350 are sarcastic. Dataset 2 has been used by Joshi et al. (2014) and it consists of 843 snippets comprising movie reviews and normalized tweets out of which 443 are positive and 400 are negative. Eye-movement data of 7 and 5 readers is available for each snippet for dataset 1 and 2 respectively. 2. CNN Variants: With text component alone we have three variants such as STATICTEXT, NONSTATICTEXT and MULTICHANNELTEXT (refer to Section 4.1). Similarly, with gaze component we have variants such as FIXATION, SACCADE and MULTICHANNELGAZE (refer to Section 4.2). With both text and gaze components, 9 more variants could be experimented with. 3. Hyper-parameters: For text component, we experiment with filter widths (H) of [3, 4]. For the gaze component, 2D filters (J2) set to [3 × 3], [4× 4] respectively. The max pooling 2D window, M2, is set to [2 × 2]. In both gaze and text components, number of filters is set to 150, resulting in 150 feature maps for each window. These model hyper-parameters are fixed by trial and error and are possibly good enough to provide a first level insight into our system. Tuning of hyperparameters might help in improving the performance of our framework, which is on our future research agenda. 4. Regularization: For regularization dropout is employed on the penultimate layer with a constraint on l2-norms of the weight vectors (Hinton et al., 2012). Dropout prevents co-adaptation of hidden units by randomly dropping out - i.e., setting to zero - a proportion p of the hidden units\nduring forward propagation. We set p to 0.25. 5. Training: We use ADADELTA optimizer (Zeiler, 2012), with a learning rate of 0.1. The input batch size is set to 32 and number of training iterations (epochs) is set to 200. 10% of the training data is used for validation. 6. Use of pre-trained embeddings: Initializing the embedding layer with of pre-trained embeddings can be more effective than random initialization (Kim, 2014). In our experiments, we have used embeddings using word2vec facilitated by Mikolov et al. (2013) (best results obtained with embedding dimension of 50). We have also tried randomly initializing the embeddings but better results are obtained with pre-trained embeddings. 7. Comparison with existing work: For sentiment analysis, we compare our systems’s accuracy (for both datasets 1 and 2) with Mishra et al. (2016c)’s systems that rely on handcrafted text and gaze features. For sarcasm detection, we compare Mishra et al. (2016b)’s sarcasm classifier with ours using dataset 1 (with available gold standard labels for sarcasm). We follow the same 10-fold traintest configuration as these existing works for consistency.\n\n6 Results\nIn this section, we discuss the results for different model variants for sentiment polarity and sarcasm detection tasks.\n\n6.1 Results for Sentiment Analysis Task\nTable 1 presents results for sentiment analysis task. For dataset 1, different variants of our CNN architecture outperform the best systems reported by Mishra et al. (2016c), with a maximum F-score improvement of 3.8%. This improvement is statistically significant of p < 0.05 as confirmed by McNemar test. Moreover, we observe an F-score improvement of around 5% for CNNs with both gaze and text components as compared to CNNs with only text components (similar to the system by Kim (2014)), which is also statistically significant (with p < 0.05).\nFor dataset 2, CNN based approaches do not perform better than manual feature based approaches. However, variants with both text and gaze components outperform the ones with only text component (Kim, 2014), with a maximum Fscore improvement of 2.9%. We observe that for dataset 2, training accuracy reaches 100 within\n6\n501\n502\n503\n504\n505\n506\n507\n508\n509\n510\n511\n512\n513\n514\n515\n516\n517\n518\n519\n520\n521\n522\n523\n524\n525\n526\n527\n528\n529\n530\n531\n532\n533\n534\n535\n536\n537\n538\n539\n540\n541\n542\n543\n544\n545\n546\n547\n548\n549\n550\n551\n552\n553\n554\n555\n556\n557\n558\n559\n560\n561\n562\n563\n564\n565\n566\n567\n568\n569\n570\n571\n572\n573\n574\n575\n576\n577\n578\n579\n580\n581\n582\n583\n584\n585\n586\n587\n588\n589\n590\n591\n592\n593\n594\n595\n596\n597\n598\n599\nDataset1 Dataset2\nConfiguration P R F P R F\nTraditional systems based on\nNäive Bayes 63.0 59.4 61.14 50.7 50.1 50.39 Multi-layered Perceptron 69.0 69.2 69.2 66.8 66.8 66.8\ntextual features SVM (Linear Kernel) 72.8 73.2 72.6 70.3 70.3 70.3 Systems by Mishra et al. (2016c) Gaze based (Best) 61.8 58.4 60.05 53.6 54.0 53.3 Text + Gaze (Best) 73.3 73.6 73.5 71.9 71.8 71.8\nCNN with only text input (Kim, 2014) STATICTEXT 63.85 61.26 62.22 55.46 55.02 55.24 NONSTATICTEXT 72.78 71.93 72.35 60.51 59.79 60.14 MULTICHANNELTEXT 72.17 70.91 71.53 60.51 59.66 60.08\nCNN with only gaze Input\nFIXATION 60.79 58.34 59.54 53.95 50.29 52.06 SACCADE 64.19 60.56 62.32 51.6 50.65 51.12 MULTICHANNELGAZE 65.2 60.35 62.68 52.52 51.49 52\nCNN with both text and gaze Input\nSTATICTEXT + FIXATION 61.52 60.86 61.19 54.61 54.32 54.46 STATICTEXT + SACCADE 65.99 63.49 64.71 58.39 56.09 57.21 STATICTEXT + MULTICHANNELGAZE 65.79 62.89 64.31 58.19 55.39 56.75 NONSTATICTEXT + FIXATION 73.01 70.81 71.9 61.45 59.78 60.60 NONSTATICTEXT + SACCADE 77.56 73.34 75.4 65.13 61.08 63.04 NONSTATICTEXT + MULTICHANNELGAZE 79.89 74.86 77.3 63.93 60.13 62 MULTICHANNELTEXT + FIXATION 74.44 72.31 73.36 60.72 58.47 59.57 MULTICHANNELTEXT + SACCADE 78.75 73.94 76.26 63.7 60.47 62.04 MULTICHANNELTEXT + MULTICHANNELGAZE 78.38 74.23 76.24 64.29 61.08 62.64\nTable 1: Results for different traditional feature based systems and CNN model variants for the task of sentiment analysis. Abbreviations (P,R,F)→ Precision, Recall, F-score. SVM→Support Vector Machine\n25 epochs with validation accuracy stable around 50%, indicating the possibility of overfitting. Tuning the regularization parameters specific to dataset 2 may help here. Even though CNN might not be proving to be a choice as good as handcrafted features for dataset 2, the bottom line remains that incorporation of gaze data into CNN consistently improves the performance over onlytext-based CNN variants.\n\n6.2 Results for Sarcasm Detection Task\nFor sarcasm detection, our CNN model variants outperform traditional systems by a maximum margin of 11.27% (Table 2). However, the improvement by adding the gaze component to the CNN network is just 1.36%, which is statistically insignificant over CNN with text component. While inspecting the sarcasm dataset, we observe a clear difference between the vocabulary of sarcasm and non-sarcasm classes in our dataset. This, perhaps, was captured well by the text component, especially the variant with only non-static embeddings.\n\n7 Discussion\nIn this section, some important observations from our experiments are discussed. • Effect of embedding dimension variation: Embedding dimension has proven to have a deep impact on the performance of neural systems (dos\nSantos and Gatti, 2014; Collobert et al., 2011). We repeated our experiments by varying the embedding dimensions in the range of [50-300]4 and observed that reducing embedding dimension improves the F-scores by a little margin. Best results are obtained when the embedding dimension is as low as 50. Small embedding dimensions are probably reducing the chances of over-fitting when the data size is small. We also observe that for different embedding dimensions, performance of CNN with both gaze and text components is consistently better than that with only text component. • Effect of static / non static text channels: Nonstatic embedding channel has a major role in tuning embeddings for sentiment analysis by bringing adjectives expressing similar sentiment close to each other (e.g, good and nice), where as static channel seems to prevent over-tuning of embeddings (over-tuning often brings verbs like love closer to the pronoun I in embedding space, purely due to higher co-occurrence of these two words in sarcastic examples). • Effect of fixation / saccade channels: For sentiment detection, saccade channel seems to be handing text having semantic incongruity (due to the presence of irony / sarcasm) better. Fixation channel does not help much, may be because of higher variance in fixation duration. For sarcasm\n4a standard range (Liu et al., 2015; Melamud et al., 2016)\n7\n601\n602\n603\n604\n605\n606\n607\n608\n609\n610\n611\n612\n613\n614\n615\n616\n617\n618\n619\n620\n621\n622\n623\n624\n625\n626\n627\n628\n629\n630\n631\n632\n633\n634\n635\n636\n637\n638\n639\n640\n641\n642\n643\n644\n645\n646\n647\n648\n649\n650\n651\n652\n653\n654\n655\n656\n657\n658\n659\n660\n661\n662\n663\n664\n665\n666\n667\n668\n669\n670\n671\n672\n673\n674\n675\n676\n677\n678\n679\n680\n681\n682\n683\n684\n685\n686\n687\n688\n689\n690\n691\n692\n693\n694\n695\n696\n697\n698\n699\nConfiguration P R F\nTraditional systems based on\nNäive Bayes 69.1 60.1 60.5 Multi-layered Perceptron 69.7 70.4 69.9\ntextual features SVM (Linear Kernel) 72.1 71.9 72 Systems by Riloff et al. (2013) Text based (Ordered) 49 46 47 Text + Gaze (Unordered) 46 41 42\nSystem by Joshi et al. (2015)\nText based (best) 70.7 69.8 64.2\nSystems by Mishra et al. (2016b)\nGaze based (Best) 73 73.8 73.1 Text based (Best) 72.1 71.9 72 Text + Gaze (Best) 76.5 75.3 75.7\nCNN with only text input (Kim, 2014) STATICTEXT 67.17 66.38 66.77 NONSTATICTEXT 84.19 87.03 85.59 MULTICHANNELTEXT 84.28 87.03 85.63\nCNN with only gaze input\nFIXATION 74.39 69.62 71.93 SACCADE 68.58 68.23 68.40 MULTICHANNELGAZE 67.93 67.72 67.82\nCNN with both text and gaze Input\nSTATICTEXT + FIXATION 72.38 71.93 72.15 STATICTEXT + SACCADE 73.12 72.14 72.63 STATICTEXT + MULTICHANNELGAZE 71.41 71.03 71.22 NONSTATICTEXT + FIXATION 87.42 85.2 86.30 NONSTATICTEXT + SACCADE 84.84 82.68 83.75 NONSTATICTEXT + MULTICHANNELGAZE 84.98 82.79 83.87 MULTICHANNELTEXT + FIXATION 87.03 86.92 86.97 MULTICHANNELTEXT + SACCADE 81.98 81.08 81.53 MULTICHANNELTEXT + MULTICHANNELGAZE 83.11 81.69 82.39\nTable 2: Results for different traditional feature based systems and CNN model variants for the task of sarcasm detection on dataset 1. Abbreviations (P,R,F)→ Precision, Recall, F-score\ndetection, fixation and saccade channels perform with similar accuracy when employed separately. Accuracy reduces with gaze multichannel, may be because of higher variation of both fixations and saccades across sarcastic and non-sarcastic classes, as opposed to sentiment classes.\n• Effectiveness of the CNN learned features To examine how good the features learned by the CNN are, we analyzed the features for a few example cases. Figure 4 presents some of the testexamples for the task of sarcasm detection. Example 1 contains sarcasm while examples 2, 3 and 4 are non-sarcastic. To see if there is any difference in the automatically learned features between text-only and combined text and gaze variants, we examine the feature vector (of dimension 150) for the examples obtained from different model variants. Output of the hidden layer after merge layer is considered as features learned by the network. We plot the features, in the form of color-bars, following Li et al. (2016) - denser colors representing higher feature values. In Figure 4, we show only two (representative) model variants viz., MULTICHANNELTEXT and MULTICHANNELTEXT+ MULTICHANNELGAZE. As one can see, addition of gaze information helps\nto generate features with more subtle differences (marked by blue rectangular boxes) for sarcastic and non-sarcastic texts. It is also interesting to note that in the marked region, features for the sarcastic texts exhibit more intensity than the nonsarcastic ones - perhaps capturing the notion that sarcasm typically conveys an intensified negative opinion. This difference is not clear in feature vectors learned by text only systems for instances like example 2, which has been incorrectly classified by MULTICHANNELTEXT. Example 4 is incorrectly classified by both the systems, perhaps due to lack of context. In cases like this, addition of gaze information does not help much in learning more distinctive features, as it becomes difficult for even humans to classify such texts.\n\n8 Related Work\nSentiment and sarcasm classification are two important problems in NLP and have been the focus of research for many communities for quite some time. Popular sentiment and sarcasm detection systems are feature based and are based on unigrams, bigrams etc. (Dave et al., 2003; Ng et al., 2006), syntactic properties (Martineau and Finin, 2009; Nakagawa et al., 2010), semantic properties\n8\n701\n702\n703\n704\n705\n706\n707\n708\n709\n710\n711\n712\n713\n714\n715\n716\n717\n718\n719\n720\n721\n722\n723\n724\n725\n726\n727\n728\n729\n730\n731\n732\n733\n734\n735\n736\n737\n738\n739\n740\n741\n742\n743\n744\n745\n746\n747\n748\n749\n750\n751\n752\n753\n754\n755\n756\n757\n758\n759\n760\n761\n762\n763\n764\n765\n766\n767\n768\n769\n770\n771\n772\n773\n774\n775\n776\n777\n778\n779\n780\n781\n782\n783\n784\n785\n786\n787\n788\n789\n790\n791\n792\n793\n794\n795\n796\n797\n798\n799\n1. I would like to live in Manchester, England. The transition between Manchester and death would be unnoticeable. (Sarcastic, Negative Sentiment)\n2. We really did not like this camp. After a disappointing summer, we switched to another camp, and all of us much happier on all fronts! (Non Sarcastic, Negative Sentiment)\n3. Helped me a lot with my panics attack I take 6 mg a day for almost 20 years can't stop of course but make me feel very comfortable (Non Sarcastic, Positive Sentiment)\n4. Howard is the King and always will be, all others are weak clones. (Non Sarcastic, Positive Sentiment)\n(a) MultichannelText + MultichannelGaze (b) MultichannelText\nFigure 4: Visualization of representations learned by two variants of the network for sarcasm detection task. The output of the Merge layer (of dimension 150) are plotted in the form of colour-bars. Plots with thick red borders correspond to wrongly predicted examples.\n(Balamurali et al., 2011). For sarcasm detection, supervised approaches rely on (a) Unigrams and Pragmatic features (González-Ibánez et al., 2011; Barbieri et al., 2014; Joshi et al., 2015) (b) Stylistic patterns (Davidov et al., 2010) and patterns related to situational disparity (Riloff et al., 2013) and (c) Hastag interpretations (Liebrecht et al., 2013; Maynard and Greenwood, 2014). Recent systems are based on variants of deep neural network built on the top of embeddings. A few representative works in this direction for sentiment analysis are based on CNNs (dos Santos and Gatti, 2014; Kim, 2014; Tang et al., 2014), RNNs (Dong et al., 2014; Liu et al., 2015) and combined architecture (Wang et al., 2016). Few works exist on using deep neural networks for sarcasm detection, one of which is by (Ghosh and Veale, 2016) that uses a combination of RNNs and CNNs.\nEye-tracking technology is a relatively new NLP, with a very few systems directly making use of gaze data in prediction frameworks. Klerke et al. (2016) present a novel multi-task learning approach for sentence compression using labeled data, while, Barrett and Søgaard (2015) discriminate between grammatical functions using gaze features. The closest works to ours is by Mishra et al. (2016b) and Mishra et al. (2016c) that introduce feature engineering based on both gaze\nand text data for sentiment and sarcasm detection tasks. These recent advancements motivate us to explore the cognitive NLP paradigm .\n\n9 Conclusion and Future Directions\nIn this work, we proposed a multimodal ensemble of features, automatically learned using variants of CNNs from text and readers’ eye-movement data, for the tasks of sentiment and sarcasm classification. On multiple published datasets for which gaze information is available, our systems could achieve significant performance improvements over (a) systems that rely on handcrafted gaze and textual features and (b) CNN based systems that rely on text input alone. An analysis of the learned features confirms that the combination of automatically learned features is indeed capable of representing deep linguistic subtleties in text that pose challenges to sentiment and sarcasm classifiers. Our future agenda includes: (a) optimizing the CNN framework hyper-parameters (e.g., filter width, dropout, embedding dimensions etc.) to obtain better results, (b) exploring the applicability of our technique for document-level sentiment analysis and (c) applying our framework on related problems, such as emotion analysis, text summarization and question-answering.\n9\n801\n802\n803\n804\n805\n806\n807\n808\n809\n810\n811\n812\n813\n814\n815\n816\n817\n818\n819\n820\n821\n822\n823\n824\n825\n826\n827\n828\n829\n830\n831\n832\n833\n834\n835\n836\n837\n838\n839\n840\n841\n842\n843\n844\n845\n846\n847\n848\n849\n850\n851\n852\n853\n854\n855\n856\n857\n858\n859\n860\n861\n862\n863\n864\n865\n866\n867\n868\n869\n870\n871\n872\n873\n874\n875\n876\n877\n878\n879\n880\n881\n882\n883\n884\n885\n886\n887\n888\n889\n890\n891\n892\n893\n894\n895\n896\n897\n898\n899\n",
    "rationale": "- Strengths:\n\n(1) A deep CNN framework is proposed to extract and combine cognitive features\nwith textual features for sentiment analysis and sarcasm detection. \n\n(2) The ideas is interesting and novelty.\n\n- Weaknesses:\n\n(1) Replicability would be an important concern. Researchers cannot replicate\nthe system/method for improvement due to lack of data for feature extraction. \n\n- General Discussion:\n\nOverall, this paper is well written and organized. The experiments are\nconducted carefully for comparison with previous work and the analysis is\nreasonable. I offer some comments as follows.\n\n(1)           Does this model be suitable on sarcastic/non-sarcastic utterances?\nThe\nauthors should provide more details for further analysis. \n\n(2)           Why the eye-movement data would be useful for\nsarcastic/non-sarcastic\nsentiment classification beyond the textual features? The authors should\nprovide more explanations.",
    "rating": 5
  },
  {
    "title": "FRUSTRATINGLY SHORT ATTENTION SPANS IN NEURAL LANGUAGE MODELING",
    "abstract": "Neural language models predict the next token using a latent representation of the immediate token history. Recently, various methods for augmenting neural language models with an attention mechanism over a differentiable memory have been proposed. For predicting the next token, these models query information from a memory of the recent history which can facilitate learning midand long-range dependencies. However, conventional attention mechanisms used in memoryaugmented neural language models produce a single output vector per time step. This vector is used both for predicting the next token as well as for the key and value of a differentiable memory of a token history. In this paper, we propose a neural language model with a key-value attention mechanism that outputs separate representations for the key and value of a differentiable memory, as well as for encoding the next-word distribution. This model outperforms existing memoryaugmented neural language models on two corpora. Yet, we found that our method mainly utilizes a memory of the five most recent output representations. This led to the unexpected main finding that a much simpler model based only on the concatenation of recent output representations from previous time steps is on par with more sophisticated memory-augmented neural language models.",
    "text": "1 INTRODUCTION\nAt the core of language models (LMs) is their ability to infer the next word given a context. This requires representing context-specific dependencies in a sequence across different time scales. On the one hand, classical N -gram language models capture relevant dependencies between words in short time distances explicitly, but suffer from data sparsity. Neural language models, on the other hand, maintain and update a dense vector representation over a sequence where time dependencies are captured implicitly (Mikolov et al., 2010). A recent extension of neural sequence models are attention mechanisms (Bahdanau et al., 2015), which can capture long-range connections more directly. However, we argue that applying such an attention mechanism directly to neural language models requires output vectors to fulfill several purposes at the same time: they need to (i) encode a distribution for predicting the next token, (ii) serve as a key to compute the attention vector, as well as (iii) encode relevant content to inform future predictions.\nWe hypothesize that such overloaded use of output representations makes training the model difficult and propose a modification to the attention mechanism which separates these functions explicitly, inspired by Miller et al. (2016); Ba et al. (2016); Reed & de Freitas (2015); Gulcehre et al. (2016). Specifically, at every time step our neural language model outputs three vectors. The first is used to encode the next-word distribution, the second serves as key, and the third as value for an attention mechanism. We term the model key-value-predict attention and show that it outperforms existing memory-augmented neural language models on the Children’s Book Test (CBT, Hill et al., 2016) and a new corpus of 7500 Wikipedia articles. However, we observed that this model pays attention mainly to the previous five memories. We thus also experimented with a much simpler model that only uses a concatenation of output vectors from the previous time steps for predicting the next token. This simple model is on par with more sophisticated memory-augmented neural language models. Thus, our main finding is that modeling short attention spans properly works well and provides notable\nimprovements over a neural language model with attention. Conversely, it seems to be notoriously hard to train neural language models to leverage long-range dependencies.\nIn this paper, we investigate various memory-augmented neural language models and compare them against previous architectures. Our contributions are threefold: (i) we propose a key-value attention mechanism that uses specific output representations for querying a sliding-window memory of previous token representations, (ii) we demonstrate that while this new architecture outperforms previous memory-augmented neural language models, it mainly utilizes a memory of the previous five representations, and finally (iii) based on this observation we experiment with a much simpler but effective model that uses the concatenation of three previous output representations to predict the next word.\n\n2 METHODS\nIn the following, we discuss methods for extending neural language models with differentiable memory. We first present a standard attention mechanism for language modeling (§2.1). Subsequently, we introduce two methods for separating the usage of output vectors in the attention mechanism: (i) using a dedicated key and value (§2.2), and (ii) further separating the value into a memory value and a representation that encodes the next-word distribution (§2.3). Finally, we describe a very simple method that concatenates previous output representations for predicting the next token (§2.4).\n\n2.1 ATTENTION FOR NEURAL LANGUAGE MODELING\nAugmenting a neural language model with attention (Bahdanau et al., 2015) is straight-forward. We simply take the previous L output vectors as memory Yt = [ht−L · · · ht−1] ∈ Rk×L where k is the output dimension of a Long Short-Term Memory (LSTM) unit (Hochreiter & Schmidhuber, 1997). This memory could in principle contain all previous output representations, but for practical reasons we only keep a sliding window of the previous L outputs. Let ht ∈ Rk be the output representation at time step t and 1 ∈ RL be a vector of ones. The attention weights α ∈ RL are computed from a comparison of the current and previous LSTM outputs. Subsequently, the context vector rt ∈ Rk is calculated from a sum over previous output vectors weighted by their respective attention value. This can be formulated as\nMt = tanh(W Y Yt + (W hht)1 T ) ∈ Rk×L (1)\nαt = softmax(wTMt) ∈ R1×L (2) rt = Ytα T ∈ Rk (3)\nwhereW Y ,W h ∈ Rk×k are trainable projection matrices andw ∈ Rk is a trainable vector. The final representation that encodes the next-word distribution is computed from a non-linear combination of the attention-weighted representation rt of previous outputs and the final output vector ht via\nh∗t = tanh(W rrt +W xht) ∈ Rk (4)\nwhere W r,W x ∈ Rk×k are trainable projection matrices. An overview of this architecture is depicted in Figure 1a. Lastly, the probablity distribution yt for the next word is represented by\nyt = softmax(W ∗h∗t + b) ∈ R|V | (5)\nwhereW ∗ ∈ R|V |×k and b ∈ R|V | are a trainable projection matrix and bias, respectively.\n\n2.2 KEY-VALUE ATTENTION\nInspired by Miller et al. (2016); Ba et al. (2016); Reed & de Freitas (2015); Gulcehre et al. (2016), we introduce a key-value attention model that separates output vectors into keys used for calculating the attention distribution αt, and a value part used for encoding the next-word distribution and context representation. This model is depicted in Figure 1b. Formally, we rewrite Equations 1-4 as follows:[\nkt vt\n] = ht ∈ R2k (6)\nMt = tanh(W Y [kt−L · · · kt−1] + (W hkt)1T ) ∈ Rk×L (7)\nαt = softmax(wTMt) ∈ R1×L (8) rt = [vt−L · · · vt−1]αT ∈ Rk (9) h∗t = tanh(W rrt +W xvt) ∈ Rk (10)\nIn essence, Equation 7 compares the key at time step t with the previous L keys to calculate the attention distribution αt which is then used in Equation 9 to obtain a weighted context representation from values associated with these keys.\n\n2.3 KEY-VALUE-PREDICT ATTENTION\nEven with a key-value separation, a potential problem is that the same representation vt is still used both for encoding the probability distribution of the next word and for retrieval from the memory via the attention later. Thus, we experimented with another extension of this model where we further separate ht into a key, a value and a predict representation where the latter is only used for encoding the next-word distribution (see Figure 1c). To this end, equations 6 and 10 are replaced by[\nkt vt pt\n] = ht ∈ R3k (11)\nh∗t = tanh(W rrt +W xpt) ∈ Rk (12)\nMore precisely, the output vector ht is divided into three equal parts: key, value and predict. In our implementation we simply split the output vector ht into kt, vt and pt. To this end the hidden dimension of the key-value-predict attention model needs to be a multiplicative of three. Consequently, the dimensions of kt, vt and pt are 100 for a hidden dimension of 300.\n\n2.4 N -GRAM RECURRENT NEURAL NETWORK\nNeural language models often work best in combination with traditional N -gram models (Mikolov et al., 2011; Chelba et al., 2013; Williams et al., 2015; Ji et al., 2016; Shazeer et al., 2015), since the former excel at generalization while the latter ensure memorization. In addition, from initial experiments with memory-augmented neural language models, we found that usually only the previous five output representations are utilized. This is in line with observations by Tran et al. (2016). Hence, we experiment with a much simpler architecture depicted in Figure 1d. Instead of an attention mechanism, the output representations from the previous N − 1 time steps are directly used to calculate next-word probabilities. Specifically, at every time step we split the LSTM output into N − 1 vectors [h1t , . . . ,hN−1t ] and replace Equation 4 with\nh∗t = tanh WN  h 1 t ...\nhN−1t−N+1\n  ∈ Rk (13)\nwhere WN ∈ Rk×(N−1)k is a trainable projection matrix. This model is related to higher-order RNNs (Soltani & Jiang, 2016) with the difference that we do not incorporate output vectors from the previous steps into the hidden state but only use them for predicting the next word. Furthermore, note that at time step t the first part of the output vector h1t will contribute to predicting the next word, the second part h2t will contribute to predicting the second word thereafter, and so on. As the output vectors from the N − 1 previous time-steps are used to score the next word, we call the resulting model an N -gram RNN.\n\n3 RELATED WORK\nEarly attempts of using memory in neural networks have been undertaken by Taylor (1959) and Steinbuch & Piske (1963) by performing nearest-neighbor operations on input vectors and fitting parametric models to the retrieved sets. The dedicated use of external memory in neural architectures has more recently witnessed increased interest. Weston et al. (2015) introduced Memory Networks to explicitly segregate memory storage from the computation of the neural network, and Sukhbaatar et al. (2015) trained this model end-to-end with an attention-based memory addressing mechanism. The Neural Turing Machines by Graves et al. (2014) add an external differentiable memory with read-write functions to a controller recurrent neural network, and has shown promising results in simple sequence tasks such as copying and sorting. These models make use of external memory, whereas our model directly uses a short sequence from the history of tokens to dynamically populate an addressable memory.\nIn sequence modeling, RNNs such as LSTMs (Hochreiter & Schmidhuber, 1997) maintain an internal memory state as they process an input sequence. Attending over previous state outputs on top of an RNN encoder has improved performances in a wide range of tasks, including machine translation (Bahdanau et al., 2015), recognizing textual entailment (Rocktäschel et al., 2016), sentence summarization (Rush et al., 2015), image captioning (Xu et al., 2015) and speech recognition (Chorowski et al., 2015).\nRecently, Cheng et al. (2016) proposed an architecture that modifies the standard LSTM by replacing the memory cell with a memory network (Weston et al., 2015). Another proposal for conditioning on previous output representations are Higher-order Recurrent Neural Networks (HORNNs, Soltani & Jiang, 2016). Soltani & Jiang found it useful to include information from multiple preceding RNN states when computing the next state. This previous work centers around preceding state vectors, whereas we investigate attention mechanisms on top of RNN outputs, i.e. the vectors used for predicting the next word. Furthermore, instead of pooling we use attention vectors to calculate a context representation of previous memories.\nYang et al. (2016) introduced a reference-aware neural language model where at every position a latent variable determines from which source a target token is generated, e.g., by copying entries from a table or referencing entities that were mentioned earlier.\nAnother class of models that include memory into sequence modeling are Recurrent Memory Networks (RMNs) (Tran et al., 2016). Here, a memory block accesses the most recent input words to selectively attend over relevant word representations from a global vocabulary. RMNs use a global memory with two input word vector look-up tables for the attention mechanism, and consequently have a large number of trainable parameters. Instead, we proposed models that need much fewer parameters by producing the vectors that will be attended over in the future, which can be seen as a memory that is dynamically populated by the language model.\nFinally, the functional separation of look-up keys and memory content has been found useful for Memory Networks (Miller et al., 2016), Neural Programmer-Interpreters (Reed & de Freitas, 2015), Dynamic Neural Turing Machines (Gulcehre et al., 2016), and Fast Associative Memory (Ba et al., 2016). We apply and extend this principle to neural language models.\n\n4 EXPERIMENTS\nWe evaluate models on two different corpora for language modeling. The first is a subset of the Wikipedia corpus.1 It consists of 7500 English Wikipedia articles (dump from 6 Feb 2015) belonging to one of the following categories: People, Cities, Countries, Universities, and Novels. We chose these categories as we expect articles in these categories to often contain references to previously mentioned entities. Subsequently, we split this corpus into a train, development, and test part, resulting in corpora of 22.5M words, 1.2M and 1.2M words, respectively. We map all numbers to a dedicated numerical symbol N and restrict the vocabulary to the 77K most frequent words, encompassing 97% of the training vocabulary. All other words are replaced by the UNK symbol. The average length of sentences is 25 tokens. In addition to this Wikipedia corpus, we also run experiments on the Children’s Book Test (CBT Hill et al., 2016). While this corpus is designed for cloze-style question-answering, in this paper we use it to test how well language models can exploit wider linguistic context.\n\n4.1 TRAINING PROCEDURE\nWe use ADAM (Kingma & Ba, 2015) with an initial learning rate of 0.001 and a mini-batch size of 64 for optimization. Furthermore, we apply gradient clipping at a gradient norm of 5 (Pascanu et al., 2013). The bias of the LSTM’s forget gate is initialized to 1 (Jozefowicz et al., 2016), while other parameters are initialized uniformly from the range (−0.1, 0.1). Backpropagation Through Time (Rumelhart et al., 1985; Werbos, 1990) was used to train the network with 20 steps of unrolling. We reset the hidden states between articles for the Wikipedia corpus and between stories for CBT, respectively. We take the best configuration based on performance on the validation set and evaluate it on the test set.\n\n5 RESULTS\nIn the first set of experiments we explore how well the proposed models and Tran et al.’s Recurrentmemory Model can make use of histories of varying lengths. Perplexity results for different attention window sizes on the Wikipedia corpus are summarized in Figure 2a. The average attention these models pay to specific positions in the history is illustrated in Figure 3. We observed that although our models attend over tokens further in the past more often than the Recurrent-memory Model, attending over a longer history does not significantly improve the perplexity of any attentive model.\nThe much simpler N -gram RNN model achieves comparable results (Figure 2b) and seems to work best with a history of the previous three output vectors (4-gram RNN). As a result, we choose the 4-gram model for the following N -gram RNN experiments.\n1The wikipedia corpus is available at https://goo.gl/s8cyYa.\n\n5.1 COMPARISON WITH STATE-OF-THE-ART MODELS\nIn the next set of experiments, we compared our proposed models against a variety of state-of-the-art models on the Wikipedia and CBT corpora. Results are shown in Figure 2c and 2d, respectively. Note that the models presented here do not achieve state-of-the-art on CBT as they are language models and not tailored towards cloze-sytle question answering. Thus, we merely use this corpus for comparing different neural language model architectures. We reimplemented the Recurrent-Memory model by Tran et al. (2016) with the temporal matrix and gating composition function (RM+tM-g).\nFurthermore, we reimplemented Higher Order Recurrent Neural Networks (HORNNs) by Soltani & Jiang (2016).\nTo ensure a comparable number of parameters to a vanilla LSTM model, we adjusted the hidden size of all models to have roughly the same total number of model parameters. The attention window size N for the N -gram RNN model was set to 4 according to the best validation set perplexity on the Wikipedia corpus. Below we discuss the results in detail.\nAttention By using a neural language model with an attention mechanism over a dynamically populated memory, we observed a 3.2 points lower perplexity over a vanilla LSTM on Wikipedia, but only notable differences for predicting verbs and prepositions in CBT. This indicates that incorporating mechanisms for querying previous output vectors is useful for neural language modeling.\nKey-Value Decomposing the output vector into a key-value paired memory improves the perplexity by 7.0 points compared to a baseline LSTM, and by 1.9 points compared to the RM(+tM-g) model. Again, for CBT we see only small improvements.\nKey-Value-Predict By further separating the output vector into a key, value and next-word prediction part, we get the lowest perplexity and gain 9.4 points over a baseline LSTM, a 4.3 points compared to RM(+tM-g), and 2.4 points compared to only splitting the output into a key and value. For CBT, we see an accuracy increase of 1.0 percentage points for verbs, and 1.7 for prepositions. As stated earlier, the performance of the Key-Value-Predict model does not improve significantly when increasing the attention window size. This leads to the conclusion that none of the attentive models investigated in this paper can utilize a large memory of previous token representations. Moreover, none of the presented methods differ significantly for predicting common nouns and named entities in CBT.\nN -gram RNN Our main finding is that the simple modification of using output vectors from the previous time steps for the next-word prediction leads to perplexities that are on par with or better than more complicated neural language models with attention. Specifically, the 4-gram RNN achieves only slightly worse perplexities than the Key-Value-Predict architecture.\n\n6 CONCLUSION\nIn this paper, we observed that using an attention mechanism for neural language modeling where we separate output vectors into a key, value and predict part outperform simpler attention mechanisms on a Wikipedia corpus and the Children Book Test (CBT, Hill et al., 2016). However, we found that all attentive neural language models mainly utilize a memory of only the most recent history and fail to exploit long-range dependencies. In fact, a much simpler N -gram RNN model, which only uses a concatenation of output representations from the previous three time steps, is on par with more sophisticated memory-augmented neural language models. Training neural language models that take long-range dependencies into account seems notoriously hard and needs further investigation. Thus, for future work we want to investigate ways to encourage attending over a longer history, for instance by forcing the model to ignore the local context and only allow attention over output representations further behind the local history.\n",
    "rationale": "This paper focusses on attention for neural language modeling and has two major contributions:\n\n1. Authors propose to use separate key, value, and predict vectors for attention mechanism instead of a single vector doing all the 3 functions. This is an interesting extension to standard attention mechanism which can be used in other applications as well.\n2. Authors report that very short attention span is sufficient for language models (which is not very surprising) and propose an n-gram RNN which exploits this fact.\n\nThe paper has novel models for neural language modeling and some interesting messages. Authors have done a thorough experimental analysis of the proposed ideas on a language modeling task and CBT task.\n\nI am convinced with authors’ responses for my pre-review questions.\n\nMinor comment: Ba et al., Reed & de Freitas, and Gulcehre et al. should be added to the related work section as well.\n",
    "rating": 5
  },
  {
    "title": "CANE: Context-Aware Network Embedding for Relation Modeling",
    "abstract": "Network embedding (NE) is playing a critical role in network analysis, due to its ability to represent vertices with efficient low-dimensional embedding vectors. However, existing NE models aim to learn a fixed context-free embedding for each vertex, and neglect the diverse roles when interacting with other vertices. In this paper, we assume that one vertex usually shows different aspects when interacting with different neighbor vertices, and should own different embeddings respectively. Therefore, we present ContextAware Network Embedding (CANE), a novel NE model to address this issue. CANE learns context-aware embeddings for vertices with mutual attention mechanism and is expected to model the semantic relationships between vertices more precisely. In experiments, we compare our model with existing NE models on three real-world datasets. Experimental results shows that CANE achieves significant improvement than state-of-the-art methods on link prediction, and comparable performance on vertex classification.",
    "text": "1 Introduction\nNetwork embedding (NE), i.e., network representation learning (NRL), aims to map vertices of a network into a low-dimensional space according to their structural roles in the network. NE provides an efficient and effective way to represent and manage large-scale networks, alleviating the computation and sparsity issues of conventional symbol-based representations. Hence, NE is attracting many research interests in recent years (Perozzi et al., 2014; Tang et al., 2015; Grover and\nLeskovec, 2016), and achieves promising performance on many network analysis tasks including link prediction, vertex classification, and community detection.\nIn real-world social networks, it is intuitive that one vertex may demonstrate various aspects when interacting with different neighbor vertices. For example, a researcher usually collaborates with different partners on diverse research topics (as illustrated in Fig. 1), a social-media user contacts with various friends sharing distinct interests, and a web page links with multiple pages for different purposes. However, most existing NE methods only arrange one single embedding vector to each vertex, and give rise to the following two invertible issues: (1) These methods cannot cope with the aspect transition of a vertex flexibly when interacting with different neighbors. (2) In these models, a vertex tends to force the embeddings of its neighbors close to each other, which may be not case all the time. For example, the left user and right user in Fig. 1, share less common interests, but are learned to be close to each other since they both link to the middle person. This will accordingly\n2\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n112\n113\n114\n115\n116\n117\n118\n119\n120\n121\n122\n123\n124\n125\n126\n127\n128\n129\n130\n131\n132\n133\n134\n135\n136\n137\n138\n139\n140\n141\n142\n143\n144\n145\n146\n147\n148\n149\n150\n151\n152\n153\n154\n155\n156\n157\n158\n159\n160\n161\n162\n163\n164\n165\n166\n167\n168\n169\n170\n171\n172\n173\n174\n175\n176\n177\n178\n179\n180\n181\n182\n183\n184\n185\n186\n187\n188\n189\n190\n191\n192\n193\n194\n195\n196\n197\n198\n199\nmake vertex embeddings indiscriminative. To address these issues, we aim to propose a Context-Aware Network Embedding (CANE) framework for modeling relationships between vertices precisely. More specifically, we present CANE on information networks, where each vertex also contains rich external information such as text, labels or other meta-data, and the significance of context is more critical for NE in this scenario. Without loss of generality, we implement CANE on text-based information networks in this paper, which can be easily extended to other types of information networks.\nIn conventional NE models, each vertex is represented as a static embedding vector, denoted as context-free embedding. On the contrary, CANE assigns dynamic embeddings to a vertex according to different neighbors it interacts with, named as context-aware embedding . Take a vertex u and its neighbor vertex v for example. The contextfree embedding of u remains unchanged when interacting with different neighbors. On the contrary, the context-aware embedding of u is dynamic when confronting different neighbors.\nWhen u interacting with v, their context embeddings with respect to each other are derived from their text information, Su and Sv respectively. For each vertex, we can easily use neural models, such as convolutional neural networks (Blunsom et al., 2014; Johnson and Zhang, 2014; Kim, 2014) and recurrent neural networks (Kiros et al., 2015; Tai et al., 2015), to build context-free text-based embedding. In order to realize context-aware textbased embeddings, we introduce the selective attention scheme and build mutual attention between u and v into these neural models. The mutual attention is expected to guide neural models to emphasize those words that are focused by its neighbor vertices and eventually obtain contextaware embeddings.\nBoth context-free embeddings and contextaware embeddings of each vertex can be efficiently learned together via concatenation using existing NE methods such as DeepWalk (Perozzi et al., 2014), LINE (Tang et al., 2015) and node2vec (Grover and Leskovec, 2016).\nWe conduct experiments on three real-world datasets of different areas. Experimental results on link prediction reveal the effectiveness of our framework as compared to other state-of-the-art methods. The results suggest that, context-aware\nembeddings are critical for network analysis, especially for those tasks concerning about complicated interactions between vertices such as link prediction. We also explore the performance of our framework via vertex classification and case studies, which again confirms the flexibility and superiority of our models.\n\n2 Related Work\nWith the rapid growth of large-scale social networks, network embedding, i.e. network representation learning has been proposed as a critical technique for network analysis tasks.\nIn recent years, there have been a large number of NE models proposed to learn efficient vertex embeddings (Tang and Liu, 2009; Cao et al., 2015; Wang et al., 2016). For example, DeepWalk (Perozzi et al., 2014) performs random walks over networks and introduces an efficient word representation learning model, Skip-Gram (Mikolov et al., 2013a), to learn network embeddings. LINE (Tang et al., 2015) optimizes the joint and conditional probabilities of edges in large-scale networks to learn vertex representations. Node2vec (Grover and Leskovec, 2016) modifies the random walk strategy in DeepWalk into biased random walks to explore the network structure more efficiently. Nevertheless, most of these NE models only encode the structural information into vertex embeddings, without considering heterogenous information accompanied with vertices in real-world social networks.\nTo address this issue, researchers make great efforts to incorporate heterogenous information into conventional NE models. For instance, Yang et al. (2015) present text-associated DeepWalk (TADW) to improve matrix factorization based DeepWalk with text information. Tu et al. (2016) propose max-margin DeepWalk (MMDW) to learn discriminative network representations by utilizing labeling information of vertices. Chen et al. (2016) propose group-enhanced network embedding (GENE) to integrate existing group information in NE. Sun et al. (2016) regard text content as a special kind of vertices, and propose contextenhanced network embedding (CENE) through leveraging both structural and textural information to learn network embeddings.\nTo the best of our knowledge, all existing NE models focus on learning context-free embeddings, but ignore the diverse roles when a vertex\n3\n201\n202\n203\n204\n205\n206\n207\n208\n209\n210\n211\n212\n213\n214\n215\n216\n217\n218\n219\n220\n221\n222\n223\n224\n225\n226\n227\n228\n229\n230\n231\n232\n233\n234\n235\n236\n237\n238\n239\n240\n241\n242\n243\n244\n245\n246\n247\n248\n249\n250\n251\n252\n253\n254\n255\n256\n257\n258\n259\n260\n261\n262\n263\n264\n265\n266\n267\n268\n269\n270\n271\n272\n273\n274\n275\n276\n277\n278\n279\n280\n281\n282\n283\n284\n285\n286\n287\n288\n289\n290\n291\n292\n293\n294\n295\n296\n297\n298\n299\ninteracts with others. In contrast, we assume that a vertex has different embeddings according to which vertex it interacts with, and propose CANE to learn context-aware vertex embeddings.\n\n3 Problem Formulation\nWe first give basic notations and definitions in this work. Suppose there is an information network G = (V,E, T ), where V is the set of vertices, E ⊆ V ×V are edges between vertices, and T denotes the text information of vertices. Each edge eu,v ∈ E represents the relationship between two vertices (u, v), with an associated weight wu,v. Here, the text information of a specific vertex v ∈ V is represented as a word sequence Sv = (w1, w2, . . . , wnv), where nv = |Sv|. NRL aims to learn a low-dimensional embedding v ∈ Rd for each vertex v ∈ V according to its network structure and associated information, e.g. text and labels. Note that, d |V | is the dimension of representation space.\nDefinition 1. Context-free Embeddings: Conventional NRL models learn context-free embedding for each vertex. It means the embedding of a vertex is fixed and won’t change with respect to its context information (i.e., another vertex it interacts with).\nDefinition 2. Context-aware Embeddings: Different from existing NRL models that learn context-free embeddings, CANE learns various embeddings for a vertex according to its different contexts. Specifically, for an edge eu,v, CANE learns context-aware embeddings v(u) and u(v).\n\n4 The Method\n\n\n4.1 Overall Framework\nTo take full use of both network structure and associated text information, we propose two types of embeddings for a vertex v, i.e., structurebased embedding vs and text-based embedding vt. Structure-based embedding is able to capture the information in the network structure, while text-based embedding can capture the textual meanings lying in the associated text information. With these embeddings, we can simply concatenate them and obtain the vertex embeddings as v = vs⊕vt, where ⊕ indicates the concatenation operation. Note that, the text-based embedding vt can be either context-free or context-aware, which will be introduced detailedly in section 4.4 and 4.5\nrespectively. When vt is context-aware, the overall vertex embeddings v will be context-aware as well.\nWith above definitions, CANE aims to maximize the overall objective of edges as follows:\nL = ∑ e∈E L(e). (1)\nHere, the objective of each edge L(e) consists of two parts as follows:\nL(e) = Ls(e) + Lt(e), (2)\nwhere Ls(e) denotes the structure-based objective and Lt(e) represents the text-based objective.\nIn the following part, we give detailed introduction to the two objectives respectively.\n\n4.2 Structure-based Objective\nWithout loss of generality, we assume the network is directed, as an undirected edge can be considered as two directed edges with opposite directions and equal weights.\nThus, the structure-based objective aims to measure the log-likelihood of a directed edge using the structure-based embeddings as\nLs(e) = wu,v log p(v s|us). (3)\nFollowing LINE (Tang et al., 2015), we define the conditional probability of v generated by u in Eq. (3) as\np(vs|us) = exp(u s · vs)∑\nz∈V exp(u s · zs)\n. (4)\n\n4.3 Text-based Objective\nVertices in real-world social networks usually accompany with associated text information. Therefore, we propose the text-based objective to take advantage of these text information, as well as learn text-based embeddings for vertices.\nThe text-based objective Lt(e) can be defined with various measurements. To be compatible with Ls(e), we define Lt(e) as follows:\nLt(e) = α ·Ltt(e) + β ·Lts(e) + γ ·Lst(e), (5)\nwhere α, β and γ control the weights of various parts, and\nLtt(e) = wu,v log p(v t|ut), Lts(e) = wu,v log p(v t|us), Lst(e) = wu,v log p(v s|ut).\n(6)\n4\n301\n302\n303\n304\n305\n306\n307\n308\n309\n310\n311\n312\n313\n314\n315\n316\n317\n318\n319\n320\n321\n322\n323\n324\n325\n326\n327\n328\n329\n330\n331\n332\n333\n334\n335\n336\n337\n338\n339\n340\n341\n342\n343\n344\n345\n346\n347\n348\n349\n350\n351\n352\n353\n354\n355\n356\n357\n380\n381\n382\n383\n384\n385\n386\n387\n388\n389\n390\n391\n392\n393\n394\n395\n396\n397\n398\n399\nThe conditional probabilities in Eq. (6) map the two types of vertex embeddings into the same representation space, but do not enforce them to be identical for the consideration of their own characteristics. Similarly, we employ softmax function for calculating the probabilities, as in Eq. (4).\nThe structure-based embeddings are regarded as parameters, the same as in conventional NE models. But for text-based embeddings, we intend to obtain them from associated text information of vertices. Besides, the text-based embeddings can be obtained either in context-free ways or in context-aware ones. In the following sections, we will give detailed introduction respectively.\n\n4.4 Context-Free Text Embedding\nThere has been a variety of neural models to obtain text embeddings from a word sequence, such as convolutional neural networks (CNN) (Blunsom et al., 2014; Johnson and Zhang, 2014; Kim, 2014) and recurrent neural networks (RNN) (Kiros et al., 2015; Tai et al., 2015).\nIn this work, we investigate different neural networks for text modeling, including CNN, Bidirectional RNN (Schuster and Paliwal, 1997) and GRU (Cho et al., 2014), and employ the best performed CNN, which can capture the local semantic dependency among words.\nTaking the word sequence of a vertex as input, CNN obtains the text-based embedding through three layers, i.e. looking-up, convolution and pooling.\nLooking-up. Given a word sequence S = (w1, w2, . . . , wn), the looking-up layer transforms each word wi ∈ S into its corresponding word embedding wi ∈ Rd ′ and obtains embedding sequence as S = (w1,w2, . . . ,wn). Here, d′ indicates the dimension of word embeddings.\nConvolution. After looking-up, the convolution layer extracts local features of input embedding sequence S. To be specific, it performs convolution operation over a sliding window of length l using a convolution matrix C ∈ Rd×(l×d′) as follows:\nxi = C · Si:i+l−1 + b, (7)\nwhere Si:i+l−1 denotes the concatenation of word embeddings within the i-th window and b is the bias vector. Note that, we add zero padding vectors (Hu et al., 2014) at the edge of the sentence.\nMax-pooling. To obtain the text embedding vt, we operate max-pooling and non-linear transfor-\nmation over {xi0, . . . ,xin} as follows:\nri = tanh(max(x i 0, . . . ,x i n)), (8)\nAt last, we encode the text information of a vertex with CNN and obtain its text-based embedding vt = [r1, . . . , rd]\nT . As vt is irrelevant to the other vertices it interacts with, we name it as contextfree text embedding.\n\n4.5 Context-Aware Text Embedding\nAs stated before, we assume that a specific vertex plays different roles when interacting with others vertices. In other words, each vertex should have its own points of focus about a specific vertex, which leads to its context-aware text embeddings.\nTo achieve this, we employ mutual attention to obtain context-aware text embedding. It enables the pooling layer in CNN to be aware of the vertex pair in an edge, in a way that text information from a vertex can directly affect the text embedding of the other vertex, and vice versa.\nIn Fig. 2, we give an illustration of the generating process of context-aware text embedding. Given an edge eu,v with two corresponding text sequences Su and Sv, we can get the matrices P ∈ Rd×m and Q ∈ Rd×n through convolution layer. Here, m and n represent the lengths of Su\n5\n401\n402\n403\n404\n405\n406\n407\n408\n409\n410\n411\n412\n413\n414\n415\n416\n417\n418\n419\n420\n421\n422\n423\n424\n425\n426\n427\n428\n429\n430\n431\n432\n433\n434\n435\n436\n437\n438\n439\n440\n441\n442\n443\n444\n445\n446\n447\n448\n449\n450\n451\n452\n453\n454\n455\n456\n457\n458\n459\n460\n461\n462\n463\n464\n465\n466\n467\n468\n469\n470\n471\n472\n473\n474\n475\n476\n477\n478\n479\n480\n481\n482\n483\n484\n485\n486\n487\n488\n489\n490\n491\n492\n493\n494\n495\n496\n497\n498\n499\nand Sv respectively. By introducing an attentive matrix A ∈ Rd×d, we compute the correlation matrix F ∈ Rm×n as follows:\nF = tanh(PTAQ). (9)\nNote that, each element Fi,j in F represents the pair-wise correlation score between two hidden vectors, i.e., Pi and Qj .\nAfter that, we conduct pooling operations along rows and columns of F to generate the importance vectors, named as row-pooling and column pooling respectively. According to our experiments, mean-pooling performs better than max-pooling. Thus, we employ mean-pooling operation as follows:\ngpi = mean(Fi,1, . . . ,Fi,n), gqi = mean(F1,i, . . . ,Fm,i). (10)\nThe importance vectors of P and Q are obtained as gp = [gp1 , . . . , g p m]T and gq = [gq1, . . . , g q n]T .\nNext, we employ softmax function to transform importance vectors gp and gq to attention vectors ap and aq. For instance, the i-th element of ap is formalized as follows:\napi = exp(gpi )∑\nj∈[1,m] exp(g p j ) . (11)\nAt last, the context-aware text embeddings of u and v are computed as\nut(v) = Pa p, vt(u) = Qa q.\n(12)\nNow, given an edge (u, v), we can obtain the context-aware embeddings of vertices with their structure embeddings and context-aware text embeddings as u(v) = us⊕ut(v) and v(u) = v s⊕vt(u).\n\n4.6 Optimization of CANE\nAccording to Eq. (3) and Eq. (6), CANE aims to maximize several conditional probabilities between u ∈ {us,ut(v)} and v ∈ {v\ns,vt(u)}. It is intuitive that optimizing the conditional probability using softmax function is computationally expensive. Thus, we employ negative sampling (Mikolov et al., 2013b) and transform the objective into the following form:\nlog σ(uT ·v)+ k∑\ni=1\nEz∼P (v)[log σ(−uT ·z)], (13)\nwhere k is the number of negative samples and σ represents the sigmoid function. P (v) ∝ dv3/4 denotes the distribution of vertices, where dv is the out-degree of v.\nAfterwards, we employ Adam (Kingma and Ba, 2015) to optimize the transformed objective.\n\n5 Experiments\nIn order to investigate the effectiveness of CANE on modeling relationships between vertices, we conduct experiments of link prediction on several real-world datasets. Besides, we also employ vertex classification to verify whether contextaware embeddings of a vertex can compose a highquality context-free embedding in return.\n\n5.1 Datasets\nDatasets Cora HepTh Zhihu\n#Vertices 2, 277 1, 038 10, 000 #Edges 5, 214 1, 990 43, 894 #Labels 7 − −\nTable 1: Statistics of Datasets.\nWe select three real-world network datasets as follows:\nCora1 is a typical paper citation network constructed by (McCallum et al., 2000). After filtering out papers without text information, there are 2, 277 machine learning papers in this network, which are divided into 7 categories.\nHepTh2 (High Energy Physics Theory) is another citation network from arXiv3 released by (Leskovec et al., 2005). We filter out papers without abstract information and retain 1, 038 papers at last.\nZhihu4 is the largest online Q&A website in China. Users follow each other and answer questions in this site. We randomly crawl 10, 000 active users from Zhihu, and take the descriptions of their concerned topics as text information.\nThe detailed statistics are listed in Table 1.\n\n5.2 Baselines\nWe employ the following methods as baselines:\nStructure-only: DeepWalk (Perozzi et al., 2014) performs random walks over networks and employ Skip-Gram 1https://people.cs.umass.edu/∼mccallum/data.html 2https://snap.stanford.edu/data/cit-HepTh.html 3https://arxiv.org/ 4https://www.zhihu.com/\n6\n501\n502\n503\n504\n505\n506\n507\n508\n509\n510\n511\n512\n513\n514\n515\n516\n517\n518\n519\n520\n521\n522\n523\n524\n525\n526\n527\n528\n529\n530\n531\n532\n533\n534\n535\n536\n537\n538\n539\n540\n541\n542\n543\n544\n545\n546\n547\n548\n549\n550\n551\n552\n553\n554\n555\n556\n557\n558\n559\n560\n561\n562\n563\n564\n565\n566\n567\n568\n569\n570\n571\n572\n573\n574\n575\n576\n577\n578\n579\n580\n581\n582\n583\n584\n585\n586\n587\n588\n589\n590\n591\n592\n593\n594\n595\n596\n597\n598\n599\n%Removed edges 15% 25% 35% 45% 55% 65% 75% 85% 95%\nDeepWalk 56.0 63.0 70.2 75.5 80.1 85.2 85.3 87.8 90.3 LINE 55.0 58.6 66.4 73.0 77.6 82.8 85.6 88.4 89.3 node2vec 55.9 62.4 66.1 75.0 78.7 81.6 85.9 87.3 88.2\nNaive Combination 72.7 82.0 84.9 87.0 88.7 91.9 92.4 93.9 94.0 TADW 86.6 88.2 90.2 90.8 90.0 93.0 91.0 93.4 92.7 CENE 72.1 86.5 84.6 88.1 89.4 89.2 93.9 95.0 95.9\nCANE (text only) 78.0 80.5 83.9 86.3 89.3 91.4 91.8 91.4 93.3 CANE (w/o attention) 85.8 90.5 91.6 93.2 93.9 94.6 95.4 95.1 95.5\nCANE 86.8 91.5 92.2 93.9 94.6 94.9 95.6 96.6 97.7\nTable 2: AUC values on Cora. (α = 1.0, β = 0.3, γ = 0.3)\nmodel (Mikolov et al., 2013a) to learn vertex embeddings.\nLINE (Tang et al., 2015) learns vertex embeddings in large-scale networks using first-order and second-order proximities.\nNode2vec (Grover and Leskovec, 2016) proposes a biased random walk algorithm based on DeepWalk to explore neighborhood architecture more efficiently.\nStructure and Text: Naive Combination: We simply concatenate the best-performed structure-based embeddings with CNN based embeddings to represent the vertices.\nTADW (Yang et al., 2015) employs matrix factorization to incorporate text features of vertices into network embeddings.\nCENE (Sun et al., 2016) leverages both structure and textural information by regarding text content as a special kind of vertices, and optimizes the probabilities of heterogenous links.\n\n5.3 Evaluation Metrics and Experiment Settings\nFor link prediction, we adopt a standard evaluation metric AUC (Hanley and McNeil, 1982), which represents the probability that vertices in a random unobserved link are more similar than those in a random nonexistent link.\nFor vertex classification, we employ L2regularized logistic regression (L2R-LR) (Fan et al., 2008) to train classifiers, and evaluate the classification accuracies of various methods.\nTo be fair, we set the embedding dimension to 200 for all methods. In LINE, we set the number of negative samples to 5; we learn the 100 dimensional first-order and second-order embeddings respectively, and concatenate them to form the 200 dimensional embeddings. In node2vec, we employ grid search and select the best per-\nformed hyper-parameters for training. We also employ grid search to set the hyper-parameters α, β and γ in CANE. Besides, we set the number of negative samples k to 1 in CANE to speed up the training process. To demonstrate the effectiveness of considering attention mechanism and two types of objectives in Eqs. (3) and (6), we design three versions of CANE for evaluation, i.e., CANE with text only, CANE without attention and CANE.\n\n5.4 Link Prediction\nAs shown in Table 2, Table 3 and Table 4, we evaluate the AUC values while removing different ratios of edges on Cora, HepTh and Zhihu respectively. Note that, when we only keep 5% edges for training, most vertices are isolated, which results in the poor and meaningless performance of all the methods. Thus, we omit the results under this training ratio. From these tables, we have the following observations:\n(1) Our proposed CANE consistently achieves significant improvement comparing to all the baselines on all different datasets and different training ratios. It indicates the effectiveness of CANE when applied to link prediction task, and verifies that CANE has the capability of modeling relationships between vertices precisely.\n(2) What calls for special attention is that, both CENE and TADW exhibit unstable performance under various training ratios. Specifically, CENE performs poorly under small training ratios, because it reserves much more parameters (e.g., convolution kernels and word embeddings) than TADW, which need more data for training. Different from CENE, TADW performs much better under small training ratios, because DeepWalk based methods can explore the sparse network structure well through random walks even with limited edges. However, it achieves poor performance\n7\n601\n602\n603\n604\n605\n606\n607\n608\n609\n610\n611\n612\n613\n614\n615\n616\n617\n618\n619\n620\n621\n622\n623\n624\n625\n626\n627\n628\n629\n630\n631\n632\n633\n634\n635\n636\n637\n638\n639\n640\n641\n642\n643\n644\n645\n646\n647\n648\n649\n650\n651\n652\n653\n654\n655\n656\n657\n658\n659\n660\n661\n662\n663\n664\n665\n666\n667\n668\n669\n670\n671\n672\n673\n674\n675\n676\n677\n678\n679\n680\n681\n682\n695\n696\n697\n698\n699\n%Removed edges 15% 25% 35% 45% 55% 65% 75% 85% 95%\nDeepWalk 55.2 66.0 70.0 75.7 81.3 83.3 87.6 88.9 88.0 LINE 53.7 60.4 66.5 73.9 78.5 83.8 87.5 87.7 87.6 node2vec 57.1 63.6 69.9 76.2 84.3 87.3 88.4 89.2 89.2\nNaive Combination 78.7 82.1 84.7 88.7 88.7 91.8 92.1 92.0 92.7 TADW 87.0 89.5 91.8 90.8 91.1 92.6 93.5 91.9 91.7 CENE 86.2 84.6 89.8 91.2 92.3 91.8 93.2 92.9 93.2\nCANE (text only) 83.8 85.2 87.3 88.9 91.1 91.2 91.8 93.1 93.5 CANE (w/o attention) 84.5 89.3 89.2 91.6 91.1 91.8 92.3 92.5 93.6\nCANE 90.0 91.2 92.0 93.0 94.2 94.6 95.4 95.7 96.3\nTable 3: AUC values on HepTh. (α = 0.7, β = 0.2, γ = 0.2)\n%Removed edges 15% 25% 35% 45% 55% 65% 75% 85% 95%\nDeepWalk 56.6 58.1 60.1 60.0 61.8 61.9 63.3 63.7 67.8 LINE 52.3 55.9 59.9 60.9 64.3 66.0 67.7 69.3 71.1 node2vec 54.2 57.1 57.3 58.3 58.7 62.5 66.2 67.6 68.5\nNaive Combination 55.1 56.7 58.9 62.6 64.4 68.7 68.9 69.0 71.5 TADW 52.3 54.2 55.6 57.3 60.8 62.4 65.2 63.8 69.0 CENE 56.2 57.4 60.3 63.0 66.3 66.0 70.2 69.8 73.8\nCANE (text only) 55.6 56.9 57.3 61.6 63.6 67.0 68.5 70.4 73.5 CANE (w/o attention) 56.7 59.1 60.9 64.0 66.1 68.9 69.8 71.0 74.3\nCANE 56.8 59.3 62.9 64.5 68.9 70.4 71.4 73.6 75.4\nTable 4: AUC values on Zhihu. (α = 1.0, β = 0.3, γ = 0.3)\nunder large ones, as its simplicity and the limitation of bag-of-words assumption. On the contrary, CANE has a stable performance on various situations. It demonstrates the flexibility and robustness of CANE.\n(3) By introducing attention mechanism, the learnt context-aware embeddings obtain considerable improvements than the ones without attention. It verifies our assumption that a specific vertex should play different roles when interacting with other vertices, and thus benefits the relevant link prediction task.\nTo summarize, all the above observations demonstrate that CANE is able to learn highquality context-aware embeddings, which are conducive to estimating the relationship between vertices precisely. Moreover, the experimental results on link prediction task state the effectiveness and robustness of CANE.\n\n5.5 Vertex Classification\nIn CANE, we obtain various embeddings of a vertex according to the vertex it connects to. It’s intuitive that the obtained context-aware embeddings are naturally applicable to link prediction task. However, network analysis tasks, such as vertex classification and clustering, require a global embedding, rather than several context-aware embed-\ndings for each vertex. To demonstrate the capability of CANE to solve these issues, we generate the global embedding of a vertex u by simply averaging all the contextaware embeddings as follows:\nu = 1\nN ∑ (u,v)|(v,u)∈E u(v),\nwhere N indicates the number of context-aware embeddings of u.\n50\n55 60 65 70 75 80 85 90 95\n100\nDe ep\nW alk LI NE\nno de\n2v ec NC TA DW CE NE\nCA NE\n(te xt\non ly)\nCA NE\n(w /0\nat ten\ntio n) CA NE\nA cc\nur ac\ny (×\n1 00\n)\nWith the generated global embeddings, we conduct experiments of vertex classification on Cora. As shown in Fig. 3, we observe that:\n(1) CANE achieves comparable performance with state-of-the-art model CENE. It states that the\n8\n701\n702\n703\n704\n705\n706\n707\n708\n709\n710\n711\n712\n713\n714\n715\n716\n717\n718\n719\n720\n721\n722\n723\n724\n725\n726\n727\n728\n729\n730\n731\n732\n733\n734\n735\n736\n737\n738\n739\n740\n741\n742\n743\n744\n745\n746\n747\n748\n749\n775\n776\n777\n778\n779\n780\n781\n782\n783\n784\n785\n786\n787\n788\n789\n790\n791\n792\n793\n794\n795\n796\n797\n798\n799\nlearnt context-aware embeddings can transform into high-quality context-free embeddings through simple average operation, which can be further employed to other network analysis tasks.\n(2) With the introduction of mutual attention mechanism, CANE has an encouraging improvement than the one without attention, which is in accordance with the results of link prediction. It denotes that CANE is flexible to various network analysis tasks.\n\n5.6 Case Study\nTo demonstrate the significance of mutual attention on selecting meaningful features from text information, we visualize the heat maps of two vertex pairs in Fig. 4. Note that, every word in this figure accompanies with various background colors. The stronger the background color is, the larger the weight of this word is. The weight of each word is calculated according to the attention weights as follows.\nFor each vertex pair, we can get the attention weight of each convolution window according to Eq. (11). To obtain the weights of words, we assign the attention weight to each word in this window, and add the attention weights of a word together as its final weight.\nWe select three connected vertices in Cora for example, denoted as A, B and C. From Fig. 4, we observe that, though there exists citation relations with identical paper A, paper B and C concern about different parts of A. The attention weights over A in edge #1 are assigned to “reinforcement learning”. On the contrary, the weights in edge #2 are assigned to “machine learning’”, “supervised learning algorithms” and “complex stochastic models”. Moreover, all these key elements in A can find corresponding words in B and C. It’s intuitive that these key elements give an exact explanation on the citation relations. The discovered significant correlations between vertex pairs reflects the effectiveness of mutual attention mechanism, as well as the capability of CANE for modeling relations precisely.\n\n6 Conclusion and Future Work\nIn this paper, we propose the concept of ContextAware Network Embedding (CANE) for the first time, which aims to learn various context-aware embeddings for a vertex according to the neighbors it interacts with. Specifically, we implement\nCANE on text-based information networks with proposed mutual attention mechanism, and conduct experiments on several real-world information networks. Experimental results on link prediction demonstrate that CANE is effective for modeling the relationship between vertices. Besides, the learnt context-aware embeddings can compose high-quality context-free embeddings.\nWe will explore the following directions in future:\n(1) We have investigated the effectiveness of CANE on text-based information networks. In future, we will strive to implement CANE on wider variety of information networks with multi-modal data, such as labels, images and so on.\n(2) CANE encodes latent relations between vertices into their context-aware embeddings. Furthermore, there usually exist explicit relations in social networks (e.g., families, friends and colleagues relations between social network users), which are expected to be critical to NE. Thus, we want to explore how to incorporate and predict these explicit relations between vertices in NE.\n9\n801\n802\n803\n804\n805\n806\n807\n808\n809\n810\n811\n812\n813\n814\n815\n816\n817\n818\n819\n820\n821\n822\n823\n824\n825\n826\n827\n828\n829\n830\n831\n832\n833\n834\n835\n836\n837\n838\n839\n840\n841\n842\n843\n844\n845\n846\n847\n848\n849\n850\n851\n852\n853\n854\n855\n856\n857\n858\n859\n860\n861\n862\n863\n864\n865\n866\n867\n868\n869\n870\n871\n872\n873\n874\n875\n876\n877\n878\n879\n880\n881\n882\n883\n884\n885\n886\n887\n888\n889\n890\n891\n892\n893\n894\n895\n896\n897\n898\n899\n",
    "rationale": "This paper addresses the network embedding problem by introducing a neural\nnetwork model which uses both the network structure and associated text on the\nnodes, with an attention model to vary the textual representation based on the\ntext of the neighboring nodes.\n\n- Strengths:\n\nThe model leverages both the network and the text to construct the latent\nrepresentations, and the mutual attention approach seems sensible.\n\nA relatively thorough evaluation is provided, with multiple datasets,\nbaselines, and evaluation tasks.\n\n- Weaknesses:\n\nLike many other papers in the \"network embedding\" literature, which use neural\nnetwork techniques inspired by word embeddings to construct latent\nrepresentations of nodes in a network, the previous line of work on\nstatistical/probabilistic modeling of networks is ignored.  In particular, all\n\"network embedding\" papers need to start citing, and comparing to, the work on\nthe latent space model of Peter Hoff et al., and subsequent papers in both\nstatistical and probabilistic machine learning publication venues:\n\nP.D. Hoff, A.E. Raftery, and M.S. Handcock. Latent space approaches to social\nnetwork analysis. J. Amer. Statist. Assoc., 97(460):1090–1098, 2002.\n\nThis latent space network model, which embeds each node into a low-dimensional\nlatent space, was written as far back as 2002, and so it far pre-dates neural\nnetwork-based network embeddings.\n\nGiven that the aim of this paper is to model differing representations of\nsocial network actors' different roles, it should really cite and compare to\nthe mixed membership stochastic blockmodel (MMSB):\n\nAiroldi, E. M., Blei, D. M., Fienberg, S. E., & Xing, E. P. (2008). Mixed\nmembership stochastic blockmodels. Journal of Machine Learning Research.\n\nThe MMSB allows each node to randomly select a different \"role\" when deciding\nwhether to form each edge.\n\n- General Discussion:\n\nThe aforementioned statistical models do not leverage text, and they do not use\nscalable neural network implementations based on negative sampling, but they\nare based on well-principled generative models instead of heuristic neural\nnetwork objective functions and algorithms.  There are more recent extensions\nof these models and inference algorithms which are more scalable, and which do\nleverage text.\n\nIs the difference in performance between CENE and CANE in Figure 3\nstatistically insignificant? (A related question: were the experiments repeated\nmore than once with random train/test splits?)\n\nWere the grid searches for hyperparameter values, mentioned in Section 5.3,\nperformed with evaluation on the test set (which would be problematic), or on a\nvalidation set, or on the training set?",
    "rating": 5
  }
]