[
  {
    "title": "Automatic Annotation and Evaluation of Error Types for Grammatical Error Correction",
    "abstract": "Until now, error type performance for Grammatical Error Correction (GEC) systems could only be measured in terms of recall because system output is not annotated. In this paper, we overcome this problem by using a linguisticallyenhanced alignment to automatically extract the edits between parallel original and corrected sentences and then classify them using a new dataset-independent rule-based classifier. As human experts rated the predicted error types as “Good” or “Acceptable” in at least 95% of cases, we applied our approach to the system output produced in the CoNLL-2014 shared task to carry out a detailed analysis of system error type performance for the first time.",
    "text": "1 Introduction\nThe Conference on Natural Language Learning (CoNLL) shared task of 2014 (Ng et al., 2014) required teams to build systems that were capable of correcting all types of grammatical errors in learner text. While the submitted systems were evaluated against text that had been explicitly annotated with error type information, the teams themselves were not required to annotate their output in a similar way. This mismatch ultimately meant that a detailed error type analysis of each system was impossible and that error type performance could only be measured in terms of recall.\nThe main aim of this paper is to rectify this situation and provide a method by which unannotated error correction data can be automatically annotated with error type information. This is important because some systems may be more effective at correcting certain error types than oth-\ners, yet this information is otherwise concealed in an overall score. Although several new metrics and methodologies for Grammatical Error Correction (GEC) have been proposed since the end of the CoNLL-2014 shared task (Felice and Briscoe, 2015; Bryant and Ng, 2015; Napoles et al., 2015; Grundkiewicz et al., 2015), none of these are currently capable of producing individual error type scores.\nOur approach consists of two main steps. First, we automatically extract the edits between parallel original and corrected sentences by means of a linguistically-enhanced alignment algorithm (Felice et al., 2016), and second, we classify them according to a new rule-based framework specifically designed with error type evaluation in mind. This enables us to automatically annotate system hypothesis corrections with the same alignment and error type information as the reference and hence carry out a more detailed evaluation. The tool we use to do this will be released with this paper.\n\n2 Edit Extraction\nThe first stage of automatic annotation is edit extraction. Specifically, given an original and corrected sentence pair, we need to determine the start and end boundaries of any edits. This is fundamentally an alignment problem:\nThe first attempt at automatic edit extraction was made by Swanson and Yamangil (2012), who simply used the Levenshtein distance to align original and corrected sentence pairs. As the Levenshtein distance only aligns individual tokens how-\n2\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n112\n113\n114\n115\n116\n117\n118\n119\n120\n121\n122\n123\n124\n125\n126\n127\n128\n129\n130\n131\n132\n133\n134\n135\n136\n137\n138\n139\n140\n141\n142\n143\n144\n145\n146\n147\n148\n149\n150\n151\n152\n153\n154\n155\n156\n157\n158\n159\n160\n161\n162\n163\n164\n165\n166\n167\n168\n169\n170\n171\n172\n173\n174\n175\n176\n177\n178\n179\n180\n181\n182\n183\n184\n185\n186\n187\n188\n189\n190\n191\n192\n193\n194\n195\n196\n197\n198\n199\never, they also merged all adjacent non-matches in an effort to capture multi-token edits. Xue and Hwa (2014) subsequently improved on Swanson and Yamangil’s work by training a maximum entropy classifier to predict whether edits should be merged or not.\nMost recently, Felice et al. (2016) proposed a new method of edit extraction using a linguistically-enhanced alignment supported by a set of merging rules. In particular, they incorporated various linguistic information, such as partof-speech and lemma, into the cost function of the Damerau-Levenshtein1 algorithm to make it more likely that tokens with similar linguistic properties align. This approach ultimately proved most effective at approximating human edits in several datasets (80-85 F1), and so we use it in the present study.\n\n3 Automatic Error Typing\nHaving extracted the edits, the next step is to assign them error types. While Swanson and Yamangil (2012) did this by means of maximum entropy classifiers, one disadvantage of this approach is that such classifiers are biased towards their particular training corpora. In particular, the fact that different datasets are annotated according to different standards means that it is inappropriate to evaluate the predicted error types of an in-domain corpus against the predicted error types of an out-of-domain corpus (c.f. Xue and Hwa (2014)). Instead, a dataset-agnostic error type evaluation is much more desirable.\nTo solve this problem, we took inspiration from Swanson and Yamangil’s (2012) observation that most error types are based on part-of-speech (POS) categories, and wrote a rule to classify an edit based only on its automatic POS tags. We then added another rule to differentiate Missing, Unnecessary and Replacement errors depending on whether tokens were inserted, deleted or substituted. From there, we extended our approach to classify errors that are not well-characterised by POS alone (such as Spelling or Word Order) and ensured that all types are assigned based only on automatically obtained properties of the data.\nOne of the key strengths of this approach is that by being dependent only on automatic mark-up information, our classifier is entirely dataset in-\n1Damerau-Levenshtein is an extension of Levenshtein that also handles transpositions; e.g. AB→BA\ndependent and does not require labelled training data. This is in contrast with machine learning approaches which require different classifiers for different datasets and which ultimately may not be entirely compatible with each other. Instead, our approach is analogous to automating the annotation guidelines given to human annotators.\nA second significant advantage of our approach is that it is also always possible to determine precisely why an edit was assigned to a particular error category. In contrast, human and machine learning classification decisions are often less transparent and may furthermore be subject to annotator bias. Moreover, by being fully deterministic, our approach bypasses bias effects altogether and should hence be more consistent.\n\n3.1 Automatic Markup\nThe prerequisites for our rule-based classifier are that each token in both the original and corrected sentence is POS tagged, lemmatized, stemmed and dependency parsed. We use spaCy2 v1.6 for all but the stemming, which is performed by the Lancaster Stemmer in NLTK.3 Since fine-grained POS tags are often too detailed for the purposes of error evaluation, we also map spaCy’s Penn Treebank style tags to the coarser set of Universal Dependency tags.4 We use the latest Hunspell GB-large dictionary5 to help classify non-word errors. The marked-up tokens in an edit span are then input to our classifier and an error type is returned.\n\n3.2 Error Categories\nThe complete list of 25 error types in our new framework is shown in Table 2. Note that most of them can be prefixed with ‘M:’, ‘R:’ or ‘U:’, depending on whether they describe a Missing, Replacement, or Unnecessary edit, to enable evaluation at different levels of granularity (See Appendix A for all valid combinations). This means we can choose to evaluate, for example, only replacement errors (anything prefixed by ‘R:’), only noun errors (anything suffixed with ‘NOUN’) or only replacement noun errors (‘R:NOUN’). This flexibility allows us to make more detailed observations about different aspects of system perfor-\n2https://spacy.io/ 3http://www.nltk.org/ 4http://universaldependencies.org/tagset-conversion/\nen-penn-uposf.html 5https://sourceforge.net/projects/wordlist/files/speller/ 2016.11.20/\n3\n201\n202\n203\n204\n205\n206\n207\n208\n209\n210\n211\n212\n213\n214\n215\n216\n217\n218\n219\n220\n221\n222\n223\n224\n225\n226\n227\n228\n229\n230\n231\n232\n233\n234\n235\n236\n237\n238\n239\n240\n241\n242\n243\n244\n245\n246\n247\n248\n249\n278\n279\n280\n281\n282\n283\n284\n285\n286\n287\n288\n289\n290\n291\n292\n293\n294\n295\n296\n297\n298\n299\nCode Meaning Description / Example ADJ Adjective big → wide ADJ:FORM Adjective Form Comparative or superlative adjective errors.goodest → best, bigger → biggest, more easy → easier ADV Adverb speedily → quickly CONJ Conjunction and → but CONTR Contraction n’t → not DET Determiner the → a MORPH Morphology Tokens have the same lemma, but nothing else in common.quick (adj) → quickly (adv) NOUN Noun person → people NOUN:INFL Noun Inflection Count-mass noun errors.informations → information NOUN:NUM Noun Number cat → cats NOUN:POSS Noun Possessive friends → friend’s ORTH Orthography Case and/or whitespace errors.Bestfriend → best friend OTHER Other Errors that do not fall into any other category (e.g. paraphrasing).at his best → well, job → professional PART Particle (look) in → (look) at PREP Preposition of → at PRON Pronoun ours → ourselves PUNCT Punctuation ! → . SPELL Spelling genectic → genetic, color → colour UNK Unknown The annotator detected an error but was unable to correct it. VERB Verb ambulate → walk VERB:FORM Verb Form Infinitives (with or without “to”), gerunds (-ing) and participles.to eat → eating, dancing → danced VERB:INFL Verb Inflection Misapplication of tense morphology.getted → got, fliped → flipped VERB:SVA Subject-Verb Agreement (He) have → (He) has VERB:TENSE Verb Tense Includes inflectional and periphrastic tense, modal verbs and passivization.eats → ate, eats → has eaten, eats → can eat, eats → was eaten WO Word Order only can → can only\nmance. One caveat concerning error scheme design is that it is always possible to add new categories for increasingly detailed error types; for instance, we currently label [could → should] a tense error, when it might otherwise be considered a modal error. The reason we do not call it a modal error, however, is because it would then become less clear how to handle other cases such as [can → should] and [has eaten → should eat], which might be considered a more complex combination of a modal and tense error. As it is impractical to create new categories and rules to differentiate between such narrow distinctions however, our final framework aims to be a compromise between informativeness and practicality.\n\n3.3 Classifier Evaluation\nAs our new error scheme is based only on automatically obtained properties of the data, there are no gold standard labels against which to evaluate classifier performance. For this reason, we instead carried out a small-scale manual evaluation, where\nwe simply asked 5 GEC researchers to rate the appropriateness of the predicted error categories for 200 randomly chosen edits in context (100 from FCE-test (Yannakoudakis et al., 2011) and 100 from CoNLL-2014) as “Good”, “Acceptable” or “Bad”. “Good’ meant the chosen category was the most appropriate for the given edit, “Acceptable” meant the chosen category was appropriate, but probably not optimum, while “Bad” meant the chosen category was not appropriate for the edit. Raters were warned that edit boundaries had been determined automatically, and hence might be unusual, but that they should focus on the appropriateness of the error category regardless of whether they agreed with the boundary or not.\nThe result of this evaluation is shown in Table 3. Significantly, all 5 raters individually considered at least 95% of our rule-based error types to be either “Good” or “Acceptable”, despite the degree of noise introduced by automatic edit extraction. Furthermore, whenever raters judged an edit as “Bad”, this could usually be traced back to a mistake made by the POS tagger; e.g. [ring\n4\n301\n302\n303\n304\n305\n306\n307\n308\n309\n310\n311\n312\n313\n314\n315\n316\n317\n318\n319\n320\n321\n322\n323\n324\n325\n326\n327\n328\n329\n330\n331\n332\n333\n334\n335\n336\n337\n338\n339\n340\n341\n342\n343\n344\n345\n346\n347\n348\n349\n350\n351\n352\n353\n354\n355\n356\n357\n358\n359\n360\n361\n362\n363\n364\n365\n366\n367\n368\n369\n370\n371\n372\n373\n374\n375\n376\n377\n378\n379\n380\n381\n382\n383\n384\n385\n386\n387\n388\n389\n390\n391\n392\n393\n394\n395\n396\n397\n398\n399\nRater Good Acceptable Bad 1 92.0% 4.0% 4.0% 2 89.5% 6.5% 4.0% 3 83.0% 13.0% 4.0% 4 84.5% 11.0% 4.5% 5 82.5% 15.5% 2.0% OVERALL 86.3% 10.0% 3.7%\nTable 3: The percent distribution for how each expert rated the appropriateness of the predicted error types. E.g. Rater 3 considered 83% of all predicted types to be “Good”.\n→ rings] might be considered a NOUN:NUM or VERB:SVA error depending on whether the tagger considered both sides of the edit nouns or verbs. Inter-annotator agreement was good at 0.724 κfree (Randolph, 2005).\nIn contrast, the best results using a classifier were between 50-70 F1 (Felice et al., 2016). Although these results are incomparable with previous approaches which were evaluated using a different metric and error scheme, we nevertheless believe that the high scores awarded by the raters validate the efficacy of our rule-based approach.\n\n4 CoNLL-2014 Shared Task Analysis\nTo demonstrate the value of our approach, we applied our automatic annotation tool to the data produced in the CoNLL-2014 shared task (Ng et al., 2014). In particular, we used our tool to generate annotated versions of the system output files produced by each participating team.6 Although our approach can be applied to any dataset, we chose CoNLL-2014 because it constitutes the largest collection of publicly available GEC system output.\nOne benefit of explicitly annotating the hypothesis files is that it makes evaluation much more straightforward. Specifically, if both the hypothesis and reference files are annotated in the same format, we need only compare the edits in each file to produce an F-score. In particular, for a given sentence, any edit with the same span and correction in both files is a true positive (TP), while the remaining edits in the hypothesis are false positives (FP) and the remaining edits in the reference are false negatives (FN). This is in contrast with all other metrics in GEC, which typically incorporate some sort of edit extraction or alignment component directly into their evalua-\n6http://www.comp.nus.edu.sg/∼nlp/conll14st.html\nM2 Scorer Our Approach Team Gold Auto Gold Auto AMU 35.01 35.06 32.67 32.22 CAMB 37.33 37.32 34.92 33.99 CUUI 36.79 37.64 34.15 34.68 IITB 5.90 5.97 5.77 5.74 IPN 7.09 7.69 6.12 6.15 NTHU 29.92 29.85 26.74 25.74 PKU 25.32 25.40 23.95 23.62 POST 30.88 31.02 28.43 28.00 RAC 26.68 26.89 23.39 23.21 SJTU 15.19 15.24 15.15 14.90 UFC 7.84 7.90 7.97 7.90 UMC 25.37 25.46 23.77 23.53\nTable 4: Overall scores for each team in CoNLL2014 using gold and auto references with both the M2 scorer and our simpler edit comparison approach. All scores are in terms of F0.5.\ntion algorithms (Dahlmeier and Ng, 2012; Felice and Briscoe, 2015; Napoles et al., 2015). Our approach, on the other hand, treats edit extraction and evaluation as separate tasks.\n\n4.1 Gold Reference vs. Auto Reference\nBefore evaluating the newly annotated hypothesis files against the reference, we must also address another mismatch: namely that the hypothesis edits were aligned and classified automatically, while the reference edits were aligned and classified manually using a different framework. Since evaluation is now a straightforward comparison between two files however, it is especially important that both files are processed in the same way. For instance, a hypothesis edit [have eating → has eaten] will not match the reference edits [have → has] and [eating → eaten] because the former is one edit while the latter is two edits, even though they equate to the same thing.\nWe can solve this problem by reprocessing the reference file in the same way as the hypothesis file. This means all the reference edits are subject to the same alignment and classification criteria as the hypothesis edits. While it may seem unorthodox to discard gold reference information in favour of automatic reference information, Table 4 shows that this has no significant impact on the results when using either the M2 scorer, the de facto standard of GEC evaluation (Dahlmeier and Ng, 2012), or our own approach.\n5\n401\n402\n403\n404\n405\n406\n407\n408\n409\n410\n411\n412\n413\n414\n415\n416\n417\n418\n419\n420\n421\n422\n423\n424\n425\n426\n427\n428\n429\n430\n431\n432\n433\n434\n435\n436\n437\n438\n439\n440\n441\n442\n443\n444\n445\n446\n447\n448\n449\n450\n451\n452\n453\n454\n455\n456\n457\n458\n459\n460\n461\n462\n463\n464\n465\n466\n467\n468\n469\n470\n471\n472\n473\n474\n475\n476\n477\n478\n479\n480\n481\n482\n483\n484\n485\n486\n487\n488\n489\n490\n491\n492\n493\n494\n495\n496\n497\n498\n499\nAMU CAMB CUUI IITB Type P R F0.5 P R F0.5 P R F0.5 P R F0.5\nMissing 43.61 14.36 30.98 46.32 30.00 41.77 26.71 18.62 24.57 15.38 0.59 2.56 Replacement 37.19 26.90 34.54 37.37 28.07 35.05 45.78 22.89 38.15 29.85 1.49 6.22 Unnecessary - - - 25.51 27.59 25.90 34.20 33.91 34.14 46.15 1.55 6.83\nIPN NTHU PKU POST Type P R F0.5 P R F0.5 P R F0.5 P R F0.5\nMissing 2.86 0.29 1.04 35.34 11.60 25.08 33.33 4.37 14.34 31.14 13.13 24.44 Replacement 9.87 3.86 7.53 27.61 19.15 25.37 29.62 18.32 26.37 33.16 19.32 29.00 Unnecessary 0.00 0.00 0.00 34.57 16.17 28.16 0.00 0.00 0.00 26.32 33.12 27.44\nRAC SJTU UFC UMC Type P R F0.5 P R F0.5 P R F0.5 P R F0.5\nMissing 1.49 0.27 0.78 62.50 4.42 17.24 - - - 40.08 23.57 35.16 Replacement 29.50 20.87 27.25 50.54 3.43 13.47 72.00 2.64 11.52 34.62 9.69 22.87 Unnecessary 0.00 0.00 0.00 17.65 11.51 15.95 - - - 16.89 17.33 16.98\nTable 5: Precision, recall and F0.5 for Missing, Unnecessary, and Replacement errors for each team. A dash indicates the team’s system did not attempt to correct the given error type (TP+FP = 0).\nWe validated this hypothesis for each team by means of bootstrap significance testing (Efron and Tibshirani, 1993) and found no statistically significant difference between auto and gold references (1,000 iterations, p > .05). This leads us to conclude that our auto annotations are qualitatively as good as human annotations.\nTable 4 also shows that M2 scores tend to be higher than our own, which initially led us to believe that our approach was underestimating performance. We subsequently found, however, that the M2 scorer in fact tends to overestimate performance (c.f. Felice and Briscoe (2015) and Napoles et al. (2015)).\nIn particular, given a choice between matching [have eating → has eaten] from Annotator 1 or [have → has] and [eating → eaten] from Annotator 2, the M2 scorer will always choose Annotator 2 because two true positives (TP) are worth more than one. Similarly, whenever the scorer encounters two false positives (FP) within a certain distance of each other,7 it merges them and treats them as one false positive; e.g. [is a cat → are a cats] is selected over [is → are] and [cat → cats] even though these edits are best handled separately. Ultimately, it can be said that the M2 scorer exploits its dynamic edit boundary prediction in order to maximise true positives and minimise false positives and hence produces slightly inflated scores.\n7The distance is controlled by the max unchanged words parameter which is set to 2 by default.\n\n4.2 Operation Tier\nIn our first category experiment, we simply investigated the performance of each system in terms of Unnecessary, Missing or Replacement edits. The results are shown in Table 5.\nThe most surprising result is that 5 teams failed to correctly resolve any unnecessary token errors at all (AMU, IPN, PKU, RAC, UFC). This is especially surprising given that we would expect unnecessary token errors to be easier to correct than others; a system need only detect and delete without having to propose any alternative. There is also no obvious explanation as to why these teams had difficulty with this error type because each of them employed different combinations of correction strategies including machine translation (MT), language modelling, classifiers and rules.\nIn contrast, CUUI’s classifier approach (Rozovskaya et al., 2014) was the most successful at correcting not only unnecessary token errors, but also replacement token errors, while CAMB’s hybrid MT approach (Felice et al., 2014) significantly outperformed all others in terms of missing token errors. It would hence make sense to combine these two approaches, and indeed recent research has shown this improves overall performance (Rozovskaya and Roth, 2016).\n\n4.3 General Error Types\nTable 6 shows precision, recall and F0.5 for each of the error types in our proposed framework for each team in CoNLL-2014. We refer the reader to the shared task paper for more information about\n6\n501\n502\n503\n504\n505\n506\n507\n508\n509\n510\n511\n512\n513\n514\n515\n516\n517\n518\n519\n520\n521\n522\n523\n524\n525\n526\n527\n528\n529\n530\n531\n532\n533\n534\n535\n536\n537\n538\n539\n540\n541\n542\n543\n544\n545\n546\n547\n548\n549\n550\n551\n552\n553\n554\n555\n556\n557\n558\n559\n560\n561\n562\n563\n564\n565\n566\n567\n568\n569\n570\n571\n572\n573\n574\n575\n576\n577\n578\n579\n580\n581\n582\n583\n584\n585\n586\n587\n588\n589\n590\n591\n592\n593\n594\n595\n596\n597\n598\n599\nAMU CAMB CUUI IITB IPN NTHU PKU POST RAC SJTU UFC UMC\nADJ P 7.14 9.09 - 0.00 0.00 0.00 50.00 0.00 11.11 0.00 - 4.35 R 9.38 12.82 - 0.00 0.00 0.00 6.67 0.00 3.33 0.00 - 3.57\nF0.5 7.50 9.65 - 0.00 0.00 0.00 21.74 0.00 7.58 0.00 - 4.17\nADJ:FORM P 60.00 75.00 100.00 100.00 0.00 33.33 100.00 50.00 11.54 - - 80.00 R 60.00 50.00 27.27 33.33 0.00 33.33 33.33 11.11 42.86 - - 57.14\nF0.5 60.00 68.18 65.22 71.43 0.00 33.33 71.43 29.41 13.51 - - 74.07\nADV P 11.76 12.66 0.00 0.00 0.00 0.00 0.00 - 0.00 4.76 - 7.27 R 5.88 23.26 0.00 0.00 0.00 0.00 0.00 - 0.00 3.03 - 10.00\nF0.5 9.80 13.93 0.00 0.00 0.00 0.00 0.00 - 0.00 4.27 - 7.69\nCONJ P 6.25 0.00 - - 0.00 0.00 - 0.00 - 0.00 - 0.00 R 7.69 0.00 - - 0.00 0.00 - 0.00 - 0.00 - 0.00\nF0.5 6.49 0.00 - - 0.00 0.00 - 0.00 - 0.00 - 0.00\nCONTR P 29.17 40.00 46.15 - 0.00 - - 33.33 0.00 66.67 - 28.57 R 87.50 28.57 75.00 - 0.00 - - 50.00 0.00 33.33 - 28.57\nF0.5 33.65 37.04 50.00 - 0.00 - - 35.71 0.00 55.56 - 28.57\nDET P 33.55 36.44 30.92 21.43 0.00 35.91 29.35 26.12 0.00 43.88 - 36.36 R 14.13 43.17 52.03 0.92 0.00 28.53 7.87 49.41 0.00 12.57 - 23.72\nF0.5 26.32 37.61 33.65 3.93 0.00 34.14 18.99 28.84 0.00 29.29 - 32.86\nMORPH P 54.67 57.35 52.94 15.38 2.25 25.00 21.43 30.30 27.27 50.00 36.36 34.48 R 45.56 45.35 20.00 2.70 2.78 20.25 32.14 12.82 15.19 2.74 5.06 11.63\nF0.5 52.56 54.47 39.82 7.94 2.34 23.88 22.96 23.81 23.53 11.24 16.26 24.75\nNOUN P 25.35 28.42 0.00 25.00 8.33 0.00 3.45 10.00 30.43 0.00 - 28.57 R 15.52 22.13 0.00 2.22 4.30 0.00 0.96 1.90 6.54 0.00 - 10.00\nF0.5 22.50 26.89 0.00 8.20 7.02 0.00 2.27 5.41 17.59 0.00 - 20.83\nNOUN:INFL P 55.56 60.00 50.00 - 0.00 100.00 57.14 80.00 60.00 0.00 - - R 83.33 75.00 66.67 - 0.00 40.00 57.14 66.67 60.00 0.00 - -\nF0.5 59.52 62.50 52.63 - 0.00 76.92 57.14 76.92 60.00 0.00 - -\nNOUN:NUM P 48.24 42.59 43.57 43.75 12.84 44.05 28.92 30.52 27.72 54.29 - 44.93 R 55.66 53.00 59.91 3.95 10.05 48.54 42.34 56.54 35.92 10.50 - 17.32\nF0.5 49.56 44.33 46.09 14.52 12.16 44.88 30.88 33.62 29.04 29.60 - 34.07\nNOUN:POSS P 20.00 66.67 - - - - 20.00 0.00 0.00 25.00 - 50.00 R 14.29 10.53 - - - - 5.26 0.00 0.00 4.55 - 5.00\nF0.5 18.52 32.26 - - - - 12.82 0.00 0.00 13.16 - 17.86\nORTH P 60.00 66.67 73.81 - 3.45 0.00 28.57 49.32 16.67 - - 50.00 R 11.11 40.00 59.62 - 4.55 0.00 6.90 64.29 49.12 - - 17.24\nF0.5 31.91 58.82 70.45 - 3.62 0.00 17.54 51.72 19.20 - - 36.23\nOTHER P 19.33 23.57 16.13 12.50 2.38 1.40 11.11 13.95 0.00 0.00 - 13.54 R 6.65 9.87 1.37 0.30 0.31 0.58 0.58 1.69 0.00 0.00 - 3.74\nF0.5 13.99 18.44 5.12 1.39 1.02 1.09 2.40 5.70 0.00 0.00 - 8.88\nPART P 71.43 26.67 0.00 - - 12.90 - - - 40.00 - 18.18 R 20.00 16.00 0.00 - - 16.00 - - - 9.09 - 10.00\nF0.5 47.17 23.53 0.00 - - 13.42 - - - 23.81 - 15.63\nPREP P 47.62 41.70 32.69 75.00 0.00 10.95 - 21.74 0.00 37.50 - 20.53 R 16.53 35.91 13.65 1.44 0.00 12.81 - 2.18 0.00 7.18 - 13.42\nF0.5 34.60 40.40 25.56 6.67 0.00 11.28 - 7.79 0.00 20.33 - 18.56\nPRON P 43.75 20.00 0.00 0.00 11.11 50.00 100.00 27.27 4.76 0.00 - 22.45 R 9.72 13.25 0.00 0.00 1.72 2.86 1.56 4.76 1.54 0.00 - 14.10\nF0.5 25.74 18.15 0.00 0.00 5.32 11.63 7.35 14.02 3.36 0.00 - 20.07\nPUNCT P 25.00 60.47 39.53 100.00 0.00 48.28 - 27.27 0.00 5.00 - 43.02 R 3.57 15.66 11.33 1.85 0.00 9.72 - 6.34 0.00 0.97 - 23.13\nF0.5 11.36 38.46 26.40 8.62 0.00 26.92 - 16.42 0.00 2.73 - 36.71\nSPELL P 77.78 78.43 50.00 0.00 30.77 0.00 44.58 68.27 74.60 - - 100.00 R 64.95 42.55 2.60 0.00 5.41 0.00 71.15 70.30 86.24 - - 1.32\nF0.5 74.82 67.11 10.75 0.00 15.87 0.00 48.18 68.67 76.67 - - 6.25\nVERB P 18.84 16.09 - 0.00 7.69 0.00 14.29 0.00 0.00 0.00 - 18.87 R 8.07 8.86 - 0.00 0.71 0.00 0.68 0.00 0.00 0.00 - 6.58\nF0.5 14.87 13.83 - 0.00 2.60 0.00 2.87 0.00 0.00 0.00 - 13.74\nVERB:FORM P 34.85 38.24 70.59 50.00 8.77 36.84 30.77 20.00 35.42 30.77 100.00 34.04 R 23.71 26.26 26.37 1.15 5.75 36.84 35.16 3.45 34.69 4.65 1.22 18.18\nF0.5 31.86 35.04 52.86 5.26 7.94 36.84 31.56 10.20 35.27 14.49 5.81 28.99\nVERB:INFL P 100.00 100.00 - - 100.00 100.00 50.00 100.00 100.00 - 0.00 - R 100.00 100.00 - - 50.00 50.00 50.00 50.00 100.00 - 0.00 -\nF0.5 100.00 100.00 - - 83.33 83.33 50.00 83.33 100.00 - 0.00 -\nVERB:SVA P 49.06 42.68 54.71 50.00 24.53 50.58 57.14 33.33 34.83 59.09 82.86 58.33 R 27.37 31.82 70.45 1.15 13.98 66.92 17.20 16.67 31.00 14.13 28.16 14.29\nF0.5 42.35 39.95 57.27 5.26 21.31 53.18 39.02 27.78 33.99 36.11 59.67 36.08\nVERB:TENSE P 20.55 26.05 75.00 66.67 7.14 38.89 10.61 20.00 23.27 15.38 100.00 30.51 R 8.82 17.92 5.33 1.27 1.24 4.22 4.35 2.34 21.26 2.52 1.26 10.98\nF0.5 16.23 23.88 20.74 5.92 3.66 14.71 8.24 7.97 22.84 7.60 5.99 22.50\nWO P - 38.89 0.00 66.67 - - - 0.00 0.00 - - 41.18 R - 33.33 0.00 14.29 - - - 0.00 0.00 - - 35.00\nF0.5 - 37.63 0.00 38.46 - - - 0.00 0.00 - - 39.77\nTable 6: Precision, recall and F0.5 for each team and error type. A dash indicates the team’s system did not attempt to correct the given error type (TP+FP = 0). The highest F-score for each type is highlighted.\n7\n601\n602\n603\n604\n605\n606\n607\n608\n609\n610\n611\n612\n613\n614\n615\n616\n617\n618\n619\n620\n621\n622\n623\n624\n625\n626\n627\n628\n629\n630\n631\n632\n633\n634\n635\n636\n637\n638\n639\n640\n641\n642\n643\n644\n645\n646\n647\n648\n649\n663\n664\n665\n666\n667\n668\n669\n670\n671\n672\n673\n674\n675\n676\n677\n678\n679\n680\n681\n682\n683\n684\n685\n686\n687\n688\n689\n690\n691\n692\n693\n694\n695\n696\n697\n698\n699\neach team’s system (Ng et al., 2014).\nOverall, CAMB was the most successful team in terms of error types, achieving the highest Fscore in 10 (out of 24) error categories, followed by AMU (Junczys-Dowmunt and Grundkiewicz, 2014), who scored highest in 6 categories. All but 2 teams (IITB and IPN) achieved the best score in at least 1 category, which suggests that different approaches to GEC complement different error types. Only CAMB attempted to correct at least 1 error from every category.\nRegarding individual error categories: PKU’s language model approach significantly outperformed all others in handling ADJ errors (21.74 F0.5). ADV and CONJ errors proved extremely difficult for all teams, with the best results at 13.93 F0.5 (CAMB) and 6.49 F0.5 (AMU) respectively. Although several teams built specialist classifiers for DET errors, CAMB’s hybrid MT system still slightly outperformed them (37.61 F0.5). MT approaches were most effective at correcting NOUN errors (AMU, CAMB, UMC), while fairly high scores for NOUN:NUM errors showed that this category could be successfully handled by MT (AMU, CAMB), classifiers (CUUI) or language model approaches (NTHU). Few teams attempted to correct NOUN:POSS errors, but CAMB’s system handled them the best (32.26 F0.5). CUUI’s classifier for ORTH errors significantly outperformed all other teams at 70.45 F0.5. As with DET errors, several teams employed specialist classifiers to tackle PREP errors, but CAMB’s hybrid system still worked best overall (40.40 F0.5). AMU’s MT system was most successful at correcting PRON errors (25.74 F0.5), while CAMB was most successful at correcting PUNCT errors (38.46 F0.5). Although spell checkers are widespread nowadays, many teams did not seem to employ them; this would have been an easy way to boost overall performance. CUUI’s classifier approach to VERB:FORM errors significantly outperformed other approaches (52.86 F0.5), suggesting a classifier is well-suited to this category. While UFC’s rule-based approach achieved the highest score for VERB:SVA errors (59.67 F0.5), it is worth noting that CUUI’s classifier approach was not far behind (57.27 F0.5). Finally, only 3 teams were successful at handling WO errors (CAMB, IITB and UMC), all of whom achieved similar scores of just under 40 F0.5 using MT.\n\n4.4 Detailed Error Types\nIn addition to analysing general error types, the modular design of our framework also allows us to evaluate error type performance at an even greater level of detail. For example, Table 7 shows the breakdown of Determiner errors for two teams using different approaches in terms of edit operation. Note that this is a representative example of detailed error type performance as an analysis of all error type combinations for all teams would take up too much space.\nWhile CAMB’s hybrid MT approach achieved a higher score than CUUI’s classifier approach overall (37.61 F0.5 vs. 33.65 F0.5), our more detailed evaluation reveals that actually CUUI’s approach performed better at Replacement Determiner errors than CAMB (26.53 F0.5 vs. 21.39 F0.5). This shows that even though one approach might be better than another overall, other approaches may still have complementary strengths. In fact the main weakness of CUUI’s classifier seems to be that a high recall for missing and unnecessary determiners is counterbalanced by a low precision, which suggests that minimising false positives in these categories is the most obvious avenue for improvement.\n\n4.5 Multi Token Errors\nAnother benefit of explicitly annotating all hypothesis edits is that edit spans become fixed; this means we can evaluate system performance in terms of edit size. Table 8 hence shows the overall performance for each team at correcting multitoken edits, where a multi-token edit is an edit that affects at least two tokens on either the source or\n8\n701\n702\n703\n704\n705\n706\n707\n708\n709\n710\n711\n712\n713\n714\n715\n716\n717\n718\n719\n720\n721\n722\n723\n724\n725\n726\n727\n728\n729\n730\n731\n732\n733\n734\n735\n736\n737\n738\n739\n740\n741\n742\n743\n744\n745\n746\n747\n748\n749\n767\n768\n769\n770\n771\n772\n773\n774\n775\n776\n777\n778\n779\n780\n781\n782\n783\n784\n785\n786\n787\n788\n789\n790\n791\n792\n793\n794\n795\n796\n797\n798\n799\nTeam P R F0.5 AMU 17.14 5.33 11.88 CAMB 27.22 17.06 24.32 CUUI 15.69 3.67 9.48 IITB 28.57 0.94 4.15 IPN 3.33 0.47 1.51 NTHU 0.00 0.00 0.00 PKU 25.00 1.40 5.73 POST 12.77 2.82 7.48 RAC 2.96 2.82 2.93 SJTU 10.00 0.47 1.99 UFC - - - UMC 19.82 9.82 16.47\nTable 8: Each team’s performance at correcting multi-token edits; i.e. there are at least 2 tokens on one side of the edit.\ntarget side. In the CoNLL-2014 test set, there are roughly 220 such edits (about 10% of all edits).\nIn general, teams did not do well at multi-token edits. In fact only three teams achieved scores greater than 10 F0.5 and all of them used MT (AMU, CAMB, UMC). This is significant because recent work has suggested that the main goal of GEC should be to produce fluent-sounding, rather than just grammatical, sentences, even though this often requires complex multi-token edits (Sakaguchi et al., 2016). If no system is particularly adept at correcting multi-token errors however, robust fluency correction will likely require more sophisticated methods than are currently available.\n\n4.6 Detection vs. Correction\nAnother important aspect of GEC that is less frequently reported in the literature is that of error detection; i.e. the extent to which a system can identify erroneous tokens in text. This can be calculated by comparing the edit overlap between the hypothesis and reference files regardless of the proposed correction in a manner similar to Recognition evaluation in the HOO shared tasks for GEC (Dale and Kilgarriff, 2011).\nFigure 1 hence shows how each team’s score for detection differed in relation to their score for correction. While CAMB still scored highest for detection overall, it is interesting to note that the difference between the 2nd and 3rd place contenders (CUUI and AMU) is a lot narrower. This suggests that even though AMU detected roughly as many errors as CUUI, it was less successful at correcting them. IPN and PKU are also notable for detecting\nmany more errors than they were able to correct. Although we do not do so here, our scorer is also capable of providing a detailed error type breakdown for detection.\n\n5 Conclusion\nIn this paper, we have described a method to automatically annotate parallel error correction data with explicit edit spans and error type information. This can be used to standardise existing error correction corpora or facilitate a detailed error type evaluation. The tool we use to do this will be released with this paper.\nOur approach makes use of previous work to align sentences based on linguistic intuition and then introduces a new rule-based framework to classify edits. This framework is entirely dataset independent, and relies only on automatically obtained information such as POS tags and lemmas. A small-scale evaluation of our classifier found that each rater considered >95% of the predicted error types as either “Good” (85%) or “Acceptable” (10%).\nWe demonstrated the value of our approach by carrying out a detailed evaluation of system error type performance for the first time for all teams in the CoNLL-2014 shared task on Grammatical Error Correction. We found that different systems had different strengths and weaknesses which we hope researchers can exploit to further improve general performance.\n9\n801\n802\n803\n804\n805\n806\n807\n808\n809\n810\n811\n812\n813\n814\n815\n816\n817\n818\n819\n820\n821\n822\n823\n824\n825\n826\n827\n828\n829\n830\n831\n832\n833\n834\n835\n836\n837\n838\n839\n840\n841\n842\n843\n844\n845\n846\n847\n848\n849\n850\n851\n852\n853\n854\n855\n856\n857\n858\n859\n860\n861\n862\n863\n864\n865\n866\n867\n868\n869\n870\n871\n872\n873\n874\n875\n876\n877\n878\n879\n880\n881\n882\n883\n884\n885\n886\n887\n888\n889\n890\n891\n892\n893\n894\n895\n896\n897\n898\n899\n",
    "rationale": "- Strengths: Useful application for teachers and learners; supports\nfine-grained comparison of GEC systems.\n\n- Weaknesses: Highly superficial description of the system; evaluation not\nsatisfying.\n\n- General Discussion:\n\nThe paper presents an approach of automatically enriching the output of GEC\nsystems with error types. This is a very useful application because both\nteachers and learners can benefit from this information (and many GEC systems\nonly output a corrected version, without making the type of error explicit). It\nalso allows for finer-grained comparison of GEC systems, in terms of precision\nin general, and error type-specific figures for recall and precision.\n\nUnfortunately, the description of the system remains highly superficial. The\ncore of the system consists of a set of (manually?) created rules but the paper\ndoes not provide any details about these rules. The authors should, e.g., show\nsome examples of such rules, specify the number of rules, tell us how complex\nthey are, how they are ordered (could some early rule block the application of\na later rule?), etc. -- Instead of presenting relevant details of the system,\nseveral pages of the paper are devoted to an evaluation of the systems that\nparticipated in CoNLL-2014. Table 6 (which takes one entire page) list results\nfor all systems, and the text repeats many facts and figures that can be read\noff the table. \n\nThe evaluation of the proposed system is not satisfying in several aspects. \nFirst, the annotators should have independently annotated a gold standard for\nthe 200 test sentences instead of simply rating the output of the system. Given\na fixed set of tags, it should be possible to produce a gold standard for the\nrather small set of test sentences. It is highly probable that the approach\ntaken in the paper yields considerably better ratings for the annotations than\ncomparison with a real gold standard (see, e.g., Marcus et al. (1993) for a\ncomparison of agreement when reviewing pre-annotated data vs. annotating from\nscratch). \nSecond, it is said that \"all 5 raters individually considered at least 95% of\nour rule-based error types to be either “Good” or “Acceptable”\".\nMultiple rates should not be considered individually and their ratings averaged\nthis way, this is not common practice. If each of the \"bad\" scores were\nassigned to different edits (we don't learn about their distribution from the\npaper), 18.5% of the edits were considered \"bad\" by some annotator -- this\nsounds much worse than the average 3.7%, as calculated in the paper.\nThird, no information about the test data is provided, e.g. how many error\ncategories they contain, or which error categories are covered (according to\nthe cateogories rated as \"good\" by the annotators).\nForth, what does it mean that \"edit boundaries might be unusual\"? A more\nprecise description plus examples are at need here. Could this be problematic\nfor the application of the system?\n\nThe authors state that their system is less domain dependent as compared to\nsystems that need training data. I'm not sure that this is true. E.g., I\nsuppose that Hunspell's vocabulary probably doesn't cover all domains in the\nsame detail, and manually-created rules can be domain-dependent as well -- and\nare completely language dependent, a clear drawback as compared to machine\nlearning approaches. Moreover, the test data used here (FCE-test, CoNLL-2014)\nare from one domain only: student essays.\n\nIt remains unclear why a new set of error categories was designed. One reason\nfor the tags is given: to be able to search easily for underspecified\ncategories (like \"NOUN\" in general). It seems to me that the tagset presented\nin Nicholls (2003) supports such searches as well. Or why not using the\nCoNLL-2014 tagset? Then the CoNLL gold standard could have been used for\nevaluation.\n\nTo sum up, the main motivation of the paper remains somewhat unclear. Is it\nabout a new system? But the most important details of it are left out. Is it\nabout a new set of error categories? But hardly any motivation or discussion of\nit is provided. Is it about evaluating the CoNLL-2014 systems? But the\npresentation of the results remains superficial.\n\nTypos:\n- l129 (and others): c.f. -> cf.\n- l366 (and others): M2 -> M^2 (= superscribed 2)\n- l319: 50-70 F1: what does this mean? 50-70%?\n\nCheck references for incorrect case\n- e.g. l908: esl -> ESL\n- e.g. l878/79: fleiss, kappa",
    "rating": 3
  },
  {
    "title": "Learning Cognitive Features from Gaze Data for Sentiment and Sarcasm Classification using Convolutional Neural Network",
    "abstract": "Cognitive NLP systemsi.e., NLP systems that make use of behavioral data augment traditional text based features with cognitive features extracted from eye-movement patterns, EEG signals, brain-imaging etc.. Such extraction of features is typically manual. We contend that manual extraction of features is not good enough to tackle text subtleties that characteristically prevail in complex classification tasks like sentiment analysis and sarcasm detection, and that even the extraction and choice of features should be delegated to the learning system. We introduce a framework to automatically extract cognitive features from the eye-movement / gaze data of human readers reading the text and use them as features along with textual features for the tasks of sentiment polarity and sarcasm detection. Our proposed framework is based on Convolutional Neural Network (CNN). The CNN learns features from both gaze and text and uses them to classify the input text. We test our technique on published sentiment and sarcasm labeled datasets, enriched with gaze information, to show that using a combination of automatically learned text and gaze features yields better classification performance over (i) CNN based systems that rely on text input alone and (ii) existing systems that rely on handcrafted gaze and textual features.",
    "text": "1 Introduction\nDetection of sentiment and sarcasm in usergenerated short reviews is of primary importance for social media analysis, recommendation and dialog systems. Traditional sentiment analyzers and\nsarcasm detectors face challenges that arise at lexical, syntactic, semantic and pragmatic levels (Liu and Zhang, 2012; Mishra et al., 2016c). Featurebased systems (Akkaya et al., 2009; Sharma and Bhattacharyya, 2013; Poria et al., 2014) can aptly handle lexical and syntactic challenges (e.g. learning that the word deadly conveys a strong positive sentiment in opinions such as Shane Warne is a deadly bowler, as opposed to The high altitude Himalayan roads have deadly turns). It is, however, extremely difficult to tackle subtleties at semantic and pragmatic levels. For example, the sentence “I really love my job. I work 40 hours a week to be this poor.” requires an NLP system to be able to understand that the opinion holder has not expressed a positive sentiment towards her / his job. In the absence of explicit clues in the text, it is difficult for automatic systems to arrive at a correct classification decision, as they often lack external knowledge about various aspects of the text being classified.\nMishra et al. (2016b) and Mishra et al. (2016c) show that NLP systems based on cognitive data (or simply, Cognitive NLP systems) , that of leverage eye-movement information obtained from human readers, can tackle the semantic and pragmatic challenges better. The hypothesis here is that human gaze activities are related to the cognitive processes in the brain, that combines the “external knowledge” that the a reader possesses with textual clues that she / he perceives. While incorporating behavioral information obtained from gaze-data in NLP systems is intriguing and quite plausible, especially due to the availability of low cost eye-tracking machinery (Wood and Bulling, 2014; Yamamoto et al., 2013), few methods exist for text classification and they rely on handcrafted features extracted from gaze data (Mishra et al., 2016b,c). These systems have limited capabilities due to two reasons: (a) Manually designed gaze based features may not adequately capture all\n2\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n112\n113\n114\n115\n116\n117\n118\n119\n120\n121\n122\n123\n124\n125\n126\n127\n128\n129\n130\n131\n132\n133\n134\n135\n136\n137\n138\n139\n140\n141\n142\n143\n144\n145\n146\n147\n148\n149\n162\n163\n164\n165\n179\n180\n181\n182\n183\n184\n185\n186\n187\n188\n189\n190\n191\n192\n193\n194\n195\n196\n197\n198\n199\nforms of textual subtleties (b) Eye-movement data is not as intuitive to analyze as text which makes the task of designing manual features more difficult. So, in this work, instead of handcrafting the gaze based and textual features, we try to learn feature representations from both gaze and textual data using Convolutional Neural Networks (CNNs). We test our technique on two publicly available datasets enriched with eye-movement information, used for binary classification tasks of sentiment polarity and sarcasm detection. Our experiments show that the automatically extracted features help to achieve significant classification performance improvement over (a) existing systems that rely on handcrafted gaze and textual features and (b) CNN based systems that rely on text input alone.\nThe rest of the paper is organized as follows. Section 2 discusses the motivation behind using readers’ eye-movement data in a text classification setting. Section 3 discusses on why CNNs is preferred over other available alternatives for feature extraction. The CNN architecture is proposed and discussed in Section 4. Section 5 describes our experimental setup and results are discussed in Section 6. We provide a detailed analysis of the results along with some insightful observations in Section 7. Section 8 points to relevant literature followed by Section 9 that concludes the paper. Terminology: A fixation is a relatively long stay of gaze on a visual object (such as words in text) where as a sacccade corresponds to quick shifting of gaze between two positions of rest. Forward and backward saccades are called progressions and regressions respectively. A scanpath is a line graph that contains fixations as nodes and saccades as edges.\n\n2 Eye-movement and Linguistic Subtleties\nPresence of linguistic subtleties often induces (a) surprisal (Kutas and Hillyard, 1980; Malsburg et al., 2015), due to the underlying disparity /context incongruity or (b) higher cognitive load (Rayner and Duffy, 1986), due to the presence of lexically and syntactically complex structures. While surprisal accounts for irregular saccades (Malsburg et al., 2015), higher cognitive load results in longer fixation duration (Kliegl et al., 2004).\nMishra et al. (2016b) find that presence of sarcasm in text triggers either irregular saccadic patterns or unusually high duration fixations than non-sarcastic texts (illustrated through example scanpath representations in Figure 1). For sentiment bearing texts, highly subtle eye-movement patterns are observed for semantically / pragmatically complex negative opinions (expressing irony, sarcasm, thwarted expectations etc.) than the simple ones (Mishra et al., 2016b). The association between linguistic subtleties and eye-movement patterns could be captured through sophisticated feature engineering that considers both gaze and text inputs. In our work, CNNs take the onus of feature engineering.\n3\n201\n202\n203\n204\n205\n206\n207\n208\n209\n210\n211\n212\n213\n214\n215\n216\n217\n218\n219\n220\n221\n222\n223\n224\n225\n226\n227\n228\n229\n230\n231\n232\n233\n234\n235\n236\n237\n238\n239\n240\n241\n242\n243\n244\n245\n246\n247\n248\n249\n250\n251\n252\n253\n254\n255\n256\n257\n258\n259\n260\n261\n262\n263\n264\n265\n266\n267\n268\n269\n270\n271\n272\n273\n274\n275\n276\n277\n278\n279\n280\n281\n282\n283\n284\n285\n286\n287\n288\n289\n290\n291\n292\n293\n294\n295\n296\n297\n298\n299\nT ex\nt C\nom p\non en\nt\nNon-static\nStatic\nSaccade\nFixation\nG az\ne C\nom p\non en t P1 P2 P3 P4 P5 P6 P7 P8\nN×K representation of sentences with static and non static channels\nP×G representation of sentences\nwith fixation and saccade channels\n1-D convolution operation with multiple filter width\nand feature maps\n2-D convolution operation with multiple filter row and\nColumn widths\nMax-pooling for each filter width\nMax-pooling over\nmultiple dimensions for\nmultiple filter widths\nFully connected with dropouts and softmax\noutput\nMerged pooled values\nFigure 3: Deep convolutional model for feature extraction from both text and gaze inputs\n\n3 Why Convolutional Neural Network?\nCNNs have been quite effective in learning filters for image processing tasks, filters being used to transform the input image into more informative feature space (Krizhevsky et al., 2012). Filters learned at various CNN layers are quite similar to handcrafted filters used for detection of edges, contours and removal of redundant backgrounds. We believe, a similar technique can also be applied to eye-movement data, where the learned filters will, hopefully, extract informative cognitive features. For instance, for sarcasm, we expect the network to learn filters that detect long distance saccades (refer to Figure 2 for an analogical illustration). With more number of convolution filters of different dimensions, the network may extract multiple features related to different gaze attributes (such as fixations, progressions, regressions and skips) and will be free from any form of human bias that manually extracted features are susceptible to.\n\n4 Learning Feature Representations: The CNN architecture\nFigure 3 shows the CNN architecture with two components for processing and extracting features from text and gaze inputs. The components are explained below.\n\n4.1 Text Component\nThe text component is quite similar to the one proposed by Kim (2014) for sentence classification.\nWords (in the form of one-hot representation) in the input text are first replaced by their embeddings of dimension K (ith word in the sentence represented by an embedding vector xi ∈ RK). As per Kim (2014), a multi-channel variant of CNN (referred to as MULTICHANNELTEXT) can be implemented by using two channels of embeddingsone that remains static through out training (referred to as STATICTEXT), and the other one that gets updated during training (referred to as NONSTATICTEXT). We separately experiment with static, non-static and multi-channel variants.\nFor each possible input channel of the text component, a given text is transformed into a tensor of fixed length N (padded with zero-tensors wherever necessary to tackle length variations) by concatenating the word embeddings.\nx1:N = x1 ⊕ x2 ⊕ x3 ⊕ ...⊕ xN (1)\nwhere ⊕ is the concatenation operator. To extract local features1, convolution operation is applied. Convolution operation involves a filter, W ∈ RHK , which is convolved with a window of H embeddings to produce a local feature for the H words. A local feature, ci is generated from a window of embeddings xi:i+H−1 by applying a non linear function (such as a hyperbolic tangent) over the convoluted output. Mathematically,\nci = f(W.xi:i+H−1 + b) (2)\n1features specific to a region in case of images or window of words in case of text\n4\n301\n302\n303\n304\n305\n306\n307\n308\n309\n310\n311\n312\n313\n314\n315\n316\n317\n318\n319\n320\n321\n322\n323\n324\n325\n326\n327\n328\n329\n330\n331\n332\n333\n334\n335\n336\n337\n338\n339\n340\n341\n342\n343\n344\n345\n346\n347\n348\n349\n350\n351\n352\n353\n354\n355\n356\n357\n358\n359\n360\n361\n362\n363\n364\n365\n366\n367\n368\n369\n370\n371\n372\n373\n374\n375\n376\n377\n378\n379\n380\n381\n382\n383\n384\n385\n386\n387\n388\n389\n390\n391\n392\n393\n394\n395\n396\n397\n398\n399\nwhere b ∈ R is the bias and f is the non-linear function. This operation is applied to each possible window of H words to produce a feature map (c) for the window size H .\nc = [c1, c2, c3, ..., cN−H+1] (3)\nA global feature is then obtained by applying max pooling operation2 (Collobert et al., 2011) over the feature map. The idea behind max-pooling is to capture the most important feature - one with the highest value - for each feature map.\nWe have described the process by which one feature is extracted from one filter (for illustration, red bordered portions in Figure 3 depict the case of H = 2). The model uses multiple filters (with varying window sizes) to obtain multiple features representing the text. In the MULTICHANNELTEXT variant, for a window of H words, convolution operation is separately applied on both the embedding channels. Local features learned from both the channels are concatenated before applying max-pooling.\n\n4.2 Gaze Component\nThe gaze component deals with scanpaths of multiple participants annotating the same text. Scanpaths can be pre-processed to extract two sequences of gaze data to form separate channels of input: (1) A sequence of normalized3 durations of fixations (in milliseconds) in the order in which they appear in the scanpath and (2) A sequence of position of fixations (in terms of word id) in the order in which they appear in the scanpath. These channels are related to two fundamental gaze attributes such as fixation and saccade respectively. With two channels, we thus have three possible configurations of the gaze component such as (i) FIXATION, where the input is normalized fixation duration sequence, (ii) SACCADE, where the input is fixation position sequence, and (iii) MULTICHANNELGAZE, where both the inputs channels are considered.\nFor each possible input channel, the input is in the form of a P × G matrix (with P → number of participants and G → length of the input sequence). Each element of the matrix gij ∈ R, with i ∈ P and j ∈ G, corresponds to the jth gaze attribute (either fixation duration or word id, depending on the channel) of the input sequence of\n2mean pooling does not perform well. 3scaled across participants using min-max normalization\nto reduce subjectivity\nthe ith participant. Now, unlike the text component, here we apply convolution operation across two dimensions i.e. choosing a two dimensional convolution filter W ∈ RJK (for simplicity, we have kept J = K, thus , making the dimension of W , J2). For the dimension size of J2, a local feature cij is computed from the window of gaze elements gij:(i+J−1)(j+J−1) by,\ncij = f(W.gij:(i+J−1)(j+J−1) + b) (4)\nwhere b ∈ R is the bias and f is a non-linear function. This operation is applied to each possible window of size J2 to produce a feature map (c),\nc =[c11, c12, c13, ..., c1(G−J+1),\nc21, c22, c23, ..., c2(G−J+1), ...,\nc(P−J+1)1, c(P−J+1)2, ..., c(P−J+1)(G−J+1)]\n(5)\nA global feature is then obtained by applying max pooling operation. Unlike the text component, max-pooling operator is applied to a 2D window of local features size M × N (for simplicity, we set M = N , denoted henceforth as M2). For the window of size M2, the pooling operation on c will result in as set of global features ĉJ = max{cij:(i+M−1)(j+M−1)} for each possible i, j.\nWe have described the process by which one feature is extracted from one filter (of 2D window size J2 and max-pooling window size of M2). In Figure 3, red and blue bordered portions illustrate the cases of J2 = [3, 3] and M2 = [2, 2] respectively. Like the text component, the gaze component uses multiple filters (also with varying window size) to obtain multiple features representing the gaze input. In the MULTICHANNELGAZE variant, for a 2D window of J2, convolution operation is separately applied on both fixation duration and saccade channels and local features learned from both the channels are concatenated before max-pooling is applied.\nOnce the global features are learned from both the text and gaze components, they are merged and passed to a fully connected feed forward layer (with number of units set to 150) followed by a SoftMax layer that outputs the the probabilistic distribution over the class labels.\nThe gaze component of our network is not invariant of the order in which the scanpath data is given as input- i.e., the P rows in the P × G can not be shuffled, even if each row is independent from others. The only way we can think of for\n5\n401\n402\n403\n404\n405\n406\n407\n408\n409\n410\n411\n412\n413\n414\n415\n416\n417\n418\n419\n420\n421\n422\n423\n424\n425\n426\n427\n428\n429\n430\n431\n432\n433\n434\n435\n436\n437\n438\n439\n440\n441\n442\n443\n444\n445\n446\n447\n448\n449\n450\n451\n452\n453\n454\n455\n456\n457\n458\n459\n460\n461\n462\n463\n464\n465\n466\n467\n468\n469\n470\n471\n472\n473\n474\n475\n476\n477\n478\n479\n480\n481\n482\n483\n484\n485\n486\n487\n488\n489\n490\n491\n492\n493\n494\n495\n496\n497\n498\n499\naddressing this issue is by applying convolution operations to all P × G matrices formed with all the permutations of P , capturing every possible ordering. Unfortunately, this makes the training process significantly less scalable, as the number of model parameters to be learned becomes huge. As of now, training and testing are carried out by keeping the order of the input constant.\n\n5 Experiment Setup\nWe now share several details regarding our experiments below. 1. Dataset: We experiment on sentiment and sarcasm tasks using two publicly available datasets enriched with eye-movement information. Dataset 1 has been released by Mishra et al. (2016a). It contains 994 text snippets with 383 positive and 611 negative examples. Out of the 994 snippets, 350 are sarcastic. Dataset 2 has been used by Joshi et al. (2014) and it consists of 843 snippets comprising movie reviews and normalized tweets out of which 443 are positive and 400 are negative. Eye-movement data of 7 and 5 readers is available for each snippet for dataset 1 and 2 respectively. 2. CNN Variants: With text component alone we have three variants such as STATICTEXT, NONSTATICTEXT and MULTICHANNELTEXT (refer to Section 4.1). Similarly, with gaze component we have variants such as FIXATION, SACCADE and MULTICHANNELGAZE (refer to Section 4.2). With both text and gaze components, 9 more variants could be experimented with. 3. Hyper-parameters: For text component, we experiment with filter widths (H) of [3, 4]. For the gaze component, 2D filters (J2) set to [3 × 3], [4× 4] respectively. The max pooling 2D window, M2, is set to [2 × 2]. In both gaze and text components, number of filters is set to 150, resulting in 150 feature maps for each window. These model hyper-parameters are fixed by trial and error and are possibly good enough to provide a first level insight into our system. Tuning of hyperparameters might help in improving the performance of our framework, which is on our future research agenda. 4. Regularization: For regularization dropout is employed on the penultimate layer with a constraint on l2-norms of the weight vectors (Hinton et al., 2012). Dropout prevents co-adaptation of hidden units by randomly dropping out - i.e., setting to zero - a proportion p of the hidden units\nduring forward propagation. We set p to 0.25. 5. Training: We use ADADELTA optimizer (Zeiler, 2012), with a learning rate of 0.1. The input batch size is set to 32 and number of training iterations (epochs) is set to 200. 10% of the training data is used for validation. 6. Use of pre-trained embeddings: Initializing the embedding layer with of pre-trained embeddings can be more effective than random initialization (Kim, 2014). In our experiments, we have used embeddings using word2vec facilitated by Mikolov et al. (2013) (best results obtained with embedding dimension of 50). We have also tried randomly initializing the embeddings but better results are obtained with pre-trained embeddings. 7. Comparison with existing work: For sentiment analysis, we compare our systems’s accuracy (for both datasets 1 and 2) with Mishra et al. (2016c)’s systems that rely on handcrafted text and gaze features. For sarcasm detection, we compare Mishra et al. (2016b)’s sarcasm classifier with ours using dataset 1 (with available gold standard labels for sarcasm). We follow the same 10-fold traintest configuration as these existing works for consistency.\n\n6 Results\nIn this section, we discuss the results for different model variants for sentiment polarity and sarcasm detection tasks.\n\n6.1 Results for Sentiment Analysis Task\nTable 1 presents results for sentiment analysis task. For dataset 1, different variants of our CNN architecture outperform the best systems reported by Mishra et al. (2016c), with a maximum F-score improvement of 3.8%. This improvement is statistically significant of p < 0.05 as confirmed by McNemar test. Moreover, we observe an F-score improvement of around 5% for CNNs with both gaze and text components as compared to CNNs with only text components (similar to the system by Kim (2014)), which is also statistically significant (with p < 0.05).\nFor dataset 2, CNN based approaches do not perform better than manual feature based approaches. However, variants with both text and gaze components outperform the ones with only text component (Kim, 2014), with a maximum Fscore improvement of 2.9%. We observe that for dataset 2, training accuracy reaches 100 within\n6\n501\n502\n503\n504\n505\n506\n507\n508\n509\n510\n511\n512\n513\n514\n515\n516\n517\n518\n519\n520\n521\n522\n523\n524\n525\n526\n527\n528\n529\n530\n531\n532\n533\n534\n535\n536\n537\n538\n539\n540\n541\n542\n543\n544\n545\n546\n547\n548\n549\n550\n551\n552\n553\n554\n555\n556\n557\n558\n559\n560\n561\n562\n563\n564\n565\n566\n567\n568\n569\n570\n571\n572\n573\n574\n575\n576\n577\n578\n579\n580\n581\n582\n583\n584\n585\n586\n587\n588\n589\n590\n591\n592\n593\n594\n595\n596\n597\n598\n599\nDataset1 Dataset2\nConfiguration P R F P R F\nTraditional systems based on\nNäive Bayes 63.0 59.4 61.14 50.7 50.1 50.39 Multi-layered Perceptron 69.0 69.2 69.2 66.8 66.8 66.8\ntextual features SVM (Linear Kernel) 72.8 73.2 72.6 70.3 70.3 70.3 Systems by Mishra et al. (2016c) Gaze based (Best) 61.8 58.4 60.05 53.6 54.0 53.3 Text + Gaze (Best) 73.3 73.6 73.5 71.9 71.8 71.8\nCNN with only text input (Kim, 2014) STATICTEXT 63.85 61.26 62.22 55.46 55.02 55.24 NONSTATICTEXT 72.78 71.93 72.35 60.51 59.79 60.14 MULTICHANNELTEXT 72.17 70.91 71.53 60.51 59.66 60.08\nCNN with only gaze Input\nFIXATION 60.79 58.34 59.54 53.95 50.29 52.06 SACCADE 64.19 60.56 62.32 51.6 50.65 51.12 MULTICHANNELGAZE 65.2 60.35 62.68 52.52 51.49 52\nCNN with both text and gaze Input\nSTATICTEXT + FIXATION 61.52 60.86 61.19 54.61 54.32 54.46 STATICTEXT + SACCADE 65.99 63.49 64.71 58.39 56.09 57.21 STATICTEXT + MULTICHANNELGAZE 65.79 62.89 64.31 58.19 55.39 56.75 NONSTATICTEXT + FIXATION 73.01 70.81 71.9 61.45 59.78 60.60 NONSTATICTEXT + SACCADE 77.56 73.34 75.4 65.13 61.08 63.04 NONSTATICTEXT + MULTICHANNELGAZE 79.89 74.86 77.3 63.93 60.13 62 MULTICHANNELTEXT + FIXATION 74.44 72.31 73.36 60.72 58.47 59.57 MULTICHANNELTEXT + SACCADE 78.75 73.94 76.26 63.7 60.47 62.04 MULTICHANNELTEXT + MULTICHANNELGAZE 78.38 74.23 76.24 64.29 61.08 62.64\nTable 1: Results for different traditional feature based systems and CNN model variants for the task of sentiment analysis. Abbreviations (P,R,F)→ Precision, Recall, F-score. SVM→Support Vector Machine\n25 epochs with validation accuracy stable around 50%, indicating the possibility of overfitting. Tuning the regularization parameters specific to dataset 2 may help here. Even though CNN might not be proving to be a choice as good as handcrafted features for dataset 2, the bottom line remains that incorporation of gaze data into CNN consistently improves the performance over onlytext-based CNN variants.\n\n6.2 Results for Sarcasm Detection Task\nFor sarcasm detection, our CNN model variants outperform traditional systems by a maximum margin of 11.27% (Table 2). However, the improvement by adding the gaze component to the CNN network is just 1.36%, which is statistically insignificant over CNN with text component. While inspecting the sarcasm dataset, we observe a clear difference between the vocabulary of sarcasm and non-sarcasm classes in our dataset. This, perhaps, was captured well by the text component, especially the variant with only non-static embeddings.\n\n7 Discussion\nIn this section, some important observations from our experiments are discussed. • Effect of embedding dimension variation: Embedding dimension has proven to have a deep impact on the performance of neural systems (dos\nSantos and Gatti, 2014; Collobert et al., 2011). We repeated our experiments by varying the embedding dimensions in the range of [50-300]4 and observed that reducing embedding dimension improves the F-scores by a little margin. Best results are obtained when the embedding dimension is as low as 50. Small embedding dimensions are probably reducing the chances of over-fitting when the data size is small. We also observe that for different embedding dimensions, performance of CNN with both gaze and text components is consistently better than that with only text component. • Effect of static / non static text channels: Nonstatic embedding channel has a major role in tuning embeddings for sentiment analysis by bringing adjectives expressing similar sentiment close to each other (e.g, good and nice), where as static channel seems to prevent over-tuning of embeddings (over-tuning often brings verbs like love closer to the pronoun I in embedding space, purely due to higher co-occurrence of these two words in sarcastic examples). • Effect of fixation / saccade channels: For sentiment detection, saccade channel seems to be handing text having semantic incongruity (due to the presence of irony / sarcasm) better. Fixation channel does not help much, may be because of higher variance in fixation duration. For sarcasm\n4a standard range (Liu et al., 2015; Melamud et al., 2016)\n7\n601\n602\n603\n604\n605\n606\n607\n608\n609\n610\n611\n612\n613\n614\n615\n616\n617\n618\n619\n620\n621\n622\n623\n624\n625\n626\n627\n628\n629\n630\n631\n632\n633\n634\n635\n636\n637\n638\n639\n640\n641\n642\n643\n644\n645\n646\n647\n648\n649\n650\n651\n652\n653\n654\n655\n656\n657\n658\n659\n660\n661\n662\n663\n664\n665\n666\n667\n668\n669\n670\n671\n672\n673\n674\n675\n676\n677\n678\n679\n680\n681\n682\n683\n684\n685\n686\n687\n688\n689\n690\n691\n692\n693\n694\n695\n696\n697\n698\n699\nConfiguration P R F\nTraditional systems based on\nNäive Bayes 69.1 60.1 60.5 Multi-layered Perceptron 69.7 70.4 69.9\ntextual features SVM (Linear Kernel) 72.1 71.9 72 Systems by Riloff et al. (2013) Text based (Ordered) 49 46 47 Text + Gaze (Unordered) 46 41 42\nSystem by Joshi et al. (2015)\nText based (best) 70.7 69.8 64.2\nSystems by Mishra et al. (2016b)\nGaze based (Best) 73 73.8 73.1 Text based (Best) 72.1 71.9 72 Text + Gaze (Best) 76.5 75.3 75.7\nCNN with only text input (Kim, 2014) STATICTEXT 67.17 66.38 66.77 NONSTATICTEXT 84.19 87.03 85.59 MULTICHANNELTEXT 84.28 87.03 85.63\nCNN with only gaze input\nFIXATION 74.39 69.62 71.93 SACCADE 68.58 68.23 68.40 MULTICHANNELGAZE 67.93 67.72 67.82\nCNN with both text and gaze Input\nSTATICTEXT + FIXATION 72.38 71.93 72.15 STATICTEXT + SACCADE 73.12 72.14 72.63 STATICTEXT + MULTICHANNELGAZE 71.41 71.03 71.22 NONSTATICTEXT + FIXATION 87.42 85.2 86.30 NONSTATICTEXT + SACCADE 84.84 82.68 83.75 NONSTATICTEXT + MULTICHANNELGAZE 84.98 82.79 83.87 MULTICHANNELTEXT + FIXATION 87.03 86.92 86.97 MULTICHANNELTEXT + SACCADE 81.98 81.08 81.53 MULTICHANNELTEXT + MULTICHANNELGAZE 83.11 81.69 82.39\nTable 2: Results for different traditional feature based systems and CNN model variants for the task of sarcasm detection on dataset 1. Abbreviations (P,R,F)→ Precision, Recall, F-score\ndetection, fixation and saccade channels perform with similar accuracy when employed separately. Accuracy reduces with gaze multichannel, may be because of higher variation of both fixations and saccades across sarcastic and non-sarcastic classes, as opposed to sentiment classes.\n• Effectiveness of the CNN learned features To examine how good the features learned by the CNN are, we analyzed the features for a few example cases. Figure 4 presents some of the testexamples for the task of sarcasm detection. Example 1 contains sarcasm while examples 2, 3 and 4 are non-sarcastic. To see if there is any difference in the automatically learned features between text-only and combined text and gaze variants, we examine the feature vector (of dimension 150) for the examples obtained from different model variants. Output of the hidden layer after merge layer is considered as features learned by the network. We plot the features, in the form of color-bars, following Li et al. (2016) - denser colors representing higher feature values. In Figure 4, we show only two (representative) model variants viz., MULTICHANNELTEXT and MULTICHANNELTEXT+ MULTICHANNELGAZE. As one can see, addition of gaze information helps\nto generate features with more subtle differences (marked by blue rectangular boxes) for sarcastic and non-sarcastic texts. It is also interesting to note that in the marked region, features for the sarcastic texts exhibit more intensity than the nonsarcastic ones - perhaps capturing the notion that sarcasm typically conveys an intensified negative opinion. This difference is not clear in feature vectors learned by text only systems for instances like example 2, which has been incorrectly classified by MULTICHANNELTEXT. Example 4 is incorrectly classified by both the systems, perhaps due to lack of context. In cases like this, addition of gaze information does not help much in learning more distinctive features, as it becomes difficult for even humans to classify such texts.\n\n8 Related Work\nSentiment and sarcasm classification are two important problems in NLP and have been the focus of research for many communities for quite some time. Popular sentiment and sarcasm detection systems are feature based and are based on unigrams, bigrams etc. (Dave et al., 2003; Ng et al., 2006), syntactic properties (Martineau and Finin, 2009; Nakagawa et al., 2010), semantic properties\n8\n701\n702\n703\n704\n705\n706\n707\n708\n709\n710\n711\n712\n713\n714\n715\n716\n717\n718\n719\n720\n721\n722\n723\n724\n725\n726\n727\n728\n729\n730\n731\n732\n733\n734\n735\n736\n737\n738\n739\n740\n741\n742\n743\n744\n745\n746\n747\n748\n749\n750\n751\n752\n753\n754\n755\n756\n757\n758\n759\n760\n761\n762\n763\n764\n765\n766\n767\n768\n769\n770\n771\n772\n773\n774\n775\n776\n777\n778\n779\n780\n781\n782\n783\n784\n785\n786\n787\n788\n789\n790\n791\n792\n793\n794\n795\n796\n797\n798\n799\n1. I would like to live in Manchester, England. The transition between Manchester and death would be unnoticeable. (Sarcastic, Negative Sentiment)\n2. We really did not like this camp. After a disappointing summer, we switched to another camp, and all of us much happier on all fronts! (Non Sarcastic, Negative Sentiment)\n3. Helped me a lot with my panics attack I take 6 mg a day for almost 20 years can't stop of course but make me feel very comfortable (Non Sarcastic, Positive Sentiment)\n4. Howard is the King and always will be, all others are weak clones. (Non Sarcastic, Positive Sentiment)\n(a) MultichannelText + MultichannelGaze (b) MultichannelText\nFigure 4: Visualization of representations learned by two variants of the network for sarcasm detection task. The output of the Merge layer (of dimension 150) are plotted in the form of colour-bars. Plots with thick red borders correspond to wrongly predicted examples.\n(Balamurali et al., 2011). For sarcasm detection, supervised approaches rely on (a) Unigrams and Pragmatic features (González-Ibánez et al., 2011; Barbieri et al., 2014; Joshi et al., 2015) (b) Stylistic patterns (Davidov et al., 2010) and patterns related to situational disparity (Riloff et al., 2013) and (c) Hastag interpretations (Liebrecht et al., 2013; Maynard and Greenwood, 2014). Recent systems are based on variants of deep neural network built on the top of embeddings. A few representative works in this direction for sentiment analysis are based on CNNs (dos Santos and Gatti, 2014; Kim, 2014; Tang et al., 2014), RNNs (Dong et al., 2014; Liu et al., 2015) and combined architecture (Wang et al., 2016). Few works exist on using deep neural networks for sarcasm detection, one of which is by (Ghosh and Veale, 2016) that uses a combination of RNNs and CNNs.\nEye-tracking technology is a relatively new NLP, with a very few systems directly making use of gaze data in prediction frameworks. Klerke et al. (2016) present a novel multi-task learning approach for sentence compression using labeled data, while, Barrett and Søgaard (2015) discriminate between grammatical functions using gaze features. The closest works to ours is by Mishra et al. (2016b) and Mishra et al. (2016c) that introduce feature engineering based on both gaze\nand text data for sentiment and sarcasm detection tasks. These recent advancements motivate us to explore the cognitive NLP paradigm .\n\n9 Conclusion and Future Directions\nIn this work, we proposed a multimodal ensemble of features, automatically learned using variants of CNNs from text and readers’ eye-movement data, for the tasks of sentiment and sarcasm classification. On multiple published datasets for which gaze information is available, our systems could achieve significant performance improvements over (a) systems that rely on handcrafted gaze and textual features and (b) CNN based systems that rely on text input alone. An analysis of the learned features confirms that the combination of automatically learned features is indeed capable of representing deep linguistic subtleties in text that pose challenges to sentiment and sarcasm classifiers. Our future agenda includes: (a) optimizing the CNN framework hyper-parameters (e.g., filter width, dropout, embedding dimensions etc.) to obtain better results, (b) exploring the applicability of our technique for document-level sentiment analysis and (c) applying our framework on related problems, such as emotion analysis, text summarization and question-answering.\n9\n801\n802\n803\n804\n805\n806\n807\n808\n809\n810\n811\n812\n813\n814\n815\n816\n817\n818\n819\n820\n821\n822\n823\n824\n825\n826\n827\n828\n829\n830\n831\n832\n833\n834\n835\n836\n837\n838\n839\n840\n841\n842\n843\n844\n845\n846\n847\n848\n849\n850\n851\n852\n853\n854\n855\n856\n857\n858\n859\n860\n861\n862\n863\n864\n865\n866\n867\n868\n869\n870\n871\n872\n873\n874\n875\n876\n877\n878\n879\n880\n881\n882\n883\n884\n885\n886\n887\n888\n889\n890\n891\n892\n893\n894\n895\n896\n897\n898\n899\n",
    "rationale": "- Strengths:\n\n(1) A deep CNN framework is proposed to extract and combine cognitive features\nwith textual features for sentiment analysis and sarcasm detection. \n\n(2) The ideas is interesting and novelty.\n\n- Weaknesses:\n\n(1) Replicability would be an important concern. Researchers cannot replicate\nthe system/method for improvement due to lack of data for feature extraction. \n\n- General Discussion:\n\nOverall, this paper is well written and organized. The experiments are\nconducted carefully for comparison with previous work and the analysis is\nreasonable. I offer some comments as follows.\n\n(1)           Does this model be suitable on sarcastic/non-sarcastic utterances?\nThe\nauthors should provide more details for further analysis. \n\n(2)           Why the eye-movement data would be useful for\nsarcastic/non-sarcastic\nsentiment classification beyond the textual features? The authors should\nprovide more explanations.",
    "rating": 5
  },
  {
    "title": "FRUSTRATINGLY SHORT ATTENTION SPANS IN NEURAL LANGUAGE MODELING",
    "abstract": "Neural language models predict the next token using a latent representation of the immediate token history. Recently, various methods for augmenting neural language models with an attention mechanism over a differentiable memory have been proposed. For predicting the next token, these models query information from a memory of the recent history which can facilitate learning midand long-range dependencies. However, conventional attention mechanisms used in memoryaugmented neural language models produce a single output vector per time step. This vector is used both for predicting the next token as well as for the key and value of a differentiable memory of a token history. In this paper, we propose a neural language model with a key-value attention mechanism that outputs separate representations for the key and value of a differentiable memory, as well as for encoding the next-word distribution. This model outperforms existing memoryaugmented neural language models on two corpora. Yet, we found that our method mainly utilizes a memory of the five most recent output representations. This led to the unexpected main finding that a much simpler model based only on the concatenation of recent output representations from previous time steps is on par with more sophisticated memory-augmented neural language models.",
    "text": "1 INTRODUCTION\nAt the core of language models (LMs) is their ability to infer the next word given a context. This requires representing context-specific dependencies in a sequence across different time scales. On the one hand, classical N -gram language models capture relevant dependencies between words in short time distances explicitly, but suffer from data sparsity. Neural language models, on the other hand, maintain and update a dense vector representation over a sequence where time dependencies are captured implicitly (Mikolov et al., 2010). A recent extension of neural sequence models are attention mechanisms (Bahdanau et al., 2015), which can capture long-range connections more directly. However, we argue that applying such an attention mechanism directly to neural language models requires output vectors to fulfill several purposes at the same time: they need to (i) encode a distribution for predicting the next token, (ii) serve as a key to compute the attention vector, as well as (iii) encode relevant content to inform future predictions.\nWe hypothesize that such overloaded use of output representations makes training the model difficult and propose a modification to the attention mechanism which separates these functions explicitly, inspired by Miller et al. (2016); Ba et al. (2016); Reed & de Freitas (2015); Gulcehre et al. (2016). Specifically, at every time step our neural language model outputs three vectors. The first is used to encode the next-word distribution, the second serves as key, and the third as value for an attention mechanism. We term the model key-value-predict attention and show that it outperforms existing memory-augmented neural language models on the Children’s Book Test (CBT, Hill et al., 2016) and a new corpus of 7500 Wikipedia articles. However, we observed that this model pays attention mainly to the previous five memories. We thus also experimented with a much simpler model that only uses a concatenation of output vectors from the previous time steps for predicting the next token. This simple model is on par with more sophisticated memory-augmented neural language models. Thus, our main finding is that modeling short attention spans properly works well and provides notable\nimprovements over a neural language model with attention. Conversely, it seems to be notoriously hard to train neural language models to leverage long-range dependencies.\nIn this paper, we investigate various memory-augmented neural language models and compare them against previous architectures. Our contributions are threefold: (i) we propose a key-value attention mechanism that uses specific output representations for querying a sliding-window memory of previous token representations, (ii) we demonstrate that while this new architecture outperforms previous memory-augmented neural language models, it mainly utilizes a memory of the previous five representations, and finally (iii) based on this observation we experiment with a much simpler but effective model that uses the concatenation of three previous output representations to predict the next word.\n\n2 METHODS\nIn the following, we discuss methods for extending neural language models with differentiable memory. We first present a standard attention mechanism for language modeling (§2.1). Subsequently, we introduce two methods for separating the usage of output vectors in the attention mechanism: (i) using a dedicated key and value (§2.2), and (ii) further separating the value into a memory value and a representation that encodes the next-word distribution (§2.3). Finally, we describe a very simple method that concatenates previous output representations for predicting the next token (§2.4).\n\n2.1 ATTENTION FOR NEURAL LANGUAGE MODELING\nAugmenting a neural language model with attention (Bahdanau et al., 2015) is straight-forward. We simply take the previous L output vectors as memory Yt = [ht−L · · · ht−1] ∈ Rk×L where k is the output dimension of a Long Short-Term Memory (LSTM) unit (Hochreiter & Schmidhuber, 1997). This memory could in principle contain all previous output representations, but for practical reasons we only keep a sliding window of the previous L outputs. Let ht ∈ Rk be the output representation at time step t and 1 ∈ RL be a vector of ones. The attention weights α ∈ RL are computed from a comparison of the current and previous LSTM outputs. Subsequently, the context vector rt ∈ Rk is calculated from a sum over previous output vectors weighted by their respective attention value. This can be formulated as\nMt = tanh(W Y Yt + (W hht)1 T ) ∈ Rk×L (1)\nαt = softmax(wTMt) ∈ R1×L (2) rt = Ytα T ∈ Rk (3)\nwhereW Y ,W h ∈ Rk×k are trainable projection matrices andw ∈ Rk is a trainable vector. The final representation that encodes the next-word distribution is computed from a non-linear combination of the attention-weighted representation rt of previous outputs and the final output vector ht via\nh∗t = tanh(W rrt +W xht) ∈ Rk (4)\nwhere W r,W x ∈ Rk×k are trainable projection matrices. An overview of this architecture is depicted in Figure 1a. Lastly, the probablity distribution yt for the next word is represented by\nyt = softmax(W ∗h∗t + b) ∈ R|V | (5)\nwhereW ∗ ∈ R|V |×k and b ∈ R|V | are a trainable projection matrix and bias, respectively.\n\n2.2 KEY-VALUE ATTENTION\nInspired by Miller et al. (2016); Ba et al. (2016); Reed & de Freitas (2015); Gulcehre et al. (2016), we introduce a key-value attention model that separates output vectors into keys used for calculating the attention distribution αt, and a value part used for encoding the next-word distribution and context representation. This model is depicted in Figure 1b. Formally, we rewrite Equations 1-4 as follows:[\nkt vt\n] = ht ∈ R2k (6)\nMt = tanh(W Y [kt−L · · · kt−1] + (W hkt)1T ) ∈ Rk×L (7)\nαt = softmax(wTMt) ∈ R1×L (8) rt = [vt−L · · · vt−1]αT ∈ Rk (9) h∗t = tanh(W rrt +W xvt) ∈ Rk (10)\nIn essence, Equation 7 compares the key at time step t with the previous L keys to calculate the attention distribution αt which is then used in Equation 9 to obtain a weighted context representation from values associated with these keys.\n\n2.3 KEY-VALUE-PREDICT ATTENTION\nEven with a key-value separation, a potential problem is that the same representation vt is still used both for encoding the probability distribution of the next word and for retrieval from the memory via the attention later. Thus, we experimented with another extension of this model where we further separate ht into a key, a value and a predict representation where the latter is only used for encoding the next-word distribution (see Figure 1c). To this end, equations 6 and 10 are replaced by[\nkt vt pt\n] = ht ∈ R3k (11)\nh∗t = tanh(W rrt +W xpt) ∈ Rk (12)\nMore precisely, the output vector ht is divided into three equal parts: key, value and predict. In our implementation we simply split the output vector ht into kt, vt and pt. To this end the hidden dimension of the key-value-predict attention model needs to be a multiplicative of three. Consequently, the dimensions of kt, vt and pt are 100 for a hidden dimension of 300.\n\n2.4 N -GRAM RECURRENT NEURAL NETWORK\nNeural language models often work best in combination with traditional N -gram models (Mikolov et al., 2011; Chelba et al., 2013; Williams et al., 2015; Ji et al., 2016; Shazeer et al., 2015), since the former excel at generalization while the latter ensure memorization. In addition, from initial experiments with memory-augmented neural language models, we found that usually only the previous five output representations are utilized. This is in line with observations by Tran et al. (2016). Hence, we experiment with a much simpler architecture depicted in Figure 1d. Instead of an attention mechanism, the output representations from the previous N − 1 time steps are directly used to calculate next-word probabilities. Specifically, at every time step we split the LSTM output into N − 1 vectors [h1t , . . . ,hN−1t ] and replace Equation 4 with\nh∗t = tanh WN  h 1 t ...\nhN−1t−N+1\n  ∈ Rk (13)\nwhere WN ∈ Rk×(N−1)k is a trainable projection matrix. This model is related to higher-order RNNs (Soltani & Jiang, 2016) with the difference that we do not incorporate output vectors from the previous steps into the hidden state but only use them for predicting the next word. Furthermore, note that at time step t the first part of the output vector h1t will contribute to predicting the next word, the second part h2t will contribute to predicting the second word thereafter, and so on. As the output vectors from the N − 1 previous time-steps are used to score the next word, we call the resulting model an N -gram RNN.\n\n3 RELATED WORK\nEarly attempts of using memory in neural networks have been undertaken by Taylor (1959) and Steinbuch & Piske (1963) by performing nearest-neighbor operations on input vectors and fitting parametric models to the retrieved sets. The dedicated use of external memory in neural architectures has more recently witnessed increased interest. Weston et al. (2015) introduced Memory Networks to explicitly segregate memory storage from the computation of the neural network, and Sukhbaatar et al. (2015) trained this model end-to-end with an attention-based memory addressing mechanism. The Neural Turing Machines by Graves et al. (2014) add an external differentiable memory with read-write functions to a controller recurrent neural network, and has shown promising results in simple sequence tasks such as copying and sorting. These models make use of external memory, whereas our model directly uses a short sequence from the history of tokens to dynamically populate an addressable memory.\nIn sequence modeling, RNNs such as LSTMs (Hochreiter & Schmidhuber, 1997) maintain an internal memory state as they process an input sequence. Attending over previous state outputs on top of an RNN encoder has improved performances in a wide range of tasks, including machine translation (Bahdanau et al., 2015), recognizing textual entailment (Rocktäschel et al., 2016), sentence summarization (Rush et al., 2015), image captioning (Xu et al., 2015) and speech recognition (Chorowski et al., 2015).\nRecently, Cheng et al. (2016) proposed an architecture that modifies the standard LSTM by replacing the memory cell with a memory network (Weston et al., 2015). Another proposal for conditioning on previous output representations are Higher-order Recurrent Neural Networks (HORNNs, Soltani & Jiang, 2016). Soltani & Jiang found it useful to include information from multiple preceding RNN states when computing the next state. This previous work centers around preceding state vectors, whereas we investigate attention mechanisms on top of RNN outputs, i.e. the vectors used for predicting the next word. Furthermore, instead of pooling we use attention vectors to calculate a context representation of previous memories.\nYang et al. (2016) introduced a reference-aware neural language model where at every position a latent variable determines from which source a target token is generated, e.g., by copying entries from a table or referencing entities that were mentioned earlier.\nAnother class of models that include memory into sequence modeling are Recurrent Memory Networks (RMNs) (Tran et al., 2016). Here, a memory block accesses the most recent input words to selectively attend over relevant word representations from a global vocabulary. RMNs use a global memory with two input word vector look-up tables for the attention mechanism, and consequently have a large number of trainable parameters. Instead, we proposed models that need much fewer parameters by producing the vectors that will be attended over in the future, which can be seen as a memory that is dynamically populated by the language model.\nFinally, the functional separation of look-up keys and memory content has been found useful for Memory Networks (Miller et al., 2016), Neural Programmer-Interpreters (Reed & de Freitas, 2015), Dynamic Neural Turing Machines (Gulcehre et al., 2016), and Fast Associative Memory (Ba et al., 2016). We apply and extend this principle to neural language models.\n\n4 EXPERIMENTS\nWe evaluate models on two different corpora for language modeling. The first is a subset of the Wikipedia corpus.1 It consists of 7500 English Wikipedia articles (dump from 6 Feb 2015) belonging to one of the following categories: People, Cities, Countries, Universities, and Novels. We chose these categories as we expect articles in these categories to often contain references to previously mentioned entities. Subsequently, we split this corpus into a train, development, and test part, resulting in corpora of 22.5M words, 1.2M and 1.2M words, respectively. We map all numbers to a dedicated numerical symbol N and restrict the vocabulary to the 77K most frequent words, encompassing 97% of the training vocabulary. All other words are replaced by the UNK symbol. The average length of sentences is 25 tokens. In addition to this Wikipedia corpus, we also run experiments on the Children’s Book Test (CBT Hill et al., 2016). While this corpus is designed for cloze-style question-answering, in this paper we use it to test how well language models can exploit wider linguistic context.\n\n4.1 TRAINING PROCEDURE\nWe use ADAM (Kingma & Ba, 2015) with an initial learning rate of 0.001 and a mini-batch size of 64 for optimization. Furthermore, we apply gradient clipping at a gradient norm of 5 (Pascanu et al., 2013). The bias of the LSTM’s forget gate is initialized to 1 (Jozefowicz et al., 2016), while other parameters are initialized uniformly from the range (−0.1, 0.1). Backpropagation Through Time (Rumelhart et al., 1985; Werbos, 1990) was used to train the network with 20 steps of unrolling. We reset the hidden states between articles for the Wikipedia corpus and between stories for CBT, respectively. We take the best configuration based on performance on the validation set and evaluate it on the test set.\n\n5 RESULTS\nIn the first set of experiments we explore how well the proposed models and Tran et al.’s Recurrentmemory Model can make use of histories of varying lengths. Perplexity results for different attention window sizes on the Wikipedia corpus are summarized in Figure 2a. The average attention these models pay to specific positions in the history is illustrated in Figure 3. We observed that although our models attend over tokens further in the past more often than the Recurrent-memory Model, attending over a longer history does not significantly improve the perplexity of any attentive model.\nThe much simpler N -gram RNN model achieves comparable results (Figure 2b) and seems to work best with a history of the previous three output vectors (4-gram RNN). As a result, we choose the 4-gram model for the following N -gram RNN experiments.\n1The wikipedia corpus is available at https://goo.gl/s8cyYa.\n\n5.1 COMPARISON WITH STATE-OF-THE-ART MODELS\nIn the next set of experiments, we compared our proposed models against a variety of state-of-the-art models on the Wikipedia and CBT corpora. Results are shown in Figure 2c and 2d, respectively. Note that the models presented here do not achieve state-of-the-art on CBT as they are language models and not tailored towards cloze-sytle question answering. Thus, we merely use this corpus for comparing different neural language model architectures. We reimplemented the Recurrent-Memory model by Tran et al. (2016) with the temporal matrix and gating composition function (RM+tM-g).\nFurthermore, we reimplemented Higher Order Recurrent Neural Networks (HORNNs) by Soltani & Jiang (2016).\nTo ensure a comparable number of parameters to a vanilla LSTM model, we adjusted the hidden size of all models to have roughly the same total number of model parameters. The attention window size N for the N -gram RNN model was set to 4 according to the best validation set perplexity on the Wikipedia corpus. Below we discuss the results in detail.\nAttention By using a neural language model with an attention mechanism over a dynamically populated memory, we observed a 3.2 points lower perplexity over a vanilla LSTM on Wikipedia, but only notable differences for predicting verbs and prepositions in CBT. This indicates that incorporating mechanisms for querying previous output vectors is useful for neural language modeling.\nKey-Value Decomposing the output vector into a key-value paired memory improves the perplexity by 7.0 points compared to a baseline LSTM, and by 1.9 points compared to the RM(+tM-g) model. Again, for CBT we see only small improvements.\nKey-Value-Predict By further separating the output vector into a key, value and next-word prediction part, we get the lowest perplexity and gain 9.4 points over a baseline LSTM, a 4.3 points compared to RM(+tM-g), and 2.4 points compared to only splitting the output into a key and value. For CBT, we see an accuracy increase of 1.0 percentage points for verbs, and 1.7 for prepositions. As stated earlier, the performance of the Key-Value-Predict model does not improve significantly when increasing the attention window size. This leads to the conclusion that none of the attentive models investigated in this paper can utilize a large memory of previous token representations. Moreover, none of the presented methods differ significantly for predicting common nouns and named entities in CBT.\nN -gram RNN Our main finding is that the simple modification of using output vectors from the previous time steps for the next-word prediction leads to perplexities that are on par with or better than more complicated neural language models with attention. Specifically, the 4-gram RNN achieves only slightly worse perplexities than the Key-Value-Predict architecture.\n\n6 CONCLUSION\nIn this paper, we observed that using an attention mechanism for neural language modeling where we separate output vectors into a key, value and predict part outperform simpler attention mechanisms on a Wikipedia corpus and the Children Book Test (CBT, Hill et al., 2016). However, we found that all attentive neural language models mainly utilize a memory of only the most recent history and fail to exploit long-range dependencies. In fact, a much simpler N -gram RNN model, which only uses a concatenation of output representations from the previous three time steps, is on par with more sophisticated memory-augmented neural language models. Training neural language models that take long-range dependencies into account seems notoriously hard and needs further investigation. Thus, for future work we want to investigate ways to encourage attending over a longer history, for instance by forcing the model to ignore the local context and only allow attention over output representations further behind the local history.\n",
    "rationale": "This paper focusses on attention for neural language modeling and has two major contributions:\n\n1. Authors propose to use separate key, value, and predict vectors for attention mechanism instead of a single vector doing all the 3 functions. This is an interesting extension to standard attention mechanism which can be used in other applications as well.\n2. Authors report that very short attention span is sufficient for language models (which is not very surprising) and propose an n-gram RNN which exploits this fact.\n\nThe paper has novel models for neural language modeling and some interesting messages. Authors have done a thorough experimental analysis of the proposed ideas on a language modeling task and CBT task.\n\nI am convinced with authors’ responses for my pre-review questions.\n\nMinor comment: Ba et al., Reed & de Freitas, and Gulcehre et al. should be added to the related work section as well.\n",
    "rating": 5
  },
  {
    "title": "CANE: Context-Aware Network Embedding for Relation Modeling",
    "abstract": "Network embedding (NE) is playing a critical role in network analysis, due to its ability to represent vertices with efficient low-dimensional embedding vectors. However, existing NE models aim to learn a fixed context-free embedding for each vertex, and neglect the diverse roles when interacting with other vertices. In this paper, we assume that one vertex usually shows different aspects when interacting with different neighbor vertices, and should own different embeddings respectively. Therefore, we present ContextAware Network Embedding (CANE), a novel NE model to address this issue. CANE learns context-aware embeddings for vertices with mutual attention mechanism and is expected to model the semantic relationships between vertices more precisely. In experiments, we compare our model with existing NE models on three real-world datasets. Experimental results shows that CANE achieves significant improvement than state-of-the-art methods on link prediction, and comparable performance on vertex classification.",
    "text": "1 Introduction\nNetwork embedding (NE), i.e., network representation learning (NRL), aims to map vertices of a network into a low-dimensional space according to their structural roles in the network. NE provides an efficient and effective way to represent and manage large-scale networks, alleviating the computation and sparsity issues of conventional symbol-based representations. Hence, NE is attracting many research interests in recent years (Perozzi et al., 2014; Tang et al., 2015; Grover and\nLeskovec, 2016), and achieves promising performance on many network analysis tasks including link prediction, vertex classification, and community detection.\nIn real-world social networks, it is intuitive that one vertex may demonstrate various aspects when interacting with different neighbor vertices. For example, a researcher usually collaborates with different partners on diverse research topics (as illustrated in Fig. 1), a social-media user contacts with various friends sharing distinct interests, and a web page links with multiple pages for different purposes. However, most existing NE methods only arrange one single embedding vector to each vertex, and give rise to the following two invertible issues: (1) These methods cannot cope with the aspect transition of a vertex flexibly when interacting with different neighbors. (2) In these models, a vertex tends to force the embeddings of its neighbors close to each other, which may be not case all the time. For example, the left user and right user in Fig. 1, share less common interests, but are learned to be close to each other since they both link to the middle person. This will accordingly\n2\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n112\n113\n114\n115\n116\n117\n118\n119\n120\n121\n122\n123\n124\n125\n126\n127\n128\n129\n130\n131\n132\n133\n134\n135\n136\n137\n138\n139\n140\n141\n142\n143\n144\n145\n146\n147\n148\n149\n150\n151\n152\n153\n154\n155\n156\n157\n158\n159\n160\n161\n162\n163\n164\n165\n166\n167\n168\n169\n170\n171\n172\n173\n174\n175\n176\n177\n178\n179\n180\n181\n182\n183\n184\n185\n186\n187\n188\n189\n190\n191\n192\n193\n194\n195\n196\n197\n198\n199\nmake vertex embeddings indiscriminative. To address these issues, we aim to propose a Context-Aware Network Embedding (CANE) framework for modeling relationships between vertices precisely. More specifically, we present CANE on information networks, where each vertex also contains rich external information such as text, labels or other meta-data, and the significance of context is more critical for NE in this scenario. Without loss of generality, we implement CANE on text-based information networks in this paper, which can be easily extended to other types of information networks.\nIn conventional NE models, each vertex is represented as a static embedding vector, denoted as context-free embedding. On the contrary, CANE assigns dynamic embeddings to a vertex according to different neighbors it interacts with, named as context-aware embedding . Take a vertex u and its neighbor vertex v for example. The contextfree embedding of u remains unchanged when interacting with different neighbors. On the contrary, the context-aware embedding of u is dynamic when confronting different neighbors.\nWhen u interacting with v, their context embeddings with respect to each other are derived from their text information, Su and Sv respectively. For each vertex, we can easily use neural models, such as convolutional neural networks (Blunsom et al., 2014; Johnson and Zhang, 2014; Kim, 2014) and recurrent neural networks (Kiros et al., 2015; Tai et al., 2015), to build context-free text-based embedding. In order to realize context-aware textbased embeddings, we introduce the selective attention scheme and build mutual attention between u and v into these neural models. The mutual attention is expected to guide neural models to emphasize those words that are focused by its neighbor vertices and eventually obtain contextaware embeddings.\nBoth context-free embeddings and contextaware embeddings of each vertex can be efficiently learned together via concatenation using existing NE methods such as DeepWalk (Perozzi et al., 2014), LINE (Tang et al., 2015) and node2vec (Grover and Leskovec, 2016).\nWe conduct experiments on three real-world datasets of different areas. Experimental results on link prediction reveal the effectiveness of our framework as compared to other state-of-the-art methods. The results suggest that, context-aware\nembeddings are critical for network analysis, especially for those tasks concerning about complicated interactions between vertices such as link prediction. We also explore the performance of our framework via vertex classification and case studies, which again confirms the flexibility and superiority of our models.\n\n2 Related Work\nWith the rapid growth of large-scale social networks, network embedding, i.e. network representation learning has been proposed as a critical technique for network analysis tasks.\nIn recent years, there have been a large number of NE models proposed to learn efficient vertex embeddings (Tang and Liu, 2009; Cao et al., 2015; Wang et al., 2016). For example, DeepWalk (Perozzi et al., 2014) performs random walks over networks and introduces an efficient word representation learning model, Skip-Gram (Mikolov et al., 2013a), to learn network embeddings. LINE (Tang et al., 2015) optimizes the joint and conditional probabilities of edges in large-scale networks to learn vertex representations. Node2vec (Grover and Leskovec, 2016) modifies the random walk strategy in DeepWalk into biased random walks to explore the network structure more efficiently. Nevertheless, most of these NE models only encode the structural information into vertex embeddings, without considering heterogenous information accompanied with vertices in real-world social networks.\nTo address this issue, researchers make great efforts to incorporate heterogenous information into conventional NE models. For instance, Yang et al. (2015) present text-associated DeepWalk (TADW) to improve matrix factorization based DeepWalk with text information. Tu et al. (2016) propose max-margin DeepWalk (MMDW) to learn discriminative network representations by utilizing labeling information of vertices. Chen et al. (2016) propose group-enhanced network embedding (GENE) to integrate existing group information in NE. Sun et al. (2016) regard text content as a special kind of vertices, and propose contextenhanced network embedding (CENE) through leveraging both structural and textural information to learn network embeddings.\nTo the best of our knowledge, all existing NE models focus on learning context-free embeddings, but ignore the diverse roles when a vertex\n3\n201\n202\n203\n204\n205\n206\n207\n208\n209\n210\n211\n212\n213\n214\n215\n216\n217\n218\n219\n220\n221\n222\n223\n224\n225\n226\n227\n228\n229\n230\n231\n232\n233\n234\n235\n236\n237\n238\n239\n240\n241\n242\n243\n244\n245\n246\n247\n248\n249\n250\n251\n252\n253\n254\n255\n256\n257\n258\n259\n260\n261\n262\n263\n264\n265\n266\n267\n268\n269\n270\n271\n272\n273\n274\n275\n276\n277\n278\n279\n280\n281\n282\n283\n284\n285\n286\n287\n288\n289\n290\n291\n292\n293\n294\n295\n296\n297\n298\n299\ninteracts with others. In contrast, we assume that a vertex has different embeddings according to which vertex it interacts with, and propose CANE to learn context-aware vertex embeddings.\n\n3 Problem Formulation\nWe first give basic notations and definitions in this work. Suppose there is an information network G = (V,E, T ), where V is the set of vertices, E ⊆ V ×V are edges between vertices, and T denotes the text information of vertices. Each edge eu,v ∈ E represents the relationship between two vertices (u, v), with an associated weight wu,v. Here, the text information of a specific vertex v ∈ V is represented as a word sequence Sv = (w1, w2, . . . , wnv), where nv = |Sv|. NRL aims to learn a low-dimensional embedding v ∈ Rd for each vertex v ∈ V according to its network structure and associated information, e.g. text and labels. Note that, d |V | is the dimension of representation space.\nDefinition 1. Context-free Embeddings: Conventional NRL models learn context-free embedding for each vertex. It means the embedding of a vertex is fixed and won’t change with respect to its context information (i.e., another vertex it interacts with).\nDefinition 2. Context-aware Embeddings: Different from existing NRL models that learn context-free embeddings, CANE learns various embeddings for a vertex according to its different contexts. Specifically, for an edge eu,v, CANE learns context-aware embeddings v(u) and u(v).\n\n4 The Method\n\n\n4.1 Overall Framework\nTo take full use of both network structure and associated text information, we propose two types of embeddings for a vertex v, i.e., structurebased embedding vs and text-based embedding vt. Structure-based embedding is able to capture the information in the network structure, while text-based embedding can capture the textual meanings lying in the associated text information. With these embeddings, we can simply concatenate them and obtain the vertex embeddings as v = vs⊕vt, where ⊕ indicates the concatenation operation. Note that, the text-based embedding vt can be either context-free or context-aware, which will be introduced detailedly in section 4.4 and 4.5\nrespectively. When vt is context-aware, the overall vertex embeddings v will be context-aware as well.\nWith above definitions, CANE aims to maximize the overall objective of edges as follows:\nL = ∑ e∈E L(e). (1)\nHere, the objective of each edge L(e) consists of two parts as follows:\nL(e) = Ls(e) + Lt(e), (2)\nwhere Ls(e) denotes the structure-based objective and Lt(e) represents the text-based objective.\nIn the following part, we give detailed introduction to the two objectives respectively.\n\n4.2 Structure-based Objective\nWithout loss of generality, we assume the network is directed, as an undirected edge can be considered as two directed edges with opposite directions and equal weights.\nThus, the structure-based objective aims to measure the log-likelihood of a directed edge using the structure-based embeddings as\nLs(e) = wu,v log p(v s|us). (3)\nFollowing LINE (Tang et al., 2015), we define the conditional probability of v generated by u in Eq. (3) as\np(vs|us) = exp(u s · vs)∑\nz∈V exp(u s · zs)\n. (4)\n\n4.3 Text-based Objective\nVertices in real-world social networks usually accompany with associated text information. Therefore, we propose the text-based objective to take advantage of these text information, as well as learn text-based embeddings for vertices.\nThe text-based objective Lt(e) can be defined with various measurements. To be compatible with Ls(e), we define Lt(e) as follows:\nLt(e) = α ·Ltt(e) + β ·Lts(e) + γ ·Lst(e), (5)\nwhere α, β and γ control the weights of various parts, and\nLtt(e) = wu,v log p(v t|ut), Lts(e) = wu,v log p(v t|us), Lst(e) = wu,v log p(v s|ut).\n(6)\n4\n301\n302\n303\n304\n305\n306\n307\n308\n309\n310\n311\n312\n313\n314\n315\n316\n317\n318\n319\n320\n321\n322\n323\n324\n325\n326\n327\n328\n329\n330\n331\n332\n333\n334\n335\n336\n337\n338\n339\n340\n341\n342\n343\n344\n345\n346\n347\n348\n349\n350\n351\n352\n353\n354\n355\n356\n357\n380\n381\n382\n383\n384\n385\n386\n387\n388\n389\n390\n391\n392\n393\n394\n395\n396\n397\n398\n399\nThe conditional probabilities in Eq. (6) map the two types of vertex embeddings into the same representation space, but do not enforce them to be identical for the consideration of their own characteristics. Similarly, we employ softmax function for calculating the probabilities, as in Eq. (4).\nThe structure-based embeddings are regarded as parameters, the same as in conventional NE models. But for text-based embeddings, we intend to obtain them from associated text information of vertices. Besides, the text-based embeddings can be obtained either in context-free ways or in context-aware ones. In the following sections, we will give detailed introduction respectively.\n\n4.4 Context-Free Text Embedding\nThere has been a variety of neural models to obtain text embeddings from a word sequence, such as convolutional neural networks (CNN) (Blunsom et al., 2014; Johnson and Zhang, 2014; Kim, 2014) and recurrent neural networks (RNN) (Kiros et al., 2015; Tai et al., 2015).\nIn this work, we investigate different neural networks for text modeling, including CNN, Bidirectional RNN (Schuster and Paliwal, 1997) and GRU (Cho et al., 2014), and employ the best performed CNN, which can capture the local semantic dependency among words.\nTaking the word sequence of a vertex as input, CNN obtains the text-based embedding through three layers, i.e. looking-up, convolution and pooling.\nLooking-up. Given a word sequence S = (w1, w2, . . . , wn), the looking-up layer transforms each word wi ∈ S into its corresponding word embedding wi ∈ Rd ′ and obtains embedding sequence as S = (w1,w2, . . . ,wn). Here, d′ indicates the dimension of word embeddings.\nConvolution. After looking-up, the convolution layer extracts local features of input embedding sequence S. To be specific, it performs convolution operation over a sliding window of length l using a convolution matrix C ∈ Rd×(l×d′) as follows:\nxi = C · Si:i+l−1 + b, (7)\nwhere Si:i+l−1 denotes the concatenation of word embeddings within the i-th window and b is the bias vector. Note that, we add zero padding vectors (Hu et al., 2014) at the edge of the sentence.\nMax-pooling. To obtain the text embedding vt, we operate max-pooling and non-linear transfor-\nmation over {xi0, . . . ,xin} as follows:\nri = tanh(max(x i 0, . . . ,x i n)), (8)\nAt last, we encode the text information of a vertex with CNN and obtain its text-based embedding vt = [r1, . . . , rd]\nT . As vt is irrelevant to the other vertices it interacts with, we name it as contextfree text embedding.\n\n4.5 Context-Aware Text Embedding\nAs stated before, we assume that a specific vertex plays different roles when interacting with others vertices. In other words, each vertex should have its own points of focus about a specific vertex, which leads to its context-aware text embeddings.\nTo achieve this, we employ mutual attention to obtain context-aware text embedding. It enables the pooling layer in CNN to be aware of the vertex pair in an edge, in a way that text information from a vertex can directly affect the text embedding of the other vertex, and vice versa.\nIn Fig. 2, we give an illustration of the generating process of context-aware text embedding. Given an edge eu,v with two corresponding text sequences Su and Sv, we can get the matrices P ∈ Rd×m and Q ∈ Rd×n through convolution layer. Here, m and n represent the lengths of Su\n5\n401\n402\n403\n404\n405\n406\n407\n408\n409\n410\n411\n412\n413\n414\n415\n416\n417\n418\n419\n420\n421\n422\n423\n424\n425\n426\n427\n428\n429\n430\n431\n432\n433\n434\n435\n436\n437\n438\n439\n440\n441\n442\n443\n444\n445\n446\n447\n448\n449\n450\n451\n452\n453\n454\n455\n456\n457\n458\n459\n460\n461\n462\n463\n464\n465\n466\n467\n468\n469\n470\n471\n472\n473\n474\n475\n476\n477\n478\n479\n480\n481\n482\n483\n484\n485\n486\n487\n488\n489\n490\n491\n492\n493\n494\n495\n496\n497\n498\n499\nand Sv respectively. By introducing an attentive matrix A ∈ Rd×d, we compute the correlation matrix F ∈ Rm×n as follows:\nF = tanh(PTAQ). (9)\nNote that, each element Fi,j in F represents the pair-wise correlation score between two hidden vectors, i.e., Pi and Qj .\nAfter that, we conduct pooling operations along rows and columns of F to generate the importance vectors, named as row-pooling and column pooling respectively. According to our experiments, mean-pooling performs better than max-pooling. Thus, we employ mean-pooling operation as follows:\ngpi = mean(Fi,1, . . . ,Fi,n), gqi = mean(F1,i, . . . ,Fm,i). (10)\nThe importance vectors of P and Q are obtained as gp = [gp1 , . . . , g p m]T and gq = [gq1, . . . , g q n]T .\nNext, we employ softmax function to transform importance vectors gp and gq to attention vectors ap and aq. For instance, the i-th element of ap is formalized as follows:\napi = exp(gpi )∑\nj∈[1,m] exp(g p j ) . (11)\nAt last, the context-aware text embeddings of u and v are computed as\nut(v) = Pa p, vt(u) = Qa q.\n(12)\nNow, given an edge (u, v), we can obtain the context-aware embeddings of vertices with their structure embeddings and context-aware text embeddings as u(v) = us⊕ut(v) and v(u) = v s⊕vt(u).\n\n4.6 Optimization of CANE\nAccording to Eq. (3) and Eq. (6), CANE aims to maximize several conditional probabilities between u ∈ {us,ut(v)} and v ∈ {v\ns,vt(u)}. It is intuitive that optimizing the conditional probability using softmax function is computationally expensive. Thus, we employ negative sampling (Mikolov et al., 2013b) and transform the objective into the following form:\nlog σ(uT ·v)+ k∑\ni=1\nEz∼P (v)[log σ(−uT ·z)], (13)\nwhere k is the number of negative samples and σ represents the sigmoid function. P (v) ∝ dv3/4 denotes the distribution of vertices, where dv is the out-degree of v.\nAfterwards, we employ Adam (Kingma and Ba, 2015) to optimize the transformed objective.\n\n5 Experiments\nIn order to investigate the effectiveness of CANE on modeling relationships between vertices, we conduct experiments of link prediction on several real-world datasets. Besides, we also employ vertex classification to verify whether contextaware embeddings of a vertex can compose a highquality context-free embedding in return.\n\n5.1 Datasets\nDatasets Cora HepTh Zhihu\n#Vertices 2, 277 1, 038 10, 000 #Edges 5, 214 1, 990 43, 894 #Labels 7 − −\nTable 1: Statistics of Datasets.\nWe select three real-world network datasets as follows:\nCora1 is a typical paper citation network constructed by (McCallum et al., 2000). After filtering out papers without text information, there are 2, 277 machine learning papers in this network, which are divided into 7 categories.\nHepTh2 (High Energy Physics Theory) is another citation network from arXiv3 released by (Leskovec et al., 2005). We filter out papers without abstract information and retain 1, 038 papers at last.\nZhihu4 is the largest online Q&A website in China. Users follow each other and answer questions in this site. We randomly crawl 10, 000 active users from Zhihu, and take the descriptions of their concerned topics as text information.\nThe detailed statistics are listed in Table 1.\n\n5.2 Baselines\nWe employ the following methods as baselines:\nStructure-only: DeepWalk (Perozzi et al., 2014) performs random walks over networks and employ Skip-Gram 1https://people.cs.umass.edu/∼mccallum/data.html 2https://snap.stanford.edu/data/cit-HepTh.html 3https://arxiv.org/ 4https://www.zhihu.com/\n6\n501\n502\n503\n504\n505\n506\n507\n508\n509\n510\n511\n512\n513\n514\n515\n516\n517\n518\n519\n520\n521\n522\n523\n524\n525\n526\n527\n528\n529\n530\n531\n532\n533\n534\n535\n536\n537\n538\n539\n540\n541\n542\n543\n544\n545\n546\n547\n548\n549\n550\n551\n552\n553\n554\n555\n556\n557\n558\n559\n560\n561\n562\n563\n564\n565\n566\n567\n568\n569\n570\n571\n572\n573\n574\n575\n576\n577\n578\n579\n580\n581\n582\n583\n584\n585\n586\n587\n588\n589\n590\n591\n592\n593\n594\n595\n596\n597\n598\n599\n%Removed edges 15% 25% 35% 45% 55% 65% 75% 85% 95%\nDeepWalk 56.0 63.0 70.2 75.5 80.1 85.2 85.3 87.8 90.3 LINE 55.0 58.6 66.4 73.0 77.6 82.8 85.6 88.4 89.3 node2vec 55.9 62.4 66.1 75.0 78.7 81.6 85.9 87.3 88.2\nNaive Combination 72.7 82.0 84.9 87.0 88.7 91.9 92.4 93.9 94.0 TADW 86.6 88.2 90.2 90.8 90.0 93.0 91.0 93.4 92.7 CENE 72.1 86.5 84.6 88.1 89.4 89.2 93.9 95.0 95.9\nCANE (text only) 78.0 80.5 83.9 86.3 89.3 91.4 91.8 91.4 93.3 CANE (w/o attention) 85.8 90.5 91.6 93.2 93.9 94.6 95.4 95.1 95.5\nCANE 86.8 91.5 92.2 93.9 94.6 94.9 95.6 96.6 97.7\nTable 2: AUC values on Cora. (α = 1.0, β = 0.3, γ = 0.3)\nmodel (Mikolov et al., 2013a) to learn vertex embeddings.\nLINE (Tang et al., 2015) learns vertex embeddings in large-scale networks using first-order and second-order proximities.\nNode2vec (Grover and Leskovec, 2016) proposes a biased random walk algorithm based on DeepWalk to explore neighborhood architecture more efficiently.\nStructure and Text: Naive Combination: We simply concatenate the best-performed structure-based embeddings with CNN based embeddings to represent the vertices.\nTADW (Yang et al., 2015) employs matrix factorization to incorporate text features of vertices into network embeddings.\nCENE (Sun et al., 2016) leverages both structure and textural information by regarding text content as a special kind of vertices, and optimizes the probabilities of heterogenous links.\n\n5.3 Evaluation Metrics and Experiment Settings\nFor link prediction, we adopt a standard evaluation metric AUC (Hanley and McNeil, 1982), which represents the probability that vertices in a random unobserved link are more similar than those in a random nonexistent link.\nFor vertex classification, we employ L2regularized logistic regression (L2R-LR) (Fan et al., 2008) to train classifiers, and evaluate the classification accuracies of various methods.\nTo be fair, we set the embedding dimension to 200 for all methods. In LINE, we set the number of negative samples to 5; we learn the 100 dimensional first-order and second-order embeddings respectively, and concatenate them to form the 200 dimensional embeddings. In node2vec, we employ grid search and select the best per-\nformed hyper-parameters for training. We also employ grid search to set the hyper-parameters α, β and γ in CANE. Besides, we set the number of negative samples k to 1 in CANE to speed up the training process. To demonstrate the effectiveness of considering attention mechanism and two types of objectives in Eqs. (3) and (6), we design three versions of CANE for evaluation, i.e., CANE with text only, CANE without attention and CANE.\n\n5.4 Link Prediction\nAs shown in Table 2, Table 3 and Table 4, we evaluate the AUC values while removing different ratios of edges on Cora, HepTh and Zhihu respectively. Note that, when we only keep 5% edges for training, most vertices are isolated, which results in the poor and meaningless performance of all the methods. Thus, we omit the results under this training ratio. From these tables, we have the following observations:\n(1) Our proposed CANE consistently achieves significant improvement comparing to all the baselines on all different datasets and different training ratios. It indicates the effectiveness of CANE when applied to link prediction task, and verifies that CANE has the capability of modeling relationships between vertices precisely.\n(2) What calls for special attention is that, both CENE and TADW exhibit unstable performance under various training ratios. Specifically, CENE performs poorly under small training ratios, because it reserves much more parameters (e.g., convolution kernels and word embeddings) than TADW, which need more data for training. Different from CENE, TADW performs much better under small training ratios, because DeepWalk based methods can explore the sparse network structure well through random walks even with limited edges. However, it achieves poor performance\n7\n601\n602\n603\n604\n605\n606\n607\n608\n609\n610\n611\n612\n613\n614\n615\n616\n617\n618\n619\n620\n621\n622\n623\n624\n625\n626\n627\n628\n629\n630\n631\n632\n633\n634\n635\n636\n637\n638\n639\n640\n641\n642\n643\n644\n645\n646\n647\n648\n649\n650\n651\n652\n653\n654\n655\n656\n657\n658\n659\n660\n661\n662\n663\n664\n665\n666\n667\n668\n669\n670\n671\n672\n673\n674\n675\n676\n677\n678\n679\n680\n681\n682\n695\n696\n697\n698\n699\n%Removed edges 15% 25% 35% 45% 55% 65% 75% 85% 95%\nDeepWalk 55.2 66.0 70.0 75.7 81.3 83.3 87.6 88.9 88.0 LINE 53.7 60.4 66.5 73.9 78.5 83.8 87.5 87.7 87.6 node2vec 57.1 63.6 69.9 76.2 84.3 87.3 88.4 89.2 89.2\nNaive Combination 78.7 82.1 84.7 88.7 88.7 91.8 92.1 92.0 92.7 TADW 87.0 89.5 91.8 90.8 91.1 92.6 93.5 91.9 91.7 CENE 86.2 84.6 89.8 91.2 92.3 91.8 93.2 92.9 93.2\nCANE (text only) 83.8 85.2 87.3 88.9 91.1 91.2 91.8 93.1 93.5 CANE (w/o attention) 84.5 89.3 89.2 91.6 91.1 91.8 92.3 92.5 93.6\nCANE 90.0 91.2 92.0 93.0 94.2 94.6 95.4 95.7 96.3\nTable 3: AUC values on HepTh. (α = 0.7, β = 0.2, γ = 0.2)\n%Removed edges 15% 25% 35% 45% 55% 65% 75% 85% 95%\nDeepWalk 56.6 58.1 60.1 60.0 61.8 61.9 63.3 63.7 67.8 LINE 52.3 55.9 59.9 60.9 64.3 66.0 67.7 69.3 71.1 node2vec 54.2 57.1 57.3 58.3 58.7 62.5 66.2 67.6 68.5\nNaive Combination 55.1 56.7 58.9 62.6 64.4 68.7 68.9 69.0 71.5 TADW 52.3 54.2 55.6 57.3 60.8 62.4 65.2 63.8 69.0 CENE 56.2 57.4 60.3 63.0 66.3 66.0 70.2 69.8 73.8\nCANE (text only) 55.6 56.9 57.3 61.6 63.6 67.0 68.5 70.4 73.5 CANE (w/o attention) 56.7 59.1 60.9 64.0 66.1 68.9 69.8 71.0 74.3\nCANE 56.8 59.3 62.9 64.5 68.9 70.4 71.4 73.6 75.4\nTable 4: AUC values on Zhihu. (α = 1.0, β = 0.3, γ = 0.3)\nunder large ones, as its simplicity and the limitation of bag-of-words assumption. On the contrary, CANE has a stable performance on various situations. It demonstrates the flexibility and robustness of CANE.\n(3) By introducing attention mechanism, the learnt context-aware embeddings obtain considerable improvements than the ones without attention. It verifies our assumption that a specific vertex should play different roles when interacting with other vertices, and thus benefits the relevant link prediction task.\nTo summarize, all the above observations demonstrate that CANE is able to learn highquality context-aware embeddings, which are conducive to estimating the relationship between vertices precisely. Moreover, the experimental results on link prediction task state the effectiveness and robustness of CANE.\n\n5.5 Vertex Classification\nIn CANE, we obtain various embeddings of a vertex according to the vertex it connects to. It’s intuitive that the obtained context-aware embeddings are naturally applicable to link prediction task. However, network analysis tasks, such as vertex classification and clustering, require a global embedding, rather than several context-aware embed-\ndings for each vertex. To demonstrate the capability of CANE to solve these issues, we generate the global embedding of a vertex u by simply averaging all the contextaware embeddings as follows:\nu = 1\nN ∑ (u,v)|(v,u)∈E u(v),\nwhere N indicates the number of context-aware embeddings of u.\n50\n55 60 65 70 75 80 85 90 95\n100\nDe ep\nW alk LI NE\nno de\n2v ec NC TA DW CE NE\nCA NE\n(te xt\non ly)\nCA NE\n(w /0\nat ten\ntio n) CA NE\nA cc\nur ac\ny (×\n1 00\n)\nWith the generated global embeddings, we conduct experiments of vertex classification on Cora. As shown in Fig. 3, we observe that:\n(1) CANE achieves comparable performance with state-of-the-art model CENE. It states that the\n8\n701\n702\n703\n704\n705\n706\n707\n708\n709\n710\n711\n712\n713\n714\n715\n716\n717\n718\n719\n720\n721\n722\n723\n724\n725\n726\n727\n728\n729\n730\n731\n732\n733\n734\n735\n736\n737\n738\n739\n740\n741\n742\n743\n744\n745\n746\n747\n748\n749\n775\n776\n777\n778\n779\n780\n781\n782\n783\n784\n785\n786\n787\n788\n789\n790\n791\n792\n793\n794\n795\n796\n797\n798\n799\nlearnt context-aware embeddings can transform into high-quality context-free embeddings through simple average operation, which can be further employed to other network analysis tasks.\n(2) With the introduction of mutual attention mechanism, CANE has an encouraging improvement than the one without attention, which is in accordance with the results of link prediction. It denotes that CANE is flexible to various network analysis tasks.\n\n5.6 Case Study\nTo demonstrate the significance of mutual attention on selecting meaningful features from text information, we visualize the heat maps of two vertex pairs in Fig. 4. Note that, every word in this figure accompanies with various background colors. The stronger the background color is, the larger the weight of this word is. The weight of each word is calculated according to the attention weights as follows.\nFor each vertex pair, we can get the attention weight of each convolution window according to Eq. (11). To obtain the weights of words, we assign the attention weight to each word in this window, and add the attention weights of a word together as its final weight.\nWe select three connected vertices in Cora for example, denoted as A, B and C. From Fig. 4, we observe that, though there exists citation relations with identical paper A, paper B and C concern about different parts of A. The attention weights over A in edge #1 are assigned to “reinforcement learning”. On the contrary, the weights in edge #2 are assigned to “machine learning’”, “supervised learning algorithms” and “complex stochastic models”. Moreover, all these key elements in A can find corresponding words in B and C. It’s intuitive that these key elements give an exact explanation on the citation relations. The discovered significant correlations between vertex pairs reflects the effectiveness of mutual attention mechanism, as well as the capability of CANE for modeling relations precisely.\n\n6 Conclusion and Future Work\nIn this paper, we propose the concept of ContextAware Network Embedding (CANE) for the first time, which aims to learn various context-aware embeddings for a vertex according to the neighbors it interacts with. Specifically, we implement\nCANE on text-based information networks with proposed mutual attention mechanism, and conduct experiments on several real-world information networks. Experimental results on link prediction demonstrate that CANE is effective for modeling the relationship between vertices. Besides, the learnt context-aware embeddings can compose high-quality context-free embeddings.\nWe will explore the following directions in future:\n(1) We have investigated the effectiveness of CANE on text-based information networks. In future, we will strive to implement CANE on wider variety of information networks with multi-modal data, such as labels, images and so on.\n(2) CANE encodes latent relations between vertices into their context-aware embeddings. Furthermore, there usually exist explicit relations in social networks (e.g., families, friends and colleagues relations between social network users), which are expected to be critical to NE. Thus, we want to explore how to incorporate and predict these explicit relations between vertices in NE.\n9\n801\n802\n803\n804\n805\n806\n807\n808\n809\n810\n811\n812\n813\n814\n815\n816\n817\n818\n819\n820\n821\n822\n823\n824\n825\n826\n827\n828\n829\n830\n831\n832\n833\n834\n835\n836\n837\n838\n839\n840\n841\n842\n843\n844\n845\n846\n847\n848\n849\n850\n851\n852\n853\n854\n855\n856\n857\n858\n859\n860\n861\n862\n863\n864\n865\n866\n867\n868\n869\n870\n871\n872\n873\n874\n875\n876\n877\n878\n879\n880\n881\n882\n883\n884\n885\n886\n887\n888\n889\n890\n891\n892\n893\n894\n895\n896\n897\n898\n899\n",
    "rationale": "This paper addresses the network embedding problem by introducing a neural\nnetwork model which uses both the network structure and associated text on the\nnodes, with an attention model to vary the textual representation based on the\ntext of the neighboring nodes.\n\n- Strengths:\n\nThe model leverages both the network and the text to construct the latent\nrepresentations, and the mutual attention approach seems sensible.\n\nA relatively thorough evaluation is provided, with multiple datasets,\nbaselines, and evaluation tasks.\n\n- Weaknesses:\n\nLike many other papers in the \"network embedding\" literature, which use neural\nnetwork techniques inspired by word embeddings to construct latent\nrepresentations of nodes in a network, the previous line of work on\nstatistical/probabilistic modeling of networks is ignored.  In particular, all\n\"network embedding\" papers need to start citing, and comparing to, the work on\nthe latent space model of Peter Hoff et al., and subsequent papers in both\nstatistical and probabilistic machine learning publication venues:\n\nP.D. Hoff, A.E. Raftery, and M.S. Handcock. Latent space approaches to social\nnetwork analysis. J. Amer. Statist. Assoc., 97(460):1090–1098, 2002.\n\nThis latent space network model, which embeds each node into a low-dimensional\nlatent space, was written as far back as 2002, and so it far pre-dates neural\nnetwork-based network embeddings.\n\nGiven that the aim of this paper is to model differing representations of\nsocial network actors' different roles, it should really cite and compare to\nthe mixed membership stochastic blockmodel (MMSB):\n\nAiroldi, E. M., Blei, D. M., Fienberg, S. E., & Xing, E. P. (2008). Mixed\nmembership stochastic blockmodels. Journal of Machine Learning Research.\n\nThe MMSB allows each node to randomly select a different \"role\" when deciding\nwhether to form each edge.\n\n- General Discussion:\n\nThe aforementioned statistical models do not leverage text, and they do not use\nscalable neural network implementations based on negative sampling, but they\nare based on well-principled generative models instead of heuristic neural\nnetwork objective functions and algorithms.  There are more recent extensions\nof these models and inference algorithms which are more scalable, and which do\nleverage text.\n\nIs the difference in performance between CENE and CANE in Figure 3\nstatistically insignificant? (A related question: were the experiments repeated\nmore than once with random train/test splits?)\n\nWere the grid searches for hyperparameter values, mentioned in Section 5.3,\nperformed with evaluation on the test set (which would be problematic), or on a\nvalidation set, or on the training set?",
    "rating": 5
  },
  {
    "title": "Understanding Image and Text Simultaneously: a Dual Vision-Language Machine Comprehension Task",
    "abstract": "We introduce a new multi-modal task for computer systems, posed as a combined vision-language comprehension challenge: identifying the most suitable text describing a scene, given several similar options. Accomplishing the task entails demonstrating comprehension beyond just recognizing “keywords” (or key-phrases) and their corresponding visual concepts. Instead, it requires an alignment between the representations of the two modalities that achieves a visually-grounded “understanding” of various linguistic elements and their dependencies. This new task also admits an easy-to-compute and wellstudied metric: the accuracy in detecting the true target among the decoys. The paper makes several contributions: an effective and extensible mechanism for generating decoys from (human-created) image captions; an instance of applying this mechanism, yielding a large-scale machine comprehension dataset (based on the COCO images and captions) that we make publicly available; human evaluation results on this dataset, informing a performance upper-bound; and several baseline and competitive learning approaches that illustrate the utility of the proposed task and dataset in advancing both image and language comprehension. We also show that, in a multi-task learning setting, the performance on the proposed task is positively correlated with the end-to-end task of image captioning.",
    "text": "1 Introduction\nThere has been a great deal of interest in multimodal artificial intelligence research recently, bringing together the fields of Computer Vision and Natural Language Processing. This interest has been fueled in part by the availability of many large-scale image datasets with textual annotations. Several vision+language tasks have been proposed around these datasets (Hodosh et al., 2013; Karpathy and Fei-Fei, 2015; Lin et al., 2014; Antol et al., 2015). Image Captioning (Hodosh et al., 2013; Donahue et al., 2014; Karpathy and Fei-Fei, 2015; Fang et al., 2015; Kiros et al., 2015; Vinyals et al., 2015; Mao et al., 2015; Xu et al., 2015b) and Visual Question Answering (Malinowski and Fritz, 2014; Malinowski et al., 2015; Tu et al., 2014; Antol et al., 2015; Yu et al., 2015; Wu et al., 2016b; Ren et al., 2015; Gao et al., 2015; Yang et al., 2016; Zhu et al., 2016; Lin and Parikh, 2016) have in particular attracted a lot of attention. The performances on these tasks have been steadily improving, owing much to the wide use of deep learning architectures (Bengio, 2009).\nA central theme underlying these efforts is the use of natural language to identify how much visual information is perceived and understood by a computer system. Presumably, a system that understands a visual scene well enough ought to be able to describe what the scene is about (thus “captioning”) or provide correct and visuallygrounded answers when queried (thus “questionanswering”).\nIn this paper, we argue for directly measuring how well the semantic representations of the visual and linguistic modalities align (in some abstract semantic space). For instance, given an image and two captions – a correct one and an incorrect yet-cunningly-similar one – can we both qualitatively and quantitatively measure the ex-\n2\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n112\n113\n114\n115\n116\n117\n118\n119\n120\n121\n122\n123\n124\n125\n126\n127\n128\n129\n130\n131\n132\n133\n134\n135\n136\n137\n138\n139\n140\n141\n142\n143\n144\n145\n146\n147\n148\n149\n150\n151\n152\n153\n154\n155\n156\n157\n158\n159\n160\n161\n162\n163\n164\n165\n166\n167\n168\n169\n170\n171\n172\n173\n174\n175\n176\n177\n178\n179\n180\n181\n182\n183\n184\n185\n186\n187\n188\n189\n190\n191\n192\n193\n194\n195\n196\n197\n198\n199\ntent to which humans can dismiss the incorrect one but computer systems blunder? Arguably, the degree of the modal alignment is a strong indicator of task-specific performance on any vision+language task. Consequentially, computer systems that can learn to maximize and exploit such alignment should outperform those that do not.\nWe take a two-pronged approach for addressing this issue. First, we introduce a new and challenging Dual Machine Comprehension (DMC) task, in which a computer system must identify the most suitable textual description from several options: one being the target and the others being “adversarialy”-chosen decoys. All options are free-form, coherent, and fluent sentences with high degrees of semantic similarity (hence, they are “cunningly similar”). A successful computer system has to demonstrate comprehension beyond just recognizing “keywords” (or key phrases) and their corresponding visual concepts; they must arrive at a coinciding and visually-grounded understanding of various linguistic elements and their dependencies. What makes the DMC task even more appealing is that it admits an easy-tocompute and well-studied performance metric: the accuracy in detecting the true target among the decoys. Second, we illustrate how solving the DMC task benefits related vision+language tasks. To this end, we render the DMC task as a classification problem, and incorporate it in a multitask learning framework for end-to-end training of joint objectives.\nOur work makes the following contributions: (1) an effective and extensible algorithm for generating decoys from human-created image captions (Section 3.2); (2) an instantiation of applying this algorithm to the COCO dataset (Lin et al., 2014), resulting in a large-scale dual machinecomprehension dataset that we make publicly available (Section 3.3); (3) a human evaluation on this dataset, which provides an upper-bound on performance (Section 3.4); (4) a benchmark study of baseline and competitive learning approaches (Section 5), which underperform humans by a substantial gap (about 20%); and (5) a multi-task learning model that simultaneously learns to solve the DMC task and the Image Captioning task (Section 4).\nOur empirical study shows that performance on the DMC task positively correlates with per-\nformance on the Image Captioning task. Therefore, besides acting as a standalone benchmark, the new DMC task can be useful in improving other complex vision+language tasks. Both suggest the DMC task as a fruitful direction for future research.\n\n2 Related work\nImage understanding is a long-standing challenge in computer vision. There has recently been a great deal of interest in bringing together vision and language understanding. Particularly relevant to our work are image captioning (IC) and visual question-answering (VQA). Both have instigated a large body of publications, a detailed exposition of which is beyond the scope of this paper. Interested readers should refer to two recent surveys (Bernardi et al., 2016; Wu et al., 2016a).\nIn IC tasks, systems attempt to generate a fluent and correct sentence describing an input image. IC systems are usually evaluated on how well the generated descriptions align with human-created captions (ground-truth). The language generation model of an IC system plays a crucial role; it is often trained such that the probabilities of the ground-truth captions are maximized (MLE training), though more advanced methods based on techniques borrowed from Reinforcement Learning have been proposed (Ranzato et al., 2015). To provide visual grounding, image features are extracted and injected into the language model. Note that language generation models need to both decipher the information encoded in the visual features, and model natural language generation.\nIn VQA tasks, the aim is to answer an input question correctly with respect to a given input image. In many variations of this task, answers are limited to single words or a binary response (“yes” or “no”) (Antol et al., 2015). The Visual7W dataset (Zhu et al., 2016) contains anaswers in a richer format such as phrases, but limits questions to “wh-”style (what, where, who, etc). The Visual Genome dataset (Krishna et al., 2016), on the other hand, can potentially define more complex questions and answers due to its extensive textual annotations.\nOur DMC task is related but significantly different. In our task, systems attempt to discriminate the best caption for an input image from a set of captions — all but one are decoys. Arguably, it is a form of VQA task, where the same default\n3\n201\n202\n203\n204\n205\n206\n207\n208\n209\n210\n211\n212\n213\n214\n215\n216\n217\n218\n219\n220\n221\n222\n223\n224\n225\n226\n227\n228\n229\n230\n231\n232\n233\n234\n235\n236\n237\n238\n239\n240\n241\n242\n243\n244\n245\n246\n247\n248\n249\n250\n251\n252\n253\n254\n255\n256\n257\n258\n259\n260\n261\n262\n263\n264\n265\n266\n267\n268\n269\n270\n271\n272\n273\n274\n275\n276\n277\n278\n279\n280\n281\n282\n283\n284\n285\n286\n287\n288\n289\n290\n291\n292\n293\n294\n295\n296\n297\n298\n299\n(thus uninformative) question is asked: Which of the following sentences best describes this image? However, unlike current VQA tasks, choosing the correct answer in our task entails a deeper “understanding” of the available answers. Thus, to perform well, a computer system needs to understand both complex scenes (visual understanding) and complex sentences (language understanding), and be able to reconcile them.\nThe DMC task admits a simple classificationbased evaluation metric: the accuracy of selecting the true target. This is a clear advantage over the IC tasks, which often rely on imperfect metrics such as BLEU (Papineni et al., 2002), ROUGE (Lin and Och, 2004), METEOR (Banerjee and Lavie, 2005), CIDEr (Vedantam et al., 2015), or SPICE (Anderson et al., 2016).\nRelated to our proposal is the work in (Hodosh et al., 2013), which frames image captioning as a ranking problem. While both share the idea of selecting captions from a large set, our framework has some important and distinctive components. First, we devise an algorithm for smart selection of candidate decoys, with the goal of selecting those that are sufficiently similar to the true targets to be challenging, and yet still be reliably identifiable by human raters. Second, we have conducted a thorough human evaluation in order to establish a performance ceiling, while also quantifying the level to which current learning systems underperform. Lastly, we show that there exists a positive correlation between the performance on the DMC task and the performance on related vision+language tasks by proposing and experimenting with a multi-task learning model. Our work is also substantially different from their more recent work (Hodosh and Hockenmaier, 2016), where only one decoy is considered and its generation is either random, or focusing on visual concept similarity (“switching people or scenes”) instead of our focus on both linguistic surface and paragraph vector embedding similarity.\n\n3 The Dual Machine Comprehension Task\n\n\n3.1 Design overview\nWe propose a new multi-modal machine comprehension task to examine how well visual and textual semantic understanding are aligned. Given an image, human evaluators or machines must accurately identify the best sentence describing the\nscene from several decoy sentences. Accuracy on this task is defined as the percentage that the true targets are identified.\nIt seems straightforward to construct a dataset for this task, as there are several existing datasets which are composed of images and their (multiple) ground-truth captions, including the popular COCO dataset (Lin et al., 2014). Thus, for any given image, it appears that one just needs to use the captions corresponding to other images as decoys. However, this naı̈ve approach could be overly simplistic as it is provides no control over the properties of the decoys.\nSpecifically, our desideratum is to recruit challenging decoys that are sufficiently similar to the targets. However, for a small number of decoys, e.g. 4-5, randomly selected captions could be significantly different from the target. The resulting dataset would be too “easy” to shed any insight on the task. Since we are also interested in human performance on this task, it is thus impractical to increase the number of decoys to raise the difficulty level of the task at the expense of demanding humans to examine tediously and unreliably a large number of decoys. In short, we need an automatic procedure to reliably create difficult sets of decoy captions that are sufficiently similar to the targets.\nWe describe such a procedure in the following. While it focuses on identifying decoy captions, the main idea is potentially adaptable to other settings. The algorithm is flexible in that the “difficulty” of the dataset can be controlled to some extent through the algorithm’s parameters.\n\n3.2 Algorithm to create an MC-IC dataset\nThe main idea behind our algorithm is to carefully define a “good decoy”. The algorithm exploits recent advances in paragraph vector (PV) models (Le and Mikolov, 2014), while also using linguistic surface analysis to define similarity between two sentences. Due to space limits, we omit a detailed introduction of the PV model. It suffices to note that the model outputs a continuouslyvalued embedding for a sentence, a paragraph, or even a document.\nThe pseudo-code for the algorithm is in the Listing 1 (the name MC-IC stands for “MachineComprehension for Image-Captions”). As input, the algorithm takes a large set C of\n4\n301\n302\n303\n304\n305\n306\n307\n308\n309\n310\n311\n312\n313\n314\n315\n316\n317\n318\n319\n320\n321\n322\n323\n324\n325\n326\n327\n328\n329\n330\n331\n332\n333\n334\n335\n336\n337\n338\n339\n340\n341\n342\n343\n344\n345\n346\n347\n348\n349\n350\n351\n352\n353\n354\n355\n356\n357\n358\n359\n360\n361\n362\n363\n364\n365\n366\n367\n368\n369\n370\n371\n372\n373\n374\n375\n376\n377\n378\n379\n380\n381\n382\n383\n384\n385\n386\n387\n388\n389\n390\n391\n392\n393\n394\n395\n396\n397\n398\n399\nAlgorithm 1: MC-IC(C, N , Score) Result: Dataset MCIC PV← OPTIMIZE-PV(C) λ← OPTIMIZE-SCORE(PV, C, Score) MCIC ← ∅ nr decoys = 4 for 〈ii, ci〉 ∈ C do\nA← [] Tci ← PV(ci)[1..N ] for cd ∈ Tci do\nscore← Score(PV, λ, cd, ci) if score > 0 then\nA.append(〈score, cd〉) end\nend if |A|≥ nr decoys then\nR← descending-sort(A) for l ∈ [1..nr decoys] do 〈score, cd〉 ← R[l] MCIC ← MCIC ∪{(〈ii, cd〉, false)} end MCIC ← MCIC ∪{(〈ii, ci〉, true)}\nend end\n〈image, {caption(s)} 〉 pairs∗, as those extracted from a variety of publicly-available corpora, including the COCO dataset (Lin et al., 2014). The output of the algorithm is the dataset MCIC which is used for the DMC task.\nConcretely, the MC-IC Algorithm has three main arguments: a dataset C = {〈ii, ci〉|1 ≤ i ≤ m} where ii is an image and ci is its ground-truth caption. For an image with multiple groundtruth captions, we split it to multiple instances with the same image and one unique groundtruth caption per instance. An integerN which controls the size of ci’s neighborhood in the embedding space defined by the paragraph vector model PV; and a function Score which is used to score theN items in each such neighborhood.\nThe first two steps of the algorithm tune several hyperparameters. The first step finds optimal settings for the PV model given the dataset C. The second finds a weight parameter λ given PV, dataset C, and the Score function. These hyperparameters are dataset-specific. Details are discussed in the next section.\nThe main body of the algorithm, the outer for loop, generates a set of nr decoys (4 here) decoys for each ground-truth caption. It accomplishes this by first extracting N candidates from the PV neighborhood of the ground-truth caption, excluding those that belong to the same image.\n∗On the order of at least hundreds of thousands of examples; smaller sets result in less challenging datasets.\nIn the inner for loop, it computes the similarity of each candidate to the ground-truth and stores them in a list A. If enough candidates are generated, the list is sorted in descending order of score. The top nr decoys captions are marked as “decoys” (i.e.false), while the ground-truth caption is marked as “target” (i.e.true).\nThe score function Score(PV, λ, c′, c) is a crucial component of the decoy selection mechanism. Its definition leverages our linguistic intuition by combining linguistic surface similarity, simSURF(c′, c), with the similarity suggested by the embedding model, simPV(c′, c):\nScore= {\n0 if simSURF≥L λ simPV+(1−λ) simSURF otherwise (1)\nwhere the common argument (c′, c) is omitted. The higher the similarity score, the more likely that c′ is a good decoy for c. Note that if the surface similarity is above the threshold L, the function returns 0, flagging that the two captions are too similar to be used as a pair of target and decoy.\nIn this work, simSURF is computed as the BLEU score between the inputs (Papineni et al., 2002) (with the brevity penalty set to 1). The embedding similarity, simPV, is computed as the cosine similarity between the two in the PV embedding space.\n\n3.3 The MCIC dataset\nWe applied the MC-IC Algorithm to the COCO dataset (Lin et al., 2014) to generate a dataset for the visual-language dual machine comprehension task. The dataset, called MCIC , is made publicly available (address anonymized). We describe the details of this dataset below.\nWe set the neighborhood size at N = 500, and the threshold atL = 0.5 (see Eq. 1). As the COCO dataset has a large body of images (thus captions) focusing on a few categories (such as sports activities), this threshold is important in discarding significantly similar captions to be decoys – otherwise, even human annotators will experience difficulty in selecting the ground-truth captions.\nThe hyperparameters of the PV model, dim (embedding dimension) and epochs (number of training epochs), are optimized in the OPTIMIZE-PV step of the MC-IC Algorithm. The main idea is to learn embeddings such that ground-truth captions from the same image have similar embeddings. Details are in the Suppl. Ma-\n5\n401\n402\n403\n404\n405\n406\n407\n408\n409\n410\n411\n412\n413\n414\n415\n416\n417\n418\n419\n420\n421\n422\n423\n424\n425\n426\n427\n428\n429\n430\n431\n432\n433\n434\n435\n436\n437\n438\n439\n440\n441\n442\n443\n444\n445\n446\n447\n448\n449\n479\n480\n481\n482\n483\n484\n485\n486\n487\n488\n489\n490\n491\n492\n493\n494\n495\n496\n497\n498\n499\nSplit dev test train total #unique˙images 2,000 2,000 110,800 114,800\n# instances 9,999 10,253 554,063 574,315\nTable 1: MCIC dataset descriptive statistics\nterial. The optimal settings are dim=1024 and epochs=5.\nLikewise, the λ in the Score function is optimized by the OPTIMIZE-SCORE step, such that ground-truth captions associated with the same image have similar scores. Details are in the Suppl. Material. The optimal λ is 0.3.\nThe resulting MCIC dataset has 574,315 instances that are in the format of {i : (〈ii, cji 〉, label j i ), j = 1 . . . 5} where label j i ∈ {true, false}. For each such instance, there is one and only one j such that the label is true. We have created a train/dev/test split such that all of the instances for the same image occur in the same split. Table 1 reports the basic statistics for the dataset. Figure 1 shows several instances from the MCIC dataset.\n\n3.4 Human performance on MCIC\nTo measure how well humans can perform on the DMC task, we randomly drew 1,000 instances from the MCIC dev set and submitted those instances to human “raters”† via a crowdsourcing platform.\nThree independent responses from 3 different raters were gathered for each instance, for a total of 3,000 responses. To ensure diversity, raters were prohibited from evaluating more than six instances or from responding to the same task instance twice. In total, 807 distinct raters were employed.\nRaters were shown one instance at a time. They were shown the image and the five caption choices (ground-truth and four decoys) and were instructed to choose the best caption for the image. To supplement the instructions, raters were initially shown a few examples from the training set with the ground-truth caption highlighted, to illustrate how to discern the most appropriate caption for the image (see the Suppl. Material for details).\nWe assessed human performance using two metrics: (1) Percentage of correct rater responses (1-human system): 81.1% (2432 out of 3000); (2) †Raters are vetted, screened and tested before working on any tasks; requirements include native-language proficiency level.\nPercentage of instances with at least 50% (i.e.2) correct responses (3-human system): 82.8% (828 out of 1000). Due to space limitation, more discussions about the human raters disagreement is available in the Suppl. Material.\n\n4 Learning Methods\nWe describe several learning methods for the dual machine comprehension (DMC) task with the MCIC dataset.\nRegression. To examine how well the two embeddings are aligned in “semantic understanding space”, a simple approach is to assume that the learners do not have access to the decoys. Instead, by accessing the ground-truth captions only,\n6\n501\n502\n503\n504\n505\n506\n507\n508\n509\n510\n511\n512\n513\n514\n515\n516\n517\n518\n519\n520\n521\n522\n523\n524\n525\n526\n527\n528\n529\n530\n531\n532\n533\n534\n535\n536\n537\n538\n539\n540\n541\n542\n543\n544\n545\n546\n547\n548\n549\n550\n551\n552\n553\n554\n555\n556\n557\n558\n559\n560\n561\n562\n563\n564\n565\n566\n567\n568\n569\n570\n571\n572\n573\n574\n575\n576\n577\n578\n579\n580\n581\n582\n583\n584\n585\n586\n587\n588\n589\n590\n591\n592\n593\n594\n595\n596\n597\n598\n599\nthe models learn a linear regressor from the image embeddings to the target captions’ embeddings (“forward regression”), or from the captions to the images (“backward regression”). With the former approach, referred as Baseline-I2C, we check whether the predicted caption for any given image is closest to its true caption. With the latter, referred as Baseline-C2I, we check whether the predicted image embedding by the ground-truth caption is the closest among predicted ones by decoy captions to the real image embeddings.\nLinear classifier. Our next approach BaselineLinM is a linear classifier learned to discriminate true targets from the decoys. Specifically, we learn a linear discriminant function f(i, c;θ) = i> θ c where θ is a matrix measuring the compatibility between two types of embeddings, cf. (Frome et al., 2013). The loss function is then given by\nL(θ) = ∑ i [max j 6=j∗ f(ii, c j i ;θ)− f(ii, c j∗ i ;θ)]+ (2)\nwhere [ ]+ is the hinge function and j indexes over all the available decoys and i indexes over all training instances. The optimization tries to increase the gap between the target cj ∗\ni and the worst “offending” decoy. We use stochastic (sub)gradient methods to optimize θ, and select the best model in terms of accuracy on the MCIC dev set.\nFFNN Model. To present our neural-network– based models, we use the following notations. Each training instance pair is a tuple 〈ii, cji 〉, where i denotes the image, and cji denotes the caption options, which can either be the target or the decoys. We use a binary variable yijk ∈ {0, 1} to denote whether j-th caption of the instance i is labeled as k, and ∑ k yijk = 1.\nWe first employ the standard feedforward neural-network models to solve the MCIC task. For each instance pair 〈ii, cji 〉, the input to the neural network is an embedding tuple 〈DNN(ii; Γ),Emb(cji ; Ω)〉, where Γ denotes the parameters of a deep convolutional neural network DNN. DNN takes an image and outputs an image embedding vector. Ω is the embedding matrix, and Emb(.) denotes the mapping from a list of word IDs to a list of embedding vectors using Ω. The loss function for our FFNN is given by: L(Γ,Ω,u) = ∑ i,j,k yijk log FNk(DNN(ii; Γ),Emb(cji ; Ω);u)\n(3)\nwhere FNk denotes the k-th output of a feedforward neural network, and ∑ k FNk(.) = 1. Our architecture uses a two hidden-layer fully connected network with Rectified Linear hidden units, and a softmax layer on top.\nThe formula in Eq. 3 is generic with respect to the number of classes. In particular, we consider a 2-class–classifier (k ∈ {0, 1}, 1 for ’yes’, this is a correct answer; 0 for ’no’, this is an incorrect answer), applied independently on all the 〈ii, cji 〉 pairs and apply one FFNN-based binary classifier for each; the final prediction is the caption with the highest ’yes’ probability among all instance pairs belonging to instance i.\nVec2seq + FFNN Model. We describe here a hybrid neural-network model that combines a recurrent neural-network with a feedforward one. We encode the image into a single-cell RNN encoder, and the caption into an RNN decoder. Because the first sequence only contains one cell, we call this model a vector-to-sequence (Vec2seq) model as a special case of Seq2seq model as in (Sutskever et al., 2014; Bahdanau et al., 2015). The output of each unit cell of a Vec2seq model (both on the encoding side and the decoding side) can be fed into an FFNN architecture for binary classification (see the Suppl. Material for an architecture illustration).\nIn addition to the classification loss (Eq. 3), we also include a loss for generating an output sequence cji based on an input ii image. We define a binary variable zijlv ∈ {0, 1} to indicate whether the lth word of cji is equal to word v. O d ijl denotes the l-th output of the decoder of instance pair 〈ii, cji 〉, O e ij denotes the output of the encoder, and Odij: denotes the concatenation of decoder outputs. With these definitions, the loss function for the Vec2seq + FFNN model is:\nL(θ,w,u) = ∑ i,j,k yijk log FNk(Oeij(ii, c j i ;θ),O d ij:(ii, c j i ;θ);u)\n+λgen ∑ i,j,l,v yij1zijlv log softmaxv(Odijl(ii, c j i ;θ);w)\n(4) where ∑\nv softmaxv(.) = 1; θ are the parameters of the Vec2seq model, which include the parameters within each unit cell, as well as the elements in the embedding matrices for images and target sequences; w are the output projection parameters that transform the output space of the decoder to the vocabulary space. u are the parameters of the\n7\n601\n602\n603\n604\n605\n606\n607\n608\n609\n610\n611\n612\n613\n614\n615\n616\n617\n618\n619\n620\n621\n622\n623\n624\n625\n626\n627\n628\n629\n630\n631\n632\n633\n634\n635\n636\n637\n638\n639\n640\n641\n642\n643\n644\n645\n646\n647\n648\n649\n650\n651\n652\n653\n654\n655\n656\n657\n658\n659\n660\n661\n662\n663\n664\n665\n666\n667\n668\n669\n670\n671\n672\n673\n674\n675\n676\n677\n678\n679\n680\n681\n682\n683\n684\n685\n686\n687\n688\n689\n690\n691\n692\n693\n694\n695\n696\n697\n698\n699\nFFNN model (Eq. 3); λgen is the weight assigned to the sequence-to-sequence generation loss. Only the true target candidates (the ones with yij1 = 1) are included in this loss, as we do not want the decoy target options to affect this computation.\nThe Vec2seq model we use here is an instantiation of the attention-enhanced models proposed in (Bahdanau et al., 2015; Chen et al., 2016). However, our current model does not support location-wise attention, as in the Show-Attendand-Tell (Xu et al., 2015a) model. In this sense, our model is an extension of the Show-and-Tell model with a single attention state representing the entire image, used as image memory representation for all decoder decisions. We apply Gated Recurrent Unit (GRU) as the unit cell (Cho et al., 2014). We also compare the influence on performance of the λgen parameter.\n\n5 Experiments\n\n\n5.1 Experimental Setup\nBaseline models For the baseline models, we use the 2048-dimensional outputs of GoogleInception-v3 (Szegedy et al., 2015) (pre-trained on ImageNet ILSSVR 2012) to represent the images, and 1024-dimensional paragraph-vector embeddings (section 3.2) to represent captions. To reduce computation time, both are reduced to 256- dimensional vectors using random projections. Neural-nets based models The experiments with these models are done using the Tensorflow package (Abadi et al., 2015). The hyper-parameter choices are decided using the hold-out development portion of the MCIC set. For modeling the input tokens, we use a vocabulary size of 8,855 types, selected as the most frequent tokens over the captions from the COCO training set (words occurring at least 5 times). The models are optimized using ADAGRAD with an initial learning rate of 0.01, and clipped gradients (maximum norm 4). We run the training procedures for 3, 000, 000 steps, with a mini-batch size of 20. We use 40 workers for computing the updates, and 10 parameter servers for model storing and (asynchronous and distributed) updating.\nWe use the following notations to refer to the neural network models: FFNNargmax 1..52-class refers to the version of feedforward neural network architecture with a 2-class–classifier (’yes’ or ’no’ for answer correctness), over which an argmax function computes a 5-way decision (i.e., the choice\nwith the highest ’yes’ probability); we henceforth refer to this model simply as FFNN.\nThe Vec2seq+FFNN refers to the hybrid model combining Vec2seq and FFNNargmax 1..52-class . The RNN part of the model uses a two-hidden–layer GRU unit-cell (Cho et al., 2014) configuration, while the FFNN part uses a two-hidden–layer architecture. The λgen hyper-parameter from the loss-function L(θ,w,u) (Eq. 4) is by default set to 1.0 (except for Section 5.3 where we directly measure its effect on performance).\nWe also include the result of a Vec2Seq model that is trained for caption generation only. To use the model for classification, we feed both image and each of its caption candidates to the Vec2Seq model, and then pick the caption candidate which has the lowest perplexity. Evaluation metrics The metrics we use to measure performance come in two flavors. First, the accuracy in detecting (the index of) the true target among the decoys provides a direct way of measuring the performance level on the comprehension task. We use this metric as the main indicator of comprehension performance. Second, because our Vec2seq+FFNN models are multitask models, they can also generate new captions given the input image. The performance level for the generation task is measured using the standard scripts measuring ROUGE-L (Lin and Och, 2004) and CIDEr (Vedantam et al., 2015), using as reference the available captions from the COCO data (around 5 for most of the images). Code for these metrics is available as part of the COCO evaluation toolkit ‡. As usual, both the hypothesis strings and the reference strings are preprocessed: remove all the non-alphabetic characters; transform all letters to lowercase, and tokenize using white space; replace all words occurring less than 5 times with an unknown token 〈UNK〉 (total vocabulary of 8,855 types); truncate to the first 30 tokens.\n\n5.2 Results\nTable 2 summarizes our main results on the comprehension task. We report the accuracies (and their standard deviations) for random choice, baselines, and neural network-based models.\nInterestingly, the Baseline-I2C model performs at the level of random choice, and much worse than the Baseline-C2I model. This discrepancy reflects the inherent difficulty in vision-\n‡https://github.com/tylin/coco-caption\n8\n701\n702\n703\n704\n705\n706\n707\n708\n709\n710\n711\n712\n713\n714\n715\n716\n717\n718\n719\n720\n721\n722\n723\n724\n725\n726\n727\n728\n729\n730\n731\n732\n733\n734\n735\n736\n737\n738\n739\n740\n741\n742\n743\n744\n745\n746\n747\n748\n749\n758\n759\n760\n761\n762\n763\n764\n765\n766\n767\n768\n769\n770\n771\n772\n773\n774\n775\n776\n777\n778\n779\n780\n781\n782\n783\n784\n785\n786\n787\n788\n789\n790\n791\n792\n793\n794\n795\n796\n797\n798\n799\nModel dim Dev Test Baseline-I2C 256 19.6 ±0.4 19.3±0.4 Baseline-C2I 256 32.8 ±0.5 32.0±0.5 Baseline-LinM 256 44.6 ±0.5 44.5±0.5 FFNN 256 56.3 ±0.5 55.1±0.5\nVec2seq+FFNN 256 60.5 ±0.5 59.0±0.5 Vec2Seq 256 15.6 ± 0.4 16.0 ± 0.4\nTable 2: Performance on the DMC Task, in accuracies (and standard deviations) on MCIC for baselines and NN models.\nLanguage tasks: for each image, there are several possible equally good descriptions, thus a linear mapping from the image embeddings to the captions might not be enough – statistically, the linear model will just predict the mean of those captions. However, for the reverse direction where the captions are the independent variables, the learned model does not have to capture the variability in image embeddings corresponding to the different but equally good captions – there is only one such image embedding.\nNonlinear neural networks overcome these modeling limitations. The results clearly indicate their superiority over the baselines. The Vec2seq+FFNN model obtains the best results, with accuracies of 60.5% (dev) and 59.0% (test); the accuracy numbers indicate that the Vec2seq+FFNN architecture is superior to the non-recursive fully-connected FFNN architecture (at 55.1% accuracy on test). In the Suppl. Material, we show the impact on performance of the mebedding dimension and neural-network sizes.\nLast but not least, the Vec2Seq model performs the worst in the classification task. This result indicates that a caption generation model alone is unable to discriminate among captions that are close in a pretrained embedding space, even when it has a good caption generation performance (CIDEr 0.983 on dev and 0.927 on test).\n\n5.3 DMC and Image Captioning\nIn this section, we compare models with different values of λgen in Eq. 4. This parameter allows for a natural progression from learning for the DMC task only (λgen = 0) to focusing more on the image captioning loss (λgen = 16).\nThe results in Table 3 illustrate one of the main points of this paper. That is, the ability to perform the comprehension task (as measured by the accuracy metric) positively correlates with the ability to perform other tasks that require machine comprehension, such as caption generation. At\nλgen = 4, the Vec2seq+FFNN model not only has a high accuracy of detecting the ground-truth option, but it also generates its own captions given the input image, with an accuracy measured on MCIC at 0.9890 (dev) and 0.9380 (test) CIDEr scores. On the other hand, at an accuracy level of about 59% (on test, at λgen = 0.1), the generation performance is at only 0.9010 (dev) and 0.8650 (test) CIDEr scores.\nWe note that there is an inherent trade-off between prediction accuracy and generation performance, as seen for λgen values above 4.0. This agrees with the intuition that training a Vec2seq+FFNN model using a loss L(θ,w,u) with a larger λgen means that the ground-truth detection loss (the first term of the loss in Eq.4) may get overwhelmed by the word-generation loss (the second term). However, our empirical results suggest that there is value in training models with a multi-task setup, in which both the comprehension side as well as the generation side are carefully tuned to maximize performance.\n\n6 Discussion\nWe have proposed and described in detail a new multi-modal machine comprehension task (DMC). The underlying hypothesis is that computer systems that can be shown to perform increasingly well on this task will do so by constructing a visually-grounded understanding of various linguistic elements and their dependencies.\nThe Vec2seq+FFNN architecture can be trained end-to-end to display both the ability to choose the most likely text associated with an image, as well as the ability to generate a complex description of that image. The empirical results validate that improvements in comprehension and generation happen in tandem.\n9\n801\n802\n803\n804\n805\n806\n807\n808\n809\n810\n811\n812\n813\n814\n815\n816\n817\n818\n819\n820\n821\n822\n823\n824\n825\n826\n827\n828\n829\n830\n831\n832\n833\n834\n835\n836\n837\n838\n839\n840\n841\n842\n843\n844\n845\n846\n847\n848\n849\n850\n851\n852\n853\n854\n855\n856\n857\n858\n859\n860\n861\n862\n863\n864\n865\n866\n867\n868\n869\n870\n871\n872\n873\n874\n875\n876\n877\n878\n879\n880\n881\n882\n883\n884\n885\n886\n887\n888\n889\n890\n891\n892\n893\n894\n895\n896\n897\n898\n899\n",
    "rationale": "- Strengths:\n\nAuthors generate a dataset of “rephrased” captions and are planning to make\nthis dataset publicly available.\n\nThe way authors approached DMC task has an advantage over VQA or caption\ngeneration in terms of metrics. It is easier and more straightforward to\nevaluate problem of choosing the best caption. Authors use accuracy metric.\nWhile for instance caption generation requires metrics like BLUE or Meteor\nwhich are limited in handling semantic similarity.\n\nAuthors propose an interesting approach to “rephrasing”, e.g. selecting\ndecoys. They draw decoys form image-caption dataset. E.g. decoys for a single\nimage come from captions for other images. These decoys however are similar to\neach other both in terms of surface (bleu score) and semantics (PV similarity).\nAuthors use lambda factor to decide on the balance between these two components\nof the similarity score. I think it would be interesting to employ these for\nparaphrasing.\n\nAuthors support their motivation for the task with evaluation results. They\nshow that a system trained with the focus on differentiating between similar\ncaptions performs better than a system that is trained to generate captions\nonly. These are, however, showing that system that is tuned for a particular\ntask performs better on this task.\n\n- Weaknesses:\n\n It is not clear why image caption task is not suitable for comprehension task\nand why author’s system is better for this. In order to argue that system can\ncomprehend image and sentence semantics better one should apply learned\nrepresentation, e.g. embeddings. E.g. apply representations learned by\ndifferent systems on the same task for comparison.\n\nMy main worry about the paper is that essentially authors converge to using\nexisting caption generation techniques, e.g. Bahdanau et al., Chen et al.\n\nThey way formula (4) is presented is a bit confusing. From formula it seems\nthat both decoy and true captions are employed for both loss terms. However, as\nit makes sense, authors mention that they do not use decoy for the second term.\nThat would hurt mode performance as model would learn to generate decoys as\nwell. The way it is written in the text is ambiguous, so I would make it more\nclear either in the formula itself or in the text. Otherwise it makes sense for\nthe model to learn to generate only true captions while learning to distinguish\nbetween true caption and a decoy.\n\n- General Discussion:\n\nAuthors formulate a task of Dual Machine Comprehension. They aim to accomplish\nthe task by challenging computer system to solve a problem of choosing between\ntwo very similar captions for a given image. Authors argue that a system that\nis able to solve this problem has to “understand” the image and captions\nbeyond just keywords but also capture semantics of captions and their alignment\nwith image semantics.\n\nI think paper need to make more focus on why chosen approach is better than\njust caption generation and why in their opinion caption generation is less\nchallenging for learning image and text representation and their alignment.\n\nFor formula (4). I wonder if in the future it is possible to make model to\nlearn “not to generate” decoys by adjusting second loss term to include\ndecoys but with a negative sign. Did authors try something similar?",
    "rating": 5
  },
  {
    "title": "Identifying 1950s American Jazz Composers: Fine-Grained IsA Extraction via Modifier Composition",
    "abstract": "We present a method for populating fine-grained classes (e.g., “1950s American jazz composers”) with instances (e.g., Charles Mingus). While stateof-the-art methods tend to treat class labels as single lexical units, the proposed method considers each of the individual modifiers in the class label relative to the head. An evaluation on the task of reconstructing Wikipedia category pages demonstrates a >10 point increase in AUC, over a strong baseline relying on widely-used Hearst patterns.",
    "text": "1 Introduction\nSubstantial attention has been paid to automatically acquiring taxonomic knowledge, like that “Charles Mingus” is a “composer”, from text (Snow et al., 2006; Shwartz et al., 2016). The majority of approaches for extracting such “IsA” relations rely on lexical patterns as the primary signal of whether an instance belongs to a class: for example, observing a pattern like “X such as Y” is a strong indication that Y is an instance of class X (Hearst, 1992).\nMethods based on these “Hearst patterns” assume that class labels can be treated as atomic lexicalized units. This assumption has several significant weakness. First, in order to recognize an instance of a class, these patternbased methods require that the entire class label be observed verbatim in text. The requirement is reasonable for class labels containing a single word, but in practice, there are many possible fine-grained classes: not only “composers” but also “1950s American jazz composers”. The probability that a given label will appear in its entirety within one of the\nexpected patterns is very low, even in large amounts of text. Second, when class labels are treated as though they cannot be decomposed, every class label must be modeled independently, even those containing overlapping words (“American jazz composer”, “French jazz composer”). As a result, the number of meaning representations to be learned is exponential in the length of the class label, and quickly becomes intractable. Thus, compositional models of taxonomic relations are necessary for better language understanding.\nWe introduce a compositional approach for reasoning about fine-grained class labels. Our approach is based on the notion from formal semantics in which modifiers (“1950s”) correspond to properties which differentiate instances of a subclass (“1950s composers”) from instances of the superclass (“composers”) (Heim and Kratzer, 1998). Our method consists of two stages: interpreting each modifier relative to the head (“composers active during 1950s”), and using the interpretations to identify instances of the class from text (Figure 1). Our main contributions are: 1) a compositional method for IsA extraction, which in-\n2\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n112\n113\n114\n115\n116\n117\n118\n119\n120\n121\n122\n123\n124\n125\n126\n127\n128\n129\n130\n131\n132\n133\n134\n135\n136\n137\n138\n139\n140\n141\n142\n143\n144\n145\n146\n147\n148\n149\n150\n151\n152\n153\n154\n155\n156\n157\n158\n159\n160\n161\n162\n163\n164\n165\n166\n167\n168\n169\n170\n171\n172\n173\n174\n175\n176\n177\n178\n179\n180\n181\n182\n183\n184\n185\n186\n187\n188\n189\n190\n191\n192\n193\n194\n195\n196\n197\n198\n199\nvolves a novel application of noun-phrase paraphrasing methods to the task of semantic taxonomy induction and 2) the operationalization of a formal semantics framework to address two aspects of semantics that are often kept separate in NLP: assigning intrinsic “meaning” to a phrase, and reasoning about that phrase in a truth-theoretic context.\n\n2 Related Work\nNoun Phrase Interpretation. Compound noun phrases (“jazz composer”) communicate implicit semantic relations between modifiers and the head. Many efforts to provide semantic interpretations of such phrases rely on matching the compound to pre-defined patterns or semantic ontologies (Fares et al., 2015; Séaghdha and Copestake, 2007; Tratz and Hovy, 2010; Surtani and Paul, 2015; Choi et al., 2015). Recently, interpretations may take the form of arbitrary natural language predicates (Hendrickx et al., 2013). Most approaches are supervised, comparing unseen noun compounds to the most similar phrase seen in training (Wijaya and Gianfortoni, 2011; Nulty and Costello, 2013; Van de Cruys et al., 2013). Other unsupervised approaches apply information extraction techniques to paraphrase noun compounds (Kim and Nakov, 2011; Xavier and de Lima, 2014; Pasca, 2015). They focus exclusively on providing good paraphrases for an input noun compound. To our knowledge, ours is the first attempt to use these interpretations for the downstream task of IsA relation extraction.\nSemantic Taxonomy Induction. Most efforts to learn taxonomic relations from text build on the seminal work of Hearst (1992), which observes that certain textual patterns– e.g., “X and other Y”–are high-precision indicators of whether X is a member of class Y. Recent work focuses on learning such patterns automatically from corpora (Snow et al., 2006; Shwartz et al., 2016). These IsA extraction techniques provide a key step for the more general task of knowledge base population. The “universal schema” approach (Riedel et al., 2013; Kirschnick et al., 2016; Verga et al., 2016), which infers relations using matrix factorization, often includes Hearst patterns as input features. Graphical (Bansal et al., 2014)\nand joint inference models (Movshovitz-Attias and Cohen, 2015) typically require Hearst patterns to define an inventory of possible classes. A separate line of work avoids Hearst patterns by instead exploiting semi-structured data from HTML markup (Wang and Cohen, 2009; Dalvi et al., 2012; Pasupat and Liang, 2014). These approaches all share the limitation that, in practice, in order for a class to be populated with instances, the entire class label has to have been observed verbatim in text. This requirement limits the ability to handle arbitrarily fine-grained classes. Our work addresses this limitation by modeling fine-grained class labels compositionally. Thus the proposed method can combine evidence from multiple sentences, and can perform IsA extraction without requiring any example instances of a given class.1\n\n3 Modifiers as Functions\nFormalization. In formal semantics, modification is modeled as function application. Specifically, let MH be a class label consisting of a head H, which we assume to be a common noun, preceded by a modifier M . We use J·K to represent the “interpretation function” which maps a linguistic expression to its denotation in the world. The interpretation of a common noun is the set of entities2 in the universe U which are denoted by the noun (Heim and Kratzer, 1998):\nJHK = {e ∈ U | e is a H} (1)\nThe interpretation of a modifier M is a function that maps between sets of entities. That is, modifiers select a subset3 of the input set:\nJMK(H) = {e ∈ H | e satisfies M} (2)\nThis formalization leaves open how one decides whether or not “e satisfiesM”. This nontrivial, as the meaning of a modifier can vary depending on the class it is modifying: if e is a “good student”, e is not necessarily a “good\n1Pasupat and Liang (2014) also focuses on zero-shot IsA extraction, but exploits HTML document structure, rather than reasoning compositionally.\n2We use “entities” and “instances” interchangeably;“entities” is standard terminology in linguistics.\n3As does virtually all previous work in information extraction, we assume that modifiers are subsective, acknowledging the limitations (Kamp and Partee, 1995).\n3\n201\n202\n203\n204\n205\n206\n207\n208\n209\n210\n211\n212\n213\n214\n215\n216\n217\n218\n219\n220\n221\n222\n223\n224\n225\n226\n227\n228\n229\n230\n231\n232\n233\n234\n235\n236\n237\n238\n239\n240\n241\n242\n243\n244\n245\n246\n247\n248\n249\n250\n251\n252\n253\n254\n255\n256\n257\n258\n259\n260\n261\n262\n263\n264\n265\n266\n267\n268\n269\n270\n271\n272\n273\n274\n275\n276\n277\n278\n279\n280\n281\n282\n283\n284\n285\n286\n287\n288\n289\n290\n291\n292\n293\n294\n295\n296\n297\n298\n299\nperson”, making it difficult to model whether “e satisfies good” in general. We therefore reframe the above equation, so that the decision of whether “e satisfies M” is made by calling a binary function φM , parameterized by the class H within which e is being considered:\nJMK(H) = {e ∈ H | φM (H, e)} (3)\nConceptually, φM captures the core “meaning” of the modifier M , which is the set of properties that differentiate members of the output class MH from members of the more general input class H. This formal semantics framework has two important consequences. First, the modifier has an intrinsic “meaning”. The properties entailed by the modifier are independent of the particular state of the world. This makes it possible to make inferences about “1950s composers” even if no 1950s composers have been observed. Second, the modifier is a function that can be applied in a truth-theoretic setting. That is, applying “1950s” to the set of “composers” returns exactly the set of “1950s composers”.\nComputational Approaches. While the notion of modifiers as functions has been incorporated into computational models previously, prior work focuses on either assigning an intrinsic meaning to M or on operationalizing M in a truth-theoretic sense, but not on doing both simultaneously. For example, Young et al. (2014) focuses exclusively on the subset selection aspect of modification. That is, given a set of instances H and a modifier M , their method could return the subset MH. However, their method does not model the meaning of the modifier itself, so that, e.g., if there were no red cars in their model of the world, the phrase “red cars” would have no meaning. In contrast, Baroni and Zamparelli (2010) models the meaning of modifiers explicitly as functions which map between vector-space representations of nouns. However, their model focuses on similarity between class labels–e.g., to say that “important routes” is similar to “major roads”–and it is not obvious how the method could be operationalized in order to identify instances of those classes. A contribution of our work is to model the semantics of M intrinsically, but in a way that permits application in the\nmodel theoretic setting. We learn an explicit model of the “meaning” of a modifier M relative to a head H, represented as a distribution over properties which differentiate the members of the class MH from those of the class H. We then use this representation to identify the subset of instances of H which constitute the subclass MH.\n\n4 Learning Modifier Interpretations\n\n\n4.1 Setup\nFor each modifier M , we would like to learn the function φM from Eq. 3. Doing so makes it possible, given H and an instance e ∈ H, to decide whether e has the properties required to be an instance of MH. In general, there is no systematic way to determine the implied relation between M and H, as modifiers can arguably express any semantic relation, given the right context (Weiskopf, 2007). We therefore model the semantic relation between M and H as a distribution over properties which could potentially define the subclass MH ⊆ H. We will refer to this distribution as a “property profile” for M relative to H. We make the assumption that relations between M and H that are discussed more often are more likely to capture the important properties of the subclass MH. This assumption is not perfect (Section 4.4) but has given good results for paraphrasing noun phrases (Nakov and Hearst, 2013; Pasca, 2015). Our method for learning property profiles is based on the unsupervised method proposed by Pasca (2015), which uses query logs as a source of common sense knowledge, and rewrites noun compounds by matching MH (“American composers”) to queries of the form “H(.∗)M” (“composers from America”).\n\n4.2 Inputs\nWe assume two inputs: 1) an IsA repository, O, containing 〈e, C〉 tuples where C is a category and e is an instance of C, and 2) a fact repository, D, containing 〈s, p, o, w〉 tuples where s and o are noun phrases, p is a predicate, and w is a confidence that p expresses a true relation between s and o. Both O and D are extracted from a sample of around 1 billion Web documents in English. The supplementary material gives additional details.\n4\n301\n302\n303\n304\n305\n306\n307\n308\n309\n310\n311\n312\n313\n314\n315\n316\n317\n318\n319\n320\n321\n322\n323\n324\n325\n326\n327\n328\n329\n330\n331\n332\n333\n334\n335\n336\n337\n338\n339\n340\n341\n342\n343\n344\n345\n346\n347\n348\n349\n350\n351\n352\n353\n354\n355\n356\n357\n358\n359\n360\n361\n362\n363\n364\n365\n366\n367\n368\n369\n370\n371\n372\n373\n374\n375\n376\n377\n378\n379\n380\n381\n382\n383\n384\n385\n386\n387\n388\n389\n390\n391\n392\n393\n394\n395\n396\n397\n398\n399\nGood property profiles Bad property profiles rice dish French violinist Led Zeppelin song still life painter child actor risk manager * serve with rice * live in France Led Zeppelin write * * known for still life * have child * take risk * include rice * born in France Led Zeppelin play * * paint still life * expect child * be at risk * consist of rice * speak French Led Zeppelin have * still life be by * * play child * be aware of risk\nTable 1: Example property profiles learned by observing predicates that relate instances of class H to modifier M (I2). Results are similar when using the class label H directly (I1). We spell out inverted predicates (Section 4.2) so wildcards (*) may appear as subjects or objects.\nWe instantiate O with an IsA repository constructed by applying Hearst patterns to the Web documents. Instances are represented as automatically-disambiguated entity mentions4 which, when possible, are resolved to Wikipedia pages. Classes are represented as (non-disambiguated) natural language strings. We instantiate D with a large repository of facts extracted using in-house implementations of ReVerb (Fader et al., 2011) and OLLIE (Mausam et al., 2012). The predicates are extracted as natural language strings. Subjects and objects may be either disambiguated entity references or natural language strings. Every tuple is included in both the forward and the reverse direction. E.g. 〈jazz, perform at, venue〉 also appears as 〈venue,←perform at, jazz〉, where ← is a special character signifying inverted predicates. These inverted predicates simplify the following definitions. In total, O contains 1.1M tuples and D contains 30M tuples.\n\n4.3 Building Property Profiles\nProperties. Let I be a function which takes as input a noun phrase MH and returns a property profile for M relative to H. We define a “property” to be an SPO tuple in which the subject position5 is a wildcard, e.g. 〈∗, born in,America〉. Any instance which fills the wildcard slot then “has” the property. We expand adjectival modifiers to encompass nominalized forms using a nominalization dictionary extracted from WordNet (Miller, 1995). If MH is “American composer” and we require a tuple to have the form 〈H, p,M,w〉, we will include tuples in which the third element is either “American” or “America”.\n4“Entity mentions” may be individuals, like “Barack Obama”, but may also be concepts like “jazz”.\n5Inverse predicates capture properties in which the wildcard is conceptually the object of the relation, but occupies the subject slot in the tuple. For example, 〈venue,←perform at, jazz〉 captures that a “jazz venue” is a “venue” e such that “jazz performed at e”.\nRelating M to H Directly. We first build property profiles by taking the predicate and object from any tuple in D in which the subject is the head and the object is the modifier:\nI1(MH) = {〈〈p,M〉, w〉 | 〈H, p,M,w〉 ∈ D} (4)\nRelating M to an Instance of H. We also consider an extension in which, rather than requiring the subject to be the class label H, we require the subject to be an instance of H.\nI2(MH) = {〈〈p,M〉, w〉 | 〈e,H〉 ∈ O ∧〈e, p,M,w〉 ∈ D}\n(5)\nModifier Expansion. In practice, when building property profiles, we do not require that the object of the fact tuple match the modifier exactly, as suggested in Eq. 4 and 5. Instead, we follow Pasca (2015) and take advantage of facts involving distributionally similar modifiers. Specifically, rather than looking only at tuples in D in which the object matches M , we consider all tuples, but discount the weight proportionally to the similarity between M and the object of the tuple. Thus, I1 is computed as below:\nI1(MH) = {〈〈p,M〉, w × sim(M,N)〉 | 〈H, p,N,w〉 ∈ D} (6)\nwhere sim(M,N) is the cosine similarity between M and N . I2 is computed analogously. We compute sim using a vector space built from Web documents following Lin and Wu (2009); Pantel et al. (2009). We retain the 100 most similar phrases for each of∼10M phrases, and consider all other similarities to be 0.\n\n4.4 Analysis of Property Profiles\nTable 1 provides examples of good and bad property profiles for several MHs. In general, frequent relations between M and H capture relevant properties of MH, but it is not always\n5\n401\n402\n403\n404\n405\n406\n407\n408\n409\n410\n411\n412\n413\n414\n415\n416\n417\n418\n419\n420\n421\n422\n423\n424\n425\n426\n427\n428\n429\n430\n431\n432\n433\n434\n435\n436\n437\n438\n439\n440\n441\n442\n443\n444\n445\n446\n447\n448\n449\n450\n451\n452\n453\n454\n455\n456\n457\n458\n459\n460\n461\n462\n463\n464\n465\n466\n467\n468\n469\n470\n471\n472\n473\n474\n475\n476\n477\n478\n479\n480\n481\n482\n483\n484\n485\n486\n487\n488\n489\n490\n491\n492\n493\n494\n495\n496\n497\n498\n499\nClass label Property profile American company * based in America American composer * born in America American novel * written in America jazz album * features jazz jazz composer * writes jazz jazz venue jazz performed at *\nTable 2: Head-specific property profiles learned by relating instances of H to the modifier M (I2). Results are similar using I1.\nthe case. To illustrate, the most frequently discussed relation between “child” and “actor” is that actors have children, but this property is not indicative of the meaning of “child actor”. Qualitatively, the top-ranked interpretations learned by using the head noun directly (I1, Eq. 4) are very similar to those learned using instances of the head (I2, Eq. 5). However, I2 returns many more properties (10 on average per MH) than I1 (just over 1 on average). Anecdotally, we see that I2 captures more specific relations than does I1. For example, for “jazz composers”, both methods return “* write jazz” and “* compose jazz”, but I2 additionally returns properties like “* be major creative influence in jazz”. We compare I1 and I2 quantitatively in Section 6. Importantly, we do see that both I1 and I2 are capable of learning head-specific property profiles for a modifier. Table 2 provides examples.\n\n5 Class-Instance Identification\nInstance finding. After finding properties that relate a modifier to a head, we turn to the task of identifying instances of fine-grained classes. That is, for a given modifier M , we want to instantiate the function φM from Eq. 3. In practice, rather than being a binary function which decides whether or not e is in class MH, our instantiation, φ̂M , will return a realvalued score expressing the confidence that e is a member of MH. For notational convenience, let D(〈s, p, o〉) = w, if 〈s, p, o, w〉 ∈ D and 0 otherwise. We define φ̂M as follows:\nφ̂M (H, e) = ∑\n〈〈p,o〉,w〉∈I(MH)\nw ×D(〈e, p, o〉)\n(7) Applying M to H, then, is as in Eq. 3 except that instead of a discrete set, it returns a scored list of candidate instances:\nJMK(H) = {〈e, φ̂M (H, e)〉 | 〈e,H〉 ∈ O} (8)\nUltimately, we need to identify instances of arbitrary class labels, which may contain multiple modifiers. Given a class label C = M1 . . .MkH which contains a head H preceded by modifiers M1 . . .Mk, we generate a list of candidate instances by finding all instances of H which have some property to support every modifier:\nk⋂ i=1 {〈e, s(e)〉 | 〈e, w〉 ∈ JMiK(H) ∧w > 0} (9)\nwhere s(e) is the mean6 of the scores assigned by each separate φ̂Mi . From here on, we use Mods to refer to our method which generates lists of instances for a class using Eq. 8 and 9. When φ̂M (Eq. 7) is implemented using I1, we use the name ModsH (for “heads”). When it is implemented using I2, we use the name ModsI (for “instances”).\nWeakly Supervised Reranking. Eq. 8 uses a naive ranking in which the weight for e ∈MH is the product of how often e has been observed with some property and the weight of that property for the class MH. Thus, instances of H with overall higher counts in D receive high weights for every MH. We therefore train a simple logistic regression model to predict the likelihood that e belongs to MH. We use a small set of features7, including the raw weight as computed in Eq. 7. For training, we sample 〈e, C〉 pairs from our IsA repository O as positive examples and random pairs that were not extracted by any Hearst pattern as negative examples. We frame the task as a binary prediction of whether e ∈ C, and use the model’s confidence as the value of φ̂M in place of the function in Eq. 7.\n\n6 Evaluation\n\n\n6.1 Experimental Setup\nEvaluation Sets. We evaluate our models on their ability to return correct instances for arbitrary class labels. As a source of evaluation data, we use Wikipedia category pages8. These are pages in which the title is the name of the category (e.g., “pakistani film actresses”) and the body is a manually\n6Also tried minimum, but mean gave better results. 7Feature templates in supplementary material. 8http://en.wikipedia.org/wiki/Help:Category\n6\n501\n502\n503\n504\n505\n506\n507\n508\n509\n510\n511\n512\n513\n514\n515\n516\n517\n518\n519\n520\n521\n522\n523\n524\n525\n526\n527\n528\n529\n530\n531\n532\n533\n534\n535\n536\n537\n538\n539\n540\n541\n542\n543\n544\n545\n546\n547\n548\n549\n550\n551\n552\n553\n554\n555\n556\n557\n558\n559\n560\n561\n562\n563\n564\n565\n566\n567\n568\n569\n570\n571\n572\n573\n574\n575\n576\n577\n578\n579\n580\n581\n582\n583\n584\n585\n586\n587\n588\n589\n590\n591\n592\n593\n594\n595\n596\n597\n598\n599\n2008 california wildfires · australian army chaplains · australian boy bands · canadian business journalists · canadian military nurses · canberra urban places · cellular automaton rules · chinese rice dishes · coldplay concert tours · daniel libeskind designs · economic stimulus programs · german film critics · invasive amphibian species · log flume rides · malayalam short stories · pakistani film actresses · puerto rican sculptors · string theory books · tampa bay devil rays scouts\nTable 3: Examples of labels from UniformSet.\ncurated list of links to other pages which fall under the category. We measure the precision and recall of each method for discovering the instances listed on these pages given the page title (henceforth “class label”).\nWe collect the titles of all Wikipedia category pages, removing those in which the last word is capitalized or which contain fewer than three words. These heuristics are intended to retain compositional titles in which the head is a single common noun. We also remove any titles which contain links to sub-categories. This is to favor fine-grained classes (“pakistani film actresses”) over coarse-grained ones (“film actresses”). We perform heuristic modifier chunking in order to group together multiword modifiers (e.g., “puerto rican”); for details, see supplementary material. From the resulting list of class labels, we draw two samples of 100 labels each, enforcing that no H appear as the head of more than three class labels per sample. The first sample is chosen uniformly at random (denoted UniformSet). The second (WeightedSet) is weighted so that the probability of drawing M1 . . .MkH is proportional to the total number of class labels in whichH appears as the head. Available at http://anonymized, these different evaluation sets are intended to evaluate performance on the head versus the tail of class label distribution, since information retrieval methods often perform differently on different parts of the distribution. On average, there are 17 instances per category in UniformSet and 19 in WeightedSet. Table 3 gives example class labels from UniformSet.\nBaselines. We implement two baselines using our IsA repository (O as defined in Section 4.1). Our simplest baseline ignores modifiers altogether, and simply assumes that any instance of H is an instance of MH, regardless of M . In this case the confidence value for 〈e,MH〉 is equivalent to that for 〈e,H〉. We\nrefer to this baseline simply as Baseline. Our second, stronger baseline uses the IsA repository directly to identify instances of the finegrained class C = M1 . . .MkH. That is, we consider e to be an instance of the class if 〈e, C〉 ∈ O, meaning the entire class label appeared in a source sentence matching some Hearst pattern. We refer to this baseline as Hearst. The weight used to rank the candidate instances is the confidence value assigned by the Hearst pattern extraction (Section 4.2).\nCompositional Models. As a baseline compositional model, we augment the Hearst baseline via set intersection. Specifically, for a class C = M1 . . .MkH, if each of the MiH appears in O independently, we take the instances of C to be the intersection of the instances of each of the MiH. We assign the weight of an instance e to be the sum of the weights associated with each independent modifier. We refer to this method as Hearst∩. We contrast this with our proposed model which recognizes instances of a fine-grained class by 1) assigning a meaning to each modifier in the form of a property profile and 2) checking whether a candidate instance exhibits these properties. We refer to the versions of our method as ModsH and ModsI , as described in Section 5. When relevant, we use “raw” to refer to the version in which instances are ranked using raw weights and “RR” to refer to the version in which instances are ranked using logistic regression (Section 5). We also try using the proposed methods to extend rather than replace the Hearst baseline. We combine predictions by merging the ranked lists produced by each system: i.e. the score of an instance is the inverse of the sum of its ranks in each of the input lists. If an instance does not appear at all in an input list, its rank in that list is set to a large constant value. We refer to these combination systems as Hearst+ModsH and Hearst+ModsI .\n\n6.2 Results\nPrecision and Coverage. We first compare the methods in terms of their coverage, the number of class labels for which the method is able to find some instance, and their precision, to what extent the method is able to correctly rank true instances of the class above\n7\n601\n602\n603\n604\n605\n606\n607\n608\n609\n610\n611\n612\n613\n614\n615\n616\n617\n618\n619\n620\n621\n622\n623\n624\n625\n626\n627\n628\n629\n630\n631\n632\n633\n634\n635\n636\n637\n638\n639\n640\n641\n642\n643\n644\n645\n646\n647\n648\n649\n650\n651\n652\n653\n654\n655\n656\n657\n658\n659\n660\n661\n662\n663\n664\n665\n666\n678\n679\n680\n681\n682\n683\n684\n685\n686\n687\n688\n689\n690\n691\n692\n693\n694\n695\n696\n697\n698\n699\nFlemish still life painters: Clara Peeters · Willem Kalf · Jan Davidsz de Heem · Pieter Claesz · Peter Paul Rubens · Frans Snyders · Jan Brueghel the Elder · Hans Memling · Pieter Bruegel the Elder · Caravaggio · Abraham Brueghel Pakistani cricket captains: Salman Butt · Shahid Afridi · Javed Miandad · Azhar Ali · Greg Chappell · Younis Khan · Wasim Akram · Imran Khan · Mohammad Hafeez · Rameez Raja · Abdul Hafeez Kardar · Waqar Younis · Sarfraz Ahmed Thai buddhist temples: Wat Buddhapadipa · Wat Chayamangkalaram · Wat Mongkolratanaram · Angkor Wat · Preah Vihear Temple ·Wat Phra Kaew ·Wat Rong Khun ·Wat Mahathat Yuwaratrangsarit · Vat Phou · Tiger Temple · Sanctuary of Truth · Wat Chalong · Swayambhunath · Mahabodhi Temple · Tiger Cave Temple · Harmandir Sahib\nTable 4: Instances extracted for several fine-grained classes from Wikipedia. Lists shown are from ModsI . Instances in italics were also returned by Hearst∩. Strikethrough denotes incorrect.\nnon-instances. We report total coverage, the number of labels for which the method returns any instance, and correct coverage, the number of labels for which the method returns a correct instance. For precision, we compute the average precision (AP) for each class label. AP ranges from 0 to 1, where 1 indicates that all positive instances were ranked above all negative instances. We report mean average precision (MAP), which is the mean of the APs across all the class labels. MAP is only computed over class labels for which the method returns something, meaning methods are not punished for returning empty lists.\nUniformSet WeightedSet Coverage MAP Coverage MAP\nBaseline 95 / 70 0.01 98 / 74 0.01 Hearst 9 / 9 0.63 8 / 8 0.80 Hearst∩ 13 / 12 0.62 9 / 9 0.80 ModsH raw 56 / 32 0.23 50 / 30 0.16 ModsH RR 56 / 32 0.29 50 / 30 0.25 ModsI raw 62 / 36 0.18 59 / 38 0.20 ModsI RR 62 / 36 0.24 59 / 38 0.23\nTable 5: Coverage and precision for populating Wikipedia category pages with instances. “Coverage” is the number of class labels (out of 100) for which at least one instance was returned, followed by the number for which at least one correct instance was returned. “MAP” is mean average precision. MAP does not punish methods for returning empty lists, thus favoring the baseline (see Figure 2).\nTable 4 gives examples of instances returned for several class labels and Table 5 shows the precision and coverage for each of the methods. Figure 2 illustrates how the single mean AP score (as reported in Table 5) can misrepresent the relative precision of different methods. In combination, Table 5 and Figure 2 demonstrate that the proposed methods extract instances about as well as the baseline, whenever the baseline can extract anything at all; i.e. the proposed method does not cause a precision drop on classes covered by the base-\nline. In addition, there are many classes for which the baseline is not able to extract any instances, but the proposed method is.\nTable 5 also reveals that the reranking model (RR) consistently increases MAP for the proposed methods. Therefore, going forward, we only report results using the reranking model (i.e. ModsH and ModsI will refer to ModsH RR and ModsI RR, respectively).\nManual Re-Annotation. It possible that true instances of a class are missing from our Wikipedia reference set, and thus that our precision scores underestimate the actual precision of the systems. We therefore manually verify the top 10 predictions of each of the systems for a random sample of 25 class labels. We choose class labels for which Hearst was able to return at least one instance, in order to ensure reliable precision estimates. For each of these labels, we manually check the top 10 instances proposed by each method to determine whether each belongs to the class. Table 6 shows the precision scores for each method computed against the\n8\n701\n702\n703\n704\n705\n706\n707\n708\n709\n710\n711\n712\n713\n714\n715\n716\n717\n718\n719\n720\n721\n722\n723\n724\n725\n726\n727\n728\n729\n730\n731\n732\n733\n734\n735\n736\n737\n738\n739\n740\n741\n742\n743\n744\n745\n746\n747\n748\n749\n750\n751\n752\n753\n754\n755\n756\n757\n758\n759\n760\n761\n762\n763\n764\n765\n766\n767\n768\n769\n770\n771\n772\n773\n774\n775\n776\n777\n778\n779\n780\n781\n782\n783\n784\n785\n786\n787\n788\n789\n790\n791\n792\n793\n794\n795\n796\n797\n798\n799\n(a) Uniform random sample (UniformSet). (b) Weighted random sample (WeightedSet).\nFigure 3: ROC curves for selected methods (Hearst in blue, proposed in red). Given a ranked list of instances, ROC curves plot true positives vs. false positives retained by setting various cutoffs. The curve becomes linear once all remaining instances have the same score (e.g., 0), as this makes it impossible to add true positives without also including all remaining false positives.\noriginal Wikipedia list of instances and against our manually-augmented list of gold instances. The overall ordering of the systems does not change, but the precision scores increase notably after re-annotation. We continue to evaluate against the Wikipedia lists, but acknowledge that reported precision is likely an underestimate of true precision.\nWikipedia Gold Hearst 0.56 0.79 Hearst∩ 0.53 0.78 ModsH 0.23 0.39 ModsI 0.24 0.42 Hearst+ModsH 0.43 0.63 Hearst+ModsI 0.43 0.63\nTable 6: P@10 before/after re-annotation; Wikipedia underestimates true precision.\nPrecision-Recall Analysis. We next look at the precision-recall tradeoff in terms of the area under the curve (AUC) achieved when each method attempts to rank the complete list of candidate instances. We take the union of all of the instances proposed by all of the methods (including the Baseline method which, given a class label M0 . . .MkH, proposes every instance of the head H as a candidate). Then, for each method, we rank this full set of candidates such that any instance returned by the method is given the score the method assigns, and every other instance is scored as 0. Table 7 reports the AUC and recall and Figure 3 plots the full ROC curves. The requirement by Hearst that class labels appear in full in a single sentence results in very low recall, which translates into very low AUC when considering the full set of candi-\ndate instances. By comparison, the proposed compositional methods make use of a larger set of sentences, and provide non-zero scores for many more candidates, resulting in a >10 point increase in AUC on both UniformSet and WeightedSet (Table 7).\nUniformSet WeightedSet AUC Recall AUC Recall\nBaseline 0.55 0.23 0.53 0.28 Hearst 0.56 0.03 0.52 0.02 Hearst∩ 0.57 0.04 0.53 0.02 ModsH 0.68 0.08 0.60 0.06 ModsI 0.71 0.09 0.65 0.09 Hearst∩+ModsH 0.70 0.09 0.61 0.08 Hearst∩+ModsI 0.73 0.10 0.66 0.10\nTable 7: Recall of instances on Wikipedia category pages, measured against the full set of instances from all pages in sample. AUC captures tradeoff between true and false positives.\n\n7 Conclusion\nWe have presented an approach to IsA extraction which takes advantage of the compositionality of natural language. Existing approaches often treat class labels as atomic units which must be observed in full in order to be populated with instances. As a result, current methods are not able to handle the infinite number of classes describable in natural language, most of which never appear in text. Our method reasons about each modifier in the label individually, in terms of the properties that it implies about the instances. This approach allows us to harness information that is spread across multiple sentences, significantly increasing in the number of finegrained classes which we are able to populate.\n9\n801\n802\n803\n804\n805\n806\n807\n808\n809\n810\n811\n812\n813\n814\n815\n816\n817\n818\n819\n820\n821\n822\n823\n824\n825\n826\n827\n828\n829\n830\n831\n832\n833\n834\n835\n836\n837\n838\n839\n840\n841\n842\n843\n844\n845\n846\n847\n848\n849\n850\n851\n852\n853\n854\n855\n856\n857\n858\n859\n860\n861\n862\n863\n864\n865\n866\n867\n868\n869\n870\n871\n872\n873\n874\n875\n876\n877\n878\n879\n880\n881\n882\n883\n884\n885\n886\n887\n888\n889\n890\n891\n892\n893\n894\n895\n896\n897\n898\n899\n",
    "rationale": "- Strengths:\n\nThis paper presents an approach for fine-grained IsA extraction by learning\nmodifier interpretations. The motivation of the paper is easy to understand and\nthis is an interesting task. In addition, the approach seems solid in general\nand the experimental results show that the approach increases in the number of\nfine-grained classes that can be populated.\n\n- Weaknesses:\n\nSome parts of the paper are hard to follow. It is unclear to me why D((e, p,\no)) is multiplied by w in Eq (7) and why the weight for e in Eq. (8) is\nexplained as the product of how often e has been observed with some property\nand the weight of that property for the class MH. In addition, it also seems\nunclear how effective introducing compositional models itself is in increasing\nthe coverage. I think one of the major factors of the increase of the coverage\nis the modifier expansion, which seems to also be applicable to the baseline\n'Hearst'. It would be interesting to see the scores 'Hearst' with modifier\nexpansion.\n\n- General Discussion:\n\nOverall, the task is interesting and the approach is generally solid. However,\nsince this paper has weaknesses described above, I'm ambivalent about this\npaper.\n\n- Minor comment:\n\nI'm confused with some notations. For example, it is unclear for me what 'H'\nstands for. It seems that 'H' sometimes represents a class such as in (e, H)\n(- O, but sometimes represents a noun phrase such as in (H, p, N, w) (- D. Is\nmy\nunderstanding correct?\n\nIn Paragraph \"Precision-Recall Analysis\", why the authors use area under the\nROC curve instead of area under the Precision-Recall curve, despite the\nparagraph title \"Precision-Recall Analysis\"?\n\n- After reading the response:\n\nThank you for the response. I'm not fully satisfied with the response as to the\nmodifier expansion. I do not think the modifier expansion can be applied to\nHearst as to the proposed method. However, I'm wondering whether there is no\nway to take into account the similar modifiers to improve the coverage of\nHearst. I'm actually between 3 and 4, but since it seems still unclear how\neffective introducing compositional models itself is, I keep my recommendation\nas it is.",
    "rating": 5
  },
  {
    "title": "Investigating Different Context Types and Representations for Learning Word Embeddings",
    "abstract": "The number of word embedding models is growing every year. Most of them learn word embeddings based on the cooccurrence information of words and their contexts. However, it’s still an open question what is the best definition of context. We provide the first systematical investigation of different context types and context representations for learning word embeddings. Comprehensive experiments are conducted to evaluate their effectiveness under 6 tasks, which give us some insights about context selection. We hope that this paper, along with the published code, can serve as a guideline of choosing context for our community.",
    "text": "1 Introduction\nRecently, there is a growing research interest on word embedding models, where words are embedded into low-dimensional real vectors. Words that share similar meanings tend to have short distances in the vector space. The trained word embeddings are not only useful by themselves (e.g. used for calculating word similarities) but also effective when used as the input of the downstream models, such as part-of-speech tagging, chunking, named entity recognition (Collobert and Weston, 2008; Collobert et al., 2011) and text classification (Socher et al., 2013; Kim, 2014).\nFor almost all word embedding models, the training objectives are based on the Distributed Hypothesis (Harris, 1954), which can be stated as: “words that occur in the same contexts tend to have similar meanings”. The “context” is usually defined as the words which precede and follow the target word within some fixed distance in most word embedding models with various architec-\ntures (Bengio et al., 2003; Mnih and Hinton, 2007; Mikolov et al., 2013b; Pennington et al., 2014). Among them, Global Vectors (GloVe) proposed by Pennington et al. (2014), Continuous Skip-Gram (CSG) 1 and Continuous Bag-Of-Words (CBOW) proposed by Mikolov et al. (2013a) achieve stateof-the-art results on a wide range of linguistic tasks, and scales well to corpus with billion words.\nRecently, Levy and Goldberg (2014b); Ling et al. (2015) 2 improve CSG and CBOW by introducing position-aware context representation, where each contextual word is associated with their relative position to the target word. Levy and Goldberg (2014a) propose DEPS, which takes the words that are connected to target word in dependency parse tree as context.\nDespite all these efforts, there is still no clear answer to the following questions due to the lack of systematical comparison: 1) Is dependencybased context more reasonable than traditional linear one? 2) Do the relative position or the dependency relation between contextual word and target word contributes to the learning process? 3) Do different word embedding models have preferences for different contexts? 4) How different contexts affect models’ performances on different tasks?\nTo answer these questions, we first classify word embedding models based on different context types (linear or dependency-based) and different context representations (word or bound word) in Table 1. We implement the models that previously not proposed and give systematical comparisons on a wide range of word similarity, word analogy, part-of-speech tagging, chunking, named entity recognition, and text classification dataset-\n1Many researches refer Continuous Skip-Gram as SG. However, in order to distinguish linear (continuous) context and dependency-based context, we refer it as CSG.\n2In these two papers, the description of position-aware context are quite different. However, their ideas is actually identical.\n2\n100 101 102 103 104 105 106 107 108 109 110 111 112 113 114 115 116 117 118 119 120 121 122 123 124 125 126 127 128 129 130 131 132 133 134 135 136 137 138 139 140 141 142 143 144 145 146 147 148 149\n150\n151\n152\n153\n154\n155\n156\n157\n158\n159\n160\n161\n162\n163\n164\n165\n166\n167\n168\n169\n170\n171\n172\n173\n174\n175\n176\n177\n178\n179\n180\n181\n182\n183\n184\n185\n186\n187\n188\n189\n190\n191\n192\n193\n194\n195\n196\n197\n198\n199\nBasic Model Context Representation\nContext Type Linear Dependency-based\ngeneralized (unbound) word CSG (Mikolov et al., 2013a) this work Skip-Gram bound word Structured SG (Ling et al., 2015)POSIT (Levy and Goldberg, 2014b) Deps\n(Levy and Goldberg, 2014a) generalized (unbound) word CBOW (Mikolov et al., 2013a) this work\nBag-Of-Words bound word CWINDOW (Ling et al., 2015) this work generalized (unbound) word GloVe (Pennington et al., 2014) this work\nGloVe bound word this work this work\nTable 1: Research summarization of Generalized Skip-Gram, Bag-Of-Words and GloVe with different context types and context representations. For linear context, bound word indicates word associated with positional information. For dependency-based context, bound word indicates word associated with dependency relation.\ns. Experimental results suggest that although it’s hard to find any universal insight (i.e. one context works consistently better than the other), the characteristics of different contexts on different models are concluded according to specific tasks. We expect this paper to be a useful complement in the word embedding literature.\n\n2 Methodology\nIn this section, we first introduce different contexts in detail and discuss their strength and weakness. We then show how CSG, CBOW and GloVe can be generalized to use these contexts.\n\n2.1 Context Types\nIt is necessary to discover more effective ways of defining “context”. In the current literature, there are mainly two types of contexts: linear (most word embedding models) and dependency-based (DEPS (Levy and Goldberg, 2014a)). Linear context is defined as the positional neighbours of the target word in texts. Dependency-based context is defined as the syntactic neighbours of the target word based on dependency parse tree, as shown in Figure 1 .\nCompared to linear context, dependency-based context is more focused and can capture more long-range contexts. For example in Figure 1, linear context does not consider the word-context pair “discovers telescope”, while dependency-based context contains this information. Dependency-based context can also exclude some uninformative word-context pairs like “with star” and “telescope with”.\nin the text. The context vocabulary C is thus identical to the word vocabulary W . However, this restriction is not r quired by the model; contexts need not corr spond to words, and the number of context-types can be substantially larger than the number of word-types. W generalize SKIPGRAM by replacing the bag-of-words contexts with arbitrary contexts.\nIn this paper we experiment with dependencybased syntactic contexts. Syntactic contexts capture different information than bag-of-word contexts, as we demon trate u ing the sentence “Au - tralian scientist discovers star with telesc p ”.\nLinear Bag-of-Words Contexts This is the context used by word2vec and many other neural embeddings. Using a window of size k around the targ t word w, 2k contexts are produced: the k words before and the k words after w. For k = 2, the contexts f the target word w are −2, w−1, w+1, w+2. In our example, the contexts of discovers are Australian, scientist, star, with.2\nNote that a context window of size 2 may miss some important contexts (telescope is n t a cont xt of discovers), wh le including s me accidental ones (Australian is a ontext discover ). Moreover, the contexts are unmarked, resulting in discovers being a context of both stars and scientist, which may result in stars and scientists ending up as neighbours in the embedded space. A window size of 5 is commonly used to capture broad topical content, whereas smaller windows contain more focused information about the target word.\nDependency-Based Contexts An alternative to the bag-of-words approach is to derive contexts based on the syntactic relations the word participates in. This is facilitated by recent advances in parsing technology (Goldberg and Nivre, 2012; Goldberg and Nivre, 2013) that allow parsing to syntactic dependencies with very high speed and near state-of-the-art accuracy.\nAfter parsing each sentence, we derive word contexts as follows: for a target word w with modifiers m1, . . . ,mk and a head h, we consider the contexts (m1, lbl1), . . . , (mk, lblk), (h, lbl−1h ),\n2word2vec’s implementation is slightly more complicated. The software defaults to prune rare words based on their frequency, and has an option for sub-sampling the frequent words. These pruning and sub-sampling happen before the context extraction, leading to a dynamic window size. In addition, the window size is not fixed to k but is sampled uniformly in the range [1, k] for each word.\nAustralian scientist discovers star with telescope\namod nsubj dobj\nprep\npobj\nAustralian scientist discovers star telescope\namod nsubj dobj\nprep with\nWORD CONTEXTS\naustralian scientist/amod−1\nscientist australian/amod, discovers/nsubj−1\ndiscovers scientist/nsubj, star/dobj, telescope/prep with star discovers/dobj−1 telescope discovers/prep with−1\nFigure 1: Dependency-based context extraction example. Top: preposition relations are collapsed into single arcs, making telescope a direct modifier of discovers. Bottom: the contexts extracted for each word in the sentence.\nwhere lbl is the type of the dependency relation between the head and the modifier (e.g. nsubj, dobj, prep with, amod) and lbl−1 is used to mark the inverse-relation. Relations that include a preposition are “collapsed” prior to context extraction, by directly connecting the head and the object of the preposition, and subsuming the preposition itself into the dependency label. An example of the dependency context extraction is given in Figure 1.\nNotice that syntactic dependencies are both more inclusive and more focused than bag-ofwords. They capture relations to words that are far apart and thus “out-of-reach” with small window bag-of-words (e.g. the instrument of discover is telescope/prep with), and also filter out “coincidental” contexts which are within the window but not directly related to the target word (e.g. Australian is not used as the context for discovers). In addition, the contexts are typed, indicating, for example, that stars are objects of discovery and scientists are subjects. We thus expect the syntactic contexts to yield more focused embeddings, capturing more functional and less topical similarity.\n4 Experiments and Evaluation\nWe experiment with 3 training conditions: BOW5 (bag-of-words contexts with k = 5), BOW2 (same, with k = 2) and DEPS (dependency-based syntactic contexts). We modified word2vec to support arbitrary contexts, and to output the context embeddings in addition to the word embeddings. For bag-of-words contexts we used the original word2vec implementation, and for syntactic contexts, we used our modified version. The negative-sampling parameter (how many negative contexts to sample for every correct one) was 15.\nFigure 1: Illustration of dependency parse tree for sentence “Australian scientist discovers star with telescope”. Note that preposition relation is collapsed in the bottom sub-figure, where telescope is con idered as a direct modifier of discovers. We use the collapsed version of dependency in all of our experiments the same as Levy and Goldberg (2014b)\n\n2.2 Context Representations\nIn CSG and CBOW, contexts are represented by words without any additional information. Levy and Goldberg (2014b); Ling et al. (2015) improve them by introducing position-bound words, where each contextual word is associated with their relative position to the target word. This allows CSG and CBOW to distinguish different sequential positions and capture context’s structural information. We name the method that bind additional information with the contextual word as bound (context) representation, as opposited to unbound (context) representation where word is used alone.\nFor dependency-based context, the original DEPS uses bound representation by default: words are associated with their dependency relation to the target word. Similar to bound representation in linear context type, this allows word embedding models to capture more dependency information. An example is shown in Table 2. In this paper, we\n3\n201\n202\n203\n204\n205\n206\n207\n208\n209\n210\n211\n212\n213\n214\n215\n216\n217\n218\n219\n220\n221\n222\n223\n224\n225\n226\n227\n228\n229\n230\n231\n232\n233\n234\n235\n236\n237\n238\n239\n240\n241\n242\n243\n244\n245\n246\n247\n248\n249\n250\n251\n252\n253\n254\n255\n256\n257\n258\n259\n260\n261\n262\n263\n264\n265\n266\n267\n268\n269\n270\n271\n272\n273\n274\n275\n276\n277\n278\n279\n280\n281\n282\n283\n284\n285\n286\n287\n288\n289\n290\n291\n292\n293\n294\n295\n296\n297\n298\n299\nContext Representation\nContext Type Linear Dependency-based\nunbound australian, scientist, star, with\nscientist, star, telescope\nbound australian/-2, scientist/-1, star/+1, with/+2 scientist/nsubj, star/dobj, telescope/prep with\nTable 2: Illustration of bound and unbound representations under linear and dependency-based context types. This example is based on Figure 1 and the target word is “discovers”.\nalso investigate the simpler context representation where no dependency relation is considered. This also makes a fair comparison with linear context models like CSG, CBOW and GloVe, since they do not use bound representation either.\nIntuitively, bound representation should work better than unbound representation, since it is more sophisticated by considering position or dependency relation. However, this is not always the case in practice. The biggest drawback of word embeddings learned with bound context type is the ignorance of syntax. The bound representation already contains a certain degree of syntactic information, thus word embedding models can not learn it from the input word-context pairs. Another drawback is that bound context representation is sparse, especially for dependency-based context. There are 47 dependency relations in dependency parse tree. Although not every combination of dependency relations and words appear in the wordcontext pair collection, it still enlarges the context vocabulary about 5 times in practice.\nCompared to context types (linear and dependency-based), the choice of context representations (bound and unbound) have more effects to the quality of the learned word embeddings. Bound representation transfers each contextual word into a new one, and the word-context pairs are changed completely. As for context types, a lot of word-context pairs in linear context type also appear in dependency-based context type. For example, in Table 2, “scientist” and “star” are considered as the contexts of “discovers” in both context types.\n\n2.3 Generalization\nFor convenience, we first define the collection of word-context pairs as P . P can be merged based on the words to form a collection M with size of\nLinear (window size 1) Dependency-based\nP (australian, scientist) (scientist, australian) (scientist, discovers) (discovers, scientist) (discovers, star) . . .\n(australian, scientist) (scientist, australian) (scientist, discovers) (discovers, scientist) (discovers, star) (discovers, telescope) . . .\nM (australian, scientist) (scientist, australian, discovers) (discovers, scientist, star) . . .\n(australian, scientist) (scientist, australian, discovers) (discovers, scientist, star, telescope) . . .\nM (australian, scientist, 1) (scientist, australian, 1) (scientist, discovers, 1) (discovers, scientist, 1) (discovers, star, 1) . . . (australian, scientist, 1) (scientist, australian, 1) (scientist, discovers, 1) (discovers, scientist, 1) (discovers, star, 1) (discovers, telescope, 1) . . .\nTable 3: Illustration of collection P , M andM for sentence “australian scientist discovers star with telescope”. Unbound representation is used in this example. Words in the collections are Bold. Contexts and numbers in the collections are Normal.\n|C|. Each element (w, c1, c2, .., cnw) ∈ M is the word w and its contexts, where nw is the number of word w’s contexts. P can also be merged based on both words and contexts to form a collection M . Each element (w, c,#(w, c)) ∈ M is the word w, context c, and the times they appear in collection P . An example of these collections is shown in Table 3.\n\n2.3.1 Generalized Bag-Of-Words\nThe objective function of Generalized Bag-OfWords (GBOW) is defined as:\n∑ (w,c1,..,cnw )∈M log p\n( w ∣∣∣∣∣ nw∑ i=1 ~ci ) (1)\nWith negative sampling technique, the log probability is calculated by:\nlog σ ( ~w ·\nnw∑ i=1 ~ci\n) −\nK∑ k=1 log σ\n( ~wN ·\nnw∑ 1=i ~ci ) (2)\nwhere σ is the sigmoid function, K is the negative sampling size, ~w and ~c is the vector for word w and c respectively. The negatively sampled word wN is randomly selected based on its unigram distribution ( #(w)∑\nw #(w) )ds, where #(w) is the number\nof times that word w appears in the corpus, ds is the distribution smoothing hyper-parameter which is usually defined as 0.75.\n4\n301\n302\n303\n304\n305\n306\n307\n308\n309\n310\n311\n312\n313\n314\n315\n316\n317\n318\n319\n320\n321\n322\n323\n324\n325\n326\n327\n328\n329\n330\n331\n332\n333\n334\n335\n336\n337\n338\n339\n340\n341\n342\n343\n344\n345\n346\n347\n348\n349\n350\n351\n352\n353\n354\n355\n356\n357\n358\n359\n360\n361\n362\n363\n364\n365\n366\n367\n368\n369\n370\n371\n372\n373\n374\n375\n376\n377\n378\n379\n380\n381\n382\n383\n384\n385\n386\n387\n388\n389\n390\n391\n392\n393\n394\n395\n396\n397\n398\n399\nNote that in the original CBOW (Mikolov et al., 2013a) with negative sampling technique, the probability is actually p (c| ∑ ~wi) instead of\np (w| ∑ ~ci). In another word, the original CBOW uses the sum of word vectors to predict context. This works well for linear context. But for dependency-based context with bound representation, there is only one word available for predicting its context. For example in Figure 1, the context “scientist/nsubj” can only be predicted by word “discovers”. However, a word can be predicted by the sum of several contexts. Due to this reason, we exchange the role of word and context in GBOW. The negative sampling objective is also changed from context cN to word wN .\n\n2.3.2 Generalized Skip-Gram\nFor generalized Skip-Gram (GSG), the definition is straightforward and the objective function actually needs no modification (Levy and Goldberg, 2014b). However, in order to make it consistent with our GBOW, we also exchange the role of word and context. The objective function of GSG is defined as:∑\n(w,c)∈P log p (w|~c)\n= ∑\n(w,c)∈P\n[ log σ (~w · ~c)−\nK∑ k=1 log σ ( ~wN · ~c) ] (3)\n\n2.3.3 GloVe\nUnlike GSG and GBOW, GloVe explicitly optimizes a log-bilinear regression model based on word co-occurrence matrix. Since GloVe is already a very generalized model, with the previous defined collection M , the final objective function is written as:∑ (w,c)∈M f(#(w, c))(~w ·~c+ ~bw+ ~bc− log#(w, c)) (4) where ~bw and ~bc are biases for word and context. f is a non-decreasing weighting function and ensures that large #(w, c) is not over-weighted.\nNote that the inputs of GSG, GBOW and Glove are the collections P , M and M respectively. Once the corpus and hyper-parameters are fixed, these collections (and thus the learned word embeddings) are determined only by the choice of context types and representations.\n\n3 Experiments\nWe evaluate the effectiveness of different context types and representations on word similarity, word analogy, part-of-speech tagging, chunking, named entity recognition, and text classification tasks. In this section, we first describe the training details of word embedding models. We then report and discuss the experimental results on each task. Detailed numerical results can be found in Supplemental Material.\n\n3.1 Training Details\nPreviously, the word2vecf toolkit 3 (Levy et al., 2015) extends the word2vec toolkit 4 (Mikolov et al., 2013b) to accept the input of collection P rather than raw corpus. This makes CSG model accept arbitrary contexts (e.g. dependencybased context). However, CBOW and GloVe are not considered in that toolkit. We implement word2vecPM toolkit, a further extension of word2vecf, which supports generalized SG, CBOW and GloVe with the input of collection P , M and M respectively.\nWe use English Wikipedia (August 2013 dump) as the training corpus in all of our experiments. The Stanford CoreNLP (Manning et al., 2014) is used for dependency parsing. All words and contexts are converted to lower case after parsing. Words and contexts that appear less than 100 times in the collection P are directly ignored. Note that this is slightly different from ignoring rare words that appear less than 100 times in the vocabulary based on the corpus, since each word may appear more times in the collection than that in the vocabulary.\nMost hyper-parameters are the same as Levy et al. (2015)’s best configuration. For example, negative sampling size K is set to 5 for GSG and 2 for GBOW. Distribution smoothing cds is set to 0.75. No dynamic context or “dirty” sub-sampling is used. The window size wn is fixed to 2 for constructing linear context, which ensures the number of the (merged) word-context pair collection for both linear context and dependency-based context is comparable. The number of iteration is set to 2, 5 and 30 for GSG, GBOW and GloVe respectively. Unless otherwise noted, the number of word embedding dimension is set to 500. Since the aim\n3https://bitbucket.org/yoavgo/ word2vecf\n4http://code.google.com/p/word2vec/\n5\n401\n402\n403\n404\n405\n406\n407\n408\n409\n410\n411\n412\n413\n414\n415\n416\n417\n418\n419\n420\n421\n422\n423\n424\n425\n426\n427\n428\n429\n430\n431\n432\n433\n434\n435\n436\n437\n438\n439\n440\n441\n442\n443\n444\n445\n446\n447\n448\n449\n450\n451\n452\n453\n454\n455\n456\n457\n458\n459\n460\n461\n462\n463\n464\n465\n466\n467\n468\n469\n470\n471\n472\n473\n474\n475\n476\n477\n478\n479\n480\n481\n482\n483\n484\n485\n486\n487\n488\n489\n490\n491\n492\n493\n494\n495\n496\n497\n498\n499\n0 100 200 300 400 500 dimension\n0.65\n0.70\n0.75 0.80 co rr e la ti o n\nGSG on WordSim353 similarity\nlinear ctx w/ unbound rep linear ctx w/ bound rep dep ctx w/ unbound rep dep ctx w/ bound rep\n0 100 200 300 400 500 dimension\n0.45\n0.50\n0.55\n0.60\n0.65\n0.70\n0.75\n0.80\nco rr\ne la\nti o n\nGBOW on WordSim353 similarity\nlinear ctx w/ unbound rep linear ctx w/ bound rep dep ctx w/ unbound rep dep ctx w/ bound rep\n0 100 200 300 400 500 dimension\n0.55\n0.60\n0.65\n0.70\n0.75\nco rr\ne la\nti o n\nGloVe on WordSim353 similarity\nlinear ctx w/ unbound rep linear ctx w/ bound rep dep ctx w/ unbound rep dep ctx w/ bound rep\n0 100 200 300 400 500 dimension\n0.30\n0.35\n0.40\n0.45\n0.50\n0.55\n0.60\nco rr\ne la\nti o n\nGSG on WordSim353 relatedness\nlinear ctx w/ unbound rep linear ctx w/ bound rep dep ctx w/ unbound rep dep ctx w/ bound rep\n0 100 200 300 400 500 dimension\n0.20\n0.25\n0.30\n0.35\n0.40\n0.45\n0.50\n0.55\nco rr\ne la\nti o n\nGBOW on WordSim353 relatedness\nlinear ctx w/ unbound rep linear ctx w/ bound rep dep ctx w/ unbound rep dep ctx w/ bound rep\n0 100 200 300 400 500 dimension\n0.30\n0.35\n0.40\n0.45\n0.50\n0.55\nco rr\ne la\nti o n\nGloVe on WordSim353 relatedness\nlinear ctx w/ unbound rep linear ctx w/ bound rep dep ctx w/ unbound rep dep ctx w/ bound rep\nFigure 2: Results on WordSim353 (similarity and relatedness) dataset.\nof this paper is not comparing the performance of different word embedding models, the results of GSG, GBOW and GloVe are reported respectively.\n\n3.2 Word Similarity Task\nWord similarity task aims at producing semantic similarity scores of word pairs, which are compared with the human scores using Spearman’s correlation. The cosine distance is used for generating similarity scores between two word vectors. WordSim353 (Finkelstein et al., 2001) dataset with similarity and relatedness partition (Zesch et al., 2008; Agirre et al., 2009) is used for this task.\nPrevious researches (Levy and Goldberg, 2014a; Melamud et al., 2016) conclude that compared to linear context, dependency-based context can capture more functional similarity (e.g. tiger/cat) rather than topical similarity (relatedness) (e.g., tiger/jungle). However, their experiments do not distinguish the effect of differen-\nt context representations: unbound representation is used for linear context (Mikolov et al., 2013b) while bound representation is used for dependency-based context (Levy and Goldberg, 2014a). Moreover, only CSG model is compared.\nWe revisit those claims based on more systematical experiments. As shown in Figure 2’s top-left sub-figure, compared to linear context (solid and dotted blue line), the better results of dependencybased context for GSG and GloVe (solid and dotted red line) on ws353’s similarity partition confirms its ability of capturing functional similarity. However, the good performances of dependencybased context do not fully transfer to GBOW. Although dependency-based context with bound representation (dotted red line) for GBOW is still the best performer, dependency-based context with unbound representation (solid red line) for GBOW performs worst on ws353’s similarity partition. Distinguishing bound representation from unbound representation is important.\nNote that the results are also reversed on ws353’s relatedness partition (Figure 2’s right subfigures), which shows the use of linear context is more suitable for capturing topical similarity.\nOverall, dependency-based context type does not get all the credit for capturing functional similarity. Context representations play an important role for word similarity task. It’s only safe to say that dependency-based context captures functional similarity with the “help” of bound representation. In contrast, linear context type captures topical similarity with the “help” of unbound representation.\n\n3.3 Word Analogy Task\nWord analogy task aims at answering the questions like “a is to b as c is to ?”. For example, “London is to Britain as Tokyo is to Japan”. We follow the evaluation protocol in Levy and Goldberg (2014b), answering the questions using both 3CosAdd (additive) and 3CosMul (multiplicative) functions. Our experiments show that 3CosMul works consistently better than 3CosAdd, thus only the results of 3CosMul are reported. We follow previous researches and use Google’s analogy dataset (Mikolov et al., 2013a) (with semantic and syntactic partition) in our experiments.\nAs shown in Figure 3, we observe that context representation plays an important role in word analogy task. The choice of context representa-\n6\n501\n502\n503\n504\n505\n506\n507\n508\n509\n510\n511\n512\n513\n514\n515\n516\n517\n518\n519\n520\n521\n522\n523\n524\n525\n526\n527\n528\n529\n530\n531\n532\n533\n534\n535\n536\n537\n538\n539\n540\n541\n542\n543\n544\n545\n546\n547\n548\n549\n550\n551\n552\n553\n554\n555\n556\n557\n558\n559\n560\n561\n562\n563\n564\n565\n566\n567\n568\n569\n570\n571\n572\n573\n574\n575\n576\n577\n578\n579\n580\n581\n582\n583\n584\n585\n586\n587\n588\n589\n590\n591\n592\n593\n594\n595\n596\n597\n598\n599\n0 100 200 300 400 500 dimension\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7 0.8 a cc u ra cy ( % )\nGSG on Google Sem (3CosMul)\nlinear ctx w/ unbound rep linear ctx w/ bound rep dep ctx w/ unbound rep dep ctx w/ bound rep\n0 100 200 300 400 500 dimension\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\na cc\nu ra\ncy (\n% )\nGBOW on Google Sem (3CosMul)\nlinear ctx w/ unbound rep linear ctx w/ bound rep dep ctx w/ unbound rep dep ctx w/ bound rep\n0 100 200 300 400 500 dimension\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\na cc\nu ra\ncy (\n% )\nGloVe on Google Sem (3CosMul)\nlinear ctx w/ unbound rep linear ctx w/ bound rep dep ctx w/ unbound rep dep ctx w/ bound rep\n0 100 200 300 400 500 dimension\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\na cc\nu ra\ncy (\n% )\nGSG on Google Syn (3CosMul)\nlinear ctx w/ unbound rep linear ctx w/ bound rep dep ctx w/ unbound rep dep ctx w/ bound rep\n0 100 200 300 400 500 dimension\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\na cc\nu ra\ncy (\n% )\nGBOW on Google Syn (3CosMul)\nlinear ctx w/ unbound rep linear ctx w/ bound rep dep ctx w/ unbound rep dep ctx w/ bound rep\n0 100 200 300 400 500 dimension\n0.0\n0.1\n0.2\n0.3\n0.4\n0.5\n0.6\n0.7\n0.8\na cc\nu ra\ncy (\n% )\nGloVe on Google Syn (3CosMul)\nFigure 3: Results on Google (Sem and Syn) dataset.\ntion (word or bound word) actually has much larger impact than the choice of context type (linear or dependency). The results on Google Syn dataset (Figure 3’s sub-figures in the second column) is perhaps the most evident. The performance of linear context and dependency-based context with unbound representation is similar. However, when bound representation is used, the performance of GSG and GBOW drops more than 30 percent for dependency-based context and around 20 percent for linear context. The main reason for this phenomenon is that the bound representation already contains syntactic information, thus word embedding models can not learn it from the input wordcontext pairs. It can also be observed that GloVe is more sensitive to different context representations than Skip-Gram and CBOW, which is probably due to its explicitly defined/optimized objective function.\n\n3.4 POS, Chunking and NER Tasks\nAlthough intrinsic evaluations like word similarity and word analogy tasks could provide direct\ninsights of different context types and representations, the experimental results above cannot be directly translated to the typical uses of word embeddings. For example, these tasks aren’t necessarily correlated with downstream tasks’ performances, as shown in (Schnabel et al., 2015; Linzen, 2016; Chiu et al., 2016). More extrinsic tasks should be considered.\nIn this subsection, we evaluate the effectiveness of different word embedding models with different contexts on Part-of-Speech Tagging (POS), Chunking and Named Entity Recognition (NER) tasks. These tasks can be categorized as sequence labeling. It aims at automatically assigning words in texts with labels. CoNLL 2000 shared task 5 is used as benchmark for POS and Chunking. CoNLL 2003 shared task 6 is used as benchmark for NER.\nInspired by the evaluation protocol used in Kiros et al. (2015), we restrict the predicting model to simple linear classifier. The classifier’s input for predicting the label of word wi is simply the concatenation of vectors ~wi−2, ~wi−1, ~wi, ~wi+1, ~wi+2. This ensures the quality of embedding models is directly evaluated, and their strengths and weaknesses are easily observed.\nAs shown in Figure 4, the overall trends of GSG, GBOW and GloVe are similar. When the same context type is used, bound representation (dotted line) outperforms unbound representation (solid line) on all datasets. Sequence labeling tasks tend to classify words with the same syntax to the same category. The ignorance of syntax for word embeddings which are learned by bound representation becomes beneficial. Moreover, dependencybased context type works better than linear context type in most cases. These results suggest that linear context type with unbound representations (as in traditional CSG and CBOW) may not be the best choice of input word vectors for sequence labeling. Bound representations should always be used and dependency-based context type is also worth considering. Again, similar to that on word analogy task, GloVe is more sensitive to different context representations than Skip-Gram and CBOW on sequence labeling tasks.\n5http://www.cnts.ua.ac.be/conll2000/ chunking\n6http://www.cnts.ua.ac.be/conll2003/ ner\n7\n601\n602\n603\n604\n605\n606\n607\n608\n609\n610\n611\n612\n613\n614\n615\n616\n617\n618\n619\n620\n621\n622\n623\n624\n625\n626\n627\n628\n629\n630\n631\n632\n633\n634\n635\n636\n637\n638\n639\n640\n641\n642\n643\n644\n645\n646\n647\n648\n649\n650\n651\n652\n653\n654\n655\n656\n657\n658\n659\n660\n661\n662\n663\n664\n665\n666\n667\n668\n669\n670\n671\n672\n673\n674\n675\n676\n677\n678\n679\n680\n681\n682\n683\n684\n685\n686\n687\n688\n689\n690\n691\n692\n693\n694\n695\n696\n697\n698\n699\n0 100 200 300 400 500 dimension\n84\n86\n88\n90\n92\n94\n96\n98\na cc\nu ra\ncy (\n% )\nGSG on Part-of-Speech Tagging\nlinear ctx w/ unbound rep linear ctx w/ bound rep dep ctx w/ unbound rep dep ctx w/ bound rep\n0 100 200 300 400 500 dimension\n86\n88\n90\n92\n94\n96\n98\na cc\nu ra\ncy (\n% )\nGBOW on Part-of-Speech Tagging\nlinear ctx w/ unbound rep linear ctx w/ bound rep dep ctx w/ unbound rep dep ctx w/ bound rep\n0 100 200 300 400 500 dimension\n80\n82\n84\n86\n88\n90\n92\n94\n96\na cc\nu ra\ncy (\n% )\nGloVe on Part-of-Speech Tagging\nlinear ctx w/ unbound rep linear ctx w/ bound rep dep ctx w/ unbound rep dep ctx w/ bound rep\n0 100 200 300 400 500 dimension\n65\n70\n75\n80\n85\n90\nF1 -s\nco re\n( %\n)\nGSG on Chunking\nlinear ctx w/ unbound rep linear ctx w/ bound rep dep ctx w/ unbound rep dep ctx w/ bound rep\n0 100 200 300 400 500 dimension\n65\n70\n75\n80\n85\n90\nF1 -s\nco re\n( %\n)\nGBOW on Chunking\nlinear ctx w/ unbound rep linear ctx w/ bound rep dep ctx w/ unbound rep dep ctx w/ bound rep\n0 100 200 300 400 500 dimension\n60\n65\n70\n75\n80\n85\n90\nF1 -s\nco re\n( %\n)\nGloVe on Chunking\nlinear ctx w/ unbound rep linear ctx w/ bound rep dep ctx w/ unbound rep dep ctx w/ bound rep\n0 100 200 300 400 500 dimension\n66\n68\n70\n72\n74\n76\n78\nF1 -s\nco re\n( %\n)\nGSG on Named Entity Recognition\nlinear ctx w/ unbound rep linear ctx w/ bound rep dep ctx w/ unbound rep dep ctx w/ bound rep\n0 100 200 300 400 500 dimension\n66\n68\n70\n72\n74\n76\nF1 -s\nco re\n( %\n)\nGBOW on Named Entity Recognition\nlinear ctx w/ unbound rep linear ctx w/ bound rep dep ctx w/ unbound rep dep ctx w/ bound rep\n0 100 200 300 400 500 dimension\n60\n62\n64\n66\n68\n70\n72\n74\n76\nF1 -s\nco re\n( %\n)\nGloVe on Named Entity Recognition\nlinear ctx w/ unbound rep linear ctx w/ bound rep dep ctx w/ unbound rep dep ctx w/ bound rep\nFigure 4: Results on POS, Chunking and NER tasks.\n\n3.5 Text Classification Task\nFinally, we evaluate the effectiveness of different word embedding models with different contexts on text classification task. Text classification is one of the most popular and well-studied tasks in natural language processing. Recently, deep neural networks are dominant on this task (Socher et al., 2013; Kim, 2014; Dai and Le, 2015). They often need pre-trained word embeddings as inputs to improve their performances. Similar to the previous evaluation of sequence labeling tasks, instead of building complex deep neural networks, we use a simpler classification method called Neural Bag-of-Words to directly evaluate the word embeddings: texts are first represented by the sum of their belonging words’ vectors, then a Logistic Regression Classifier is built upon them for classification.\nDifferent word embedding models are evaluated on 5 text classification datasets. The first 3 datasets are sentence-level: short movie review sentiment (MR) (Pang and Lee, 2005), customer product re-\nModel Context Context Sentence-level Document-levelType Rep. MR CR Subj RT-2k IMDB\nGSG linear word 76.1 78.3 90.9 83.5 85.2bound 75.3 79.0 90.4 82.2 85.2\ndep word 76.0 77.7 90.7 84.8 85.1bound 75.0 77.5 90.0 84.7 84.5\nGBOW linear word 74.9 77.9 90.4 82.0 85.0bound 74.1 77.8 90.3 80.7 84.1\ndep word 75.0 77.6 90.1 82.4 84.9bound 73.5 78.2 89.9 80.7 83.4\nGloVe linear word 73.4 76.7 89.6 79.2 83.5bound 73.2 77.5 90.0 79.8 83.4\ndep word 74.0 77.7 89.5 81.3 83.5bound 72.5 76.7 88.8 79.2 83.5 Random word embeddings 63.9 72.8 79.9 72.2 77.2\nTable 4: Results on 5 text classification datasets.\nviews (CR) (Nakagawa et al., 2010), and subjectivity/objectivity classification (SUBJ) (Pang and Lee, 2004). The other 2 datasets are documentlevel with multiple sentences: full-length movie review (RT-2k) (Pang and Lee, 2004), and IMDB movie review (IMDB) (Maas et al., 2011).\nAs shown in Table 4, pre-trained word embeddings outperform random word embeddings by a large margin. This further strengthens previous\n8\n701\n702\n703\n704\n705\n706\n707\n708\n709\n710\n711\n712\n713\n714\n715\n716\n717\n718\n719\n720\n721\n722\n723\n724\n725\n726\n727\n728\n729\n730\n731\n732\n733\n734\n735\n736\n737\n738\n739\n740\n741\n742\n743\n744\n745\n746\n747\n748\n749\n750\n751\n752\n753\n754\n755\n756\n757\n758\n759\n760\n761\n762\n763\n764\n765\n766\n767\n768\n769\n770\n771\n772\n773\n774\n775\n776\n777\n778\n779\n780\n781\n782\n783\n784\n785\n786\n787\n788\n789\n790\n791\n792\n793\n794\n795\n796\n797\n798\n799\nresearches that pre-trained word embeddings are crucial for text classification. Unlike that on previous tasks, different models’ results are actually very similar on text classification task. Text classification has less focus on syntax and function similarity. This leads to the phenomenon that models which use bound representation perform worse than those which use unbound representation on all datasets except CR. Models that use dependency-based context type and linear context type are comparable. These observations suggest that simple linear context type with unbound representations (as in traditional CSG and CBOW) is still the best choice of pre-training word embeddings for text classification, which is already used in most researches.\n\n4 Related Work\nPreviously, there are researches which directly compare different word embedding models. Lai et al. (2016) compare 6 word embedding models using different corpora and hyper-parameters. Levy and Goldberg (2014c) show the theoretical equivalence of CSG and PPMI matrix factorization. Levy et al. (2015) further discuss the connections between 4 word embedding models (PPMI, PPMI+SVD, CSG, GloVe) and re-evaluate them with the same hyper-parameters. Suzuki and Nagata (2015) investigate different configurations of CSG and Glove, then merge them into a unified form. Yin and Schutze (2016) propose 4 ensemble methods and show their effectiveness over individual word embeddings.\nThere are also researches which focus on evaluating different context types in learning word embeddings. Vulic and Korhonen (2016) compare CSG and dependency-based models on various languages. The results suggest that dependencybased models are able to detect functional similarity in English. However, the advantages of dependency-based context over linear context on other languages are not as promising as that on English. Bansal et al. (2014) investigate different embedding models for parsing task and show that dependency-based context is more suitable than linear context on this task. Melamud et al. (2016) investigate the performance of CSG, Deps and a substitute-based word embedding models (Yatbaz et al., 2012) 7, which shows that different types of\n7We do not consider this type of context, since it performs consistently worse than the other two context types. The\nintrinsic tasks have clear preference to particular types of contexts. On the other hand, for extrinsic tasks, the optimal context types need to be carefully tuned on specific dataset. However, context representations (bound and unbound) are not evaluated in these models. Moreover, they focus only on the more popular and intuitive CSG model, but not on CBOW and GloVe.\n\n5 Conclusion\nTo the best of our knowledge, this paper provides the first systematical investigation of different context types and representations for learning word embeddings. We evaluate different models on intrinsic property analysis (word similarity and word analogy), sequence labeling tasks (POS, Chunking and NER) and text classification task.\nOverall, the tendency of different models on different tasks is similar. However, most tasks have clear preference for different context types and representations. Context representations play a more important role than context types for learning word embeddings. More precisely: 1) Unbound representation is more suitable for syntactic word analogy than bound representation. Bound representation already contains syntactic information, which makes it difficult to learn syntactic aware word embeddings based on the input wordcontext pairs. 2) No matter which type of context to be used, bound representation is essential for sequence labeling tasks, which benefits from its ability of capturing functional similarity. In contrast, unbound representation, which is suitable for capturing topical similarity, doesn’t contribute to sequence labeling tasks. 3) Linear context with unbound representation (Skip-Gram) is still the best choice for text classification task. Linear context is enough for capturing topical similarity compared to dependency-based context. Words’ position information is generally useless for text classification, which makes bound representation contribute less to this task.\nIn the spirit of transparent and reproducible experiments, the word2vecPM toolkit in Supplemental Material will be published online. We hope researchers will take advantage of the code for further improvements and applications to other tasks.\nsame observation is made by Melamud et al. (2016); Vulic and Korhonen (2016)\n9\n801\n802\n803\n804\n805\n806\n807\n808\n809\n810\n811\n812\n813\n814\n815\n816\n817\n818\n819\n820\n821\n822\n823\n824\n825\n826\n827\n828\n829\n830\n831\n832\n833\n834\n835\n836\n837\n838\n839\n840\n841\n842\n843\n844\n845\n846\n847\n848\n849\n850\n851\n852\n853\n854\n855\n856\n857\n858\n859\n860\n861\n862\n863\n864\n865\n866\n867\n868\n869\n870\n871\n872\n873\n874\n875\n876\n877\n878\n879\n880\n881\n882\n883\n884\n885\n886\n887\n888\n889\n890\n891\n892\n893\n894\n895\n896\n897\n898\n899\n",
    "rationale": "- Strengths:\n\nThis paper presents a 2 x 2 x 3 x 10 array of accuracy results based on\nsystematically changing the parameters of embeddings models:\n\n(context type, position sensitive, embedding model, task), accuracy\n\n- context type ∈ {Linear, Syntactic}\n- position sensitive ∈ {True, False}\n- embedding model ∈ {Skip Gram, BOW, GLOVE}\n- task ∈ {Word Similarity, Analogies, POS, NER, Chunking, 5 text classific.\ntasks}\n\nThe aim of these experiments was to investigate the variation in\nperformance as these parameters are changed. The goal of the study itself\nis interesting for the ACL community and similar papers have appeared\nbefore as workshop papers and have been well cited, such as Nayak et al.'s\npaper mentioned below.\n\n- Weaknesses:\nSince this paper essentially presents the effect of systematically changing the\n\ncontext types and position sensitivity, I will focus on the execution of the\ninvestigation and the analysis of the results, which I am afraid is not \nsatisfactory.\n\nA) The lack of hyper-parameter tuning is worrisome. E.g.\n   - 395 Unless otherwise notes, the number of word embedding dimension is set\nto 500.\n   - 232 It still enlarges the context vocabulary about 5 times in practice.\n   - 385 Most hyper-parameters are the same as Levy et al' best configuration.\n\n  This is worrisome because lack of hyperparameter tuning makes it difficult to\nmake statements like method A is better than method B. E.g. bound methods may\nperform better with a lower dimensionality than unbound models, since their\neffective context vocabulary size is larger.\n\nB) The paper sometimes presents strange explanations for its results. E.g.\n   - 115 \"Experimental results suggest that although it's hard to find any \nuniversal insight, the characteristics of different contexts on different\nmodels are concluded according to specific tasks.\"\n\n   What does this sentence even mean? \n\n   - 580 Sequence labeling tasks tend to classify words with the same syntax \nto the same category. The ignorance of syntax for word embeddings which  are\nlearned by bound representation becomes beneficial. \n\n   These two sentences are contradictory, if a sequence labeling task\n   classified words with \"same syntax\" to same category then syntx becomes\n   a ver valuable feature. Bound representation's ignorance of syntax\n   should cause a drop in performance just like other tasks which does not\n   happen.\n\nC) It is not enough to merely mention Lai et. al. 2016 who have also done a\n   systematic study of the word embeddings, and similarly the paper \n   \"Evaluating Word Embeddings Using a Representative Suite of Practical\n   Tasks\", Nayak, Angeli, Manning. appeared at the repeval workshop at \n   ACL 2016. should have been cited. I understand that the focus of Nayak\n   et al's paper is not exactly the same as this paper, however they\n   provide recommendations about hyperparameter tuning and experiment\n   design and even provide a web interface for automatically running\n   tagging experiments using neural networks instead of the \"simple linear\n   classifiers\" used in the current paper.\n\nD) The paper uses a neural BOW words classifier for the text classification\ntasks\n   but a simple linear classifier for the sequence labeling tasks. What is\n   the justification for this choice of classifiers? Why not use a simple\n   neural classifier for the tagging tasks as well? I raise this point,\n   since the tagging task seems to be the only task where bound\n   representations are consistently beating the unbound representations,\n   which makes this task the odd one out. \n\n- General Discussion:\nFinally, I will make one speculative suggestion to the authors regarding\nthe analysis of the data. As I said earlier, this paper's main contribution is\nan\nanalysis of the following table.\n(context type, position sensitive, embedding model, task, accuracy)\nSo essentially there are 120 accuracy values that we want to explain in\nterms of the aspects of the model. It may be beneficial to perform\nfactor analysis or some other pattern mining technique on this 120 sample data.",
    "rating": 3
  },
  {
    "title": "Gated Self-Matching Networks for Reading Comprehension and Question Answering",
    "abstract": "In this paper, we present the gated selfmatching networks for reading comprehension style question answering, which aims to answer questions from a given passage. We first match the question and passage with gated attention-based recurrent networks to obtain the question-aware passage representation. Then we propose a self-matching attention mechanism to refine the representation by matching the passage against itself, which effectively encodes information from the whole passage. We finally employ the pointer networks to locate the positions of answers from the passages. We conduct extensive experiments on the SQuAD dataset. The single model achieves 71.3% on the evaluation metrics of exact match on the hidden test set, while the ensemble model further boosts the results to 75.9%. At the time1 of submission of the paper, our model holds the first place on the SQuAD leaderboard for both single and ensemble model.",
    "text": "1 Introduction\nIn this paper, we focus on reading comprehension style question answering which aims to answer questions given a passage or document. We specifically focus on the Stanford Question Answering Dataset (SQuAD) (Rajpurkar et al., 2016), a largescale dataset for reading comprehension and question answering which is manually created through crowdsourcing. SQuAD constrains answers to the space of all possible spans within the reference passage, which is different from cloze-style reading comprehension datasets (Hermann et al.,\n1On Feb. 6, 2017\n2015; Hill et al., 2016) in which answers are single words or entities. Moreover, SQuAD requires different forms of logical reasoning to infer the answer (Rajpurkar et al., 2016).\nRapid progress has been made since the release of the SQuAD dataset. Wang and Jiang (2016b) build question-aware passage representation with match-LSTM (Wang and Jiang, 2016a), and predict answer boundaries in the passage with pointer networks (Vinyals et al., 2015). Seo et al. (2016) introduce bi-directional attention flow networks to model question-passage pairs at multiple levels of granularity. Xiong et al. (2016) propose dynamic co-attention networks which attend the question and passage simultaneously and iteratively refine answer predictions. Lee et al. (2016) and Yu et al. (2016) predict answers by ranking continuous text spans within passages.\nInspired by Wang and Jiang (2016b), we introduce a gated self-matching network, illustrated in Figure 1, an end-to-end neural network model for reading comprehension and question answering. Our model consists of four parts: 1) the recurrent network encoder to build representation for questions and passages separately, 2) the gated matching layer to match the question and passage, 3) the self-matching layer to aggregate information from the whole passage, and 4) the pointernetwork based answer boundary prediction layer. The key contributions of this work are three-fold.\nFirst, we propose a gated attention-based recurrent network, which adds an additional gate to the attention-based recurrent networks (Bahdanau et al., 2014; Rocktäschel et al., 2015; Wang and Jiang, 2016a), to account for the fact that words in the passage are of different importance to answer a particular question for reading comprehension and question answering. In Wang and Jiang (2016a), words in a passage with their corresponding attention-weighted question context are en-\n2\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n112\n113\n114\n115\n116\n117\n118\n119\n120\n121\n122\n123\n124\n125\n126\n127\n128\n129\n130\n131\n132\n133\n134\n135\n136\n137\n138\n139\n140\n141\n142\n143\n144\n145\n146\n147\n148\n149\n150\n151\n152\n153\n154\n155\n156\n157\n158\n159\n160\n161\n162\n163\n164\n165\n166\n167\n168\n169\n170\n171\n172\n173\n174\n175\n176\n177\n178\n179\n180\n181\n182\n183\n184\n185\n186\n187\n188\n189\n190\n191\n192\n193\n194\n195\n196\n197\n198\n199\ncoded together to produce question-aware passage representation. By introducing a gating mechanism, our gated attention-based recurrent network assigns different levels of importance to passage parts depending on their relevance to the question, masking out irrelevant passage parts and emphasizing the important ones.\nSecond, we introduce a self-matching mechanism, which can effectively aggregate evidence from the whole passage to infer the answer. Through a gated matching layer, the resulting question-aware passage representation effectively encodes question information for each passage word. However, recurrent networks can only memorize limited passage context in practice despite its theoretical capability. One answer candidate is often unaware of the clues in other parts of the passage. To address this problem, we propose a self-matching layer to dynamically refine passage representation with information from the whole passage. Based on question-aware passage representation, we employ gated attention-based recurrent networks on passage against passage itself, aggregating evidence relevant to the current passage word from every word in the passage. A gated attention-based recurrent network layer and self-matching layer dynamically enrich each passage representation with information aggregated from both question and passage, enabling subsequent network to better predict answers.\nLastly, the proposed method yields state-of-theart results against strong baselines. Our single model achieves 71.3% exact match accuracy on the hidden SQuAD test set, while the ensemble model further boosts the result to 75.9%. At the time of submission of this paper, our model holds the first place on the SQuAD leader board.\n\n2 Task Description\nFor reading comprehension style question answering, a passage P and question Q are given, our task is to predict an answer A to question Q based on information found in P. The SQuAD dataset further constrains answer A to be a continuous subspan of passage P. Answer A often includes nonentities and can be much longer phrases. This setup challenges us to understand and reason about both the question and passage in order to infer the answer. Table 1 shows a simple example from the SQuAD dataset.\nPassage: Tesla later approached Morgan to ask for more funds to build a more powerful transmitter. When asked where all the money had gone, Tesla responded by saying that he was affected by the Panic of 1901, which he (Morgan) had caused. Morgan was shocked by the reminder of his part in the stock market crash and by Tesla’s breach of contract by asking for more funds. Tesla wrote another plea to Morgan, but it was also fruitless. Morgan still owed Tesla money on the original agreement, and Tesla had been facing foreclosure even before construction of the tower began. Question: On what did Tesla blame for the loss of the initial money? Answer: Panic of 1901\nTable 1: An example from the SQuAD dataset.\n\n3 Gated Self-Matching Networks\nFigure 1 gives an overview of the gated selfmatching networks. First, the question and passage are processed by a bi-directional recurrent network (Mikolov et al., 2010) separately. We then match the question and passage with gated attention-based recurrent networks, obtaining question-aware representation for the passage. On top of that, we apply self-matching attention to aggregate evidence from the whole passage and refine the passage representation, which is then fed into the output layer to predict the boundary of the answer span.\n\n3.1 Question and Passage Encoder\nConsider a question Q = {wQt }mt=1 and a passage P = {wPt }nt=1. We first convert the words to their respective word-level embeddings ({eQt }mt=1 and {ePt }nt=1) and character-level embeddings ({cQt }mt=1 and {cPt }nt=1). The character-level embeddings are generated by taking the final hidden states of a bi-directional recurrent neural network (RNN) applied to embeddings of characters in the token. Such character-level embeddings have been shown to be helpful to deal with out-ofvocab (OOV) tokens. We then use a bi-directional RNN to produce new representation uQ1 , . . . , u Q m and uP1 , . . . , u P n of all words in the question and passage respectively:\nuQt = BiRNNQ(u Q t−1, [e Q t , c Q t ]) (1) uPt = BiRNNP (u P t−1, [e P t , c P t ]) (2)\nWe choose to use Gated Recurrent Unit (GRU) (Cho et al., 2014) in our experiment since it performs similarly to LSTM (Hochreiter and Schmidhuber, 1997) but is computationally cheaper.\n3\n201\n202\n203\n204\n205\n206\n207\n208\n209\n210\n211\n212\n213\n214\n215\n216\n217\n218\n219\n220\n221\n222\n223\n224\n225\n226\n227\n228\n229\n230\n231\n232\n233\n234\n235\n236\n237\n238\n239\n240\n241\n242\n243\n244\n245\n246\n247\n248\n249\n250\n251\n252\n253\n254\n255\n256\n257\n258\n259\n260\n261\n262\n263\n264\n265\n266\n267\n268\n269\n270\n271\n272\n273\n274\n275\n276\n277\n278\n279\n280\n281\n282\n283\n284\n285\n286\n287\n288\n289\n290\n291\n292\n293\n294\n295\n296\n297\n298\n299\n𝑢1 𝑄 𝑢2 𝑄 𝑢𝑚 𝑄\nQuestion\nAttention Question\nVector\n𝑣1 𝑃 𝑣2 𝑃 𝑣3 𝑃\n𝑢1 𝑃 𝑢2 𝑃 𝑢3 𝑃\nPassage\n𝑣1 𝑃 𝑣2 𝑃 𝑣3 𝑃 𝑣𝑛 𝑃\nℎ1 𝑃 ℎ2 𝑃 ℎ3 𝑃\nAttention\nℎ1 𝑎 ℎ2 𝑎\nQuestion and Passage GRU Layer\nQuestion and Passage Matching Layer\nPassage Self-Matching Layer\nOutput Layer Start End\n𝑢𝑛 𝑃…\n…𝑣𝑛 𝑃…\nℎ𝑛 𝑃…\nWhen was tested The delay in\n…\ntest……\n𝑟𝑄\nFigure 1: Gated Self-Matching Networks structure overview.\n\n3.2 Gated Attention-based Recurrent Networks\nWe propose a gated attention-based recurrent network to incorporate question information into passage representation. It is a variant of attentionbased recurrent networks, with an additional gate to determine the importance of information in the passage regarding a question. Given question and passage representation {uQt }mt=1 and {uPt }nt=1, Rocktäschel et al. (2015) propose generating sentence-pair representation {vPt }nt=1 via soft-alignment of words in the question and passage as follows:\nvPt = RNN(v P t−1, ct) (3)\nwhere ct = att(uQ, [uPt , v P t−1]) is an attentionpooling vector of the whole question (uQ):\nstj = v Ttanh(WQu u Q j + W P u u P t + W P v v P t−1)\nati = exp(s t i)/Σ m j=1exp(s t j)\nct = Σ m i=1a t iu Q i (4)\nEach passage representation vPt dynamically incorporates aggregated matching information from the whole question.\nWang and Jiang (2016a) introduce matchLSTM, which takes uPt as an additional input into the recurrent network:\nvPt = RNN(v P t−1, [u P t , ct]) (5)\nTo determine the importance of passage parts and attend to the ones relevant to the question, we add another gate to the input ([uPt , ct]) of RNN:\ngt = sigmoid(Wg[u P t , ct])\n[uPt , ct] ∗ = gt [uPt , ct] (6)\nDifferent from the gates in LSTM or GRU, the additional gate is based on the current passage word and its attention-pooling vector of the question, which focuses on the relation between the question and current passage word. The gate effectively model the phenomenon that only parts of the passage are relevant to the question in reading comprehension and question answering. [uPt , ct] ∗ is utilized in subsequent calculations instead of [uPt , ct]. We call this gated attention-based recurrent networks. It can be applied to variants of RNN, such as GRU and LSTM. We also conduct experiments to show the effectiveness of the additional gate on both GRU and LSTM.\n\n3.3 Self-Matching Attention\nThrough gated attention-based recurrent networks, question-aware passage representation {vPt }nt=1 is generated to pinpoint important parts in the passage. One problem with such representation is that it has very limited knowledge of context. One answer candidate is often oblivious to important\n4\n301\n302\n303\n304\n305\n306\n307\n308\n309\n310\n311\n312\n313\n314\n315\n316\n317\n318\n319\n320\n321\n322\n323\n324\n325\n326\n327\n328\n329\n330\n331\n332\n333\n334\n335\n336\n337\n338\n339\n340\n341\n342\n343\n344\n345\n346\n347\n348\n349\n350\n351\n352\n353\n354\n355\n356\n357\n358\n359\n360\n361\n362\n363\n364\n365\n366\n367\n368\n369\n370\n371\n372\n373\n374\n375\n376\n377\n378\n379\n380\n381\n382\n383\n384\n385\n386\n387\n388\n389\n390\n391\n392\n393\n394\n395\n396\n397\n398\n399\ncues in the passage outside its surrounding window. Moreover, there exists some sort of lexical or syntactic divergence between the question and passage in the majority of SQuAD dataset (Rajpurkar et al., 2016). Passage context is necessary to infer the answer. To address this problem, we propose directly matching the question-aware passage representation against itself. It dynamically collects evidence from the whole passage for words in passage and encodes the evidence relevant to the current passage word and its matching question information into the passage representation hPt :\nhPt = BiRNN(h P t−1, [v P t , ct]) (7)\nwhere ct = att(vP , vPt ) is an attention-pooling vector of the whole passage (vP ):\nstj = v Ttanh(WPv v P j + W P̃ v v P t )\nati = exp(s t i)/Σ n j=1exp(s t j)\nct = Σ n i=1a t iv P i (8)\nAn additional gate as in gated attention-based recurrent networks is applied to [vPt , ct] to adaptively control the input of RNN.\nSelf-matching extracts evidence from the whole passage according to the current passage word and question information.\n\n3.4 Output Layer\nWe follow Wang and Jiang (2016b) and use pointer networks (Vinyals et al., 2015) to predict the start and end position of the answer. In addition, we use an attention-pooling over the question representation to generate the initial hidden vector for the pointer network. Given the passage representation {hPt }nt=1, the attention mechanism is utilized as a pointer to select the start position (p1) and end position (p2) from the passage, which can be formulated as follows:\nstj = v Ttanh(WPh h P j + W a hh a t−1)\nati = exp(s t i)/Σ n j=1exp(s t j)\npt = arg max(at1, . . . , a t n) (9)\nHere hat−1 represents the last hidden state of the answer recurrent network (pointer network). The input of the answer recurrent network is the attention-pooling vector based on current predicted probability at:\nct = Σ n i=1a t ih P i\nhat = RNN(h a t−1, ct) (10)\nWhen predicting the start position, hat−1 represents the initial hidden state of the answer recurrent network. We utilize the question vector rQ as the initial state of the answer recurrent network. rQ = att(uQ, V Qr ) is an attention-pooling vector of the question based on the parameter V Qr :\nsj = v Ttanh(WQu u Q j + W Q v V Q r )\nai = exp(si)/Σ m j=1exp(sj)\nrQ = Σmi=1aiu Q i (11)\nTo train the network, we minimize the sum of the negative log probabilities of the ground truth start and end position by the predicted distributions.\n\n4 Experiment\n\n\n4.1 Implementation Details\nWe specially focus on the SQuAD dataset to train and evaluate our model, which has garnered a huge attention over the past few months. SQuAD is composed of 100,000+ questions posed by crowd workers on 536 Wikipedia articles. The dataset is randomly partitioned into a training set (80%), a development set (10%), and a test set (10%). The answer to every question is a segment of the corresponding passage.\nWe use the tokenizer from Stanford CoreNLP (Manning et al., 2014) to preprocess each passage and question. The Gated Recurrent Unit (Cho et al., 2014) variant of LSTM is used throughout our model. For word embedding, we use pretrained case-sensitive GloVe embeddings2 (Pennington et al., 2014) for both questions and passages, and it is fixed during training; We use zero vectors to represent all out-of-vocab words. We utilize 1 layer of bi-directional GRU to compute character-level embeddings and 3 layers of bi-directional GRU to encode questions and passages, the gated attention-based recurrent network for question and passage matching is also encoded bidirectionally in our experiment. The hidden vector length is set to 75 for all layers. We also apply dropout (Srivastava et al., 2014) between layers with a dropout rate of 0.2. The model is optimized with AdaDelta (Zeiler, 2012) with an initial learning rate of 1.\n2Downloaded from http://nlp.stanford.edu/ data/glove.840B.300d.zip.\n5\n401\n402\n403\n404\n405\n406\n407\n408\n409\n410\n411\n412\n413\n414\n415\n416\n417\n418\n419\n420\n421\n422\n423\n424\n425\n426\n427\n428\n429\n430\n431\n432\n433\n434\n435\n436\n437\n438\n439\n440\n441\n442\n443\n444\n445\n446\n447\n448\n449\n450\n451\n452\n453\n454\n455\n456\n457\n458\n459\n460\n461\n462\n463\n464\n465\n466\n467\n468\n469\n470\n471\n472\n473\n482\n483\n484\n485\n486\n487\n488\n489\n490\n491\n492\n493\n494\n495\n496\n497\n498\n499\nDev Set Test Set Single model EM / F1 EM / F1 LR Baseline (Rajpurkar et al., 2016) 40.0 / 51.0 40.4 / 51.0 Dynamic Chunk Reader (Yu et al., 2016) 62.5 / 71.2 62.5 / 71.0 Match-LSTM with Ans-Ptr (Wang and Jiang, 2016b) 64.1 / 73.9 64.7 / 73.7 Dynamic Coattention Networks (Xiong et al., 2016) 65.4 / 75.6 66.2 / 75.9 RaSoR (Lee et al., 2016) 66.4 / 74.9 - / - BiDAF (Seo et al., 2016) 68.0 / 77.3 68.0 / 77.3 jNet (USTC&National Research Council Canada&York University) - / - 68.7 / 77.4 Multi-Perspective Matching (Wang et al., 2016) - / - 68.9 / 77.8 Gated Self-Matching Networks 71.1 / 79.5 71.3 / 79.7 Ensemble model Fine-Grained Gating (Yang et al., 2016) 62.4 / 73.4 62.5 / 73.3 Match-LSTM with Ans-Ptr (Wang and Jiang, 2016b) 67.6 / 76.8 67.9 / 77.0 RaSoR (Lee et al., 2016) 68.2 / 76.7 - / - Dynamic Coattention Networks (Xiong et al., 2016) 70.3 / 79.4 71.6 / 80.4 BiDAF (Seo et al., 2016) 73.3 / 81.1 73.3 / 81.1 Multi-Perspective Matching (Wang et al., 2016) - / - 73.8 / 81.3 Gated Self-Matching Networks 75.6 / 82.8 75.9 / 82.9 Human Performance (Rajpurkar et al., 2016) 80.3 / 90.5 77.0 / 86.8 FastQA* (German Research Center for Artificial Intelligence) - / - 68.4 / 77.1 FastQAExt* (German Research Center for Artificial Intelligence) - / - 70.8 / 78.9\nTable 2: The performance of our gated self-matching networks and competing approaches. * indicates that there is no explicit label to suggest whether the model is a single or ensemble model.\nSingle Model EM / F1 Gated Self-Matching (GRU) 71.1 / 79.5 -Character embedding 70.3 / 78.9 -Gating 67.9 / 77.1 -Self-Matching 66.9 / 76.4 -Gating, -Self-Matching 65.2 / 74.7\nTable 3: Ablation tests of single model on the SQuAD dev set.\n\n4.2 Main Results\nTwo metrics are utilized to evaluate model performance: Exact Match (EM) and F1 score. EM measures the percentage of the prediction that matches one of the ground truth answers exactly. F1 measures the overlap between the prediction and ground truth answers which takes the maximum F1 over all of the ground truth answers. The scores on dev set are evaluated by the official script3. Since the test set is hidden, we are required to submit the model to Stanford NLP group to obtain the test scores.\nTable 2 shows exact match and F1 scores on the 3Downloaded from http://stanford-qa.com\ndev and test set4 of our model and competing approaches. As we can see, our method clearly outperforms the baseline and several strong state-ofthe-art systems for both single model and ensembles.\n\n4.3 Ablation Study\nWe do ablation tests on the dev set to analyze the contribution of components of gated self-matching networks. As illustrated in Table 3, the gated attention-based recurrent network (GARNN) and self-matching attention mechanism positively contribute to the final results of gated self-matching\n4Extracted from SQuAD leaderboard http: //stanford-qa.com on Feb. 6, 2017.\n6\n501\n502\n503\n504\n505\n506\n507\n508\n509\n510\n511\n512\n513\n514\n515\n516\n517\n518\n519\n520\n521\n522\n523\n524\n525\n526\n527\n528\n529\n530\n531\n532\n533\n534\n535\n536\n537\n538\n539\n540\n541\n542\n543\n544\n545\n546\n547\n548\n549\n550\n551\n552\n553\n554\n555\n556\n557\n558\n559\n560\n561\n562\n563\n564\n565\n566\n567\n568\n569\n570\n571\n572\n573\n574\n575\n576\n577\n578\n579\n580\n581\n582\n583\n584\n585\n586\n587\n588\n589\n590\n591\n592\n593\n594\n595\n596\n597\n598\n599\nFigure 2: Part of the attention matrices for self-matching. Each row is the attention weights of the whole passage for the current passage word. The darker the color is the higher the weight is. Some key evidence relevant to the question-passage tuple is more encoded into answer candidates.\nnetworks. Removing self-matching results in 4.2 point EM drop, which reveals that information in the passage plays an important role. Characterlevel embeddings contribute towards the model’s performance since it can better handle out-ofvocab or rare words. To show the effectiveness of GARNN for variant RNNs, we conduct experiments on the base model (Wang and Jiang, 2016b) of different variant RNNs. The base model match the question and passage via a variant of attentionbased recurrent network (Wang and Jiang, 2016a), and employ pointer networks to predict the answer. Character-level embeddings are not utilized. As shown in Table 4, the gate introduced in question and passage matching layer is helpful for both GRU and LSTM on the SQuAD dataset.\n\n5 Discussion\n\n\n5.1 Encoding Evidence from Passage\nTo show the ability of the model for encoding evidence from the passage, we draw the alignment of the passage against itself in self-matching. The attention weights are shown in Figure 2, in which the darker the color is the higher the\nweight is. We can see that key evidence aggregated from the whole passage is more encoded into the answer candidates. For example, the answer “Egg of Columbus” pays more attention to the key information “Tesla”, “device” and the lexical variation word “known” that are relevant to the question-passage tuple. The answer “world classic of epoch-making oratory” mainly focuses on the evidence “Michael Mullet”, “speech” and lexical variation word “considers”. For other words, the attention weights are more evenly distributed between evidence and some irrelevant parts. Selfmatching do adaptively aggregate evidence for words in passage.\n\n5.2 Result Analysis\nTo further analyse the model’s performance, we show the exact match and F1 score for different question types (Figure 3(a)), different answer lengths (Figure 3(b)), different passage lengths (Figure 3(c)) and different question lengths (Figure 3(d)). As we can see, the type of “what” takes up a majority of questions. Our model is better at “when” and “who” questions, but poorly on “why”\n7\n601\n602\n603\n604\n605\n606\n607\n608\n609\n610\n611\n612\n613\n614\n615\n616\n617\n618\n619\n620\n621\n622\n623\n624\n625\n626\n627\n628\n629\n630\n631\n632\n633\n634\n635\n636\n637\n638\n639\n640\n641\n642\n643\n644\n645\n646\n647\n648\n649\n650\n651\n652\n653\n654\n655\n656\n657\n658\n659\n660\n661\n662\n663\n664\n665\n666\n667\n668\n669\n670\n671\n672\n673\n674\n675\n676\n677\n678\n679\n680\n681\n682\n683\n684\n685\n686\n687\n688\n689\n690\n691\n692\n693\n694\n695\n696\n697\n698\n699\n(a) (b)\n(c) (d)\nFigure 3: Model performance on different question types (a), different answer lengths (b), different passage lengths (c), different question lengths (d). The point on the x-axis of figure (c) and (d) represent the datas whose passages length or questions length are between the value of current point and last point.\nquestions. From the Graph 3(b), the majority of answers are in short length. With the increase of answer length, the performance of our model obviously drops. Besides, the gap between exact match and F1 widens as answer length increases, which indicates our model could locate the core of the answer to some extent. From Graph 3(c) and 3(d), the majority of passages and questions are not too long. Moreover, we discover that the performance remains stable with the increase in length, the obvious drop in longer passage is mainly because the proportion is too small. Our model is largely agnostic to long passages and focuses on important part of the passage.\n\n6 Related Work\nReading Comprehension and Question Answering Dataset Benchmark datasets play an important role in recent progress in reading comprehension and question answering research. Existing datasets can be classified into two categories according to whether they are manually labeled. Those that are labeled by humans are always in high quality (Richardson et al., 2013; Berant et al., 2014; Yang et al., 2015), but are too small for training modern data-intensive models. Those that\nare automatically generated from natural occurring data can be very large (Hill et al., 2016; Hermann et al., 2015), which allow the training of more expressive models. However, they are in cloze style, in which the goal is to predict the missing word (often a named entity) in a passage. Moreover, Chen et al. (2016) have shown that the CNN / Daily News dataset (Hermann et al., 2015) requires less reasoning than previously thought, and conclude that performance is almost saturated.\nDifferent from above datasets, the SQuAD provides a large and high-quality dataset. The answers in SQuAD often include non-entities and can be much longer phrase, which is more challenging than cloze-style datasets. Moreover, Rajpurkar et al. (2016) show that the dataset retains a diverse set of answers and requires different forms of logical reasoning, including multi-sentence reasoning. MS MARCO (Nguyen et al., 2016) is also a large-scale dataset. The questions in the dataset are real anonymized queries issued through Bing or Cortana and the passages are related web pages. For each question in the dataset, several related passages are provided. However, the answers are human generated, which is different from SQuAD where answers must be a span of the passage.\n8\n701\n702\n703\n704\n705\n706\n707\n708\n709\n710\n711\n712\n713\n714\n715\n716\n717\n718\n719\n720\n721\n722\n723\n724\n725\n726\n727\n728\n729\n730\n731\n732\n733\n734\n735\n736\n737\n738\n739\n740\n741\n742\n743\n744\n745\n746\n747\n748\n749\n750\n751\n752\n753\n754\n755\n756\n757\n758\n759\n760\n761\n762\n763\n764\n765\n766\n767\n768\n769\n770\n771\n772\n773\n774\n775\n776\n777\n778\n779\n780\n781\n782\n783\n784\n785\n786\n787\n788\n789\n790\n791\n792\n793\n794\n795\n796\n797\n798\n799\nEnd-to-end Neural Networks for Reading Comprehension Along with cloze-style datasets, several powerful deep learning models (Hermann et al., 2015; Hill et al., 2016; Chen et al., 2016; Kadlec et al., 2016; Sordoni et al., 2016; Cui et al., 2016; Trischler et al., 2016; Dhingra et al., 2016; Shen et al., 2016) have been introduced to solve this problem. Hermann et al. (2015) first introduce attention mechanism into reading comprehension. Hill et al. (2016) propose a windowbased memory network for CBT dataset. Kadlec et al. (2016) introduce pointer networks with one attention step to predict the blanking out entities. Sordoni et al. (2016) propose an iterative alternating attention mechanism to better model the links between question and passage. Trischler et al. (2016) solve cloze-style question answering task by combining an attentive model with a reranking model. Dhingra et al. (2016) propose iteratively selecting important parts of the passage by a multiplying gating function with the question representation. Cui et al. (2016) propose a two-way attention mechanism to encode the passage and question mutually. Shen et al. (2016) propose iteratively inferring the answer with a dynamic number of reasoning steps and is trained with reinforcement learning.\nNeural network-based models demonstrate the effectiveness on the SQuAD dataset. Wang and Jiang (2016b) combine match-LSTM and pointer networks to produce the boundary of the answer. Xiong et al. (2016) and Seo et al. (2016) employ variant coattention mechanism to match the question and passage mutually. Xiong et al. (2016) propose a dynamic pointer network to iteratively infer the answer. Yu et al. (2016) and Lee et al. (2016) solve SQuAD by ranking continuous text spans within passage. Yang et al. (2016) present a fine-grained gating mechanism to dynamically combine word-level and character-level representation and model the interaction between questions and passages. Wang et al. (2016) propose matching the context of passage with the question from multiple perspectives.\nDifferent from the above models, we introduce self-matching attention in our model. It dynamically refines the passage representation by looking over the whole passage and aggregating evidence relevant to the current passage word and question, allowing our model make full use of passage information. Weightedly attending to word context\nhas been proposed in several works. Ling et al. (2015) propose considering window-based contextual words differently depending on the word and its relative position. Cheng et al. (2016) propose a novel LSTM network to encode words in a sentence which considers the relation between the current token being processed and its past tokens in the memory. Parikh et al. (2016) apply this method to encode words in a sentence according to word form and its distance. Since passage information relevant to question is more helpful to infer the answer in reading comprehension, we apply self-matching based on question-aware representation and gated attention-based recurrent networks. It helps our model mainly focus on question-relevant evidence in the passage and dynamically look over the whole passage to aggregate evidence.\nAnother key component of our model is the attention-based recurrent network, which has demonstrated success in a wide range of tasks. Bahdanau et al. (2014) first propose attentionbased recurrent networks to infer word-level alignment when generating the target word. Hermann et al. (2015) introduce word-level attention into reading comprehension to model the interaction between questions and passages. Rocktäschel et al. (2015) and Wang and Jiang (2016a) propose determining entailment via word-by-word matching. The gated attention-based recurrent network is a variant of attention-based recurrent network with an additional gate to model the fact that passage parts are of different importance to the particular question for reading comprehension and question answering.\n\n7 Conclusion\nIn this paper, we present gated self-matching networks for reading comprehension and question answering. We introduce the gated attentionbased recurrent networks and self-matching attention mechanism to obtain representation for the question and passage, and then use the pointernetworks to locate answer boundaries. Our model achieves state-of-the-art results on the SQuAD dataset, outperforming several strong competing systems. As for future work, we are applying the gated self-matching networks to other reading comprehension and question answering datasets, such as the MS MARCO dataset (Nguyen et al., 2016).\n9\n801\n802\n803\n804\n805\n806\n807\n808\n809\n810\n811\n812\n813\n814\n815\n816\n817\n818\n819\n820\n821\n822\n823\n824\n825\n826\n827\n828\n829\n830\n831\n832\n833\n834\n835\n836\n837\n838\n839\n840\n841\n842\n843\n844\n845\n846\n847\n848\n849\n850\n851\n852\n853\n854\n855\n856\n857\n858\n859\n860\n861\n862\n863\n864\n865\n866\n867\n868\n869\n870\n871\n872\n873\n874\n875\n876\n877\n878\n879\n880\n881\n882\n883\n884\n885\n886\n887\n888\n889\n890\n891\n892\n893\n894\n895\n896\n897\n898\n899\n",
    "rationale": "This paper presents the gated self-matching network for reading comprehension\nstyle question answering. There are three key components in the solution: \n\n(a) The paper introduces the gated attention-based recurrent network to obtain\nthe question-aware representation for the passage. Here, the paper adds an\nadditional gate to attention-based recurrent networks to determine the\nimportance of passage parts and attend to the ones relevant to the question.\nHere they use word as well as character embeddings to handle OOV words.\nOverall, this component is inspired from Wang and Jiang 2016.\n\n(b) Then the paper proposes a self-matching attention mechanism to improve the\nrepresentation for the question and passage by looking at wider passage context\nnecessary to infer the answer. This component is completely novel in the paper.\n\n(c) At the output layer, the paper uses pointer networks to locate answer\nboundaries. This is also inspired from Wang and Jiang 2016\n\nOverall, I like the paper and think that it makes a nice contribution.\n\n- Strengths:\n\nThe paper clearly breaks the network into three component for descriptive\npurposes, relates each of them to prior work and mentions its novelties with\nrespect to them. It does a sound empirical analysis by describing the impact of\neach component by doing an ablation study. This is appreciated.\n\nThe results are impressive!\n\n- Weaknesses:\n\nThe paper describes the results on a single model and an ensemble model. I\ncould not find any details of the ensemble and how was it created. I believe it\nmight be the ensemble of the character based and word based model. Can the\nauthors please describe this in the rebuttal and the paper.\n\n- General Discussion:\n\nAlong with the ablation study, it would be nice if we can have a\nqualitative analysis describing some example cases where the components of\ngating, character embedding, self embedding, etc. become crucial ... where a\nsimple model doesn't get the question right but adding one or more of these\ncomponents helps. This can go in some form of appendix or supplementary.",
    "rating": 5
  },
  {
    "title": "End-to-End Neural Relation Extraction with Global Optimization",
    "abstract": "Neural networks have recently shown promising results for relation extraction. State-of-the-art models cast the task as an end-to-end problem, solved incrementally using a local classifier. Yet previous work using statistical models have demonstrated that global optimization can achieve better performances compared to local classification. We build a globally optimized neural model for end-to-end relation extraction, proposing novel LSTM features in order to better learn representations. Experiments show that our model is highly effective, achieving the best performances on two standard benchmarks.",
    "text": "1 Introduction\nExtracting entities (Florian et al., 2006, 2010) and relations (Zhao and Grishman, 2005; Jiang and Zhai, 2007; Sun et al., 2011; Plank and Moschitti, 2013) from unstructured texts have been two central tasks in information extraction (Grishman, 1997; Doddington et al., 2004). Traditional approaches to relation extraction take entity recognition as a predecessor step in a pipeline (Zelenko et al., 2003; Chan and Roth, 2011), predicting relations between given entities.\nIn recent years, there has been a surge of interest in performing end-to-end relation extraction, jointly recognizing entities and relations given free text inputs (Li and Ji, 2014; Miwa and Sasaki, 2014; Miwa and Bansal, 2016; Gupta et al., 2016). End-to-end learning prevents error propagation in the pipeline approach, and allows cross-task dependencies to be modeled explicitly for entity recognition. As a result, it gives better relation extraction accuracies compared to pipelines.\nMiwa and Bansal (2016) were among the first to use neural networks for end-to-end relation extraction, showing highly promising results. In particular, they used a bidirectional LSTM (Graves et al., 2013) to learn hidden word representations under a sentential context, and further leveraged a tree-structured LSTMs (Tai et al., 2015) to encode syntactic information, given the output of a parser. The resulting representations are then used for making local decisions for entity and relation extraction incrementally, leading to much improved results compared with the best statistical model (Li and Ji, 2014). This demonstrates the strength of neural representation learning for endto-end relation extraction.\nOn the other hand, Miwa and Bansal (2016)’s model is trained locally, without considering structural correspondences between incremental decisions. This is unlike existing statistical methods, which utilize well-studied structured prediction methods to address the problem (Li and Ji, 2014; Miwa and Sasaki, 2014). As has been commonly understood, learning local decisions for structured prediction can lead to label bias (Lafferty et al., 2001), which prevents globally optimal structures from receiving optimal scores by the model. We address this potential issue by building a structural neural model for end-to-end relation extraction, following a recent line of efforts on globally optimized models for neural structured prediction (Zhou et al., 2015; Watanabe and Sumita, 2015; Andor et al., 2016; Wiseman and Rush, 2016).\nIn particular, we follow Miwa and Bansal (2016), casting the task as an end-to-end tablefilling problem. This is different from the actionbased method of Li and Ji (2014), yet has shown to be more flexible and accurate (Miwa and Sasaki, 2014). We take a different approach to representation learning, addressing two potential limitations of Miwa and Bansal (2016).\n2\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n112\n113\n114\n115\n116\n117\n118\n119\n120\n121\n122\n123\n124\n125\n126\n127\n128\n129\n130\n131\n132\n133\n134\n135\n136\n137\n138\n139\n140\n141\n142\n143\n144\n145\n146\n147\n148\n149\n154\n155\n156\n157\n158\n159\n160\n161\n162\n163\n164\n165\n166\n167\n168\n169\n170\n171\n172\n173\n174\n175\n176\n177\n178\n179\n180\n181\n182\n183\n184\n185\n186\n187\n188\n189\n190\n191\n192\n193\n194\n195\n196\n197\n198\n199\nFirst, Miwa and Bansal (2016) rely on external syntactic parsers for obtaining syntactic information, which is crucial for relation extraction (Culotta and Sorensen, 2004; Zhou et al., 2005; Bunescu and Mooney, 2005; Qian et al., 2008). However, parsing errors can lead to encoding inaccuracies of tree-LSTMs, thereby hurting relation extraction potentially. We take an alternative approach to integrating syntactic information, by taking the hidden LSTM layers of a bi-affine attention parser (Dozat and Manning, 2016) to augment input representations. Pretrained for parsing, such hidden layers contain rich syntactic information on each word, yet do not explicitly represent parsing decisions, thereby avoiding potential issues caused by incorrect parses.\nOur method is also free from a particular syntactic formalism, such as dependency grammar, constituent grammar or CCG, requiring only hidden representations on word that contain syntactic information. In contrast, the method of Miwa and Bansal (2016) must consider tree LSTM formulations that are specific to grammar formalisms, which can be very different (Tai et al., 2015).\nSecond, Miwa and Bansal (2016) did not explicitly learn the representation of segments when predicting entity boundaries or making relation classification decisions, which can be intuitively highly useful. We take the LSTM-Minus method of Wang and Chang (2016), modelling a segment as the difference between its last and first LSTM hidden vectors. This method is highly efficient, yet gives as accurate results as compared to more complex neural network structures to model a span of words (Cross and Huang, 2016).\nEvaluation on two benchmark datasets shows that our method outperforms previous methods of Miwa and Bansal (2016), Li and Ji (2014) and Miwa and Sasaki (2014), giving the best reported results on both benchmarks. Our code is available under GPL at https://github.com/〈anonymized〉.\n\n2 Model\n\n\n2.1 Task Definition\nAs shown in Figure 1, the goal of relation extraction is to mine relations from raw texts. It consists of two sub-tasks, namely entity detection, which recognizes valid entities, and relation classification, which determines the relation categories over entity pairs. We follow recent studies and recognize entities and relations as one single task.\n\n2.2 Method\nWe follow Miwa and Sasaki (2014) and Gupta et al. (2016), treating relation extraction as a tablefilling problem, performing entity detection and relation classification using a single incremental model, which is similar in spirit to Miwa and Bansal (2016). Figure 2 shows an example of the table-filling process.\nFormally, given a sentence w1w2 · · ·wn, we maintain a table Tn×n, where T (i, j) denotes the relation between wi and wj . When i = j, T (i, j) denotes an entity boundary label. We map entity words into labels under the BILOU (Begin, Inside, Last, Outside, Unit) scheme, assuming that there are no overlapping entities in one sentence (Li and Ji, 2014; Miwa and Sasaki, 2014; Miwa and Bansal, 2016). Only the upper triangular table is necessary for indicating the relations.\nWe adopt the close-first left-to-right strategy (Miwa and Sasaki, 2014) to map the twodimensional table into a sequence, in order to fill the table incrementally. During the table-filling process, we take two label sets for entity detection (i = j) and relation classification (i < j), respectively. The labels for entity detection include {B*, I-*, L-*, O, U-* }, where * denotes the entity type, and the labels for relation classification are {−→∗ ,←−∗ }, where * denotes the relation category.\nAt each step, given a partially-filled table T , we determine the most suitable label l for the next step (order in Figure 2) using a scoring function:\nscore(T, l) = WlhT , (1)\nwhere Wl is a model parameter and hT is the vector representation of T . Based on the function, we aim to find the best label sequence l1 · · · lm, where m = n(n+1)2 , and the resulting sequence of partially-filled tables is T0T1 · · ·Tm, where Ti = FILL(Ti−1, li), and T0 is an empty table. Different from previous work, we investigate a structural\n3\n201\n202\n203\n204\n205\n206\n207\n208\n209\n210\n211\n212\n213\n214\n215\n216\n217\n218\n219\n220\n221\n222\n223\n224\n225\n226\n227\n228\n229\n230\n231\n232\n233\n234\n235\n236\n237\n238\n239\n240\n241\n242\n243\n244\n245\n246\n247\n248\n249\n250\n251\n252\n253\n254\n255\n256\n257\n258\n259\n260\n261\n262\n263\n264\n265\n266\n267\n268\n269\n270\n271\n272\n273\n274\n275\n276\n277\n278\n279\n280\n281\n282\n283\n284\n285\n286\n287\n288\n289\n290\n291\n292\n293\n294\n295\n296\n297\n298\n299\nAssociated Press writer Patrick McDowell in Kuwait City\nAssociated Press writer Patrick McDowell\nin Kuwait\nCity\n1 B-ORG 9 ⊥ 16 ⊥ 22 ⊥ 27 ⊥ 31 ⊥ 34 ⊥ 36 ⊥ 2 L-ORG 10←−−−−−ORG-AFF 17 ⊥ 23 ⊥ 28 ⊥ 32 ⊥ 35 ⊥\n3 U-PER 11 ⊥ 18 ⊥ 24 ⊥ 29 ⊥ 33 ⊥ 4 B-PER 12 ⊥ 19 ⊥ 25 ⊥ 30 ⊥\n5 L-PER 13 ⊥ 20 ⊥ 26−−−→PHYS 6 O 14 ⊥ 21 ⊥\n7 B-GPE 15 ⊥ 8 L-GPE\nFigure 2: Table-filling example, where numbers indicate the filling order.\nhw\n⊕ ewe′w et hchar ⊕ ⊕ CNN\ncharacter sequence\nFigure 3: Word representations.\nmodel that is optimized for the label sequence l1 · · · lm globally, rather than for each li locally.\n\n2.3 Representation Learning\nAt the ith step, we determine the label li of the next table slot based on the current hypothesis Ti−1. Following Miwa and Bansal (2016), we use a neural network to learn the vector representation of Ti−1, and then use Equation 1 to rank candidate next labels. There are two types of input features, including the word sequence w1w2 · · ·wn, and the readily filled label sequence l1l2 · · · li−1. We build a neural network to represent Ti−1.\n\n2.3.1 Word Representation\nShown in Figure 3, we represent each word wi by hwi using its word form, POS tag and characters. Based on the word form, we use two different embeddings, one being obtained by using a randomly initialized look-up table Ew, tuned during training and represented by ew, and the other being a pretrained external word embedding from E′w, which is fixed and represented by e′w.\n1 For a POS tag t, its embedding et is obtained from a look-up table Et similar to Ew.\nThe above two components have also been used by Miwa and Bansal (2016). In addition, we enhance the word representation by its character sequence (Ballesteros et al., 2015; Lample et al., 2016), using a convolution neural network (CNN) to derive a character-based word representation hchar, which has been demonstrated effective for\n1We use the set of pre-trained glove word embeddings available at http://nlp.stanford.edu/data/glove.6B.zip as external word embeddings.\nhi−1...... hi ...... hj ......\nhseg = hj − hi−1\nFigure 4: Segment representation.\nseveral NLP tasks (dos Santos and Gatti, 2014). We obtain the final hwi based on a non-linear feedforward layer on e′w ⊕ ew ⊕ et ⊕ hchar, where ⊕ denotes concatenation.\n\n2.3.2 Label Representation\nIn addition to the word sequence, the history label sequence l1l2 · · · li−1, especially the labels representing detected entities, is also useful disambiguation. For example, the previous entity boundary label can be helpful to deciding the boundary label of the current word. During relation classification, the types of the entities involved can indicate the relation category between them. We exploit the diagonal label sequence of partial table T , which denotes entity boundaries of words, to enhance the representation learning. We obtain a word’s entity boundary label embedding el by a randomly initialized looking-up table El.\n\n2.3.3 LSTM Features\nWe follow Miwa and Bansal (2016), learning global context representations using LSTMs. Three basic LSTM structures are used: a leftto-right word LSTM ( −−−−→ LSTMw), a right-to-left word LSTM ( ←−−−− LSTMw) and a left-to-right entity boundary label LSTM ( −−−−→ LSTMe). Each LSTM derives a sequence of hidden vectors for inputs. For example, for w1w2 · · ·wn, −−−−→ LSTMw gives hw,→1 h w,→ 2 · · ·h w,→ n .\nDifferent from Miwa and Bansal (2016), who use the output hidden vectors {hi} of LSTMs to represent words, we exploit segment representations as well. In particular, for a segment of text [i, j], the representation is computed by using LSTM-Minus (Wang and Chang, 2016), shown by Figure 4, where hj − hi−1 in a left-to-right LSTM\n4\n301\n302\n303\n304\n305\n306\n307\n308\n309\n310\n311\n312\n313\n314\n315\n316\n317\n318\n319\n320\n321\n322\n323\n324\n325\n326\n327\n328\n329\n330\n331\n332\n333\n334\n335\n336\n337\n338\n339\n340\n341\n342\n343\n344\n345\n346\n347\n348\n349\n350\n351\n352\n353\n354\n355\n356\n357\n358\n359\n360\n361\n362\n363\n364\n365\n366\n367\n368\n369\n370\n371\n372\n373\n374\n375\n376\n377\n378\n379\n380\n381\n382\n383\n384\n385\n386\n387\n388\n389\n390\n391\n392\n393\n394\n395\n396\n397\n398\n399\n...... ...... ......\n...... ...... ......\n...... ......\nhT\n−−−−→ LSTMw\n←−−−− LSTMw\n−−−−→ LSTMe\n...... j − 1 j ...... i− 1 i i+ 1 ......\nconcatenate\nfeed-forward\n(a) entity detection\n...... ...... ...... ...... ......\n...... ...... ...... ...... ......\n...... ...... ...... ...... ......\nhT\n−−−−→ LSTMw\n←−−−− LSTMw\n−−−−→ LSTMe\nleft entityi middle entityj right\nconcatenate\nfeed-forward\n(b) relation classification\nFigure 5: Feature representation.\nand hi − hj+1 in a right-to-left LSTM are used to represent the segment [i, j]. The segment representations can reflect entities in a sentence, and thus can be potentially useful for both entity detection and relation extraction.\n\n2.3.4 Feature Representation\nWe use separate feature representations for entity detection and relation classification, both of which are induced according to the above three LSTM structures. In particular, we first extract a set of base neural features, and then concatenate them and feed them into a non-linear neural layer for entity detection and relation classification, respectively. Figure 5 shows the overall representation.\n[Entity Detection] Figure 5(a) shows the feature representation method for the entity detection actions. First, we extract six feature vectors from the three basic LSTMs, three of which are word features, namely hw,→i , h w,← i and h e,→ i−1 , and the remaining are segment features, namely hw,→[j,i−1], hw,←[j,i−1] and h e,→ [j,i−1], where j denotes the start po-\nsition of the previous entity.2 The six vectors are concatenated and then fed into a non-linear layer for entity detection.\n[Relation Classification] Figure 5(b) shows the feature representation method for relation classification. Similar to entity detection, we extract a set of features from the basic LSTMs (\n−−−−→ LSTMw,←−−−−\nLSTMw and −−−−→ LSTMe), and then concatenate them for a non-linear classification layer. The differences between relation classification with entity detection lie in the range of hidden layers from LSTMs. For relation classification between i and j, we split each LSTM into five segments according to the two entities ended with i and j. Formally, let [s(i), i] and [s(j), j] denote the two entities above, where s(·) denotes the start position of an entity, the resulted segments are [0, s(i)− 1] (i.e., left, in Figure 5(b)), [s(i), i] (i.e., entityi), [i + 1, s(j) − 1] (i.e., middle), [s(j), j] (i.e., entityj) and [j + 1, n] (i.e., right), respectively. For the word LSTMs, we extract all five segment features, while the entity label LSTM, we only use the segment features of entityi and entityj .\n\n2.3.5 Syntactic Features\nPrevious work has shown that syntactic features are useful for relation extraction (Zhou et al., 2005). For example, the shortest dependency path has been used by several relation extraction models (Bunescu and Mooney, 2005; Miwa and Bansal, 2016). Here we propose a novel method to integrate syntax, without need for prior knowledge on concrete syntactic structures.\nIn particular, many state-of-the-art syntactic parsers use encoder-decoder neural models (Buys and Blunsom, 2015; Kiperwasser and Goldberg, 2016), where the encoder represents the features of the input sentences. For example, LSTM over the input word/tag sequences has been used frequently (Kiperwasser and Goldberg, 2016). The decoder can also leverage partially-parsed results, such as features from partial syntactic trees. Table 1 shows the encoder structures of three state-ofthe-art dependency parsers.\nOur method is to dump the encoder source representations of state-of-the-art parsers, and then use them directly as part of input embeddings in our proposed model. Denoting the dumped syntactic features on each word as hsyn1 h syn 2 · · ·h syn n , we feed them into a non-linear neural layer, 2The non-entity word is treated as a special unit entity to extract segmental features.\n5\n401\n402\n403\n404\n405\n406\n407\n408\n409\n410\n411\n412\n413\n414\n415\n416\n417\n418\n419\n420\n421\n422\n423\n424\n425\n426\n427\n428\n429\n430\n431\n432\n433\n434\n435\n436\n437\n438\n439\n440\n441\n442\n443\n444\n445\n446\n447\n448\n449\n450\n451\n452\n453\n454\n455\n456\n457\n458\n459\n460\n461\n462\n463\n464\n465\n466\n467\n468\n469\n470\n471\n472\n473\n474\n475\n476\n477\n478\n479\n480\n481\n482\n483\n484\n485\n486\n487\n488\n489\n490\n491\n492\n493\n494\n495\n496\n497\n498\n499\nModels Encoder LAS S-LSTM (2015) 1-Layer LSTM 90.9\nK&G (2016) 2-Layer Bi-LSTM 91.9 D&M (2016) 4-Layer Bi-LSTM 93.8\nTable 1: The encoder structures and performances of three state-of-the-art dependency parsers, where S-LSTM (2015) refers to Dyer et al. (2015), K&G (2016) refers to the best parser of Kiperwasser and Goldberg (2016), D&M (2016) refers to Dozat and Manning (2016), and LAS (labeled attachment score) is the major evaluation metric for dependency parsing.\nand then generate two LSTMs (bi-directional) based on the outputs, namely\n−−−−→ LSTMsyn and←−−−− LSTMsyn, respectively, augmenting the original three LSTMs into five LSTMs. Features are extracted from the two new LSTMs in the same way as from the basic bi-directional word LSTMs.\nWe exploit the parser of Dozat and Manning (2016) to extract syntactic features, since it achieves the current best performance for dependency parsing. Our method can be easily generalized to the use of other parsers, which are potentially useful for our task as well. For example, we can use a constituent parser in the same way.\n\n2.4 Training and Search\n\n\n2.4.1 Local Optimization\nPrevious work (Miwa and Bansal, 2016; Gupta et al., 2016) trains model parameters by modeling each step for labeling one input sentence separately. Given a partial table T , we first obtain its neural representation hT , and then compute the next label scores {l1, l2, · · · , ls} using Equation 1. The output scores are regularized into a probability distribution {pl1 , pl2 , · · · , pls} by using a softmax layer. Our training objective is to minimize the cross-entropy loss between this output distribution with the gold-standard distribution:\nloss(T, lgi ,Θ) = − log plgi , (2)\nwhere lgi is the gold-standard next label for T , and Θ is the set of all model parameters. We refer this training method as local optimization, because it maximizes the score of the gold-standard label at each step locally.\nDuring the decoding phase, the greedy search strategy is applied in consistence with the training. At each step, we find the highest-scored label\nAlgorithm 1 Beam-search. agenda← { (empty table, score=0.0) } for i in 1 · · ·max-step\nnext scored tables← { } for scored table in agenda\nlabels← NEXTLABELS(scored table) for next label in labels\nnew← FILL(scored table, next label) ADDITEM(next scored tables, new)\nagenda← TOP-B(next scored tables, B)\nbased on the current partial table, before going on to the next step.\n\n2.4.2 Global Optimization\nWe exploit the global optimization strategy of Andor et al. (2016), maximizing the cumulative score of the gold-standard label sequence for one sentence as a unit. Global optimization has achieved success for several NLP tasks under the neural setting (Zhou et al., 2015; Watanabe and Sumita, 2015). For relation extraction, global learning gives the best performances under the discrete setting (Li and Ji, 2014; Miwa and Sasaki, 2014). We study such models here under the neural setting.\nGiven a label sequence of l1l2 · · · li, the score of Ti is defined as follows:\nscore(Ti) = i∑\nj=0\nscore(Tj−1, lj)\n= score(Ti−1) + score(Ti−1, li),\n(3)\nwhere score(T0) = 0 and score(Ti−1, li) is computed by Equation 1. By this definition, we maximize the scores of all gold-standard partial tables.\nAgain cross-entropy loss is used to perform model updates. At each step i, the objective function is defined by:\nloss(x, T gi ,Θ) = − log pT gi\n= − log score(T gi )∑ T ′i score(T ′i ) ,\n(4)\nwhere x denotes the input sentence, T gi denotes the gold-standard state at step i, and T ′i are all partial tables that can be reached at step i.\nThe major challenge is to compute pT gi , because we cannot traverse all partial tables that are valid at step i, since their count increases exponentially by the step number. We follow Andor\n6\n501\n502\n503\n504\n505\n506\n507\n508\n509\n510\n511\n512\n513\n514\n515\n516\n517\n518\n519\n520\n521\n522\n523\n524\n525\n526\n527\n528\n529\n530\n531\n532\n533\n534\n535\n536\n537\n538\n539\n540\n541\n542\n543\n544\n545\n546\n547\n548\n549\n558\n564\n565\n566\n567\n568\n569\n570\n571\n572\n573\n574\n575\n576\n577\n578\n579\n580\n581\n582\n583\n584\n585\n586\n587\n588\n589\n590\n591\n592\n593\n594\n595\n596\n597\n598\n599\net al. (2016), approximating the probability by using beam search and early-update.\nShown in Algorithm 1, we use standard beam search, using an agenda to maintain B highestscored partially-filled tables at each step. When each action of table filling is taken, all hypotheses in the agenda are expanded by enumerating the next labels, and the B highest-scored resulting tables are used to replace the agenda for the next step. Search begins with the agenda containing an empty table, and finishes when all cells of the tables in the agenda have been filled. When the beam size is 1, the algorithm is the same as greedy decoding. When the beam size is larger than 1, however, error propagation from greedy steps can be alleviated. For training, the same beam search algorithm is applied to training examples, and early-update (Collins and Roark, 2004) is used to fix search errors.\n\n3 Experiments\n\n\n3.1 Data and Evaluation\nWe evaluate the proposed model on two datasets, namely the ACE05 data and the corpus of Roth and Yih (2004) (CONLL04), respectively. The ACE05 data defines seven coarse-grained entity types and six coarse-grained relation categories, while the CONLL04 data defines four entity types and five relation categories.\nFor the ACE05 dataset, we follow Li and Ji (2014) and Miwa and Bansal (2016), splitting and preprocessing the dataset into training, development and test sets.3 For CONLL04, we follow Miwa and Sasaki (2014) to split and preprocess the dataset into training and test corpus, and divide 10% of the training corpus for development.\nWe use the micro F1-measure as the major metric to evaluate model performances, treating an entity as correct when its head region and type are both exactly matched,4 and regard a relation as correct when the argument entities and the relation category are all correct.\n\n3.2 Parameter Tuning\nWe update all model parameters by back propagation using Adam (Kingma and Ba, 2014) with a learning rate 10−3, using gradient clipping by\n3https://github.com/tticoin/LSTM-ER/. 4For the ACE05 dataset, the head region is defined by the corpus, and for the CONLL04 dataset, the head region covers the entire scope of an entity.\na max norm 10 and l2-regularization by a parameter 10−5. The dimension sizes of various vectors in neural network structure are shown in Table 2. All the hyper-parameters are tuned by development experiments. All experiments are conducted under gcc version 4.9.4 (Ubuntu 4.9.4- 2ubuntu1 14.04.1), on an Intel(R) Xeon(R) CPU E5-2670 @ 2.60GHz.\nOnline training is used to learn model parameters, traversing over the entire training examples by 300 iterations. We select the best iteration model according to the development results. In particular, we exploit pre-training techniques to learn better model parameters (Wiseman and Rush, 2016). For the local model, we follow Miwa and Bansal (2016), training parameters only for entity detection during the first 20 iterations. For the global model, we pretrain our model using local optimization for 40 iterations, before conducting beam global optimization.\n\n3.3 Development Experiments\nWe conduct several development experiments on the ACE05 development dataset.\n\n3.3.1 Feature Ablation Tests\nWe consider the baseline system with no syntactic features using local training. Compared with Miwa and Bansal (2016), we introduce characterlevel features, and in addition exploit segmental features for entity detection. Feature ablation experiments are conducted for the two types of features. As shown in Table 3, without character-level features, the F-scores of entity detection and relation classification decrease 0.6% and 0.7%, respectively. Without segmental features for entity\n7\n601\n602\n603\n604\n605\n606\n607\n608\n609\n610\n611\n612\n613\n614\n615\n616\n617\n618\n619\n620\n621\n622\n623\n624\n625\n626\n627\n628\n629\n630\n631\n632\n633\n634\n635\n636\n637\n638\n639\n640\n641\n642\n643\n644\n645\n646\n647\n648\n649\n655\n663\n664\n665\n666\n667\n668\n669\n670\n671\n672\n673\n674\n675\n676\n677\n678\n679\n680\n681\n682\n683\n684\n685\n686\n687\n688\n689\n690\n691\n692\n693\n694\n695\n696\n697\n698\n699\nModel Beam F1 Speed Local 1 50.9 95.6\nLocal(+SS) 1 51.2 95.1\nGlobal 1 51.4 95.3 3 51.8 52.0 5 52.6 36.9\nTable 4: Comparisons between local and global models, where SS denotes scheduled sampling, and speed is measured by the number of sentences per second.\ndetection, the baseline model loses 1.3% in entity detection, which results in an error propagation of 1.1% for relation classification. The results demonstrate that the two types of new features we use are useful for relation extraction.\n\n3.3.2 Local v.s. Global\nWe study the influence of training strategies for the relation extraction model without using syntactic features. For the local model, we apply the scheduled sampling strategy (Bengio et al., 2015), which has been shown to improve the performances by Miwa and Bansal (2016).\nTable 4 shows the relation F1 scores. Scheduled sampling achieves improved F-measure scores for the local model. With the same greedy search strategy, the globally normalized model gives slightly better results than the local model with scheduled sampling. The performance of the global model increases with a larger beam size. However, the decoding speed becomes intolerably slow when the beam size increases beyond 5. Thus we exploit a beam size of 5 for global training considering both performance and efficiency.\n\n3.3.3 Syntactic Features\nWe examine the effectiveness of the proposed syntactic features. Table 5 shows the developmental results using both local and global optimization. The proposed features improve the relation performances significantly under both settings, where the p-values are below 10−4 by using pairwise ttest, demonstrating that our use of syntactic features is highly effective for relation extraction.\n\n3.4 Final Results\nTable 6 shows the final results on the test datasets of ACE05 and CONLL04. We show several topperforming systems in the table as well, where M&B (2016) refers to Miwa and Bansal (2016), who exploit end-to-end LSTM neural networks\nwith local optimization, and L&J (2014) and M&S (2014) refer to Li and Ji (2014) and Miwa and Sasaki (2014), respectively, which are both globally optimized models using discrete features, giving the top F-scores among statistical models.5\nOverall, neural models give better performances than statistical models, and global optimization can give improved performances as well. Our final model achieves the best performances on both datasets. Compared with the best reported results, our model gives improvements of 1.9% on ACE05, and 6.8% on CONLL04.\n\n3.5 Analysis\nWe conduct analysis on the ACE05 test dataset in order to understand our models in depth. We focus on two major contributions by our model, first examining the influences of global optimization, and then studying the gains by using the proposed syntactic features.\nGlobal optimization aims to find the best label sequences, rather than the best label locally at each step. Thus intuitively global optimization should give better accuracies at the sentence level. We verify this by examining the sentence-level accuracies, where one sentence is regarded as correct when all the labels in the resulted table are correct. Figure 6 shows the result, which is consistent with our intuition. On the one hand, the sentence-\n5Gupta et al. (2016) proposed a locally optimized model but used a different test dataset from CONLL04 and a different evaluation method, reporting entity and relation F-scores of 93.6% and 72.1%, respectively. Their results are not directly comparable to the results in Table 6. In particular, they regard an entity as correct if at least one token is tagged correctly, which influences the results significantly since multiword entities accounts for over 50% of all entities.\n8\n701\n702\n703\n704\n705\n706\n707\n708\n709\n710\n711\n712\n713\n714\n715\n716\n717\n718\n719\n720\n721\n722\n723\n724\n725\n726\n727\n728\n729\n730\n731\n732\n733\n734\n735\n736\n737\n738\n739\n740\n741\n742\n743\n744\n745\n746\n747\n748\n749\n750\n751\n752\n753\n754\n755\n756\n757\n758\n759\n760\n761\n762\n763\n764\n765\n766\n767\n768\n769\n770\n771\n772\n773\n774\n775\n776\n777\n778\n779\n780\n781\n782\n783\n784\n785\n786\n787\n788\n789\n790\n791\n792\n793\n794\n795\n796\n797\n798\n799\n5 10 15 20 25 ≥30 20\n40\n60\n80 100 A cc ur ac y( % )\nglobal local\nFigure 6: Sentence-level accuracies with respect to sentence length.\n1 2 3 4 5 ≥6 30\n40\n50\n60\n70\nFsc\nor e(\n% )\n+syn -syn\nFigure 7: F-scores with respect to the distance between entity pairs.\nlevel accuracies of the globally normalized model are consistently better than the local model. On the other hand, the accuracy decreases sharply as the sentence length increases, with the local model suffering more severely from larger sentences.\nTo understand the effectiveness of the proposed syntactic features, we examine the relation Fscores with respect to entity distances. Miwa and Bansal (2016) exploit the shortest dependency path, which can make the distance between two entities closer compared with their sequential distance, thus facilitating relation extraction. We verify whether the proposed syntactic features can benefit our model similarly. As shown in Figure 7, the F-scores of entity-pairs with large distances see apparent improvements, demonstrating that our use of syntactic features has a similar effect compared to the shortest dependency path.\n\n4 Related Work\nEntity recognition (Florian et al., 2004, 2006; Ratinov and Roth, 2009; Florian et al., 2010; Kuru et al., 2016) and relation extraction (Zhao and Grishman, 2005; Jiang and Zhai, 2007; Zhou et al., 2007; Qian and Zhou, 2010; Chan and Roth, 2010; Sun et al., 2011; Plank and Moschitti, 2013; Verga et al., 2016) have received much attention in the NLP community. The dominant methods treat the two tasks separately, where relation extraction is performed assuming that entity boundaries have been given (Zelenko et al., 2003; Miwa et al.,\n2009; Chan and Roth, 2011; Lin et al., 2016). Several studies find that extracting entities and relations jointly can benefit both tasks. Early work conducts joint inference for separate models (Ji and Grishman, 2005; Roth and Yih, 2004, 2007). Recent work shows that joint learning and decoding with a single model brings more benefits for the two tasks (Li and Ji, 2014; Miwa and Sasaki, 2014; Miwa and Bansal, 2016; Gupta et al., 2016), and we follow this line of work in the study.\nLSTM features have been extensively exploited for NLP tasks, including tagging (Huang et al., 2015; Lample et al., 2016), parsing (Kiperwasser and Goldberg, 2016; Dozat and Manning, 2016), relation classification (Xu et al., 2015; Vu et al., 2016; Miwa and Bansal, 2016) and sentiment analysis (Li et al., 2015; Teng et al., 2016). Based on the output of LSTM structures, Wang and Chang (2016) introduce segmental features, and apply it to dependency parsing. The same method is applied to constituent parsing by Cross and Huang (2016). We exploit this segmental representation for relation extraction.\nGlobal optimization and normalization has been successfully applied on many NLP tasks that involve structural prediction (Lafferty et al., 2001; Collins, 2002; McDonald et al., 2010; Zhang and Clark, 2011), using traditional discrete features. For neural models, it has recently received increasing interests (Zhou et al., 2015; Andor et al., 2016; Xu, 2016; Wiseman and Rush, 2016), and improved performances can be achieved with global optimization accompanied by beam search. Our work is in line with these efforts. To our knowledge, we are the first to apply globally optimized neural models for end-to-end relation extraction, achieving the best results on standard benchmarks.\n\n5 Conclusion\nWe proposed a novel relation extraction model using neural network, based on the table-filling framework proposed by Miwa and Sasaki (2014). Feature representations are learned from several LSTM structures over the inputs, and a simple method is used to integrate syntactic information into our model without the need of parser outputs. In addition, global optimization is taken to make use of structural information more effectively. Compared with previous work, our final model achieved the best performances on two benchmark datasets.\n9\n801\n802\n803\n804\n805\n806\n807\n808\n809\n810\n811\n812\n813\n814\n815\n816\n817\n818\n819\n820\n821\n822\n823\n824\n825\n826\n827\n828\n829\n830\n831\n832\n833\n834\n835\n836\n837\n838\n839\n840\n841\n842\n843\n844\n845\n846\n847\n848\n849\n850\n851\n852\n853\n854\n855\n856\n857\n858\n859\n860\n861\n862\n863\n864\n865\n866\n867\n868\n869\n870\n871\n872\n873\n874\n875\n876\n877\n878\n879\n880\n881\n882\n883\n884\n885\n886\n887\n888\n889\n890\n891\n892\n893\n894\n895\n896\n897\n898\n899\n",
    "rationale": "- Strengths:\n\n - The paper is clearly written and well-structured. \n\n - The system newly applied several techniques including global optimization to\nend-to-end neural relation extraction, and the direct incorporation of the\nparser representation is interesting.\n\n - The proposed system has achieved the state-of-the-art performance on both\nACE05 and CONLL04 data sets.\n\n - The authors include several analyses.\n\n- Weaknesses:\n\n - The approach is incremental and seems like just a combination of existing\nmethods.  \n\n - The improvements on the performance (1.2 percent points on dev) are\nrelatively small, and no significance test results are provided.\n\n- General Discussion:\n\n- Major comments:\n\n - The model employed a recent parser and glove word embeddings. How did they\naffect the relation extraction performance?\n\n - In prediction, how did the authors deal with illegal predictions?\n\n- Minor comments:\n\n - Local optimization is not completely \"local\". It \"considers structural\ncorrespondences between incremental decisions,\" so this explanation in the\nintroduction is misleading.\n\n - Points in Figures 6 and 7 should be connected with straight lines, not\ncurves.\n\n - How are entities represented in \"-segment\"?\n\n - Some citations are incomplete. Kingma et al. (2014) is accepted to ICLR,\nand Li et al. (2014) misses pages.",
    "rating": 3
  },
  {
    "title": "Morphology Generation for Statistical Machine Translation using Deep Learning Techniques",
    "abstract": "Morphology in unbalanced languages remains a big challenge in the context of machine translation. In this paper, we propose to de-couple machine translation from morphology generation in order to better deal with the problem. We investigate the morphology simplification with a reasonable trade-off between expected gain and generation complexity. For the Chinese-Spanish task, optimum morphological simplification is in gender and number. For this purpose, we design a new classification architecture which, compared to other standard machine learning techniques, obtains the best results. This proposed neural-based architecture consists of several layers: an embedding, a convolutional followed by a recurrent neural network and, finally, ends with sigmoid and softmax layers. We obtain classification results over 98% accuracy in gender classification, over 93% in number classification, and an overall translation improvement of 0.7 METEOR.",
    "text": "1 Introduction\nMachine Translation (MT) is evolving from different perspectives. One of the most popular paradigms is still Statistical Machine Translation (SMT), which consists in finding the most probable target sentence given the source sentence using probabilistic models based on co-ocurrences. Recently, deep learning techniques applied to natural language processing, speech recognition and image processing and even in MT have reached quite successful results. Early stages of deep learning applied to MT include using neural language modeling for rescoring (Schwenk et al., 2007). Later,\ndeep learning has been integrated in MT in many different steps (Zhand and Zong, 2015). Nowadays, deep learning has allowed to develop an entire new paradigm, which within one-year of development has achieved state-of-the-art results (Jean et al., 2015) for some language pairs.\nIn this paper, we are focusing on a challenging translation task, which is Chinese-to-Spanish. This translation task has the characteristic that we are going from an isolated language in terms of morphology (Chinese) to a fusional language (Spanish). This means that for a simple word in Chinese (e.g. 鼓励 ), the corresponding translation has many different morphology inflexions (e.g. alentar, alienta, alentamos, alientan ...), which depend on the context. It is still difficult for MT in general (no matter which paradigm) to extract information from the source context to give the correct translation.\nWe propose to divide the problem of translation into translation and then a postprocessing of morphology generation. This has been done before, e.g. (Toutanova et al., 2008; Formiga et al., 2013), as we will review in the next section. However, the main contribution of our work is that we are using deep learning techniques in morphology generation. This gives us significant improvements in translation quality.\nThe rest of the paper is organised as follows. Section 2 describes the related work both in morphology generation approaches and in ChineseSpanish translation. Section 3 overviews the phrase-based MT approach together with an explanation of the divide and conquer approach of translating and generating morphology. Section 4 details the architecture of the morphology generation module and it reports the main classification techniques that are used for morphology generation. Section 5 describes the experimental framework. Section 6 reports and discusses both clas-\n2\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n112\n113\n114\n115\n116\n117\n118\n119\n120\n121\n122\n123\n124\n125\n126\n127\n128\n129\n130\n131\n132\n133\n134\n135\n136\n137\n138\n139\n140\n141\n142\n143\n144\n145\n146\n147\n148\n149\n150\n151\n152\n153\n154\n155\n156\n157\n158\n159\n160\n161\n162\n163\n164\n165\n166\n167\n168\n169\n170\n171\n172\n173\n174\n175\n176\n177\n178\n179\n180\n181\n182\n183\n184\n185\n186\n187\n188\n189\n190\n191\n192\n193\n194\n195\n196\n197\n198\n199\nsification and translation results, which show significant improvements. Finally, section 7 summarises the main conclusions and further work.\n\n2 Related Work\nIn this section we are reviewing the previous related works on morphology generation for MT and on Chinese-Spanish MT approaches.\nMorphology generation There have been many works in morphological generation and some of them are in the context of the application of MT. In this cases, MT is faced in two-steps: first step where the source is translated to a simplified target text that has less morphology variation than the original target; and then, second step, a postprocessing module (morphology generator) adds the proper inflections. To name a few of these works, for example, (Toutanova et al., 2008) build maximum entropy markov models for inflection prediction of stems; (Clifton and Sarkar, 2011) and (Kholy and Habash, 2012) use conditional random fields (CFR) to predict one or more morphological features; and (Formiga et al., 2013) use Support Vector Machines (SVMs) to predict verb inflections. Other related works are in the context of Part-of-Speech (PoS) tagging generation such as (Giménez and Màrquez, 2004) in which a model is trained to predict each individual fragment of a PoS tag by means of machine learning algorithms. The main difference is that in PoS tagging the word itself has information about morphological inflection, whereas in our task, we do not have this information.\nIn this paper, we use deep learning techniques to morphology generation or classification. Based on the fact that Chinese does not have number and gender inflections and Spanish does, (Costajussà, 2015) show that simplification in gender and number has the best trade-off between improving translation and keeping the morphology generation complexity at a low level.\nChinese-Spanish There are few works in Chinese-Spanish MT despite being two of the most spoken languages in the world. Most of these works are based on comparing different pivot strategies like standard cascade or pseudo-corpus (Costa-jussà et al., 2012). Also it is important to mention that, in 2008, there were two tasks organised by the popular IWSLT\nevaluation campaign1 (International Workshop on Spoken Language Translation) between these two languages (Paul, 2008). The first task was based on a direct translation for Chinese-Spanish. The second task provided corpus in Chinese-English and English-Spanish and asked participants to provide Chinese-Spanish translation through pivot techniques. The second task obtained better results than direct translation because of the larger corpus provided. Differently, (Costa-jussà and Centelles, 2016) present the first rule-based MT system for Chinese to Spanish. Authors describe a hybrid method for constructing this system taking advantage of available resources such as parallel corpora that are used to extract dictionaries and lexical and structural transfer rules. Finally, it is worth mentioning that novel successful neural approximations (Jean et al., 2015), already mentioned in the introduction, have not yet achieved state-of-the-art results for this language pair (Costa-jussà et al., 2017).\n\n3 Machine Translation Architecture\nIn this section, we review the baseline system which is a standard phrase-based MT system and explain the architecture that we are using by dividing the problem of translation into: morphologically simplified translation and morphology generation.\n\n3.1 Phrase-based MT baseline system\nThe popular phrase-based MT system (Koehn et al., 2003) focuses on finding the most probable target text given a source text. In the last 20 years, the phrase-based system has dramatically evolved introducing new techniques and modifying the architecture; for example, replacing the noisychannel for the log-linear model which combines a set of feature functions in the decoder, including the translation and language model, the reordering model and the lexical models. There is a widely used open-source software, Moses (Koehn et al., 2007), which englobes a large community that helps in the progress of the system. As a consequence, phrase-based MT is a commoditized technology used at the academic and commercial level. However, there are still many challenges to solve, such as morphology generation.\n1http://iwslt2010.fbk.eu\n3\n201\n202\n203\n204\n205\n206\n207\n208\n209\n210\n211\n212\n213\n214\n215\n216\n217\n218\n219\n220\n221\n222\n223\n224\n225\n226\n227\n228\n229\n230\n231\n232\n233\n234\n235\n236\n237\n238\n239\n240\n241\n242\n243\n244\n245\n246\n247\n248\n249\n250\n251\n252\n253\n254\n255\n256\n257\n258\n259\n260\n261\n262\n263\n264\n265\n266\n267\n268\n269\n270\n271\n272\n273\n274\n275\n276\n277\n278\n279\n280\n281\n282\n283\n284\n285\n286\n287\n288\n289\n290\n291\n292\n293\n294\n295\n296\n297\n298\n299\n\n3.2 Divide and conquer MT architecture: simplified translation and morphology generation\nMorphology generation is not always achieved in the standard phrase-based system. This may be due to the fact that phrase-based MT uses a limited source context information to translate. Therefore, we are proposing to follow a similar strategy to previous works (Toutanova et al., 2008; Formiga et al., 2013), where authors do a first translation from source to a morphology-based simplified target and then, use the morphology generation module that transforms the simplified translation into the full form output.\n\n4 Morphological Generation Module\nIn order to design the morphology generation module, we have to decide the morphology simplification we are applying to the translation. Since we are focusing on Chinese-to-Spanish task and based on (Costa-jussà, 2015), the simplification which achieves the best trade-off among highest translation gain and lowest complexity of morphological generation is the simplification in number and gender. Table 1 shows examples of this simplification. The main challenge of this task is that number and gender are generated from words where this inflection information (both number and gender) has been removed beforehand.\nWith these results at hand, we propose an architecture of the morphology generation module, which is language independent and it is easily generalizable to other simplification schemes.\nThe morphology generation procedure is summarised as follows and further detailed in the next subsections.\n• Feature selection. We have investigated different set of features including information from both source and target languages.\n• Classification. We propose a new deep learning classification architecture composed of different layers.\n• Rescoring and rules. We generate different alternatives of the classification output and rerank them using a language model. After, we use hand-crafted rules that allow to solve some specific problems.\nThis procedure is depicted in Figure 1, in which we can see that each of the above processes gener-\nFigure 1: Block Diagram for Morphology Generation\nates the needed input for the next step. Figure also shows in red the main subprocesses that have been developed on this work.\n\n4.1 Feature selection\nWe propose to compare several features for recovering morphological information. Given that both Chinese and simplified Spanish languages do not contain explicit morphology information, we start by simply using windows of words as source of information. We follow Collobert’s approach in which each word is represented by a fixed size window of words in which the central element is the one to classify (Collobert et al., 2011).\nIn our case, we experiment with three different inputs: (1) Chinese window; (2) simplified Spanish window; (3) Spanish window adding information about its correspondant word in Chinese, i.e. information about pronouns and the number of characters in the word. The main advantage of the second one is that it is not dependant on the alignment file generated during translation.\nOur classifiers did not have to train all types of words. Some types of words, such as prepositions (a, ante, cabo, de...), do not have gender or number. Therefore our system was trained using only determiners, adjectives, verbs, pronouns and nouns which are the ones that present morphology\n4\n301\n302\n303\n304\n305\n306\n307\n308\n309\n310\n311\n312\n313\n314\n315\n316\n317\n318\n319\n320\n321\n322\n323\n324\n325\n326\n327\n328\n329\n330\n331\n332\n333\n334\n335\n336\n337\n338\n339\n340\n341\n342\n343\n344\n345\n346\n347\n348\n349\n350\n351\n352\n353\n354\n355\n356\n357\n358\n359\n360\n361\n362\n363\n364\n365\n366\n367\n368\n369\n370\n371\n372\n373\n374\n375\n376\n377\n378\n379\n380\n381\n382\n383\n384\n385\n386\n387\n388\n389\n390\n391\n392\n393\n394\n395\n396\n397\n398\n399\nEsnum decidir[VMIP3N0] examinar[VMN0000] el[DA0MN0] c uestión[NCFN000] en[SPS00] el[DA0MN0] perı́odo[NCM N000] de[SPS00] sesión[NCFN000] el[DA0MN0] tema[NCMN000] titular [AQ0MN0] “[Fp] cuestión[NCFN000] relativo[AQ0FN0] a[SPS00] el[DA0MN0] derecho[NCMN000] humano[AQ0MN0] “[Fp] .[Fp] Esgen decidir[VMIP3S0] examinar[VMN0000] el[DA0GS0] cuestión [NCGS000] en[SPS00] el[DA0GS0] perı́odo[NCGS000] de [SPS00] sesión[NCGS000] el[DA0GS0] tema[NCGS000] titular [AQ0GS0] “[Fp] cuestión[NCGS000] relativo[AQ0GS0] a[SPS00] el[DA0GS0] derecho[NCGS000] humano[AQ0GS0] “[Fp] .[Fp] Esnumgen decidir[VMIP3N0] examinar[VMN0000] el[DA0GN0 ] cuestión[NCGN000] en[SPS00] el[DA0GN0] perı́odo[NCGN000] de[SPS00] sesión[NCGN000] el[DA0GN0] tema[NCGN000] titular [AQ0GN0] “[Fp] cuestión[NCGN000] relativo[AQ0GN00 ] a[SPS00] el[DA0GN0] derecho[NCGN000] humano[AQ0GN0] “[Fp] .[Fp] Es Decide examinar la cuestión en el perı́odo de sesiones el tema titulad o “ Cuestiones relativas a los derechos humanos ” .\nTable 1: Example of Spanish simplification into number, gender and both\n.\nvariations in gender or number. However, note that all types of words are used in the windows.\n\n4.2 Classification architecture\nDescription We propose to train two different models: one to retrieve gender and another to retrieve number. Each model decides among three different classes. Classes for gender classifier are masculine (M ), femenine (F ) and none (N ); and classes for number classifier are singular (S), plural (P ) and none (N ) 2. Again, we inspire our architecture in previous Collobert’s proposal and we modify it by adding a recurrent neural network. This recurrent neural network is relevant because it keeps information about previous elements in a sequence and, in our classification problem, context words are very relevant. As a recurrent neural network, we use a Long Short Term Memory (LSTM) (Hochreiter and Schmidhuber, 1997) that is proven efficient to deal with sequence NLP challenges (Sutskever et al., 2014). This kind of recurrent neural network is able to maintain information for several elements in the sequence and to forget it when needed. Figure 2 shows an overview of the different layers involved in the final classification architecture, which are detailed as follows:\nEmbedding. We represent each word as its index in the vocabulary, i.e. every word is represented as one discrete value: E(w) = W,W ∈ Rd, w being the index of the word in the sorted vocabulary and d the user chosen size of the array. Then, each word is represented as a numeric array and each window is a matrix.\nConvolutional. We add a convolutional neural network. This step allows the system to detect some common patterns between the different\n2None means that there is no need to specify gender or number information because the word is invariant in these terms. This happens for types of words (determiners, nouns, verbs, pronouns and adjectives) that can have gender and number in other cases.\nwords. This layer’s input consists in W l matrix of multidimensional arrays of size n · d, where n is the window length (in words) and d is the size of the array created by the previous embedding layer. This layer’s output is a matrix of the same size as the input.\nMax Pooling. This layer allows to extract most relevant features from the input data and reduces feature vectors to half.\nLSTM. Each feature array is treated individually, generating a fixed size representation hi of the ith word using information of all the previous words (in the sequence). This layer’s output, h, is the result of the last element of the sequence using information from all previous words.\nSigmoid. This layer smoothes results obtained by previous layer and compresses results to the interval [−1, 1]. This layer’s input is a fixed size vector of shape 1 ·n where n is the number of neurons in the previous LSTM layer. This layer’s output is a vector of length c equal to the number of classes to predict.\nSoftmax. This layer allows to show results as probabilities by ensuring that the returned value of each class belongs to the [0, 1) interval and all classes add up 1.\nMotivation Our input data is PoS tagged and morphogically simplified before the classification architecture which largely reduces the information that can be extracted from individual words in the vocabulary. In addition, we can encounter out-ofvocabulary words for which no morphological information can be extracted.\nThe main source of information available is the context in which the word can be found in the sentence. Considering the window as a sequence enforces the behaviour a human would have while reading the sentence. The information of a word consists in itself and the words that surround it. Sometimes information preceeds the word and sometimes information is after the word. Words\n5\n401\n402\n403\n404\n405\n406\n407\n408\n409\n410\n411\n412\n413\n414\n415\n416\n417\n418\n419\n420\n421\n422\n423\n424\n425\n426\n427\n428\n429\n430\n431\n432\n433\n434\n435\n436\n437\n438\n439\n440\n441\n442\n443\n444\n445\n446\n447\n448\n449\n460\n461\n462\n463\n464\n465\n466\n467\n468\n469\n470\n471\n472\n473\n474\n475\n476\n477\n478\n479\n480\n481\n482\n483\n484\n485\n486\n487\n488\n489\n490\n491\n492\n493\n494\n495\n496\n497\n498\n499\nFigure 2: Neural network overview.\n(like adjetives), which are modifying or complementing another word, generally take information from preceeding words. For example, in the sequence casa blanca, the word blanca could also be blanco, blancos or blancas but because noun and adjective are required to have gender and number agreement, the femenine word casa forces the femenine for blanca. While, for example, determiners usually take information from posterior words. This fact motivates that the word to classify has to be placed at the center of the window.\nFinally, given that we rely only on the context information since words themselves may not have any information, makes the recurrent neural network a key element in our architecture. The output h of the layer can be considered a context vector of the whole window maintaining information of all the previously encountered words (in the same window).\n\n4.3 Rescoring and rules\nAt this point in the pipeline, we have two models (gender and number) that allow us to generate the full Part-of-Speech (PoS) tag by combining the best results of both classifiers.\nHowever, in order to improve the overall performance, we add a rescoring step followed by some hand-crafted rules. To generate the different alternatives, we represent every sentence as a graph (see Figure 3), with the following properties:\n• Each word is represented as a layer of the graph and each node represents a class of the\nclassification model.\n• A node only has edges with all the nodes of the next layer. This way we force the original sentence order.\n• An edge’s weight is the probability given by the classification model.\n• Each accepted path in the graph starts in the first layer and ends in the last one. This acyclic structure finds the best path in linear time, due to the fact that it goes through all layers and it picks the node with the greatest weight. One layer can have either 1 element (the word does not need to be classified, e.g. prepositions) or 3 elements (the word needs to be classified among the three number or gender categories).\n• Add the weight of a previously trained target language model.\nWe used Yen’s algorithm (Yen, 1971) to find the best path, which has an associated cost of O(KN2logN), being K the number of paths to find.\nSee pseudo code in Algorithm 1, where A is the set paths chosen in the graph. The algorithm ends when this A set contains K paths or no further paths available to explore. B contains all suboptimal paths that can be elected in future iterations.\nThere are two special cases that the models were not able to treat and we apply specific rules: (1) conjunctions y and o are replaced by e and u if they are placed in front of vowels. This could not be generated during translation because both words share the same tag and lemma; (2) verbs with a pronoun as a suffix, producirse, second to\n6\n501\n502\n503\n504\n505\n506\n507\n508\n509\n510\n511\n512\n513\n514\n515\n516\n517\n518\n519\n520\n521\n522\n523\n524\n525\n526\n527\n528\n529\n530\n531\n532\n533\n534\n535\n536\n537\n538\n539\n540\n541\n542\n543\n544\n545\n546\n547\n548\n549\n550\n551\n552\n553\n554\n555\n556\n557\n558\n559\n560\n561\n562\n563\n564\n565\n566\n567\n568\n569\n570\n571\n572\n573\n574\n575\n576\n577\n578\n579\n580\n581\n582\n583\n584\n585\n586\n587\n588\n589\n590\n591\n592\n593\n594\n595\n596\n597\n598\n599\nlast syllabe stretched (palabras llanas) and ending in a vowel are not accentuated. However, after adding the suffix, these words should be accentuated because they become palabras esdrújulas, which happen to be always accentuated.\nData: G Graph of the sentence, K Result: best k paths in G initialization; A[0] = bestPath(G,0,final); B = [] ; i = 0; for i <K do\nfor i in range(0, len(A[K-1])-1) do spurNode = A[K-1][i]; root = A[K-1][0;i]; for path in A do\nif root = path[0:i] then remove edge(i-1,i) from G;\nend end for node in root and node != spurNode do\nremoves node node from G; end spurPath = bestPath(G,spurnode, final) totalPath = root + spurPath; B.append(totalPath); restore edges from G; restore nodes from G; if B is empty then\nbreak; end B.sort(); A.append(B[0]); B.pop();\nend end\nAlgorithm 1: Pseudo-code for k-best paths generation.\n\n5 Experimental framework\nIn this section, we describe the data used for experimentation together with the corresponding preprocessing. In addition, we detail chosen parameters for the MT system and the classification algorithm.\n\n5.1 Data and preprocessing\nOne of the main contributions of this work is using the Chinese-Spanish language pair. In the last years, there has appeared more and more resources for this language pair available in (Ziemski et al.,\nL Set S W V ES Train Small 58.6K 2.3M 22.5K Large 3.0M 51.7M 207.5K\nDevelopment 990 43.4K 5.4k Test 1K 44.2K 5.5K\nZH Train Small 58.6K 1.6M 17.8K Large 3.0M 43.9M 373.5K\nDevelopment 990 33K 3.7K Test 1K 33.7K 3.8K\nTable 2: Corpus Statistics. Number of sentences (S),words (W), vocabulary (V). M stands for millions and K stands for thousands.\n2016) or from TAUS corporation3. Therefore, differently from previous works on this language pair, we can test our approach in both a small and large data sets.\n• A small training corpus by using the United Nations Corpus (UN) (Rafalovitch and Dale, 2009).\n• A large training corpus by using, in addition to the UN corpus, the TAUS corpus, the Bible corpus (Chew et al., 2006) and the BTEC (Basic Traveller Expressions Corpus) (Takezawa, 2006). The TAUS corpus is around 2,890,000 sentences, the Bible corpus about 30,000 sentences and the BTEC corpus about 20,000 sentences.\nCorpus statistics are shown in Table 2. Development and test sets are taken from UN corpus.\nCorpus preprocessing consisted in tokenization, filtering empty sentences and longer than 50 words, Chinese segmentation by means of the ZhSeg (Dyer, 2016), Spanish lowercasing, filtering pairs of sentences with more than 10% of nonChinese characters in the Chinese side and more than 10% of non-Spanish characters in the Spanish side. Spanish PoS tagging was done using Freeling (Padró and Stanilovsky, 2012). All chunking or name entity recognition was disabled to preserve the original number of words.\n\n5.2 MT Baseline\nMoses has been trained using default parameters, which include: grow-diag-final word alignment symmetrization, lexicalized reordering, relative frequencies (conditional and posterior probabilities) with phrase discounting, lexical weights, phrase bonus, accepting phrases up to length 10, 5-gram language model with kneser-ney smoothing, word bonus and MERT optimisation.\n\n5.3 Classification parameters\nTo generate the classification architecture we used the library keras (Chollet, 2015) for creating and ensambling the different layers. Using NVIDIA GTX Titan X GPUs with 12GB of memory and 3072 CUDA Cores, each classifier is trained on aproximately 1h and 12h for the small and large corpus, respectively.\n3http://www.taus.net\n7\n601\n602\n603\n604\n605\n606\n607\n608\n609\n610\n611\n612\n613\n614\n615\n616\n617\n618\n619\n620\n621\n622\n623\n624\n625\n626\n627\n628\n629\n630\n631\n632\n633\n634\n635\n636\n637\n638\n639\n640\n641\n642\n643\n644\n645\n646\n647\n648\n649\n650\n651\n652\n653\n654\n655\n656\n657\n658\n659\n660\n661\n662\n663\n664\n665\n666\n667\n668\n669\n670\n671\n672\n673\n674\n675\n676\n677\n678\n679\n680\n681\n682\n683\n684\n685\n686\n687\n688\n689\n690\n691\n692\n693\n694\n695\n696\n697\n698\n699\nRegarding classification parameters, experimentation has shown that number and gender classification tasks have different requirements. Table 3 summarizes these parameters. The best window size is 9 and 7 words for number and gender, respectively. In both cases increasing this size lowers the accuracy of the system. The vocabulary size is fixed as a trade-off between giving enough information to the system to perform the classification while removing enough words to train the classifier for unknown words. The embedding size of 128 results in stable training, while further increasing this value augmented the training time and hardware cost. The filter size in the convolutional layer reached best results when it was slightly smaller than the window size, being 7 and 5 the best values for number and gender classification, respectively. Finally, increasing LSTM nodes up to 70 improved significantly for both classifiers.\nTable 3: Values of the different parameters of the classifiers\nParameter Small Large Num Gen Num Gen Window size 9 7 9 7 Vocabulary size 7000 9000 15000 15000 Embedding 128 128 128 128 Filter size 7 5 7 5 LSTM nodes 70 70 70 70\nFor windows, we only used the simplified Spanish translation. In Table 4 we can see that testing different sources of information with the classifier of number for the small corpus. Adding Chinese has a negative effect in the classifier accuracy.\n\n5.4 Rescoring and Full form generation\nAs a rescoring tool, we use the one available in Moses 4. We trained a standard n-gram language model with the SRILM toolkit (Stolcke, 2002).\nIn order to generate the final full form we use the full PoS tag, generated from the postprocessing step, and the lemma, taken from the morphology-simplified translation output. Then, we use the vocabulary and conjugations rules pro-\n4https://github.com/mosessmt/mosesdecoder/tree/master/scripts/nbest-rescore\nTable 4: Accuracy of the classifier of number using different sources of information.\nFeatures Accuracy (%) Chinese window 72 Spanish window 93,4 Chinese + Spanish window 86\nvided by Freeling. Freeling’s coverage raises the 99%. When a word is not found in the dictionary, we test all gender and/or number inflections in descendant order of probability until a match is found. If none matched, the lemma is used as translation, which usually happens only in the case of cities or demonyms.\n\n6 Evaluation\nIn this section we discuss the results obtained both in classification and in the final translation task.\nTable 5 shows results for the classification task both number and gender and with the different corpus sets. We have contrasted our proposed classification architecture based on neural networks with standard machine learning techniques such as linear, cuadratic and sigmoid kernels SVMs (Cortes and Vapnik, 1995), random forests (Breiman, 2001), convolutional(Fukushima, 1980) and LSTM(Hochreiter and Schmidhuber, 1997) neural networks (NN). All algorithms were tested using features and parameters described in previous sections with the exception of random forests in which we added the one hot encoding representation of the words to the features.\nWe observe that our proposed architecture achieves by large the best results in all tasks. It is also remarkable that the accuracy is lower using the bigger corpus, this is due to the fact that the small set consisted in texts of the same domain and the vocabulary had a better representation of specific words such as country names.\nTable 5: Classification results. In bold, best results. Num stands for Number and Gen, for Gender\nAlgorithm Small Large Num Gen Num Gen Naive Bayes 61.3 53.5 61.3 53.5 Lineal kernel SVM 68.1 71.7 65.8 69.3 Cuadratic kernel SVM 77.8 81.3 77.6 82.7 Sigmoid kernel SVM 83,1 87.4 81.5 84.2 Random Forest 81.6 91.8 77.8 88.1 Convolutional NN 81.3 93.9 83.9 94.2 LSTM NN 68.1 73.3 70.8 71.4 CNN + LSTM 93.7 98.4 88.9 96.1\nTabla 6 shows translation results. We show both the Oracle and the result in terms of METEOR (Banerjee and Lavie, 2005). We observe improvement in most cases (when classifying number, gender, both and rescoring), but best results are obtained when classifying number and gender and rescoring number in the large corpus, obtaining a\n8\n701\n702\n703\n704\n705\n706\n707\n708\n709\n710\n711\n712\n713\n714\n715\n716\n717\n718\n719\n720\n721\n722\n723\n724\n725\n726\n727\n728\n729\n730\n731\n732\n733\n734\n735\n736\n737\n738\n739\n740\n741\n742\n743\n744\n745\n746\n747\n748\n749\n750\n751\n752\n753\n754\n755\n756\n757\n758\n759\n760\n761\n762\n763\n764\n765\n766\n767\n768\n769\n770\n771\n772\n773\n774\n775\n776\n777\n778\n779\n780\n781\n782\n783\n784\n785\n786\n787\n788\n789\n790\n791\n792\n793\n794\n795\n796\n797\n798\n799\nTable 6: METEOR results. In bold, best results. Num stands for Number and Gen, for Gender\nSet System UN Oracle Result Small Baseline - 55.29 +Num 55.60 55.35 +Gen 55.45 55.39 +Num&Gen 56.81 55.48 +Num&Gen +Rescoring(Num&Gen) - 54.91 +Num&Gen +Rescoring(Num) - 55.56 Large Baseline - 56.98 +Num 58.87 57.51 +Gen 57.56 57.32 +Num&Gen 62.41 57.13 +Num&Gen Rescoring - 57.74\ngain up to +0.7 METEOR. Rescoring step improves final results. Note that rescoring was only applied to number classification because gender classification model has a low classification error (bellow 2%) which makes it harder to further decrease it. Additionally, gender and number classification scores are not be comparable and not easily integrated in Yen’s algorithm.\n\n7 Conclusions\nChinese-to-Spanish translation task is challenging, specially because of Spanish being morphologically rich compared to Chinese. Main contributions of this paper include correctly de-coupling the translation and morphological generation tasks and proposing a new classification architecture, based on deep learning, for number and gender.\nStandard phrase-based MT procedure is changed to first translating into a morphologically simplified target (in terms of number and gender); then, introducing the classification algorithm, based on a new proposed neural network-based architecture, that retrieves the simplified morphology; and composing the final full form by using the standard Freeling dictionary.\nResults of the proposed neural-network architecture in the classification task compared to standard algorithms (SVM or random forests) are significantly better and results in the translation task achieve up to 0.7 METEOR improvement. As further work, we intend to further simplify morphology and extend the scope of the classification.\n",
    "rationale": "The paper describes a method for improving two-step translation using deep\nlearning. Results are presented for Chinese->Spanish translation, but the\napproach seems to be largely language-independent.\n\nThe setting is fairly typical for two-step MT. The first step translates into a\nmorphologically underspecified version of the target language. The second step\nthen uses machine learning to fill in the missing morphological categories and\nproduces the final system output by inflecting the underspecified forms (using\na morphological generator). The main novelty of this work is the choice of deep\nNNs as classifiers in the second step. The authors also propose a rescoring\nstep which uses a LM to select the best variant.\n\nOverall, this is solid work with good empirical results: the classifier models\nreach a high accuracy (clearly outperforming baselines such as SVMs) and the\nimprovement is apparent even in the final translation quality.\n\nMy main problem with the paper is the lack of a comparison with some\nstraightforward deep-learning baselines. Specifically, you have a structured\nprediction problem and you address it with independent local decisions followed\nby a rescoring step. (Unless I misunderstood the approach.) But this is a\nsequence labeling task which RNNs are well suited for. How would e.g. a\nbidirectional LSTM network do when trained and used in the standard sequence\nlabeling setting? After reading the author response, I still think that\nbaselines (including the standard LSTM) are run in the same framework, i.e.\nindependently for each local label. If that's not the case, it should have been\nclarified better in the response. This is a problem because you're not using\nthe RNNs in the standard way and yet you don't justify why your way is better\nor compare the two approaches.\n\nThe final re-scoring step is not entirely clear to me. Do you rescore n-best\nsentences? What features do you use? Or are you searching a weighted graph for\nthe single optimal path? This needs to be explained more clearly in the paper.\n(My current impression is that you produce a graph, then look for K best paths\nin it, generate the inflected sentences from these K paths and *then* use a LM\n-- and nothing else -- to select the best variant. But I'm not sure from\nreading the paper.) This was not addressed in the response.\n\nYou report that larger word embeddings lead to a longer training time. Do they\nalso influence the final results?\n\nCan you attempt to explain why adding information from the source sentence\nhurts? This seems a bit counter-intuitive -- does e.g. the number information\nnot get entirely lost sometimes because of this? I would appreciate a more\nthorough discussion on this in the final version, perhaps with a couple of\nconvincing examples.\n\nThe paper contains a number of typos and the general level of English may not\nbe sufficient for presentation at ACL.\n\nMinor corrections:\n\ncontext of the application of MT -> context of application for MT\n\nIn this cases, MT is faced in two-steps -> In this case, MT is divided into two\nsteps\n\nmarkov -> Markov\n\nCFR -> CRF\n\ntask was based on a direct translation -> task was based on direct translation\n\ntask provided corpus -> task provided corpora\n\nthe phrase-based system has dramatically -> the phrase-based approach...\n\ninvestigated different set of features -> ...sets of features\n\nwords as source of information -> words as the source...\n\ncorrespondant -> corresponding\n\nClasses for gender classifier -> Classes for the...\n\nfor number classifier -> for the...\n\nThis layer's input consists in -> ...consists of\n\nto extract most relevant -> ...the most...\n\nSigmoid does not output results in [-1, 1] but rather (0, 1). A tanh layer\nwould produce (-1, 1).\n\ninformation of a word consists in itself -> ...of itself\n\nthis $A$ set -> the set $A$\n\nempty sentences and longer than 50 words -> empty sentences and sentences\nlonger than...\n\nclassifier is trained on -> classifier is trained in\n\naproximately -> approximately\n\ncoverage raises the 99% -> coverage exceeds 99% (unless I misunderstand)\n\nin descendant order -> in descending order\n\ncuadratic -> quadratic (in multiple places)\n\nbut best results -> but the best results\n\nRescoring step improves -> The rescoring step...\n\nare not be comparable -> are not comparable",
    "rating": 5
  }
]
