[
  {
    "title": "DETERMINISTIC PAC-BAYESIAN GENERALIZATION BOUNDS FOR DEEP NETWORKS VIA GENERALIZING NOISE-RESILIENCE",
    "abstract": "The ability of overparameterized deep networks to generalize well has been linked to the fact that stochastic gradient descent (SGD) finds solutions that lie in flat, wide minima in the training loss – minima where the output of the network is resilient to small random noise added to its parameters. So far this observation has been used to provide generalization guarantees only for neural networks whose parameters are either stochastic or compressed. In this work, we present a general PAC-Bayesian framework that leverages this observation to provide a bound on the original network learned – a network that is deterministic and uncompressed. What enables us to do this is a key novelty in our approach: our framework allows us to show that if on training data, the interactions between the weight matrices satisfy certain conditions that imply a wide training loss minimum, these conditions themselves generalize to the interactions between the matrices on test data, thereby implying a wide test loss minimum. We then apply our general framework in a setup where we assume that the pre-activation values of the network are not too small (although we assume this only on the training data). In this setup, we provide a generalization guarantee for the original (deterministic, uncompressed) network, that does not scale with product of the spectral norms of the weight matrices – a guarantee that would not have been possible with prior approaches.",
    "text": "1 INTRODUCTION\nModern deep neural networks contain millions of parameters and are trained on relatively few samples. Conventional wisdom in machine learning suggests that such models should massively overfit on the training data, as these models have the capacity to memorize even a randomly labeled dataset of similar size (Zhang et al., 2017; Neyshabur et al., 2015). Yet these models have achieved state-ofthe-art generalization error on many real-world tasks. This observation has spurred an active line of research (Soudry et al., 2018; Brutzkus et al., 2018; Li & Liang, 2018) that has tried to understand what properties are possessed by stochastic gradient descent (SGD) training of deep networks that allows these networks to generalize well.\nOne particularly promising line of work in this area (Neyshabur et al., 2017; Arora et al., 2018) has been bounds that utilize the noise-resilience of deep networks on training data i.e., how much the training loss of the network changes with noise injected into the parameters, or roughly, how wide is the training loss minimum. While these have yielded generalization bounds that do not have a severe exponential dependence on depth (unlike other bounds that grow with the product of spectral norms of the weight matrices), these bounds are quite limited: they either apply to a stochastic version of the classifier (where the parameters are drawn from a distribution) or a compressed version of the classifier (where the parameters are modified and represented using fewer bits).\nIn this paper, we revisit the PAC-Bayesian analysis of deep networks in Neyshabur et al. (2017; 2018) and provide a general framework that allows one to use noise-resilience of the deep network\non training data to provide a bound on the original deterministic and uncompressed network. We achieve this by arguing that if on the training data, the interaction between the ‘activated weight matrices’ (weight matrices where the weights incoming from/outgoing to inactive units are zeroed out) satisfy certain conditions which results in a wide training loss minimum, these conditions themselves generalize to the weight matrix interactions on the test data.\nAfter presenting this general PAC-Bayesian framework, we specialize it to the case of deep ReLU networks, showing that we can provide a generalization bound that accomplishes two goals simultaneously: i) it applies to the original network and ii) it does not scale exponentially with depth in terms of the products of the spectral norms of the weight matrices; instead our bound scales with more meaningful terms that capture the interactions between the weight matrices and do not have such a severe dependence on depth in practice. We note that all but one of these terms are indeed quite small on networks in practice. However, one particularly (empirically) large term that we use is the reciprocal of the magnitude of the network pre-activations on the training data (and so our bound would be small only in the scenario where the pre-activations are not too small). We emphasize that this drawback is more of a limitation in how we characterize noise-resilience through the specific conditions we chose for the ReLU network, rather than a drawback in our PAC-Bayesian framework itself. Our hope is that, since our technique is quite general and flexible, by carefully identifying the right set of conditions, in the future, one might be able to derive a similar generalization guarantee that is smaller in practice.\nTo the best of our knowledge, our approach of generalizing noise-resilience of deep networks from training data to test data in order to derive a bound on the original network that does not scale with products of spectral norms, has neither been considered nor accomplished so far, even in limited situations.\n\n2 BACKGROUND AND RELATED WORK\nOne of the most important aspects of the generalization puzzle that has been studied is that of the flatness/width of the training loss at the minimum found by SGD. The general understanding is that flatter minima are correlated with better generalization behavior, and this should somehow help explain the generalization behavior (Hochreiter & Schmidhuber, 1997; Hinton & van Camp, 1993; Keskar et al., 2017). Flatness of the training loss minimum is also correlated with the observation that on training data, adding noise to the parameters of the network results only in little change in the output of the network – or in other words, the network is noise-resilient. Deep networks are known to be similarly resilient to noise injected into the inputs (Novak et al., 2018); but note that our theoretical analysis relies on resilience to parameter perturbations.\nWhile some progress has been made in understanding the convergence and generalization behavior of SGD training of simple models like two-layered hidden neural networks under simple data distributions (Neyshabur et al., 2015; Soudry et al., 2018; Brutzkus et al., 2018; Li & Liang, 2018), all known generalization guarantees for SGD on deeper networks – through analyses that do not use noise-resilience properties of the networks – have strong exponential dependence on depth. In particular, these bounds scale either with the product of the spectral norms of the weight matrices (Neyshabur et al., 2018; Bartlett et al., 2017) or their Frobenius norms (Golowich et al., 2018). In practice, the weight matrices have a spectral norm that is as large as 2 or 3, and an even larger Frobenius norm that scales with √ H where H is the width of the network i.e., maximum number of hidden units per layer. 1 Thus, the generalization bound scales as say, 2D or HD/2, where D is the depth of the network.\nAt a high level, the reason these bounds suffer from such an exponential dependence on depth is that they effectively perform a worst case approximation of how the weight matrices interact with each other. For example, the product of the spectral norms arises from a naive approximation of the Lipschitz constant of the neural network, which would hold only when the singular values of the\n1To understand why these values are of this order in magnitude, consider the initial matrix that is randomly initialized with independent entries with variance 1/ √ H . It can be shown that the spectral norm of this matrix, with high probability, lies near its expected value, near 2 and the Frobenius norm near its expected value which is √ H . Since SGD is observed not to move too far away from the initialization regardless of H (Nagarajan & Kolter, 2017), these values are more or less preserved for the final weight matrices.\nweight matrices all align with each other. However, in practice, for most inputs to the network, the interactions between the activated weight matrices are not as adverse.\nBy using noise-resilience of the networks, prior approaches (Arora et al., 2018; Neyshabur et al., 2017) have been able to derive bounds that replace the above worst-case approximation with smaller terms that realistically capture these interactions. However, these works are limited in critical ways. Arora et al. (2018) use noise-resilience of the network to modify and “compress” the parameter representation of the network, and derive a generalization bound on the compressed network. While this bound enjoys a better dependence on depth because its applies to a compressed network, the main drawback of this bound is that it does not apply on the original network. On the other hand, Neyshabur et al. (2017) take advantage of noise-resilience on training data by incorporating it within a PAC-Bayesian generalization bound (McAllester, 1999a). However, their final guarantee is only a bound on the expected test loss of a stochastic network.\nIn this work, we revisit the idea in Neyshabur et al. (2017), by pursuing the PAC-Bayesian framework (McAllester, 1999a) to answer this question. The standard PAC-Bayesian framework provides generalization bounds for the expected loss of a stochastic classifier, where the stochasticity typically corresponds to Gaussian noise injected into the parameters output by the learning algorithm. However, if the classifier is noise-resilient on both training and test data, one could extend the PAC-Bayesian bound to a standard generalization guarantee on the deterministic classifier.\nOther works have used PAC-Bayesian bounds in different ways in the context of neural networks. Langford & Caruana (2001); Dziugaite & Roy (2017) optimize the stochasticity and/or the weights of the network in order to numerically compute good (i.e., non-vacuous) generalization bounds on the stochastic network. Neyshabur et al. (2018) derive generalization bounds on the original, deterministic network by working from the PAC-Bayesian bound on the stochastic network. However, as stated earlier, their work does not make use of noise resilience in the networks learned by SGD.\nOUR CONTRIBUTIONS The key contribution in our work is a general PAC-Bayesian framework for deriving generalization bounds while leveraging the noise resilience of a deep network. While our approach is applied to deep networks, we note that it is general enough to be applied to other classifiers.\nIn our framework, we consider a set of conditions that when satisfied by the network, makes the output of the network noise-resilient at a particular input datapoint. For example, these conditions could characterize the interactions between the activated weight matrices at a particular input. To provide a generalization guarantee, we assume that the learning algorithm has found weights such that these conditions hold for the weight interactions in the network on training data (which effectively implies a wide training loss minimum). Then, as a key step, we generalize these conditions over to the weight interactions on test data (which effectively implies a wide test loss minimum) 2. Thus, with the guarantee that the classifier is noise-resilient both on training and test data, we derive a generalization bound on the test loss of the original network.\nFinally, we apply our framework to a specific set up of ReLU based feedforward networks. In particular, we first instantiate the above abstract framework with a set of specific conditions, and then use the above framework to derive a bound on the original network. While very similar conditions have already been identified in prior work (Arora et al., 2018; Neyshabur et al., 2017) (see Appendix G for an extensive discussion of this), our contribution here is in showing how these conditions generalize from training to test data. Crucially, like these works, our bound does not have severe exponential dependence on depth in terms of products of spectral norms.\nWe note that in reality, all but one of our conditions on the network do hold on training data as necessitated by the framework. The strong, non-realistic condition we make is that the pre-activation values of the network are sufficiently large, although only on training data; however, in practice a small proportion of the pre-activation values can be arbitrarily small. Our generalization bound scales inversely with the smallest absolute value of the pre-activations on the training data, and hence in practice, our bound would be large.\n2Note that we can not directly assume these conditions to hold on test data, as that would be ‘cheating’ from the perspective of a generalization guarantee.\nIntuitively, we make this assumption to ensure that under sufficiently small parameter perturbations, the activation states of the units are guaranteed not to flip. It is worth noting that Arora et al. (2018); Neyshabur et al. (2017) too require similar, but more realistic assumptions about pre-activation values that effectively assume only a small proportion of units flip under noise. However, even under our stronger condition that no such units exist, it is not apparent how these approaches would yield a similar bound on the deterministic, uncompressed network without generalizing their conditions to test data. We hope that in the future our work could be developed further to accommodate the more realistic conditions from Arora et al. (2018); Neyshabur et al. (2017).\n\n3 A GENERAL PAC-BAYESIAN FRAMEWORK\nIn this section, we present our general PAC-Bayesian framework that uses noise-resilience of the network to convert a PAC-Bayesian generalization bound on the stochastic classifier to a generalization bound on the deterministic classifier.\nNOTATION. Let KL(⋅∥⋅) denote the KL-divergence. Let ∥⋅∥ , ∥⋅∥∞ denote the `2 norm and the `∞ norms of a vector, respectively. Let ∥⋅∥2 , ∥⋅∥F , ∥⋅∥2,∞ denote the spectral norm, Frobenius norm and maximum row `2 norm of a matrix, respectively. Consider a K-class learning task where the labeled datapoints (x, y) are drawn from an underlying distribution D over X × {1,2,⋯,K} where X ∈ RN . We consider a classifier parametrized by weightsW . For a given input x and class k, we denote the output of the classifier by f (x;W) [k]. In our PAC-Bayesian analysis, we will use U ∼ N (0, σ2) to denote parameters whose entries are sampled independently from a Gaussian, andW + U to denote the entrywise addition of the two sets of parameters. We use ∥W∥2F to denote ∑ D d=1 ∥Wd∥ 2 F . Given a training set S of m samples, we let (x, y) ∼ S to denote uniform sampling from the set. Finally, for any γ > 0, let Lγ(f (x;W) , y) denote a margin-based loss such that the loss is 0 only when f (x;W) [y] ≥ maxj≠y f (x;W) [j]+γ, and 1 otherwise. Note that L0 corresponds to 0-1 error. See Appendix A for more notations.\nTRADITIONAL PAC-BAYESIAN BOUNDS. The PAC-Bayesian framework (McAllester, 1999a;b) allows us to derive generalization bounds for a stochastic classifier. Specifically, let W̃ be a random variable in the parameter space whose distribution is learned based on training data S. Let P be a prior distribution in the parameter space chosen independent of the training data. The PAC-Bayesian framework yields the following generalization bound on the 0-1 error of the stochastic classifier that holds with probability 1 − δ over the draw of the training set S of m samples3:\nEW̃[E(x,y)∼D[L0(f (x;W̃) , y)]] ≤ EW̃[E(x,y)∼S[L0(f (x;W̃) , y)]] + Õ ( √ KL(W̃∥P )/m)\nTypically, and in the rest of this discussion, W̃ is a Gaussian with covariance σ2I for some σ > 0 centered at the weightsW learned based on the training data. Furthermore, we will set P to be a Gaussian with covariance σ2I centered at the random initialization of the network like in Dziugaite & Roy (2017), instead of at the origin, like in Neyshabur et al. (2018). This is because the resulting KL-divergence – which depends on the distance between the means of the prior and the posterior – is known to be smaller, and to save a √ H factor in the bound (Nagarajan & Kolter, 2017).\n\n3.1 OUR FRAMEWORK\nTo extend the above PAC-Bayesian bound to a standard generalization bound on a deterministic classifierW , we need to replace the training and the test loss of the stochastic classifier with that of the original, deterministic classifier. However, in doing so, we will have to introduce extra terms in the upper bound to account for the perturbation suffered by the train and test loss under the Gaussian perturbation of the parameters. To tightly bound these two terms, we need that the network is noise-resilient on training and test data respectively. Our hope is that if the learning algorithm has found weights such that the network is noise-resilient on the training data, we can then generalize this noise-resilience over to test data as well, allowing us to better bound the excess terms.\n3We use Õ(⋅) to hide logarithmic factors.\nWe now discuss how noise-resilience is formalized in our framework through certain conditions on the weight matrices. Much of our discussion below is dedicated to how these conditions must be designed, as these details carry the key ideas behind how noise-resilience can be generalized from training to test data. We then present our main generalization bound and some intuition about our proof technique.\nINPUT-DEPENDENT PROPERTIES OF WEIGHTS Recall that, at a high level, the noise-resilience of a network corresponds to how little the network reacts to random parameter perturbations. Naturally, this would vary depending on the input. Hence, in our framework, we will analyze the noise-resilience of the network as a function of a given input. Specifically, we will characterize noise-resilience through conditions on how the weights of the model interact with each other for a given input. For example, in the next section, we will consider conditions of the form “the preactivation values of the hidden units in layer d, have magnitude larger than some small positive constant”. The idea is that when these conditions involving the weights and the input are satisfied, if we add noise to the weights, the output of the classifier for that input will provably suffer only little perturbation. We will more generally refer to each scalar quantity involved in these conditions, such as each of the pre-activation values, as an input-dependent property of the weights.\nWe will now formulate these input-dependent properties and the conditions on them, for a generic classifier, and in the next section, we will see how they can be instantiated in the case of deep networks. Consider a classifier for which we can define R different conditions, which when satisfied on a given input, will help us guarantee the classifier’s noise-resilience at that input i.e., bound the output perturbation under random parameter perturbations (to get an idea of what R corresponds to, in the case of deep networks, we will have a condition for each layer, and so R will scale with depth). In particular, let the rth condition be a bound involving a particular set of input-dependent properties of the weights denoted by {ρr,1(W,x, y), ρr,2(W,x, y),⋯,} – here, each element ρr,l(W,x, y) is a scalar value that depends on the weights and the input, just like pre-activation values4. Note that here the first subscript l is the index of the element in the set, and the second subscript r is the index of the set itself. Now for each of these properties, we will define a corresponding set of positive constants (that are independent ofW,x and y), denoted by {∆⋆r,1,∆ ⋆ r,2,⋯}, which we will use to specify our conditions. In particular, we say that the weightsW satisfy the rth condition on the input (x, y) if5:\n∀l, ρr,l(W,x, y) > ∆ ⋆ r,l (1)\nFor convenience, we also define an additional R+1th set to be the singleton set containing the margin of the classifier on the input: f (x;W) [y] −maxj≠y f (x;W) [j]. Note that if this term is positive (negative) then the classification is (in)correct. We will also denote the constant ∆⋆R+1,1 as γclass.\nORDERING OF THE SETS OF PROPERTIES We now impose a crucial constraint on how these sets of properties depend on each other. Roughly speaking, we want that for a given input, if the first r − 1 sets of properties approximately satisfy the condition in Equation 1, then the properties in the rth set are noise-resilient i.e., under random parameter perturbations, these properties do not suffer much perturbation. This kind of constraint would naturally hold for deep networks if we have chosen the properties carefully e.g., we will show that, for any given input, the perturbation in the pre-activation values of the dth layer is small as long as the absolute pre-activation values in the layers below d − 1 are large, and a few other norm-bounds on the lower layer weights are satisfied.\nWe formalize the above requirement by defining expressions ∆r,l(σ) that bound the perturbation in the properties ρr,l, in terms of the variance σ2 of the parameter perturbations. For any r ≤ R + 1 and for any (x, y), our framework requires the following to hold:\n4As we will see in the next section, most of these properties depend on only the unlabeled input x and not on y. But for the sake of convenience, we include y in the formulation of the input-dependent property, and use the word input to refer to x or (x, y) depending on the context\n5When we say ∀l below, we refer to the set of all possible indices l in the rth set, noting that different sets may have different cardinality.\nif ∀q < r,∀l, ρq,l(W,x, y) > 0 then\nPrU∼N (0,σ2I)[∀l ∣ρr,l(W + U ,x, y) − ρr,l(W,x, y)∣ > ∆r,l(σ)\n2 and\n∀q<r,∀l ∣ρq,l(W + U ,x, y) − ρq,l(W,x, y)∣< ∆q,l(σ)\n2 ] ≤\n1\n(R + 1) √ m . (2)\nLet us unpack the above constraint. First, although the above constraint must hold for all inputs (x, y), it effectively applies only to those inputs that satisfy the pre-condition of the if-then statement: namely, it applies only to inputs (x, y) that approximately satisfy the first r − 1 conditions in Equation 1 in that ρq,l(W,x, y) > 0 (instead of ρq,l(W,x, y) > ∆⋆q,l). Next, we discuss the second part of the above if-then statement which specifies a probability term that is required to be small for all such inputs. In words, the first event within the probability term above is the event that for a given random perturbation U , the properties involved in the rth condition suffer a large perturbation. The second is the event that the properties involved in the first r − 1 conditions do not suffer much perturbation; but, given that these r − 1 conditions already hold approximately, this second event implies that these conditions are still preserved approximately under perturbation. In summary, our constraint requires the following: for any input on which the first r − 1 conditions hold, there should be very few parameter perturbations that significantly perturb the rth set of properties while preserving the first r − 1 conditions. When we instantiate the framework, we have to derive closed form expressions for the perturbation bounds ∆r,l(σ) (in terms of only σ and the constants ∆⋆r,l). As we will see, for ReLU networks, we will choose the properties in a way that this constraint naturally falls into place in a way that the perturbation bounds ∆r,l(σ) do not grow with the product of spectral norms (Lemma E.1).\nTHEOREM STATEMENT In this setup, we have the following ‘margin-based’ generalization guarantee on the original network. That is, we bound the 0-1 test error of the network by a margin-based error on the training data. Our generalization guarantee, which scales linearly with the number of conditions R, holds under the setting that the training algorithm always finds weights such that on the training data, the conditions in Equation 1 is satisfied for all r = 1,⋯,R.\nTheorem 3.1. Let σ∗ be the maximum standard deviation of the Gaussian parameter perturbation such that the constraint in Equation 2 holds with ∆r,l(σ⋆) ≤ ∆⋆r,l ∀r ≤ R + 1 and ∀l. Then, for any δ > 0, with probability 1 − δ over the draw of samples S from Dm, for anyW we have that, ifW satisfies the conditions in Equation 1 for all r ≤ R and for all training examples (x, y) ∈ S, then\nPr(x,y)∼D [L0(f (x;W) , y)] ≤Pr(x,y)∼S [Lγclass(f (x;W) , y)]\n+ Õ ⎛ ⎜ ⎝ R\n¿ Á ÁÀ2KL(N (W, (σ⋆)2I)∥P ) + ln 2mR δ\nm − 1\n⎞ ⎟ ⎠\nThe crux of our proof (in Appendix D) lies in generalizing the conditions of Equation 1 satisfied on the training data to test data one after the other, by proving that they are noise-resilient on both training and test data. Crucially, after we generalize the first r − 1 conditions from training data to test data (i.e., on most test and training data, the r − 1 conditions are satisfied), we will have from Equation 2 that the rth set of properties are noise-resilient on both training and test data. Using the noise-resilience of the rth set of properties on test/train data, we can generalize even the rth condition to test data.\nWe emphasize a key, fundamental tool that we present in Theorem C.1 to convert a generic PACBayesian bound on a stochastic classifier, to a generalization bound on the deterministic classifier. Our technique is at a high level similar to approaches in London et al. (2016); McAllester (2003). In Section C.1, we argue how this technique is more powerful than other approaches in Neyshabur et al. (2018); Langford & Shawe-Taylor (2002); Herbrich & Graepel (2000) in leveraging the noiseresilience of a classifier. The high level argument is that, to convert the PAC-Bayesian bound, these latter works relied on a looser output perturbation bound, one that holds on all possible inputs, with high probability over all perturbations i.e., a bound on maxx ∥f (x;W) − f (x;W + U)∥∞ w.h.p over draws of U . In contrast, our technique relies on a subtly different but significantly tighter bound: a bound on the output perturbation that holds with high probability given an input i.e., a bound\non ∥f (x;W) − f (x;W + U)∥∞ w.h.p over draws of U for each x. When we do instantiate our framework as in the next section, this subtle difference is critical in being able to bound the output perturbation without suffering from a factor proportional to the product of the spectral norms of the weight matrices (which is the case in Neyshabur et al. (2018)).\n\n3. Bound on perturbation of `2 norm on the rows of the Jacobians d/d′.\nPrU[¬PERT-BOUND(W + U ,{ζ̂ ′d/d′} d d′=1,x) ∧\nPERT-BOUND(W + U , Ĉd−1,x) ∧ UNCHANGED-ACTSd−1(W + U ,x)] ≤ δ̂\n\n4 APPLICATION OF OUR FRAMEWORK TO RELU NETWORKS\nNOTATION. In this section, we apply our framework to feedforward fully connected ReLU networks of depth D (we care about D > 2) and width H (which we will assume is larger than the input dimensionality N , to simplify our proofs) and derive a generalization bound on the original network that does not scale with the product of spectral norms of the weight matrices. Let φ (⋅) denote the ReLU activation. We consider a network parameterized byW = (W1,W2,⋯,WD) such that the output of the network is computed as f (x;W) = WDφ (WD−1⋯φ (W1x)). We denote the value of the hth hidden unit on the dth layer before and after the activation by gd (x;W) [h] and fd (x;W) [h] respectively. We define Jd/d ′ (x;W) ∶= ∂gd (x;W)/∂gd ′ (x;W) to be the Jacobian of the pre-activations of layer d with respect to the pre-activations of layer d′ for d′ ≤ d (each row in this Jacobian corresponds to a unit in layer d). In short, we will call this, Jacobian d/d′. Let Z denote the random initialization of the network.\nInformally, we consider a setting where the learning algorithm satisfies the following conditions on the training data that make it noise-resilient on training data: a) the `2 norm of the hidden layers are all small, b) the pre-activation values are all sufficiently large in magnitude, c) the Jacobian of any layer with respect to a lower layer, has rows with a small `2 norm, and has a small spectral norm. We cast these conditions in the form of Equation 1 by appropriately defining the properties ρ’s and the margins ∆⋆’s in the general framework. We note that these properties are quite similar to those already explored in Arora et al. (2018); Neyshabur et al. (2017); we provide more intuition about these properties, and how we cast them in our framework in Appendix E.1.\nHaving defined these properties, we first prove in Lemma E.1 in Appendix E a guarantee equivalent to the abstract inequality in Equation 2. Essentially, we show that under random perturbations of the parameters, the perturbation in the output of the network and the perturbation in the input-dependent properties involved in (a), (b), (c) themselves can all be bounded in terms of each other. Crucially, these perturbation bounds do not grow with the spectral norms of the network.\nHaving instantiated the framework as above, we then instantiate the bound provided by the framework. Our generalization bound scales with the bounds on the properties in (a) and (c) above as satisfied on the training data, and with the reciprocal of the property in (b) i.e., the smallest absolute value of the pre-activations on the training data. Additionally, our bound has an explicit dependence on the depth of the network, which arises from the fact that we generalize R = O(D) conditions. Most importantly, our bound does not have a dependence on the product of the spectral norms of the weight matrices.\nTheorem 4.1. (shorter version; see Appendix F for the complete statement) For any margin γclass > 0, and any δ > 0, with probability 1 − δ over the draw of samples from Dm, for anyW , we have that:\nPr(x,y)∼D [L0(f (x;W) , y)] ≤Pr(x,y)∼S [Lγclass(f (x;W) , y)] + Õ (D √ ∥W −Z∥2F /((σ ⋆)2m))\nHere 1/σ⋆ equals Õ( √ Hmax{Blayer-`2 ,Bpreact,Boutput,Bjac-row-`2 ,Bjac-spec}), where\nBlayer-`2 ∶= O ⎛\n⎝ max 1≤d<D\n∑ d d′=1 ζ ⋆ d/d′α ⋆ d′−1\nα⋆d\n⎞ ⎠ ,Bpreact ∶= O ⎛ ⎝ max 1≤d<D\n∑ d d′=1 ζ ⋆ d/d′α ⋆ d′−1\n√ Hγ⋆d\n⎞\n⎠\nBoutput ∶= O ⎛\n⎝\n∑ D d=1 ζ ⋆ D/dα ⋆ d−1\n√ Hγclass\n⎞ ⎠ ,Bjac-row-`2 ∶= O ⎛ ⎝ max 1≤d′<d<D\nζ⋆d−1/d′ + ∥Wd∥2,∞∑ d−1 d′′=d′+1 ψ ⋆ d−1/d′′ζ ⋆ d′′−1/d′\nζ⋆ d/d′\n⎞\n⎠\nBjac-spec ∶= O ⎛\n⎝ max 1≤d′<d<D\nψ⋆d′′−1/d′ + ∥Wd∥2∑ d−1 d′′=d′+1 ψ ⋆ d−1/d′′ζ ⋆ d′′−1/d′\nψ⋆ d/d′\n⎞\n⎠\nwhere, the terms α⋆d, γ ⋆ d etc., are norm-bounds that hold on all training data (x, y) ∈ S as follows: α⋆d ≥ ∥f d (x;W)∥ (an upper bound on the `2 norm of each hidden layer output), γ⋆d ≤ minh ∣f d (x;W) [h]∣ (a lower bound on the absolute values of the pre-activations for each layer), ζ⋆d/d′ ≥ ∥J d/d′(x;W)∥\n2,∞ (an upper bound on the row `2 norms of the Jacobian for each\nlayer), ψ⋆d/d′ ≥ ∥J d/d′(x;W)∥\n2 (an upper bound on the spectral norm of the Jacobian for each\nlayer).\nIn Figure 1, we show how the terms in the bound vary for networks of varying depth with a small width of H = 40 on the MNIST dataset. We observe that Blayer-`2 ,Boutput,Bjac-row-`2 ,Bjac-spec typically lie in the range of [100,102] and scale with depth as ∝ 1.57D. In contrast, the equivalent term from Neyshabur et al. (2018) consisting of the product of spectral norms can be as large as 103 or 105 and scale with D more severely as 2.15D.\nThe bottleneck in our bound is Bpreact, which scales inversely with the magnitude of the smallest absolute pre-activation value of the network. In practice, this term can be arbitrarily large, even though it does not depend on the product of spectral norms/depth. This is because some hidden units can have arbitrarily small absolute pre-activation values – although this is true only for a small proportion of these units.\nTo give an idea of the typical, non-pathological magnitude of the pre-activation values, we plot two other variations of Bpreact: a) 5%-Bpreact which is calculated by ignoring 5% of the training datapoints with the smallest absolute pre-activation values and b) median-Bpreact which is calculated by ignoring half the hidden units in each layer with the smallest absolute pre-activation values for each input. We observe that median-Bpreact is quite small (of the order of 102), while 5%-Bpreact, while large (of the order of 104), is still orders of magnitude smaller than Bpreact.\nIn Figure 2 we show how our overall bound and existing product-of-spectral-norm-based bounds (Bartlett et al., 2017; Neyshabur et al., 2018) vary with depth. While our bound is orders of magnitude larger than prior bounds, the key point here is that our bound grows with depth as 1.57D while prior bounds grow with depth as 2.15D indicating that our bound should perform asymptotically better with respect to depth. Indeed, we verify that our bound obtains better values than the other existing bounds when D = 28 (see Figure 2 b). We also plot hypothetical variations of our bound replacing Bpreact with 5%-Bpreact (see “Ours-5%”) and median-Bpreact (see “Ours-Median”) both of which perform orders of magnitude better than our actual bound (note that these two hypothetical bounds do not actually hold good). In fact for larger depth, the bound with 5%-Bpreact performs better than all other bounds (including existing bounds). This indicates that the only bottleneck in our bound comes from the dependence on the smallest pre-activation magnitudes, and if this particular\ndependence is addressed, our bound has the potential to achieve tighter guarantees for even smaller D such as D = 8.\n(2018) maxx ∥x∥2D √ H∏Dd=1 ∥Wd∥2\nγclass ⋅\n√\n∑ D d=1 ∥Wd−Zd∥2F ∥Wd∥22 and Bartlett et al. (2017) maxx ∥x∥2∏ D d=1 ∥Wd∥2 γclass ⋅\n(∑ D d=1 ( ∥Wd−Zd∥2,1 ∥Wd∥ )\n2/3 ) 3/2 both of which have been modified to include distance from initialization\ninstead of distance from origin for a fair comparison. Observe the last two bounds have a plot with a larger slope than the other bounds indicating that they might potentially do worse for a sufficiently large D. Indeed, this can be observed from the plots on the right where we report the distribution of the logarithm of these bounds for D = 28 across 12 runs (although under training settings different from the experiments on the left; see Appendix F.3 for the exact details).\nWe refer the reader to Appendix F.3 for added discussion where we demonstrate how all the quantities in our bound vary with depth for H = 1280 (Figure 3, 4) and with width for D = 8,14 (Figures 5 and 6).\nFinally, as noted before, we emphasize that the dependence of our bound on the pre-activation values is a limitation in how we characterize noise-resilience through our conditions rather than a drawback in our general PAC-Bayesian framework itself. Specifically, using the assumed lower bound on the pre-activation magnitudes we can ensure that, under noise, the activation states of the units do not flip; then the noise propagates through the network in a tractable, “linear” manner. Improving this analysis is an important direction for future work. For example, one could modify our analysis to allow perturbations large enough to flip a small proportion of the activation states; one could potentially formulate such realistic conditions by drawing inspiration from the conditions in Neyshabur et al. (2017); Arora et al. (2018).\nHowever, we note that even though these prior approaches made more realistic assumptions about the magnitudes of the pre-activation values, the key limitation in these approaches is that even under our non-realistic assumption, their approaches would yield bounds only on stochastic/compressed networks. Generalizing noise-resilience from training data to test data is crucial to extending these bounds to the original network, which we accomplish.\n\n4. Bound on perturbation of spectral norm of the Jacobians d/d′.\nPrU[¬PERT-BOUND(W + U ,{ψ̂′d/d′} d d′=1,x) ∧\nPERT-BOUND(W + U , Ĉd−1,x) ∧ UNCHANGED-ACTSd−1(W + U ,x)] ≤ δ̂\nProof. For the most part of this discussion, we will consider a perturbed network where all the hidden units are frozen to be at the same activation state as they were at, before the perturbation. We will denote the weights of such a network byW[+U] and its output at the dth layer by fd (x;W[+U]). By having the activations states frozen, the Gaussian perturbations propagate linearly through the activations, effectively remaining as Gaussian perturbations; then, we can enjoy the well-established properties of the Gaussian even after they propagate.\nPERTURBATION BOUND ON THE `2 NORM OF LAYER d. We bound the change in the `2 norm of the dth layer’s output by applying a triangle inequality6 after splitting it into a sum of vectors. Each summand here (which we define as vd′ for each d′ ≤ d) is the difference in the dth layer output on introducing noise in weight matrix d′ after having introduced noise into all the first d′ − 1 weight matrices.\n∣∥fd (x;W[+Ud])∥ − ∥f d (x;W)∥∣ ≤ ∥fd (x;W[+Ud]) − f d (x;W)∥\nbecause the activations are ReLU, we can replace this with the perturbation of the pre-activation\n≤ ∥gd (x;W[+Ud]) − g d (x;W)∥\n≤ XXXXXXXXXXXXXXXXXXX d ∑ d′=1 ⎛ ⎜ ⎜ ⎜ ⎝ gd (x;W[+Ud′]) − g d (x;W[+Ud′−1]) ´¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¸¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¶ ∶=vd′ ⎞ ⎟ ⎟ ⎟ ⎠ XXXXXXXXXXXXXXXXXXX\n≤ d\n∑ d′=1\n∥vd′∥ = d\n∑ d′=1\n√\n∑ h\nv2d′,h (9)\nHere, vd′,h is the perturbation in the preactivation of hidden unit h on layer d′, brought about by perturbation of the d′th weight matrix in a network where only the first d′ − 1 weight matrices have already been perturbed.\nNow, for each h, we bound vd′,h in Equation 9. Since the activations have been frozen we can rewrite each vd′,h as the product of the hth row of the unperturbed network’s Jacobian d/d′ , followed by only the perturbation matrix Ud′ , and then the output of the layer d′ − 1. Concretely, we have7 8:\nvd′,h =\n1×Hd′ ³¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹·¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹µ Jd/d ′ (x;W)[h] Hd′×Hd′−1 ³·µ Ud′ Hd′−1×1 ³¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹·¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹µ fd ′−1\n(x;W[+Ud′−1]) ´¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¸¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¶\nspherical Gaussian\nWhat do these random variables vd′,h look like? Conditioned on Ud′−1, the second part of our expansion of vd′,h, namely, Ud′fd\n′−1 (x;W[+Ud′−1]) is a multivariate spherical Gaussian (see Lemma B.2) of the form N (0, σ2∥fd ′−1 (x;W[+Ud′−1]) ∥ 2I). As a result, conditioned on Ud′−1, vd′,h is a univariate Gaussian N (0, σ2∥Jd/d ′ (x;W)[h]∥2∥fd ′−1 (x;W[+Ud′−1])∥ 2).\nThen, we can apply a standard Gaussian tail bound (see Lemma B.1) to conclude that with probability 1 − δ̂/DH over the draws of Ud′ (conditioned on any Ud′−1), vd′,h is bounded as:\n∣vd′,h∣ ≤ σ ∥J d/d′ (x;W)[h]∥ ∥fd ′−1 (x;W[+Ud′−1])∥\n√\n2 ln 2DH\nδ̂ . (10)\nThen, by a union bound over all the hidden units on layer d, and for each d′, we have that with probability 1 − δ̂, Equation 9 is upper bounded as:\n6Specifically, for two vectors a,b, we have from triangle inequality that ∥b∥ ≤ ∥a∥ + ∥b − a∥ and ∥a∥ ≤ ∥b∥ + ∥a − b∥. As a result of this, we have: − ∥a − b∥ ≤ ∥a∥ − ∥b∥ ≤ ∥a − b∥. We use this inequality in our proof.\n7Below, we have used Hd to denote the number of units on the dth layer (and this equals H for the hidden units and K for the output layer).\n8Note that the succinct formula below holds good even for the corner case d′ = d, where the first Jacobian-row term becomes a vector with zeros on all but the hth entry and therefore only the hth row of the perturbation matrix Ud′ will participate in the expression of vd′,h.\n∑ d′\n√\n∑ h\nv2d′,h ≤ d\n∑ d′=1\nσ ∥Jd/d ′ (x;W)∥\nF ∥fd\n′−1 (x;W[+Ud′−1])∥\n√\n2 ln 2DH\nδ̂ . (11)\nUsing this we prove the probability bound in the lemma statement. To simplify notations, let us denote Ĉd−1⋃{ζ̂d/d′}dd′=1 by Ĉprev. Furthermore, we will drop redundant symbols in the arguments of the events we have defined. Then, recall that we want to upper bound the following probability (we ignore the argumentsW + U and x for brevity):\nPr [(¬PERT-BOUND ({α̂′d})) ∧ PERT-BOUND(Ĉprev) ∧ UNCHANGED-ACTSd−1]\nRecall that Equation 11 is a bound on the perturbation of the `2 norm of the dth layer’s output when the activation states are explicitly frozen. If the perturbation we randomly draw happens to satisfy UNCHANGED-ACTSd−1 then the bound in Equation 11 holds good even in the case where the activation states are not explicitly frozen. Furthermore, when PERT-BOUND(Ĉprev) holds, the bound in Equation 11 can be upper-bounded by α̂′d as defined in the lemma statement, because under PERT-BOUND(Ĉprev), the middle term in Equation 11 can be upper bounded using triangle inequality as ∥fd ′−1 (x;W[+Ud′−1])∥ ≤ ∥f d′−1 (x;W)∥ + α̂d′−1. Hence, the event above happens only for the perturbations for which Equation 11 fails and hence we have that the above probability term is upper bounded by δ̂.\nPERTURBATION BOUND ON THE PREACTIVATION VALUES OF LAYER d. Following the same analysis as above, the bound we are seeking here is essentially maxh∑dd′=1 ∣vd′,h∣. The bound follows similarly from Equation 10.\nPERTURBATION BOUND ON THE `2 NORM OF THE ROWS OF THE JACOBIAN d/d′. We split this term like we did in the previous subsection, and apply triangle equality as follows:\nmax h\n∣∥Jd/d ′ (x;W)[h]∥ − ∥Jd/d ′ (x;W[+Ud])[h]∥∣\n≤ max h\n∥Jd/d ′ (x;W)[h] − Jd/d ′ (x;W[+Ud])[h]∥\n≤ XXXXXXXXXXXXXXXXXXXXX d ∑ d′′=1 ⎛ ⎜ ⎜ ⎜ ⎜ ⎜ ⎝ Jd/d ′ (x;W[+Ud′′])[h] − J d/d′ (x;W[+Ud′′−1])[h] ´¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¸¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¶ ∶=yd′′ h ⎞ ⎟ ⎟ ⎟ ⎟ ⎟ ⎠ XXXXXXXXXXXXXXXXXXXXX\n≤ max h\nd\n∑ d′′=1\n∥yd ′′\nh ∥ = max h\nd\n∑ d′′=1\n√\n∑ h′\n(yd′′,h,h′) 2 (12)\nHere, we have defined yd ′′\nh to be the vector that corresponds to the difference in the hth row of the Jacobian d/d′ brought about by perturbing the d′′th weight matrix, given that the first d′′ − 1 matrices have already been perturbed. We use h to iterate over the units in the dth layer and h′ to iterate over the units in the d′th layer.\nNow, under the frozen activation states, when we perturb the weight matrices from 1 uptil d′, since these matrices are not involved in the Jacobian d/d′, fortunately, the Jacobian d/d′ is not perturbed (as the set of active weights in d/d′ are the same when we perturbW asW[+Ud′]). So, we will only need to bound yd′′,h,h′ for d′′ > d′.\nWhat does the distribution of yd′′,h,h′ look like for d′′ > d′? We can expand9 yd′′,h,h′ as the product of i) the hth row of the Jacobian d/d′′ ii) the perturbation matrix Ud′′ and iii) the h′th column of the Jacobian d′/d′′ − 1 for the perturbed network:\n9Again, note that the below succinct formula works even for corner cases like d′′ = d′ or d′′ = d.\nyd′′,h,h′ =\n1×Hd′′ ³¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹·¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹µ Jd/d ′′ (x;W)[h] Hd′′×Hd′′−1 ³·µ Ud′′ Hd′′−1×1 ³¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹·¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹µ Jd ′′−1/d′ (x;W[+Ud′′−1])[∶, h ′ ]\n´¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¸¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¶ spherical Gaussian\nConditioned on Ud′′−1, the second part of this expansion, namely, Ud′′Jd ′′−1/d′(x;W[+Ud′′−1])[∶ , h′] is a multivariate spherical Gaussian (see Lemma B.2) of the form N (0, σ2∥Jd ′′−1/d′(x;W[+Ud′′−1])[∶, h ′]∥2I). As a result, conditioned on Ud′′−1, yd′′,h,h′ is a univariate Gaussian N (0, σ2∥Jd/d ′′ (x;W)[h]∥2∥Jd ′′−1/d′(x;W[+Ud′′−1])[∶, h ′]∥2).\nThen, by applying a standard Gaussian tail bound we have that with probability 1 − δ̂ D2H2 over the draws of Ud′′ conditioned on Ud′′−1, each of these quantities is bounded as:\n∣yd′′,h,h′ ∣ ≤ σ∥J d/d′′ (x;W)[h]∥∥Jd ′′−1/d′ (x;W[+Ud′′−1])[∶, h ′ ]∥\n√\n2 ln D2H2\nδ̂ (13)\nWe simplify the bound on the right hand side a bit further so that it does not involve any Jacobian of layer d. Specifically, when d′′ < d, ∥Jd/d ′′ (x;W)[h]∥ can be written as the product of the spectral norm of the Jacobian d′ − 1/d′′ and the `2 norm of the hth row of Jacobian d − 1/d. Here, the latter can be upper bounded by the `2 norm of the hth row of Wd since the Jacobian (for a ReLU network) is essentially Wd but with some columns zerod out. When d = d′′, ∥Jd/d ′ (x;W)[h]∥ is essentially 1\nas the Jacobian is merely the identity matrix. Thus, we have:\n∣yd′′,h,h′ ∣ ≤ ⎧⎪⎪⎪ ⎨ ⎪⎪⎪⎩ σ ∥wdh∥ ∥J d−1/d′′(x;W)∥ 2 ∥Jd ′′−1/d′(x;W[+Ud′′−1])[∶, h ′]∥\n√ 4 ln DH\nδ̂ d′′ < d\nσ∥Jd ′′−1/d′(x;W[+Ud′′−1])[∶, h ′]∥ √\n4 ln DH δ̂\nd′′ = d\nBy a union bound on all d′′, we then get that with probability 1 − δ̂ D\nover the draws of Ud, we can upper bound Equation 12 as:\nmax h\nd\n∑ d′′=1\n√\n∑ h′\n(yd′′,h,h′) 2 ≤ σ ∥Jd−1/d ′ (x;W[+Ud′′−1])∥\nF\n√\n4 ln DH\nδ̂ +\nd−1 ∑\nd′′=d′+1 σmax h ∥wdh∥ ∥J\nd−1/d′′ (x;W)∥\n2 ∥Jd\n′′−1/d′ (x;W[+Ud′′−1])∥\nF\n√\n4 ln DH\nδ̂\nBy again applying a union bound for all d′, we get the above bound to hold simultaneously for all d′\nwith probability at least 1 − δ̂. Then, by a similar argument as in the case of the perturbation bound on the output of each layer, we get the result of the lemma.\nPERTURBATION BOUND ON THE SPECTRAL NORM OF THE JACOBIAN d/d′ . Again, we split this term and apply triangle equality as follows:\n∣∥Jd/d ′ (x;W)∥ 2 − ∥Jd/d ′ (x;W[+Ud])∥ 2 ∣ ≤ ∥Jd/d ′ (x;W) − Jd/d ′ (x;W[+Ud])∥\n2\n≤ XXXXXXXXXXXXXXXXXXX d ∑ d′′=1 ⎛ ⎜ ⎜ ⎜ ⎜ ⎝ Jd/d ′ (x;W[+Ud′′]) − J d/d′ (x;W[+Ud′′−1]) ´¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¸¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¶ ∶=Yd′′ ⎞ ⎟ ⎟ ⎟ ⎟ ⎠ XXXXXXXXXXXXXXXXXXX2\n≤ d\n∑ d′′=1\n∥Yd′′∥2 (14)\nHere, we have defined Yd′′ to be the matrix that corresponds to the difference in the Jacobian d/d′ brought about by perturbaing the the d′′th weight matrix, given that the first d′′ − 1 matrices have already been perturbed.\nAs argued before, under the frozen activation states, when we perturb the weight matrices from 1 uptil d′, since these matrices are not involved in the Jacobian d/d′, fortunately, the Jacobian d/d′ is not perturbed (as the set of active weights in d/d′ are the same when we perturbW asW[+Ud′]). So, we will only need to bound Yd′′ for d′′ > d′.\nRecall that we can exapnd Yd′′ for d′′ > d′, yd′′,h,h′ as the product of i) Jacobian d/d′′ ii) the perturbation matrix Ud′′ and iii) the Jacobian d′/d′′ − 1 for the perturbed network10:\nYd′′ =\nHd×Hd′′ ³¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹·¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹µ Jd/d ′′ (x;W) Hd′′×Hd′′−1 ³·µ Ud′′ Hd′′−1×Hd′ ³¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹·¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹µ Jd ′′−1/d′\n(x;W[+Ud′′−1]) ´¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¸¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¶\nspherical Gaussian\nNow, the spectral norm of Yd′′ is at most the products of the spectral norms of each of these three matrices. Using Lemma B.3, the spectral norm of the middle term Ud′′ can be bounded by σ √\n2H ln 2DH δ̂ with high probability 1 − δ̂ D over the draws of Ud′′ . 11\nWe will also decompose the spectral norm of the first term so that our final bound does not involve any Jacobian of the dth layer. When d′′ = d, this term has spectral norm 1 because the Jacobian d/d is essentially the identity matrix. When d′′ < d, we have that ∥Jd/d ′′ (x;W)∥\n2 ≤\n∥Jd/d−1(x;W)∥ 2 ∥Jd−1/d ′′ (x;W)∥ 2 . Furthermore, since, for a ReLU network, Jd/d−1(x;W) is effectively Wd with some columns zerod out, the spectral norm of the Jacobian is upper bounded by the spectral norm Wd.\nPutting all these together, we have that with probability 1 − δ̂ D over the draws of Ud′′ , the following holds good:\n∥Yd′′∥2 ≤ ⎧⎪⎪⎪ ⎨ ⎪⎪⎪⎩ σ ∥Wd∥2 ∥J d−1/d′′(x;W)∥ 2 ∥Jd ′′−1/d′(x;W[+Ud′′−1])∥ 2\n√ 2H ln 2DH\nδ̂ d′′ < d\nσ ∥Jd ′′−1/d′(x;W[+Ud′′−1])∥\n2\n√ 2H ln 2DH\nδ̂ d′′ = d\nBy a union bound, we then get that with probability 1 − δ̂ over the draws of Ud, we can upper bound Equation 14 as:\nd\n∑ d′′=1\n∥Yd′∥2 ≤σ ∥J d′′−1/d′\n(x;W[+Ud′′−1])∥ 2\n√\n2H ln 2DH\nδ̂\n+ σ d−1 ∑\nd′′=d′+1 ∥Wd∥2 ∥J\nd−1/d′′ (x;W)∥\n2 ∥Jd\n′′−1/d′ (x;W[+Ud′′−1])∥\n2\n√\n2H ln 2DH\nδ̂\nNote that the above bound simultaneously holds over all d′ (without the application of a union bound). Finally we get the result of the lemma by a similar argument as in the case of the perturbation bound on the output of each layer.\n10Again, note that the below succinct formula works even for corner cases like d′′ = d′ or d′′ = d. 11Although Lemma B.3 applies only to the case where Ud′′ is a H ×H matrix, it can be easily extended to the corner cases when d′′ = 1 or d′′ =D. When d′′ = 1, Ud′′ would be a H ×N matrix, where H > N ; one could imagine adding more random columns to this matrix, and applying Lemma B.3. Since adding columns does not reduce the spectral norm, the bound on the larger matrix would apply on the original matrix too. A similar argument would apply to d′′ =D, where the matrix would be K ×H .\n\n5 SUMMARY AND FUTURE WORK\nIn this work, we introduced a novel PAC-Bayesian framework for leveraging the noise-resilience of deep neural networks on training data, to derive a generalization bound on the original uncompressed, deterministic network. The main philosophy of our approach is to first generalize the noise-resilience from training data to test data using which we convert a PAC-Bayesian bound on a stochastic network to a standard margin-based generalization bound. We apply our approach to ReLU based networks and derive a bound that scales with terms that capture the interactions between the weight matrices better than the product of spectral norms.\nFor future work, the most important direction is that of removing the dependence on our strong assumption that the magnitude of the pre-activation values of the network are not too small on training data. More generally, a better understanding of the source of noise-resilience in deep ReLU networks would help in applying our framework more carefully in these settings, leading to tighter guarantees on the original network.\nACKNOWLEDGEMENTS. Vaishnavh Nagarajan was partially supported by a grant from the Bosch Center for AI.\n",
    "approval": true,
    "rationale": "This paper presents a PAC-Bayesian framework that bounds the generalization error of the learned model. While PAC-Bayesian bounds have been studied before, the focus of this paper is to study how different conditions in the network (e.g. behavior of activations) generalize from training set to the distribution. This is important since prior work have not been able to handle this issue properly and as a consequence, previous bounds are either on the networks with perturbed weights or with unrealistic assumptions on the behavior of the network for any input in the domain.\n\nI think the paper could have been written more clearly. I had a hard time following the arguments in the paper. For example, I had to start reading from the Appendix to understand what is going on and found the appendix more helpful than the main text. Moreover, the constraints should be discussed more clearly and verified through experiments.\n\nI see Constraint 2 as a major shortcoming of the paper. The promise of the paper was to avoid making assumptions on the input domain (one of the drawbacks in Neyshabur et al 2018) but the constraint 2 is on any input in the domain. In my view, this makes the result less interesting.\n\nFinally, as authors mention themselves, I think conditions in Theorem F.1 (the label should be 4.1 since it is in Section 4) could be improved with more work. More specifically, it seems that the condition on the pre-activation value can be improved by rebalancing using the positive homogeneity of ReLU activations.\n\nOverall, while I find the motivation and the approach interesting, I think this is not a complete piece of work and it can be improved significantly.\n\n===========\nUpdate: Authors have addressed my main concern, improved the presentation and added extra experiments that improve the quality of the paper.  I recommend accepting this paper. ",
    "rating": 8,
    "type": "positive"
  },
  {
    "title": "THE BREAK-EVEN POINT ON OPTIMIZATION TRAJEC- TORIES OF DEEP NEURAL NETWORKS",
    "abstract": "The early phase of training of deep neural networks is critical for their final performance. In this work, we study how the hyperparameters of stochastic gradient descent (SGD) used in the early phase of training affect the rest of the optimization trajectory. We argue for the existence of the “break-even\" point on this trajectory, beyond which the curvature of the loss surface and noise in the gradient are implicitly regularized by SGD. In particular, we demonstrate on multiple classification tasks that using a large learning rate in the initial phase of training reduces the variance of the gradient, and improves the conditioning of the covariance of gradients. These effects are beneficial from the optimization perspective and become visible after the break-even point. Complementing prior work, we also show that using a low learning rate results in bad conditioning of the loss surface even for a neural network with batch normalization layers. In short, our work shows that key properties of the loss surface are strongly influenced by SGD in the early phase of training. We argue that studying the impact of the identified effects on generalization is a promising future direction.",
    "text": "1 INTRODUCTION\nThe connection between optimization and generalization of deep neural networks (DNNs) is not fully understood. For instance, using a large initial learning rate often improves generalization, which can come at the expense of the initial training loss reduction (Goodfellow et al., 2016; Li et al., 2019; Jiang et al., 2020). In contrast, using batch normalization layers typically improves both generalization and convergence speed of deep neural networks (Luo et al., 2019; Bjorck et al., 2018). These simple examples illustrate limitations of our understanding of DNNs.\nUnderstanding the early phase of training has recently emerged as a promising avenue for studying the link between optimization and generalization of DNNs. It has been observed that applying regularization in the early phase of training is necessary to arrive at a well generalizing final solution (Keskar et al., 2017; Sagun et al., 2017; Achille et al., 2017). Another observed phenomenon is that the local shape of the loss surface changes rapidly in the beginning of training (LeCun et al., 2012; Keskar et al., 2017; Achille et al., 2017; Jastrzebski et al., 2018; Fort & Ganguli, 2019). Theoretical approaches to understanding deep networks also increasingly focus on the early part of the optimization trajectory (Li et al., 2019; Arora et al., 2019).\nIn this work, we study the dependence of the entire optimization trajectory on the early phase of training. We investigate noise in the mini-batch gradients using the covariance of gradients,1 and the local curvature of the loss surface using the Hessian. These two matrices capture important and\n∗Equal contribution. 1We define it as K = 1\nN ∑N i=1(gi − g)\nT (gi − g), where gi = g(xi, yi; θ) is the gradient of the training loss L with respect to θ on xi, N is the number of training examples, and g is the full-batch gradient.\ncomplementary aspects of optimization (Roux et al., 2008; Ghorbani et al., 2019) and generalization performance of DNNs (Jiang et al., 2020; Keskar et al., 2017; Bjorck et al., 2018; Fort et al., 2019). We include a more detailed discussion in Sec. 2.\nOur first contribution is a simplified model of the early part of the training trajectory of DNNs. Based on prior empirical work (Sagun et al., 2017), we assume that the local curvature of the loss surface (the spectral norm of the Hessian) increases or decreases monotonically along the optimization trajectory. Under this model, gradient descent reaches a point in the early phase of training at which it oscillates along the most curved direction of the loss surface. We call this point the break-even point and show empirical evidence of its existence in the training of actual DNNs.\nOur main contribution is to state and present empirical evidence for two conjectures about the dependence of the entire optimization trajectory on the early phase of training. Specifically, we conjecture that the hyperparameters of stochastic gradient descent (SGD) used before reaching the break-even point control: (1) the spectral norms of K and H, and (2) the conditioning of K and H. In particular, using a larger learning rate prior to reaching the break-even point reduces the spectral norm of K along the optimization trajectory (see Fig. 1 for an illustration of this phenomenon). Reducing the spectral norm of K decreases the variance of the mini-batch gradient, which has been linked to improved convergence speed (Johnson & Zhang, 2013).\nFinally, we apply our analysis to a network with batch normalization (BN) layers and find that our predictions are valid in this case as well. Delving deeper in this line of investigation, we show that using a large learning rate is necessary to reach better-conditioned (relatively to a network without BN layers) regions of the loss surface, which was previously attributed to BN alone (Bjorck et al., 2018; Ghorbani et al., 2019; Page, 2019).\n\n2 RELATED WORK\nImplicit regularization induced by the optimization method. The choice of the optimization method implicitly affects generalization performance of deep neural networks (Neyshabur, 2017). In particular, using a large initial learning rate is known to improve generalization (Goodfellow et al.,\n2016; Li et al., 2019). A classical approach to study these questions is to bound the generalization error using measures such as the norm of the parameters at the final minimum (Bartlett et al., 2017; Jiang et al., 2020).\nAn emerging approach is to study the properties of the whole optimization trajectory. Arora et al. (2019) suggest it is necessary to study the optimization trajectory to understand optimization and generalization of deep networks. In a related work, Erhan et al. (2010); Achille et al. (2017) show the existence of a critical period of learning. Erhan et al. (2010) argue that training, unless pretraining is used, is sensitive to shuffling of examples in the first epochs of training. Achille et al. (2017); Golatkar et al. (2019); Sagun et al. (2017); Keskar et al. (2017) demonstrate that adding regularization in the beginning of training affects the final generalization disproportionately more compared to doing so later. We continue research in this direction and study how the choice of hyperparameters in SGD in the early phase of training affects the optimization trajectory in terms of the covariance of gradients, and the Hessian.\nThe covariance of gradients and the Hessian. The Hessian quantifies the local curvature of the loss surface. Recent work has shown that the largest eigenvalues of H can grow quickly in the early phase of training (Keskar et al., 2017; Sagun et al., 2017; Fort & Scherlis, 2019; Jastrzebski et al., 2018). Keskar et al. (2017); Jastrzebski et al. (2017) studied the dependence of the Hessian (at the final minimum) on the optimization hyperparameters. The Hessian can be decomposed into two terms, where the dominant term (at least at the end of training) is the uncentered covariance of gradients G (Sagun et al., 2017; Papyan, 2019).\nThe covariance of gradients, which we denote by K, encapsulates the geometry and the magnitude of variation in gradients across different samples. The matrix K was related to the generalization error in Roux et al. (2008); Jiang et al. (2020). Closely related quantities, such as the cosine alignment between gradients computed on different examples, were recently shown to explain some aspects of deep networks generalization (Fort et al., 2019; Liu et al., 2020; He & Su, 2020). Zhang et al. (2019) argues that in DNNs the Hessian and the covariance of gradients are close in terms of the largest eigenvalues.\nLearning dynamics of deep neural networks. Our theoretical model is motivated by recent work on learning dynamics of neural networks (Goodfellow et al., 2014; Masters & Luschi, 2018; Wu et al., 2018; Yao et al., 2018; Xing et al., 2018; Jastrzebski et al., 2018; Lan et al., 2019). We are directly inspired by Xing et al. (2018) who show that for popular classification benchmarks, the cosine of the angle between consecutive optimization steps in SGD is negative. Similar observations can be found in Lan et al. (2019). Our theoretical analysis is inspired by Wu et al. (2018) who study how SGD selects the final minimum from a stability perspective. We apply their methodology to the early phase of training, and make predictions about the entire training trajectory.\n\n3 THE BREAK-EVEN POINT AND THE TWO CONJECTURES ABOUT SGD TRAJECTORY\nOur overall motivation is to better understand the connection between optimization and generalization of DNNs. In this section we study how the covariance of gradients (K) and the Hessian (H) depend on the early phase of training. We are inspired by recent empirical observations showing their importance for optimization and generalization of DNNs (see Sec. 2 for a detailed discussion).\nRecent work has shown that in the early phase of training the gradient norm (Goodfellow et al., 2016; Fort & Ganguli, 2019; Liu et al., 2020) and the local curvature of the loss surface (Jastrzebski et al., 2018; Fort & Ganguli, 2019) can rapidly increase. Informally speaking, one scenario we study here is when this initial growth is rapid enough to destabilize training. Inspired by Wu et al. (2018), we formalize this intuition using concepts from dynamical stability. Based on the developed analysis, we state two conjectures about the dependence of K and H on hyperparameters of SGD, which we investigate empirically in Sec. 4.\nDefinitions. We begin by introducing the notation. Let us denote the loss on an example (x, y) by L(x, y; θ), where θ is a D-dimensional parameter vector. The two key objects we study are the Hessian of the training loss (H), and the covariance of gradients K = 1N ∑N i=1(gi − g)T (gi − g),\nwhere gi = g(xi, yi; θ) is the gradient of L with respect to θ calculated on i-th example, N is the number of training examples, and g is the full-batch gradient. We denote the i-th normalized eigenvector and eigenvalue of a matrix A by eiA and λ i A. Both H and K are computed at a given θ, but we omit this dependence in the notation. Let t index steps of optimization, and let θ(t) denote the parameter vector at optimization step t.\nInspired by Wu et al. (2018) we introduce the following condition to quantify stability at a given θ(t). Let us denote the projection of parameters θ onto e1H by ψ = 〈θ, e1H〉. With a slight abuse of notation let g(ψ) = 〈g(θ), e1H〉. We say SGD is unstable along e1H at θ(t) if the norm of elements of sequence ψ(τ + 1) = ψ(τ)− ηg(ψ(τ)) diverges when τ →∞, where ψ(0) = θ(t). The sequence ψ(τ) represents optimization trajectory in which every step t′ > t is projected onto e1H .\nAssumptions. Based on recent empirical studies, we make the following assumptions.\n1. The loss surface projected onto e1H is a quadratic one-dimensional function of the form f(ψ) = ∑N i=1(ψ − ψ∗)2Hi. The same assumption was made in Wu et al. (2018), but\nfor all directions in the weight space. Alain et al. (2019) show empirically that the loss averaged over all training examples is well approximated by a quadratic function along e1H .\n2. The eigenvectors e1H and e 1 K are co-linear, i.e. e 1 H = ±e1K , and λ1K = αλ1H for some\nα ∈ R. This is inspired by the fact that the top eigenvalues of H can be well approximated using G (non-centered K) (Papyan, 2019; Sagun et al., 2017). Zhang et al. (2019) shows empirical evidence for co-linearity of the largest eigenvalues of K and H.\n3. If optimization is not stable along e1H at a given θ(t), λ 1 H decreases in the next step, and the\ndistance to the minimum along e1H increases in the next step. This is inspired by recent work showing training can escape a region with too large curvature compared to the learning rate (Zhu et al., 2018; Wu et al., 2018; Jastrzebski et al., 2018).\n4. The spectral norm of H, λ1H , increases during training and the distance to the minimum along e1H decreases, unless increasing λ 1 H would lead to entering a region where training\nis not stable along e1H . This is inspired by (Keskar et al., 2017; Goodfellow et al., 2016; Sagun et al., 2017; Jastrzebski et al., 2018; Fort & Scherlis, 2019; Fort & Ganguli, 2019) who show that in many settings λ1H or gradient norm increases in the beginning of training, while at the same time the overall training loss decreases.\nFinally, we also assume that S N , i.e. that the batch size is small compared to the number of training examples. These assumptions are only used to build a theoretical model for the early phase of training. Its main purpose is to make predictions about the training procedure that we test empirically in Sec. 4.\nReaching the break-even point earlier for a larger learning rate or a smaller batch size. Let us restrict ourselves to the case when training is initialized at θ(0) at which SGD is stable along e1H(0).\n2 We aim to show that the learning rate (η) and the batch size (S) determine H and K in our model, and conjecture that the same holds empirically for realistic neural networks.\nConsider two optimization trajectories for η1 and η2, where η1 > η2, that are initialized at the same θ0, where optimization is stable along e1H(t) and λ 1 H(t) > 0. Under Assumption 1 the loss surface\nalong e1H(t) can be expressed as f(ψ) = ∑N i=1(ψ − ψ∗)2Hi(t), where Hi(t) ∈ R. It can be shown that at any iteration t the necessary and sufficient condition for SGD to be stable along e1H(t) is:\n(1− ηλ1H(t))2 + s(t)2 η2(N − S) S(N − 1) ≤ 1, (1)\nwhere N is the training set size and s(t)2 = Var[Hi(t)] over the training examples. A proof can be found in (Wu et al., 2018). We call this point on the trajectory on which the LHS of Eq. 1 becomes equal to 1 for the first time the break-even point. By definition, there exists only a single break-even point on the training trajectory.\nUnder Assumption 3, λ1H(t) and λ 1 K(t) increase over time. If S = N , the break-even point is reached at λ1H(t) = 2 η . More generally, it can be shown that for η1, the break-even point is reached\n2We include a similar argument for the opposite case in App. B.\nfor a lower magnitude of λ1H(t) than for η2. The same reasoning can be repeated for S (in which case we assume N S). We state this formally and prove in App. B. Under Assumption 4, after passing the break-even point on the training trajectory, SGD does not enter regions where either λ1H or λ 1 K is larger than at the break-even point, as otherwise it would lead to increasing one of the terms in LHS of Eq. 1, and hence losing stability along e1H .\nTwo conjectures about real DNNs. Assuming that real DNNs reach the break-even point, we make the following two conjectures about their optimization trajectory.\nThe most direct implication of reaching the break-even point is that λ1K and λ 1 H at the break-even point depend on η and S, which we formalize as:\nConjecture 1 (Variance reduction effect of SGD). Along the SGD trajectory, the maximum attained values of λ1H and λ 1 K are smaller for a larger learning rate or a smaller batch size.\nWe refer to Conjecture 1 as variance reduction effect of SGD because reducing λ1K can be shown to reduce the L2 distance between the full-batch gradient, and the mini-batch gradient. We expect that similar effects exist for other optimization or regularization methods. We leave investigating them for future work.\nNext, we make another, stronger, conjecture. It is plausible to assume that reaching the break-even point affects to a lesser degree λiH and λ i K for i 6= 1 because increasing their values does not impact stability along e1H . Based on this we conjecture that:\nConjecture 2 (Pre-conditioning effect of SGD). Along the SGD trajectory, the maximum attained values of λ ∗ K\nλ1K and λ\n∗ H\nλ1H are larger for a larger learning rate or a smaller batch size, where λK ∗ and λH ∗ are the smallest non-zero eigenvalues of K and H, respectively. Furthermore, the maximum attained values of Tr(K) and Tr(H) are smaller for a larger learning rate or a smaller batch size.\nWe consider non-zero eigenvalues in the conjecture, because K has at most N − 1 non-zero eigenvalues, where N is the number of training points, which can be much smaller than D in overparametrized DNNs. Both conjectures are valid only for learning rates and batch sizes that guarantee that training converges.\nFrom the optimization perspective, the effects discussed above are desirable. Many papers in the optimization literature underline the importance of reducing the variance of the mini-batch gradient (Johnson & Zhang, 2013) and the conditioning of the covariance of gradients (Roux et al., 2008). There also exists a connection between these effects and generalization (Jiang et al., 2020), which we discuss towards the end of the paper.\n\n4 EXPERIMENTS\nIn this section we first analyse learning dynamics in the early phase of training. Next, we empirically investigate the two conjectures. In the final part we extend our analysis to a neural network with batch normalization layers.\nWe run experiments on the following datasets: CIFAR-10 (Krizhevsky, 2009), IMDB dataset (Maas et al., 2011), ImageNet (Deng et al., 2009), and MNLI (Williams et al., 2018). We apply to these datasets the following architectures: a vanilla CNN (SimpleCNN) following Keras example (Chollet et al., 2015), ResNet-32 (He et al., 2015), LSTM (Hochreiter & Schmidhuber, 1997), DenseNet (Huang et al., 2016), and BERT (Devlin et al., 2018). We also include experiments using a multi-layer perceptron trained on the FashionMNIST dataset (Xiao et al., 2017) in the Appendix. All experimental details are described in App. D.\nFollowing Dauphin et al. (2014); Alain et al. (2019), we estimate the top eigenvalues and eigenvectors of H on a small subset of the training set (e.g. 5% in the case of CIFAR-10) using the Lanczos algorithm (Lanczos, 1950). As computing the full eigenspace of K is infeasible for real DNNs, we compute the covariance using mini-batch gradients. In App. C we show empirically that (after normalization) this approximates well the largest eigenvalue, and we include other details on computing the eigenspaces.\n\n4.1 A CLOSER LOOK AT THE EARLY PHASE OF TRAINING\nFirst, we examine the learning dynamics in the early phase of training. Our goal is to verify some of the assumptions made in Sec. 3. We analyse the evolution of λ1H and λ 1 K when using η = 0.01 and η = 0.001 to train SimpleCNN on the CIFAR-10 dataset. We repeat this experiment 5 times using different random initializations of network parameters.\nVisualizing the break-even point. We visualize the early part of the optimization trajectory in Fig. 1. Following Erhan et al. (2010), we embed the test set predictions at each step of training of SimpleCNN using UMAP (McInnes et al., 2018). The background color indicates λ1K (left) and the training accuracy (right) at the iteration with the closest embedding in Euclidean distance.\nWe observe that the trajectory corresponding to the lower learning rate reaches regions of the loss surface characterized by larger λ1K , compared to regions reached at the same training accuracy in the second trajectory. Additionally, in Fig. 3 we plot the spectrum of K (left) and H (right) at the iterations when λK and λH respectively reach the highest values. We observe more outliers for the lower learning rate in the distributions of both λK and λH .\nAre λ1K and λ1H correlated in the beginning of training? The key assumption behind our theoretical model is that λ1K and λ 1 H are correlated, at least prior to reaching the break-even point. We confirm this in Fig. 2. The highest achieved λ1K and λ 1 H are larger for the smaller η. Additionally, we observe that after achieving the highest value of λ1H , further growth of λ 1 K does not translate to an increase of λ1H . This is expected as λ 1 H decays to 0 when the mean loss decays to 0 for cross entropy loss (Martens, 2016).\nDoes training become increasingly unstable in the early phase of training? According to Assumption 3, an increase of λ1K and λ 1 H translates into a decrease in stability, which we formalized as stability along e1H . Computing stability along e 1 H directly is computationally expensive. Instead, we measure a more tractable proxy. At each iteration we measure the loss on the training set before and\nafter taking the step, which we denote as ∆L (a positive value indicates a reduction of the training loss). In Fig. 2 we observe that training becomes increasingly unstable (∆L starts to take negative values) as λ1K reaches the maximum value.\nSummary. We have shown that the early phase of training is consistent with the assumptions made in our theoretical model. That is, λ1K and λ 1 H increase approximately proportionally to each other, which is also generally correlated with a decrease of a proxy of stability. Finally, we have shown qualitatively reaching the break-even point.\n\n4.2 THE VARIANCE REDUCTION AND THE PRE-CONDITIONING EFFECT OF SGD\nIn this section we test empirically Conjecture 1 and Conjecture 2. For each model we manually pick a suitable range of learning rates and batch sizes to ensure that the properties of K and H that we study have converged under a reasonable computational budget. We mainly focus on studying the covariance of gradients (K), and leave a closer investigation of the Hessian for future work. We use the batch size of 128 to compute K when we vary the batch size for training. When we vary the learning rate instead, we use the same batch size as the one used to train the model. App. C describes the remaining details on how we approximate the eigenspaces of K and H.\nWe summarize the results for SimpleCNN, ResNet-32, LSTM, BERT, and DenseNet in Fig. 4, Fig. 5, and Fig. 6. Curves are smoothed using moving average for clarity. Training curves and additional experiments are reported in App. E.\nTesting Conjecture 1. To test Conjecture 1, we examine the highest value of λ1K observed along the optimization trajectory. As visible in Fig. 4, using a higher η results in λ1K achieving a lower\nmaximum during training. Similarly, we observe that using a higher S in SGD leads to reaching a higher maximum value of λ1K . For instance, for SimpleCNN (top row of Fig. 4) we observe max(λ1K) = 0.68 and max(λ 1 K) = 3.30 for η = 0.1 and η = 0.01, respectively.\nTesting Conjecture 2. To test Conjecture 2, we compute the maximum value of λ∗K/λ1K along the optimization trajectory. It is visible in Fig. 4 that using a higher η results in reaching a larger maximum value of λ∗K/λ 1 K along the trajectory. For instance, in the case of SimpleCNN max(λ∗K/λ 1 K) = 0.37 and max(λ ∗ K/λ 1 K) = 0.24 for η = 0.1 and η = 0.01, respectively.\nA counter-intuitive effect of decreasing the batch size. Consistently with Conjecture 2, we observe that the maximum value of Tr(K) is smaller for the smaller batch size. In the case of SimpleCNN max(Tr(K)) = 5.56 and max(Tr(K)) = 10.86 for S = 10 and S = 100, respectively. Due to space constraints we report the effect of η and S on Tr(K) in other settings in App. E.\nThis effect is counter-intuitive because Tr(K) is proportional to the variance of the mini-batch gradient (see also App. C). Naturally, using a lower batch size generally increases the variance of the mini-batch gradient, and Tr(K). This apparent contradiction is explained by the fact that we measure Tr(K) using a different batch size (128) than the one used to train the model. Hence, decreasing the batch size both increases (due to approximating the gradient using fewer samples) and decreases (as predicted in Conjecture 2) the variance of the mini-batch gradient along the optimization trajectory.\nHow early in training is the break-even point reached? We find that λ1K and λ1H reach their highest values early in training, close to reaching 60% training accuracy on CIFAR-10, and 75% training accuracy on IMDB. The training and validation accuracies are reported for all the experiments in App. E. This suggests that the break-even point is reached early in training.\nThe Hessian. In the above, we have focused on the covariance of gradients. In Fig. 5 we report how λ1H depends on η and S for ResNet-32 and SimpleCNN. Consistently with prior work (Keskar et al., 2017; Jastrzebski et al., 2018), we observe that using a smaller η or using a larger S coincides with a larger maximum value of λ1H . For instance, for SimpleCNN we observe max(λ 1 H) = 26.27 and max(λ1H) = 211.17 for η = 0.1 and η = 0.01, respectively. We leave testing predictions made in Conjecture 2 about the Hessian for future work.\nLarger scale studies. Finally, we test the two conjectures in two larger scale settings: BERT fine-tuned on the MNLI dataset, and DenseNet trained on the ImageNet dataset. Due to memory constraints, we only vary the learning rate. We report results in Fig. 6. We observe that both conjectures hold in these two settings. It is worth noting that DenseNet uses batch normalization layers. In the next section we investigate closer batch-normalized networks.\nSummary. In this section we have shown evidence supporting the variance reduction (Conjecture 1) and the pre-conditioning effect (Conjecture 2) of SGD in a range of classification tasks. We also found that the above conclusions hold for MLP trained on the Fashion MNIST dataset, SGD with momentum, and SGD with learning rate decay. We include these results in App. E-G.\n\n4.3 IMPORTANCE OF LEARNING RATE FOR CONDITIONING IN NETWORKS WITH BATCH NORMALIZATION LAYERS\nThe loss surface of deep neural networks has been widely reported to be ill-conditioned (LeCun et al., 2012; Martens, 2016). Recently, Ghorbani et al. (2019); Page (2019) argued that the key reason behind the efficacy of batch normalization (Ioffe & Szegedy, 2015) is improving conditioning of the loss surface. In Conjecture 2 we suggest that using a high η (or a small S) results in improving the conditioning of K and H. A natural question that we investigate in this section is how the two phenomena are related. We study here the effect of learning rate, and report in App. H an analogous study for batch size.\nAre the two conjectures valid in networks with batch normalization layers? First, to investigate whether our conjectures hold in networks with batch normalization layers, we run similar experiments as in Sec. 4.2 with a SimpleCNN model with batch normalization layers inserted after each layer (SimpleCNN-BN), on the CIFAR-10 dataset. We test η ∈ {0.001, 0.01, 0.1, 1.0} (using η = 1.0 leads to divergence of SimpleCNN without BN). We summarize the results in Fig. 7. We observe that the evolution of λ∗K/λ 1 K and λ 1 K is consistent with both Conjecture 1 and Conjecture 2.\nA closer look at the early phase of training. To further corroborate that our analysis applies to networks with batch normalization layers, we study the early phase of training of SimpleCNN-BN, complementing the results in Sec. 4.1.\nWe observe in Fig. 7 (bottom) that training of SimpleCNN-BN starts in a region characterized by a relatively high λ1K . This is consistent with prior work showing that networks with batch normalization layers can exhibit gradient explosion in the first iteration (Yang et al., 2019). The value of λ1K then decays for all but the lowest η. This behavior is consistent with our theoretical model. We also track the norm of the scaling factor in the batch normalization layers, ‖γ‖, in the last layer of the network in Fig. 7 (bottom). It is visible that η = 1.0 and η = 0.1 initially decrease the value of ‖γ‖, which we hypothesize to be one of the mechanisms due to which high η steers optimization towards better conditioned regions of the loss surface in batch-normalized networks. Interestingly, this seems consistent with Luo et al. (2019) who argue that using mini-batch statistics in batch normalization acts as an implicit regularizer by reducing ‖γ‖.\nUsing batch normalization requires using a high learning rate. As our conjectures hold for SimpleCNN-BN, a natural question is if the loss surface can be ill-conditioned with a low learning rate even when batch normalization is used. Ghorbani et al. (2019) show that without batch normalization, mini-batch gradients are largely contained in the subspace spanned by the top eigenvectors of noncentered K. To answer this question we track ‖g‖/‖g5‖, where g denotes the mini-batch gradient, and g5 denotes the mini-batch gradient projected onto the top 5 eigenvectors of K. A value of ‖g‖/‖g5‖ close to 1 implies that the mini-batch gradient is mostly contained in the subspace spanned by the top 5 eigenvectors of K.\nWe compare two settings: SimpleCNN-BN optimized with η = 0.001, and SimpleCNN optimized with η = 0.01. We make three observations. First, the maximum and minimum values of ‖g‖/‖g5‖ are 1.90 (1.37) and 2.02 (1.09), respectively. Second, the maximum and minimum values of λ1K are 12.05 and 3.30, respectively. Finally, λ∗K/λ 1 K reaches 0.343 in the first setting, and 0.24 in the second setting. Comparing these differences to differences that are induced by using the highest η = 1.0 in SimpleCNN-BN, we can conclude that using a large learning rate is necessary to observe the effect of loss smoothing which was previously attributed to batch normalization alone (Ghorbani et al., 2019; Page, 2019; Bjorck et al., 2018). This might be directly related to the result that using a high learning rate is necessary to achieve good generalization when using batch normalization layers (Bjorck et al., 2018).\nSummary. We have shown that the effects of the learning rate predicted in Conjecture 1 and Conjecture 2 hold for a network with batch normalization layers, and that using a high learning rate is necessary in a network with batch normalization layers to improve conditioning of the loss surface, compared to conditioning of the loss surface in the same network without batch normalization layers.\n\n5 CONCLUSION\nBased on our theoretical model, we argued for the existence of the break-even point on the optimization trajectory induced by SGD. We presented evidence that hyperparameters used in the early phase of training control the spectral norm and the conditioning of K (a matrix describing noise in the mini-batch gradients) and H (a matrix describing local curvature of the loss surface) after reaching the break-even point. In particular, using a large initial learning rate steers training to better conditioned regions of the loss surface, which is beneficial from the optimization point of view.\nA natural direction for the future is connecting our observations to recent studies on the relation of measures, such as gradient variance, to the generalization of deep networks (Li et al., 2019; Jiang et al., 2020; Fort et al., 2019). Our work shows that the hyperparameters of SGD control these measures after the break-even point. Another interesting direction is to understand the connection between the existence of the break-even point and the existence of the critical learning period in training of DNNs (Achille et al., 2017).\n",
    "approval": true,
    "rationale": "The authors demonstrate that, during training, there is a point during the early phase of training that leads stochastic gradient descent (SGD) to a point where the covariance of the gradients (K) has a lower spectral norm (smaller first eigenvalue) and improved conditioning in K and the Hessian of the training loss (H).\n\nThe authors experiments seem to verify that learning rate and batch size do play a part in the spectral norm of K and the conditioning of K. My one issue is that, while effects on K produced by higher learning rates are supposed to be \"good\", the authors do not directly relate this back to model performance. From my years of experience training neural networks, I have seen many scenarios in which higher learning rates result in worse performance, even after reducing the learning rate. Can this be related back to the author's claims? Under what conditions does a higher learning rate lead to these effects on K and H and will it always lead to better model performance?\n\n\nOther comments:\nIn definitions, it says that the eigenvalue of matrix A is \\lambda_A^i, however, later in the 4th part of the assumptions, the spectral norm of H is referred to as \\lambda_1^H. Is there a difference here? Typo?\n\nThe last part of the definitions where \\Phi(\\tau) is introduced should have a formal definition for \\Phi(\\tau) as \\Phi is initially does not take any parameter \\tau.\n\nIn section 4.1, \"further growth of \\lambda_K^1 K does not translate into an increase of \\lambda_K^1\" \\lambda_K^1 is repeated. Typo?\n\n** After author response **\nChanging from weak accept to accept.\n\nThe authors have addressed my concerns about the paper.",
    "rating": 8,
    "type": "positive"
  },
  {
    "title": "DISCRETIZED NEURAL NETWORKS",
    "abstract": "Neural network quantization has become an important research area due to its great impact on deployment of large models on resource constrained devices. In order to train networks that can be effectively discretized without loss of performance, we introduce a differentiable quantization procedure. Differentiability can be achieved by transforming continuous distributions over the weights and activations of the network to categorical distributions over the quantization grid. These are subsequently relaxed to continuous surrogates that can allow for efficient gradient-based optimization. We further show that stochastic rounding can be seen as a special case of the proposed approach and that under this formulation the quantization grid itself can also be optimized with gradient descent. We experimentally validate the performance of our method on MNIST, CIFAR 10 and Imagenet classification.",
    "text": "1 INTRODUCTION\nNeural networks excel in a variety of large scale problems due to their highly flexible parametric nature. However, deploying big models on resource constrained devices, such as mobile phones, drones or IoT devices is still challenging because they require a large amount of power, memory and computation. Neural network compression is a means to tackle this issue and has therefore become an important research topic.\nNeural network compression can be, roughly, divided into two not mutually exclusive categories: pruning and quantization. While pruning (LeCun et al., 1990; Han et al., 2015) aims to make the model “smaller” by altering the architecture, quantization aims to reduce the precision of the arithmetic operations in the network. In this paper we focus on the latter. Most network quantization methods either simulate or enforce discretization of the network during training, e.g. via rounding of the weights and activations. Although seemingly straighforward, the discontinuity of the discretization makes the gradient-based optimization infeasible. The reason is that there is no gradient of the loss with respect to the parameters. A workaround to the discontinuity are the “pseudo-gradients” according to the straight-through estimator (Bengio et al., 2013), which have been successfully used for training low-bit width architectures at e.g. Hubara et al. (2016); Zhu et al. (2016).\nThe purpose of this work is to introduce a novel quantization procedure, Relaxed Quantization (RQ). RQ can bypass the non-differentiability of the quantization operation during training by smoothing it appropriately. The contributions of this paper are four-fold: First, we show how to make the set of quantization targets part of the training process such that we can optimize them with gradient descent. Second, we introduce a way to discretize the network by converting distributions over the weights and activations to categorical distributions over the quantization grid. Third, we show that we can obtain a “smooth” quantization procedure by replacing the categorical distributions with\n∗Work done while interning at Qualcomm AI Research.\nconcrete (Maddison et al., 2016; Jang et al., 2016) equivalents. Finally we show that stochastic rounding (Gupta et al., 2015), one of the most popular quantization techniques, can be seen as a special case of the proposed framework. We present the details of our approach in Section 2, discuss related work in Section 3 and experimentally validate it in Section 4. Finally we conclude and provide fruitful directions for future research in Section 5.\n\n2 RELAXED QUANTIZATION FOR DISCRETIZING NEURAL NETWORKS\nThe central element for the discretization of weights and activations of a neural network is a quantizer q(·). The quantizer receives a (usually) continous signal as input and discretizes it to a countable set of values. This process is inherently lossy and non-invertible: given the output of the quantizer, it is impossible to determine the exact value of the input. One of the simplest quantizers is the rounding function:\nq(x) = α\n⌊ x\nα +\n1\n2\n⌋ ,\nwhere α corresponds to the step size of the quantizer. With α = 1, the quantizer rounds x to its nearest integer number.\nUnfortunately, we cannot simply apply the rounding quantizer to discretize the weights and activations of a neural network. Because of the quantizers’ lossy and non-invertible nature, important information might be destroyed and lead to a decrease in accuracy. To this end, it is preferable to train the neural network while simulating the effects of quantization during the training procedure. This encourages the weights and activations to be robust to quantization and therefore decreases the performance gap between a full-precision neural network and its discretized version.\nHowever, the aforementioned rounding process is non-differentiable. As a result, we cannot directly optimize the discretized network with stochastic gradient descent, the workhorse of neural network optimization. In this work, we posit a “smooth” quantizer as a possible way for enabling gradient based optimization.\n\n2.1 LEARNING (FIXED POINT) QUANTIZERS VIA GRADIENT DESCENT\nThe proposed quantizer comprises four elements: a vocabulary, its noise model and the resulting discretization procedure, as well as a final relaxation step to enable gradient based optimization.\nThe first element of the quantizer is the vocabulary: it is the set of (countable) output values that the quantizer can produce. In our case, this vocabulary has an inherent structure, as it is a grid of ordered\nscalars. For fixed point quantization the grid G is defined as G = [ −2b−1, . . . , 0, . . . , 2b−1 − 1 ] , (1)\nwhere b is the number of available bits that allow for K = 2b possible integer values. By construction this grid of values is agnostic to the input signal x and hence suboptimal; to allow for the grid to adapt to x we introduce two free parameters, a scale α and an offset β. This leads to a learnable grid via Ĝ = αG + β that can adapt to the range and location of the input signal. The second element of the quantizer is the assumption about the input noise ; it determines how probable it is for a specific value of the input signal to move to each grid point. Adding noise to x will result in a quantizer that is, on average, a smooth function of its input. In essense, this is an application of variational optimization (Staines & Barber, 2012) to the non-differentiable rounding function, which enables us to do gradient based optimization.\nWe model this form of noise as acting additively to the input signal x and being governed by a distribution p( ). This process induces a distribution p(x̃) where x̃ = x+ . In the next step of the quantization procedure, we discretize p(x̃) according to the quantization grid Ĝ; this neccesitates the evaluation of the cumulative distribution function (CDF). For this reason, we will assume that the noise is distributed according to a zero mean logistic distribution with a standard deviation σ, i.e. L(0, σ), hence leading to p(x̃) = L(x, σ). The CDF of the logistic distribution is the sigmoid function which is easy to evaluate and backpropagate through. Using Gaussian distributions proved to be less effective in preliminary experiments. Other distributions are conceivable and we will briefly discuss the choice of a uniform distribution in Section 2.3.\nThe third element is, given the aforementioned assumptions, how the quantizer determines an appropriate assignment for each realization of the input signal x. Due to the stochastic nature of x̃, a deterministic round-to-nearest operation will result in a stochastic quantizer for x. Quantizing x in this manner corresponds to discretizing p(x̃) onto Ĝ and then sampling grid points gi from it. More specifically, we construct a categorical distribution over the grid by adopting intervals of width equal to α centered at each of the grid points. The probability of selecting that particular grid point will now be equal to the probability of x̃ falling inside those intervals:\np(x̂ = gi|x, σ) = P (x̃ ≤ (gi + α/2))− P (x̃ < (gi − α/2))) (2) = Sigmoid((gi + α/2− x)/σ)− Sigmoid((gi − α/2− x)/σ), (3)\nwhere x̂ corresponds to the quantized variable, P (·) corresponds to the CDF and the step from Equation 2 to Equation 3 is due to the logistic noise assumption. A visualization of the aforementioned process can be seen in Figure 1. For the first and last grid point we will assume that they reside within (g0 − α/2, g0 + α/2] and (gK − α/2, gK + α/2] respectively. Under this assumption we will have to truncate p(x̃) such that it only has support within (g0 − α/2, gK + α/2]. Fortunately this is easy to do, as it corresponds to just a simple modification of the CDF:\nP (x̃ ≤ c|x̃ ∈ (g0 − α/2, gK + α/2]) = P (x̃ ≤ c)− P (x̃ < (g0 − α/2))\nP (x̃ ≤ (gK + α/2))− P (x̃ < (g0 − α/2)) . (4)\nArmed with this categorical distribution over the grid, the quantizer proceeds to assign a specific grid value to x̂ by drawing a random sample. This procedure emulates quantization noise, which prevents the model from fitting the data. This noise can be reduced in two ways: by clustering the weights and activations around the points of the grid and by reducing the logistic noise σ. As σ → 0, the CDF converges towards the step function, prohibiting gradient flow. On the other hand, if is too high, the optimization procedure is very noisy, prohibiting convergence. For this reason, during optimization we initialize σ in a sensible range, such that L(x, σ) covers a significant portion of the grid. Please confer Appendix A for details. We then let σ be freely optimized via gradient descent such that the loss is minimized. Both effects reduce the gap between the function that the neural network computes during training time vs. test time. We illustrate this in Figure 2.\nThe fourth element of the procedure is the relaxation of the non-differentiable categorical distribution sampling. While we can use an unbiased gradient estimator via REINFORCE (Williams, 1992), we opt for a continuous relaxation due to high variances with REINFORCE. This is achieved by replacing the categorical distribution with a concrete distribution (Maddison et al., 2016; Jang et al., 2016). This relaxation procedure corresponds to adopting a “smooth” categorical distribution that\ncan be seen as a “noisy” softmax. Let πi be the categorical probability of sampling grid point i, i.e. πi = p(x̂ = gi); the “smoothed” quantized value x̂ can be obtained via:\nui ∼ Gumbel(0, 1), zi = exp((log πi + ui)/λ)∑ j exp((log πj + uj)/λ) , x̂ = K∑ i=1 zigi, (5)\nwhere zi is the random sample from the concrete distribution and λ is a temperature parameter that controls the degree of approximation, since as λ→ 0 the concrete distribution becomes a categorical. We have thus defined a fully differentiable “soft” quantization procedure that allows for stochastic gradients for both the quantizer parameters α, β, σ as well as the input signal x (e.g. the weights or the activations of a neural network). We refer to this algorithm as Relaxed Quantization (RQ). We summarize its forward pass as performed during training in Algorithm 1. It is also worthwhile to notice that if there were no noise at the input x then the categorical distribution would have non-zero mass only at a single value, thus prohibiting gradient based optimization for x and σ.\nOne drawback of this approach is that the smoothed quantized values defined in Equation 5 do not have to coincide with grid points, as z is not a one-hot vector. Instead, these values can lie anywhere between the smallest and largest grid point, something which is impossible with e.g. stochastic rounding (Gupta et al., 2015). In order to make sure that only grid-points are sampled, we propose an alternative algorithm RQ ST in which we use the variant of the straight-through (ST) estimator proposed in Jang et al. (2016). Here we sample the actual categorical distribution during the forward pass but assume a sample from the concrete distribution for the backward pass. While this gradient estimator is obviously biased, in practice it works as the “gradients” seem to point towards a valid direction. This effect was also recently studied at Yin et al. (2019). We perform experiments with both variants.\nAfter convergence, we can obtain a “hard” quantization procedure, i.e. select points from the grid, at test time by either reverting to a categorical distribution (instead of the continuous surrogate) or by rounding to the nearest grid point. In this paper we chose the latter as it is more aligned with the low-resource environments in which quantized models will be deployed. Furthermore, with this goal in mind, we employ two quantization grids with their own learnable scalar α, σ (and potentially β) parameters for each layer; one for the weights and one for the activations.\n\n2.2 SCALABLE QUANTIZATION VIA A LOCAL GRID\nSampling x̂ based on drawing K random numbers for the concrete distribution as described in Equation 5 can be very expensive for larger values of K. Firstly, drawing K random numbers\nAlgorithm 1 Quantization during training. Require: Input x, grid Ĝ, scale of the grid α,\nscale of noise σ, temperature λ, fuzz param. r = [Ĝ − α/2, gK + α/2] # interval points c = Sigmoid((r − x)/σ) # evaluate CDF πi = c[i+1]−c[i]+ c[K+1]−c[1]+K # categorical distr. z ∼ Concrete(π, λ) return ∑ i zigi\nAlgorithm 2 Quantization during testing. Require: Input x, scale and offset of the grid α, β,\nminimum and maximum values g0, gK y = α · round((x− β)/α) + β return min(gK ,max(g0, y)\nfor every individual weight and activation in a neural network drastically increases the number of operations required in the forward pass. Secondly, it also requires keeping many more numbers in memory for gradient computations during the backward pass. Compared to a standard neural network or stochastic rounding approaches, the proposed procedure can thus be infeasible for larger models and datasets.\nFortunately, we can make sampling x̂ independent of the grid size by assuming zero probability for grid-points that lie far away from the signal x. Specifically, by only considering grid points that are within δ standard deviations away from x, we truncate p(x̃) such that it lies within a “localized” grid around x.\nTo simplify the computation required for determining the local grid elements, we choose the grid point closest to x, bxe, as the center of the local grid (Figure 3). Since σ is shared between all elements of the weight matrix or activation, the local grid has the same width for every element.\nThe computation of the probabilities over the localized grid is similar to the truncation happening in Equation 4 and the smoothed quantized value is obtained via a manner similar to Equation 5:\nP (x̃ ≤ c|x̃ ∈ (bxe − δσ, bxe+ δσ]) = P (x̃ ≤ c)− P (x̃ < bxe − δσ) P (x̃ ≤ bxe+ δσ)− P (x̃ < bxe − δσ)\n(6)\nx̂ = ∑\ngi∈(bxe−δσ,bxe+δσ]\nzigi (7)\n\n2.3 RELATION TO STOCHASTIC ROUNDING\nOne of the pioneering works in neural network quantization has been the work of Gupta et al. (2015); it introduced stochastic rounding, a technique that is one of the most popular approaches for training neural networks with reduced numerical precision. Instead of rounding to the nearest representable value, the stochastic rounding procedure selects one of the two closest grid points with probability depending on the distance of the high precision input from these grid points. In fact, we can view stochastic rounding as a special case of RQ where p(x̃) = U(x− α2 , x+ α 2 ). This uniform distribution centered at x of width equal to the grid width α generally has support only for the closest grid point. Discretizing this distribution to a categorical over the quantization grid however assigns probabilities to the two closest grid points as in stochastic rounding, following Equation 2:\np(x̂ = ⌊x α ⌋ α |x) = P (x̃ ≤ ( ⌊x α ⌋ α+ α/2))− P (x̃ < ( ⌊x α ⌋ α− α/2)) = ⌈x α ⌉ − x α . (8)\nStochastic rounding has proven to be a very powerful quantization scheme, even though it relies on biased gradient estimates for the rounding procedure. On the one hand, RQ provides a way to circumvent this estimator at the cost of optimizing a surrogate objective. On the other hand, RQ ST makes use of the unreasonably effective straight-through estimator as used in Jang et al. (2016)\nto avoid optimizing a surrogate objective, at the cost of biased gradients. Compared to stochastic rounding, RQ ST further allows sampling of not only the two closest grid points, but also has support for more distant ones depending on the estimated input noise σ. Intuitively, this allows for larger steps in the input space without first having to decrease variance at the traversion between grid sections.\n\n3 RELATED WORK\nIn this work we focus on hardware oriented quantization approaches. As opposed to methods that focus only on weight quantization and network compression for a reduced memory footprint, quantizing all operations within the network aims to additionally provide reduced execution times. Within the body of work that considers quantizing weights and activations fall papers using stochastic rounding (Gupta et al., 2015; Hubara et al., 2016; Gysel et al., 2018; Wu et al., 2018). (Wu et al., 2018) also consider quantized backpropagation, which is out-of-scope for this work.\nFurthermore, another line of work considers binarizing (Courbariaux et al., 2015; Zhou et al., 2018) or ternarizing (Li et al., 2016; Zhou et al., 2018) weights and activations (Hubara et al., 2016; Rastegari et al., 2016; Zhou et al., 2016) via the straight-through gradient estimator (Bengio et al., 2013); these allow for fast implementations of convolutions using only bit-shift operations. In a similar vein, the straight through estimator has also been used in Cai et al. (2017); Faraone et al. (2018); Jacob et al. (2017); Zhou et al. (2017); Mishra & Marr (2017) for quantizing neural networks to arbitrary bit-precision. In these approaches, the full precision weights that are updated during training correspond to the means of the logistic distributions that are used in RQ. Furthermore, Jacob et al. (2017) maintains moving averages for the minimum and maximum observed values for activations while parameterises the network’s weights’ grids via their minimum and maximum values directly. This fixed-point grid is therefore learned during training, however without gradient descent; unlike the proposed RQ. Alternatively, instead of discretizing real valued weights, Shayer et al. (2018) directly optimize discrete distributions over them. While providing promising results, this approach does not generalize straightforwardly to activation quantization. A bayesian approach to binarized models was taken in Soudry et al. (2014), which provided encouraging results on small scale experiments with an ensemble of quantized models sampled from the approximate posterior distribution. For small vocabulary sizes (e.g. ternary weights / activations) Yin et al. (2016) proposed explicit formulas to compute the closest (according to the Euclidean distance) quantized value.\nAnother line of work quantizes networks through regularization. (Louizos et al., 2017a) formulate a variational approach that allows for heuristically determining the required bit-width precision for each weight of the model. Improving upon this work, (Achterhold et al., 2018) proposed a quantizing prior that encourages ternary weights during training. Similarly to RQ, this method also allows for optimizing the scale of the ternary grid. In contrast to RQ, this is only done implicitly via the regularization term. One drawback of these approaches is that the strength of the regularization decays with the amount of training data, thus potentially reducing their effectiveness on large datasets. Alternatively, one could directly regularize towards a set of specific values via the approach described at Yin et al. (2018).\nWeights in a neural network are usually not distributed uniformly within a layer. As a result, performing non-uniform quantization is usually more effective. (Baskin et al., 2018) employ a stochastic quantizer by first uniformizing the weight or activation distribution through a non-linear transformation and then injecting uniform noise into this transformed space. (Polino et al., 2018) propose a version of their method in which the quantizer’s code book is learned by gradient descent, resulting in a non-uniformly spaced grid. Another line of works quantizes by clustering and therefore falls into this category; (Han et al., 2015; Ullrich et al., 2017) represent each of the weights by the centroid of its closest cluster. While such non-uniform techniques can be indeed effective, they do not allow for efficient implementations on todays hardware. Nevertheless, there is encouraging recent work (Zhang et al., 2018) on non-uniform grids that can be implemented with bit operations.\nWithin the liteterature on quantizing neural networks there are many approaches that are orthogonal to our work and could potentially be combined for additional improvements. (Mishra & Marr, 2017; Polino et al., 2018) use knowledge distrillation techniques to good effect, whereas works such as (Mishra et al., 2017) modify the architecture to compensate for lower precision computations. (Zhou et al., 2017; 2018; Baskin et al., 2018) perform quantization in an step-by-step manner going from input layer to output, thus allowing the later layers to more easily adapt to the rounding errors\nintroduced. Polino et al. (2018); Faraone et al. (2018) further employ “bucketing”, where small groups of weights share a grid, instead of one grid per layer. As an example from Polino et al. (2018), a bucket size of 256 weights per grid on Resnet-18 translates to ∼ 91.4k separate scaling factors / offsets as opposed to 22 in RQ.\n\n4 EXPERIMENTS\nFor the subsequent experiments RQ will correspond to the proposed procedure that has concrete sampling and RQ ST will correspond to the proposed procedure that uses the Gumbel-softmax straight-through estimator (Jang et al., 2016) for the gradient. We did not optimize an offset for the grids in order to be able to represent the number zero exactly, which allows for sparsity and is required for zero-padding. Furthermore we assumed a grid that starts from zero when quantizing the outputs of ReLU. We provide further details on the experimental settings at Appendix A. We will also provide results of our own implementation of stochastic rounding (Gupta et al., 2015) with the dynamic fixed point format (Gysel et al., 2018) (SR+DR). Here we used the same hyperparameters as for RQ. All experiments were implemented with TensorFlow (Abadi et al., 2015), using the Keras library (Chollet et al., 2015).\n\n4.1 LENET-5 ON MNIST AND VGG7 ON CIFAR 10\nFor the first task we considered the toy LeNet-5 network trained on MNIST with the 32C5 - MP2 - 64C5 - MP2 - 512FC - Softmax architecture and the VGG 2x(128C3) - MP2 - 2x(256C3) - MP2 - 2x(512C3) - MP2 - 1024FC - Softmax architecture on the CIFAR 10 dataset. Details about the hyperparameter settings can be found in Appendix A.\nBy observing the results in Table 1, we see that our method can achieve competitive results that improve upon several recent works on neural network quantization. Considering that we achieve lower test error for 8 bit quantization than the high-precision models, we can see how RQ has a regularizing effect. Generally speaking we found that the gradient variance for low bit-widths (i.e. 2-4 bits) in RQ needs to be kept in check through appropriate learning rates.\n\n4.2 RESNET-18 AND MOBILENET ON IMAGENET\nIn order to demonstrate the effectiveness of our proposed approach on large scale tasks we considered the task of quantizing a Resnet-18 (He et al., 2016) as well as a Mobilenet (Howard et al., 2017) trained on the Imagenet (ILSVRC2012) dataset. For the Resnet-18 experiment, we started from a pre-trained full precision model that was trained for 90 epochs. We provide further details about the training procedure in Appendix C. The Mobilenet was initialized with the pretrained model available on the tensorflow github repository1. We quantized the weights of all layers, post ReLU activations and average pooling layer for various bit-widths via fine-tuning for ten epochs. Further details can be found in Appendix C.\nSome of the existing quantization works do not quantize the first (and sometimes) last layer. Doing so simplifies the problem but it can, depending on the model and input dimensions, significantly increase the amount of computation required. We therefore make use of the bit operations (BOPs) metric (Baskin et al., 2018), which can be seen as a proxy for the execution speed on appropriate hardware. In BOPs, the impact of not quantizing the first layer in, for example, the Resnet-18 model on Imagenet, becomes apparent: keeping the first layer in full precision requires roughly 1.3 times as many BOPs for one forward pass through the whole network compared to quantizing all weights and activations to 5 bits.\nFigure 4 compares a wide range of methods in terms of accuracy and BOPs. We choose to compare only against methods that employ fixed-point quantization on Resnet-18 and Mobilenet, hence do not compare with non-uniform quantization techniques, such as the one described at Baskin et al. (2018). In addition to our own implementation of (Gupta et al., 2015) with the dynamic fixed point format (Gysel et al., 2018), we also report results of “rounding”. This corresponds to simply rounding the pre-trained high-precision model followed by re-estimation of the batchnorm statistics. The grid\n1https://github.com/tensorflow/models/blob/master/research/slim/nets/ mobilenet_v1.md\nin this case is defined as the initial grid used for fine-tuning with RQ. For batchnorm re-estimation and grid initialization, please confer Appendix A.\nIn Figure 4a we observe that on ResNet-18 the RQ variants form the “Pareto frontier” in the trade-off between accuracy and efficiency, along with SYQ, Apprentice and Jacob et al. (2017). SYQ, however, employs “bucketing” and Apprentice uses distillation, both of which can be combined with RQ and improve performance. Jacob et al. (2017) does better than RQ with 8 bits, however RQ improved w.r.t. to its pretrained model, whereas Jacob et al. (2017) decreased slightly. For experimental details with Jacob et al. (2017), please confer Appendix C.1. SR+DR underperforms in this setting and is worse than simple rounding for 5 to 8 bits.\nFor Mobilenet, 4b shows that RQ is competitive to existing approaches. Simple rounding resulted in almost random chance for all of the bit configurations. SR+DR shows its strength for the 8 bit scenario, while in the lower bit regime, RQ outperforms competitive approaches.\n\n5 DISCUSSION\nWe have introduced Relaxed Quantization (RQ), a powerful and versatile algorithm for learning low-bit neural networks using a uniform quantization scheme. As such, the models trained by this method can be easily transferred and executed on low-bit fixed point chipsets. We have extensively evaluated RQ on various image classification benchmarks and have shown that it allows for the better trade-offs between accuracy and bit operations per second.\nFuture hardware might enable us to cheaply do non-uniform quantization, for which this method can be easily extended. (Lai et al., 2017; Ortiz et al., 2018) for example, show the benefits of low-bit floating point weights that can be efficiently implemented in hardware. The floating point quantization grid can be easily learned with RQ by redefining Ĝ. General non-uniform quantization, as described\nfor example in (Baskin et al., 2018), is a natural extension to RQ, whose exploration we leave to future work. For example, we could experiment with a base grid that is defined as in Zhang et al. (2018). Currently, the bit-width of every quantizer is determined beforehand, but in future work we will explore learning the required bit precision within this framework. In our experiments, batch normalization was implemented as a sequence of convolution, batch normalization and quantization. On a low-precision chip, however, batch normalization would be ”folded” (Jacob et al., 2017) into the kernel and bias of the convolution, the result of which is then rounded to low precision. In order to accurately reflect this folding at test time, future work on the proposed algorithm will emulate folded batchnorm at training time and learn the corresponding quantization grid of the modified kernel and bias. For fast model evaluation on low-precision hardware, quantization goes hand-in-hand with network pruning. The proposed method is orthogonal to pruning methods such as, for example, L0 regularization (Louizos et al., 2017b), which allows for group sparsity and pruning of hidden units.\n",
    "approval": true,
    "rationale": "The authors proposes a unified and general way of training neural network with reduced precision quantized synaptic weights and activations. The use case where such a quantization can be of use is the deployment of neural network models on resource constrained devices, such as mobile phones and embedded devices.\n\nThe paper is very well organized and systematically illustrates and motivates the ingredients that allows the authors to achieve their goal: a quantization grid with learnable position and range, stochastic quantization due to noise, and relaxing the hard categorical quantization assignment to a concrete distribution.\nThe authors then validate their method on several architectures (LeNet-5, VGG7, Resnet and mobilnet) on several datasets (MNIST, CIFAR10 and ImageNet) demonstrating competitive results both in terms of precision reduction and accuracy. \n\nMinor comments:\n- It would be interesting to know whether training with the proposed relaxed quantization method is slower than with full-precision activations and weights. It would have been informative to show learning curves comparing learning speed in the two cases.\n- It seems that this work could be generalized in a relatively straight-forward way to a case in which the quantization grid is not uniform, but instead all quantization interval are being optimized independently. It would have been interesting if the authors discussed this scenario, or at least motivated why they only considered quantization on a regular grid.\n",
    "rating": 7,
    "type": "positive"
  },
  {
    "title": "ENCODING WORD ORDER IN COMPLEX EMBEDDINGS",
    "abstract": "Sequential word order is important when processing text. Currently, neural networks (NNs) address this by modeling word position using position embeddings. The problem is that position embeddings capture the position of individual words, but not the ordered relationship (e.g., adjacency or precedence) between individual word positions. We present a novel and principled solution for modeling both the global absolute positions of words and their order relationships. Our solution generalizes word embeddings, previously defined as independent vectors, to continuous word functions over a variable (position). The benefit of continuous functions over variable positions is that word representations shift smoothly with increasing positions. Hence, word representations in different positions can correlate with each other in a continuous function. The general solution of these functions is extended to complex-valued domain due to richer representations. We extend CNN, RNN and Transformer NNs to complex-valued versions to incorporate our complex embedding (we make all code available). Experiments 1 on text classification, machine translation and language modeling show gains over both classical word embeddings and position-enriched word embeddings. To our knowledge, this is the first work in NLP to link imaginary numbers in complexvalued representations to concrete meanings (i.e., word order).",
    "text": "1 INTRODUCTION\nWhen processing text, the sequential structure of language is important, but can be computationally costly to model with neural networks (NNs) (Socher et al., 2011) due to the difficulty in parallelization. This has been alleviated by modeling word sequence not on the NN architecture level, but by adding position embeddings on the feature level. This has been done by the convolutional sequence model (ConvSeq) (Gehring et al., 2017) and the Transformer model (Vaswani et al., 2017) that replaces recurrent and convolution operations with purely attention mechanisms. More generally, vanilla position embeddings (Gehring et al., 2017) assume that individual word positions are independent and do not consider relations between neighbouring word positions. We posit that both the global absolute positions of words and their inner sequential and adjacent relationships are crucial in language. This is supported by recent empirical findings by Shaw et al. (2018) and Dai et al. (2019) who show the importance of modeling distance between sequential elements, and explicitly use extra relative position encodings to capture the relative-distance relationship of words.\nWe present a novel and principled approach to model both the global absolute positions of words and their inner sequential and adjacent relationships as follows: we extend each word embedding, previously defined as an independent vector, as a continuous function over an independent variable i.e., position. The benefit of continuous functions over variable positions is that word representations shift smoothly with increasing positions. Hence, word representations in different positions\n∗First two authors contributed equally. †Corresponding author: pzhang@tju.edu.cn 1The code is on https://github.com/iclr-complex-order/complex-order\n𝑟1\n𝑟2 𝑟3\n𝑝2 𝑝1\n𝑝3\nFigure 1: 3-dimensional complex embedding for a single word in different positions. The three wave functions (setting the initial phases as zero) show the real part of the embedding; the imaginary part has a π 2 phase difference and shows the same curves with its real-valued counterpart. The x-axis denotes the absolute position of a word and the y-axis denotes the value of each element in its word vector. Colours mark different dimensions of the embedding. The three cross points between the functions and each vertical line (corresponding to a specific position pos) represent the embedding for this word in the pos-th position.\ncan correlate with each other in a continuous function. Fig. 1 illustrates this type of word representation with a three-dimensional complex-valued embedding, where the amplitudes {r1, r2, r3} denote semantic aspects corresponding to classical word vectors, and periods {p1, p2, p3} denote how sensitive the word is to positional information. We further discuss the necessary properties of these functions to model sequential information and obtain a general solution in the form of a complexvalued embedding. Interestingly, there is a direct connection between a specific case of our general embedding and the well-known positional encoding in Vaswani et al. (2017) (see App. A).\nWe contribute (i) a novel paradigm that extends word vectors as continuous functions over changing variables like word position, and (ii) a general word embedding that models word order in a mathematically-sound manner. We integrate our complex word embeddings in state-of-the-art (SOTA) NN architectures (CNN, RNN, Transformer and experimentally find that it yields gains over both classical word embeddings and position-enriched word embeddings in text classification, machine translation and language modeling. Note that this is the first work in NLP to link imaginary numbers in complex-valued representation to concrete meanings (i.e., word order).\n\n2 MODELLING WORD ORDER IN EMBEDDING SPACE\nA Word Embedding (WE) generally defines a map fwe : N → RD from a discrete word index to a D-dimensional real-valued vector and N = {0, 1, 2, . . .}. Similarly, a Position Embedding (PE) (Gehring et al., 2017; Vaswani et al., 2017) defines another map fpe : N → RD from a discrete position index to a vector. The final embedding for word wj (wj ∈ W with index j in a given vocabulary W) in the pos-th position in a sentence is usually constructed by the sum\nf(j, pos) = fwe(j) + fpe(pos), (1)\nand f(j, pos) ∈ RD. Since both the word embedding map fw and the position embedding map fp only take integer values as word indexes or position indexes, embedding vectors for individual words or positions are trained independently. The independent training for each word vector is reasonable, since a word index is based on the order of a given arbitrary vocabulary and does not capture any specific sequential relationship with its neighboring words. However, the position index captures an ordered relationship, for instance adjacency or precedence, leading to the problem that position embeddings in individual positions (Gehring et al., 2017) are independent of each other; the ordered relationship between positions is not modelled. We refer to this as the position independence problem. This problem becomes more crucial when position embeddings are used in position-insensitive NNs, e.g., FastText (Mikolov et al., 2013b), ConvSeq (Gehring et al., 2017) and Transformer (Vaswani et al., 2017), because it is hard for such position-insensitive NNs with vanilla position embeddings (Gehring et al., 2017) to infer that wj1 in the pos-th position is close to wj2 in the pos + 1-th position, or that wj1 precedes wj2 ; instead, it is only inferred that wj1 and wj2 are in different positions, while the relative distance between them is almost unknown. Thus vanilla position embeddings (Gehring et al., 2017) cannot fully capture the sequential aspect of language.\nNext, we first introduce the necessary properties to model word order in embeddings, and then give a unique solution to meet such properties.\n\n2.1 EXTENDING VECTORS TO FUNCTIONS\nIn the general definition in Eq. 1, each dimension of the position embedding is obtained based on the discrete position indexes {0, 1, 2, ..., pos, ...}. This makes it difficult to model the ordered relationship between the positions. One solution to this problem is to build continuous functions over a variable (i.e., position index) to represent a specific word in an individual dimension. Formally, we define a general embedding as\nf(j, pos) = gj(pos) ∈ RD, (2) where gj is short for gwe(j) ∈ (F)D, indicating D functions over position index pos, and gwe(·) : N → (F)D is a mapping from a word index to D functions. By expanding the D dimension of gj , a word wj in the pos-th position can be represented as a D-dimensional vector as shown in\n[gj,1(pos), gj,2(pos), ..., gj,D(pos)] ∈ RD, (3) in which ∀gj,d(·) ∈ F : N→ R, d ∈ {1, 2, ..., D} is a function over the position index pos. To move the word wj from the current position pos to another one pos′, it needs only replace the variable pos to pos′ without changing gj .\nFunctions for words, especially continuous functions, are expected to capture smooth transformation from a position to its adjacent position therefore modeling word order. The position-independent position embedding (Gehring et al., 2017) can be considered as a special case of our definition when it only takes independent values for individual positions in the embedding function.\n\n2.2 PROPERTIES FOR THE FUNCTIONS TO CAPTURE WORD ORDER\nRelative distance is hard to compute because position indices are not visible in NNs after vector embedding (discrete position indices are necessarily embedded as vectors like words to be backpropagated with the gradient). Hence, we claim that the modeling of relative distance in NNs should be position-free: absolute position indices cannot be directly accessed in intermediate layers. Instead of processing position-free operations in NNs to capture relative distance between words, prior work (Shaw et al., 2018; Dai et al., 2019) first calculates the relative distance between words, and then feeds the relative distance as an additional feature or as embeddings/weights to NNs, instead of directly feeding with the raw position indices.\nAssume that words are embedded into RD, and let, for 1 ≤ d ≤ D, the function gj,d : N→ R be the embedding function giving the d-th coordinate of the representation of word wj (i.e., gj,d(pos) is the d-th coordinate of the embedding ofwj if it occurs at position pos. In the following, we simply write g instead of gj,d when there is no risk of confusion. Ideally, one would like there to exist a function Transformn : R → R that transforms the embedding of any word at some position pos to the embedding of a word at position pos + n such that Transformn is only dependent on the embedded value itself, but independent of the position pos, that is ∀pos : g(pos + n) = Transformn(g(pos)). Prior work in NLP (Li et al., 2019), Information Retrieval (Van Rijsbergen, 2004) and Machine Learning (Trabelsi et al., 2017) has shown the usefulness of complex numbers as richer representations. Complex word embeddings (Wang et al., 2019; Li et al., 2019; Li et al., 2018) have been used to model language. To investigate the potential of complex-valued representation, we extend the target domains of g(·) from RD to CD without losing generality, since real-valued numbers are specific complex numbers with their imaginary part being zero. This property regarding “position-free offset transformation” in complex-valued domains is formally defined in Property 1 below.\nProperty 1. Position-free offset transformation: An embedding function g : N → C is said to be a position-free offset transformation if there exists a function Transform : N × C → C (called the witness) such that for all n ≥ 1, the function Transformn(·) = Transform(n, ·) satisfies ∀pos ∈ N : g(pos+n) = Transformn(g(pos)). A position-free offset transformation g is said to be linearly witnessed if there is a function w : N→ C such that g has a witness Transform satisfying, for all n, Transform(n, pos) = Transformn(pos) = w(n) (i.e., each Transformn is a linear function).\nAdditionally, a boundedness property is necessary to ensure that the position embedding can deal with text of any length (pos could be large in a long document).\nProperty 2. Boundedness: The function over the variable position should be bounded, i.e. ∃δ ∈ R+,∀pos ∈ N, |g(pos)| ≤ δ.\nFormally, we prove the following claim that there is a unique solution that meets Properties 1 and 2 under the condition that the embedding function is linearly witnessed. We use linear functions because they are well-understood and simple with a single floating-point operation in NNs.\nClaim 1. A function g : N → C is a bounded and linearly witnessed position-free offset transformation iff it is on the form g(pos) = z2z pos 1 for z1, z2 ∈ C with |z1| ≤ 1.\nProof. Assume that g is a bounded and linearly witnessed position-free offset transformation. Then, by linear witnessing, we have for all pos, n1, n2 ∈ N:\nw(n1)w(n2)g(pos) = w(n2)g(pos + n1) = g(pos + n1 + n2) = Transformn1+n2(g(pos)) = w(n1 + n2)g(pos)\nwhencew(n1+n2) = w(n1)w(n2). Writew(1) = z1 and g(0) = z2. As n1, n2 ∈ N were arbitrary, we have w(n) = (w(1))n = zn1 for all n ∈ N. But then g(pos + n) = w(n)g(pos) = zn1 g(pos). Furthermore, observe that for pos ≥ 1, we have g(pos) = g(1+pos−1) = w(pos)g(0) = zpos1 z2 = z2z pos 1 . For pos = 0, g(0) = z2 = z2z 0 1 , whence g(pos) = z2z pos 1 , as desired. Observe that if |z1| > 1, then g(pos) is unbounded, whence we have |z1| ≤ 1. Conversely, assume that g is on the form g(pos) = z2z pos 1 with |z1| ≤ 1. Then, |g(pos)| ≤ |z2z pos 1 | ≤ |z2||z pos 1 | ≤ |z2|, whence g is bounded. Define, for each n ∈ N, w(n) = zn1 and Transformn(pos) = w(n)pos. Then, for all pos, n ∈ N,\ng(pos + n) = z2z pos+n 1 = z2z pos 1 z n 1 = g(pos)z n 1 = Transformn(g(pos))\nshowing that g is a linearly witnessed position-free offset transformation.\nFor any z ∈ C, we may write z = reiθ = r(cos θ + i sin θ). Thus, for the general form of the embedding g from Theorem 1, we have:\ng(pos) = z2z pos 1 = r2e iθ2(r1e iθ1)pos = r2r pos 1 e i(θ2+θ1pos) subject to |r1| ≤ 1 (4)\nIn implementations, the above definition of g will lead to an optimization problem due to the constraint |r1| ≤ 1. A natural and simple way to avoid this is to fix r1 = 1; note that |eix| ≡ 1, thus automatically satisfying the constraint, in contrast to a real-valued embedding where one would need to explicitly devise functions satisfying the constraint. Finally, Eq. 4 can be written in the simplified form: g(pos) = rei(ωpos+θ). Thus, one can think of g as embedding positions counterclockwise on a complex circle of radius r with a fixed period (r is the amplitude term, θ is the initial phase term, ω 2π is the frequency, and 2π ω is the period term).\n\n2.3 COMPLEX-VALUED WORD EMBEDDING\nWe now define our complex-valued word embedding g as a map taking a word index j and position word index pos to CD. For a word wj in position pos, our general complex-valued embedding is defined as f(j, pos) = gj(pos) = rjei(ωjpos+θj). Therefore, f(j, pos) is defined as:\n[rj,1e i(ωj,1pos+θj,1), ..., rj,2e i(ωj,2pos+θj,2), · · · , rj,Dei(ωj,Dpos+θj,D)] (5)\nNote that each coordinate d (1 ≤ d ≤ D) has a separate amplitude rj,d, period pj,d = 2πωj,d , and initial phase θj,d. In Fig. 1 each dimension is represented as a wave which is parameterized by an amplitude, a period/frequency, and an initial phase. The trainable parameters of the embedding are the amplitudes vector rj = [rj,1, ..., rj,D], the period/frequency related weights ωj = [ωj,1, ..., ωj,D], and the initial phase vector θj = [θj,1, ..., θj,D]. Note that the mean values of f(j, ·) over all positions are linearly dependent on the amplitude. Observe that the period/frequency determines to what degree the word is sensitive to the position. With an extremely long period (i.e., ωj very small), the complex-valued embedding is approximately constant for all possible values of pos, and hence approximates a standard word embedding. Conversely, if the period is short, the embedding will be highly sensitive to the position argument.\nIn our embedding, the mean vectors of f(j, ·) taken over all positions are linearly correlated to the amplitude embedding rj = [rj,1, ..., rj,K ] with a coefficient 2π . The amplitude rj,d of our embedding depends only on the word wj (and coordinate d), not on the position of the word, whence one can\nthink of the vector gpe(j, pos) = [ei(ωj,1pos+θj,1), · · · , ei(ωj,Dpos+θj,D)] as a “purely” positional embedding. Consequently, our complex embedding can be considered an element-wise multiplication between the word embedding gwe(j) = [rj,1, ..., rj,K ] and position embedding gpe.\nf(j, pos) = gwe(j) gpe(j, pos) (6)\nPrior work (Gehring et al., 2017; Vaswani et al., 2017) uses mean-weight addition between word embeddings fwe and position embeddings fpe (all words share the weights). In our work, word embeddings and position embeddings are decoupled to some extent by element-wise multiplication and therefore the frequency/period terms (related to ωj,d) can adaptively adjust the importance between semantic and position information for each word and each dimension. In particular, with higher frequency (i.e., large ωj,d), the final embedding will change dramatically with the changing positions, while it can be fixed for any positions with an extremely-small frequency (i.e., small ωj,d). Interestingly, the well-known position embedding in Transformer (Vaswani et al., 2017) can be seen as a degraded version of one of our specific complex word embeddings (see the Appendix A).\n\n3 EXPERIMENTAL EVALUATION\nWe evaluate our embeddings in text classification, machine translation and language modeling.\n\n3.1 TEXT CLASSIFICATION\nExperimental Setup. We use six popular text classification datasets: CR, MPQA, SUBJ, MR, SST, and TREC (see Tab. 1). We use accuracy as evaluation measure based on fixed train/dev/test splits or cross validation, as per prior work. We use Fasttext (Joulin et al., 2016), CNN (Kim, 2014), LSTM and Transformer (Vaswani et al., 2017) as NN baselines2. We use each of them: (1) without positional information; (2) with Vanilla Position Embeddings (PE) (randomly initialized and updated during training using the sum between word and position vectors (Gehring et al., 2017); (3) with Trigonometric Position Embeddings (TPE) (defining position embeddings as trigonometric functions as per Eq. 7); (4) with Complex-vanilla word embeddings (where the amplitude embedding is initialized by the pre-trained word vectors, and the phrase embedding is randomly initialized in a range from −π to π without considering word order (Wang et al., 2019)); and (5) with our order-aware complex-valued word embeddings, Complex-order (which encode position in the phase parts, train the periods, and where the amplitude embedding is also initialized by pretrained word vectors). For more details on the complex-valued extensions of NNs, see App. B and App. C.\nOur embedding generally has 3 × D × |W| parameters with D-dimensional word vectors and |W| words, while previous work (Mikolov et al., 2013b; Pennington et al., 2014) usually employs only D × |W| parameters for embedding lookup tables. To increase efficiency and facilitate fair comparison with previous work we set initial phases θj = [θj,1, ..., θj,D] to a shared constant value (such as zero). Furthermore, the period vectors ωj,d depend on word index j with length |W| and the coordinate index d with length D. To decrease the number of parameters, one can either use a word-sharing scheme (i.e., ωj,d = ω·,d), or a dimension-sharing scheme (ωj,d = ωj,·), leading to |W| ∗D + |W| and |W| ∗D +D parameters in total for the embedding layer.\n2Graph convolutional networks (GCNs) (Beck et al., 2018; Sahu et al., 2019) also encode positional information. We do not compare against them because they encode positional information inherently as part of the model, which makes redundant any additional encoding of positional information at the embedding level.\nWe search the hyper parameters from a parameter pool, with batch size in {32, 64, 128}, learning rate in {0.001, 0.0001, 0.00001}, L2-regularization rate in {0, 0.001, 0.0001}, and number of hidden layer units in {120, 128}. We use pre-trained 300-dimensional vectors from word2vec (Mikolov et al., 2013a) in all models except for Transformers. The models with trainable trigonometric position embedding produce nearly identical results compared to the non-trainable version, therefore we report the result of fixed position embeddings as per Vaswani et al. (2017). We adopt narrow convolution and max pooling in CNN, with number of filters in {64, 128}, and size of filters in {3, 4, 5}. In all Transformer models, we only use the encoder layer to extract feature information, where the layer is 1, dimension of word and inner hidden are 256 and 512 respectively, and head number is 8.\nResults. The results are shown in Tab. 2. Our complex-order embeddings outperform all other variations at all times. This gain in effectiveness comes at a negligible (or non-existent) cost in efficiency (it varies per NN architecture – see Fig. 2). CNNs are the best performing NN as expected following Bai et al. (2018). Tranformer NNs benefit the most from our complex-order embeddings, most likely because they are our weakest baseline. To contextualise these results, Tab. 3 shows classification accuracy of five typical approaches on the same datasets (as reported in the original papers). Our complex-order embeddings outperform all methods, except for the CR dataset, where InferSent is marginally better. Overall, our approach is on a par with the SOTA in embeddings.\nWe perform an ablation test (Tab. 4) on Transformer because it is the most common NN to be used with position embeddings. The two period-sharing schemas (dimension-sharing and word-sharing)\nslightly drop performance, because fewer parameters limit the representative power. Adding initial phases also hurts performance, although we observed that the loss could decrease faster in early epochs compared to the setting without offset. The negative effect of initial phases may be due to periodicity, andω cannot be directly regularized with L2-norm penalties. The sharing schemes slightly decrease the performance with less parameters. More details of the learned periods/frequencies (e.g. the distributions of periods/frequencies and case studies) are shown in App. D.\nNote that the word-sharing schema outperform the Vanilla Transformer, (both have a comparable number of parameters). If we choose<(WQ/K/V ) = =(WQ/K/V ), the additional parameters in the embedding layers will affect much less the whole parameter scale in the multiple-layer Transformer, since a embedding layer is only used in the first layer instead of the following Transformer layers.\n\n3.2 MACHINE TRANSLATION\nExperimental Setup. We use the standard WMT 2016 English-German dataset (Sennrich et al., 2016), whose training set consists of 29,000 sentence pairs. We use four baselines: basic Attentional encoder-decoder (AED) (Bahdanau et al., 2014); AED with Byte-pair encoding (BPE) subword segmentation for open-vocabulary translation (Sennrich et al., 2016); AED with extra linguistic features (morphological, part-of-speech, and syntactic dependency labels) (Sennrich & Haddow, 2016); and a 6-layer Transformer. Our approach (Transformer Complex-order) uses a batch size of 64, a head of 8, 6 layers, the rate of dropout is 0.1, and the dimension of the word embedding is 512. The embedding layer does not use initial phases, i.e., following f(j, pos) = rjei(ωjpos). We evaluate MT performance with the Bilingual Evaluation Understudy (BLEU) measure.\nResults Tab. 5 shows the MT results. Our approach outperforms all baselines. Two things are worth noting: (1) Both the vanilla Transformer and our Transformer Complex-order outperform the three Attentional encoder-decoder baselines which are based on an LSTM encoder and decoder, even when AED uses additional features. (2) Our Transformer Complex-Order outperforms the Vanilla Transformer and complex-vanilla Transformer by 1.3 and 1.1 in absolute BLEU score respectively.\n\n3.3 LANGUAGE MODELING\nExperimental Setup. We use the text8 (Mahoney, 2011) dataset, consisting of English Wikipedia articles. The text is lowercased from a to z, and space. The dataset contains 100M characters (90M for training, 5M for dev, and 5M for testing, as per Mikolov et al. (2012)). We use as baselines BN-LSTM, LN HM-LSTM RHN and Large mLSTM, which are typical recurrent NNs for language modeling in this dataset. We evaluate performance with the Bits Per Character (BPC) measure, (the lower, the better). We run the coder in Dai et al. (2019) with 6 layers for Transformer XL 6L; our model, named Transformer XL complex-order, directly replaces the word embedding with our proposed embedding under the same setting. We choose 6 layers due to limitations in computing resources. For Transformer XL Complex-order, all other parameter settings are as for Transformer XL (Dai et al., 2019). Our complex-order model does not use initial phases.\nResults. We see in Tab. 6 that our method outperforms all baselines. The first four baselines (BNLSTM, LN HM-LSTM, RHN and Large mLSTM) are based on recurrent NNs and rely on different regulation methods to become applicable in multiple-layer recurrent architectures. Transformerbased architectures can easily be stacked with multiple layers due to their advantages in parallelization, however the vanilla Transformer does not outperform the multiple-layer recurrent baselines, most likely due to its limitation of 6 layers. Our Transformer XL Complex-order outperforms its vanilla counterpart under the 6-layer setting (and also strong recurrent network baselines), demonstrating that our embedding also generalizes well in tasks with long-term dependency. With limited resources, slightly increasing the parameters in the feature layer like our proposed embedding could be more beneficial than stacking more layers with linearly increasing parameters.\n\n4 RELATED WORK\nComplex-valued NNs are not new (Georgiou & Koutsougeras, 1992; Kim & Adalı, 2003; Hirose, 2003). Complex-valued weights have been used in NNs, motivated by biology (Reichert & Serre, 2013), and also as signal processing in speech recognition (Shi et al., 2006). More recently, Arjovsky et al. (2016) shifted RNNs into the complex domain and Wolter & Yao (2018) proposed a novel complex gated recurrent cell. Trabelsi et al. (2017) also developped a complex-valued NN for computer vision and audio processing.\nComplex numbers have also been applied to text processing like (Van Rijsbergen, 2004; Melucci, 2015; Blacoe et al., 2013). Trouillon et al. (2016) adopt complex embedding for entities in Knowledge Graph Completion to represent antisymmetric relations with Hermitian dot product. Li et al. (2019); Wang et al. (2019) extend word embeddings to complex-valued fashion in quantum probability driven NNs, seeing the overview in Wang et al. (2019). However, the physical meaning of both the complex-valued entity and word embeddings is unknown, since a complex number was considered as two real numbers in a black-box learning paradigm. Our work first links the phase in complex numbers to word position to define concrete physical meaning in document representations.\n\n5 CONCLUSIONS\nWe extended word vectors to word functions with a variable i.e. position, to model the smooth shift among sequential word positions and therefore implicitly capture relative distances between words. These functions are well-defined to model the relative distances, therefore we derive a general solution in complex-valued fashion. Interestingly, the position embedding in Vaswani et al. (2017) can be considered a simplified version of our approach. We extend CNN, RNN and Transformer NNs to complex-valued versions to incorporate our complex embedding. Experiments on text classification, machine translation and language modeling show that our embeddings are more effective than vanilla position embeddings (Vaswani et al., 2017).\n",
    "approval": true,
    "rationale": "This paper proposes to learn position varying embeddings of words using complex numbers. Specifically, this work learns a position embedding by learning a continuous function that respects relative position based constraints and is bounded. The authors show that complex representations are an ideal fit for this purpose wherein the amplitude of the complex wave represents the base word embedding that is positionally invariants and the \"wave\" part encodes encodes the evolution of each dimension with position  as a periodic function with a learnable phase and period. \n\nResults are shown on text classification baselines and improvements are small. In the experiments related to machine translation and language modeling, some relevant baselines are missing (which were covered in the text classification case) , most importantly, Vaswani etal 2017 variant of complex position embeddings and \"complex-vanilla\", and all the numbers for other baselines are reported from the corresponding papers, hence it is unclear whether the improvement shown is strictly comparable or not.\n\nMoreover, a question that is unanswered is how does the periodicity affect the quality of embeddings. Basically, because of the periodic nature, the dimensions will take the same value for multiple positions spaced out according to the period. Is this a good assumption? Now, with large periods, for a finite practical length value, the periodic effects might end up not being observed but is that the case in the models that this approach learns? It would be great if authors could characterize the contexts/ words for which the periods are small and the contexts for which they are large. Basically, my concern is about the effect of getting the same embedding as output for different positions which is very likely if most of the periods learnt are small. \n\nAlso, empirical results with some other functions (maybe unbounded, or non-linear functions that do not respect relative positional constraints) would be insightful in order to assess the need for the desiderata laid out for the position sensitive functions. Finally, the \"iff\" proof needs to be cleaned up because I am not still not convinced if the proof holds oin both the directions and I believe there could be other functions with desired properties.\n\nThere are minor typos in equations like last line of \"Property 1\" related to g_pos, x.\n",
    "rating": 8,
    "type": "positive"
  },
  {
    "title": "TEMPORAL DIFFERENCE MODELS: MODEL-FREE DEEP RL FOR MODEL-BASED CONTROL",
    "abstract": "Model-free reinforcement learning (RL) is a powerful, general tool for learning complex behaviors. However, its sample efficiency is often impractically large for solving challenging real-world problems, even with off-policy algorithms such as Q-learning. A limiting factor in classic model-free RL is that the learning signal consists only of scalar rewards, ignoring much of the rich information contained in state transition tuples. Model-based RL uses this information, by training a predictive model, but often does not achieve the same asymptotic performance as model-free RL due to model bias. We introduce temporal difference models (TDMs), a family of goal-conditioned value functions that can be trained with model-free learning and used for model-based control. TDMs combine the benefits of model-free and model-based RL: they leverage the rich information in state transitions to learn very efficiently, while still attaining asymptotic performance that exceeds that of direct model-based RL methods. Our experimental results show that, on a range of continuous control tasks, TDMs provide a substantial improvement in efficiency compared to state-of-the-art model-based and model-free methods.",
    "text": "1 INTRODUCTION\nReinforcement learning (RL) algorithms provide a formalism for autonomous learning of complex behaviors. When combined with rich function approximators such as deep neural networks, RL can provide impressive results on tasks ranging from playing games (Mnih et al., 2015; Silver et al., 2016), to flying and driving (Lillicrap et al., 2015; Zhang et al., 2016), to controlling robotic arms (Levine et al., 2016; Gu et al., 2017). However, these deep RL algorithms often require a large amount of experience to arrive at an effective solution, which can severely limit their application to real-world problems where this experience might need to be gathered directly on a real physical system. Part of the reason for this is that direct, model-free RL learns only from the reward: experience that receives no reward provides minimal supervision to the learner.\nIn contrast, model-based RL algorithms obtain a large amount of supervision from every sample, since they can use each sample to better learn how to predict the system dynamics – that is, to learn the “physics” of the problem. Once the dynamics are learned, near-optimal behavior can in principle be obtained by planning through these dynamics. Model-based algorithms tend to be substantially more efficient (Deisenroth et al., 2013; Nagabandi et al., 2017), but often at the cost of larger asymptotic bias: when the dynamics cannot be learned perfectly, as is the case for most complex problems, the final policy can be highly suboptimal. Therefore, conventional wisdom holds that model-free methods are less efficient but achieve the best asymptotic performance, while model-based methods are more efficient but do not produce policies that are as optimal. ∗denotes equal contribution\nCan we devise methods that retain the efficiency of model-based learning while still achieving the asymptotic performance of model-free learning? This is the question that we study in this paper. The search for methods that combine the best of model-based and model-free learning has been ongoing for decades, with techniques such as synthetic experience generation (Sutton, 1990), partial modelbased backpropagation (Nguyen & Widrow, 1990; Heess et al., 2015), and layering model-free learning on the residuals of model-based estimation (Chebotar et al., 2017) being a few examples. However, a direct connection between model-free and model-based RL has remained elusive. By effectively bridging the gap between model-free and model-based RL, we should be able to smoothly transition from learning models to learning policies, obtaining rich supervision from every sample to quickly gain a moderate level of proficiency, while still converging to an unbiased solution.\nTo arrive at a method that combines the strengths of model-free and model-based RL, we study a variant of goal-conditioned value functions (Sutton et al., 2011; Schaul et al., 2015; Andrychowicz et al., 2017). Goal-conditioned value functions learn to predict the value function for every possible goal state. That is, they answer the following question: what is the expected reward for reaching a particular state, given that the agent is attempting (as optimally as possible) to reach it? The particular choice of reward function determines what such a method actually does, but rewards based on distances to a goal hint at a connection to model-based learning: if we can predict how easy it is to reach any state from any current state, we must have some kind of understanding of the underlying “physics.” In this work, we show that we can develop a method for learning variable-horizon goalconditioned value functions where, for a specific choice of reward and horizon, the value function corresponds directly to a model, while for larger horizons, it more closely resembles model-free approaches. Extension toward more model-free learning is thus achieved by acquiring “multi-step models” that can be used to plan over progressively coarser temporal resolutions, eventually arriving at a fully model-free formulation.\nThe principle contribution of our work is a new RL algorithm that makes use of this connection between model-based and model-free learning to learn a specific type of goal-conditioned value function, which we call a temporal difference model (TDM). This value function can be learned very efficiently, with sample complexities that are competitive with model-based RL, and can then be used with an MPC-like method to accomplish desired tasks. Our empirical experiments demonstrate that this method achieves substantially better sample complexity than fully model-free learning on a range of challenging continuous control tasks, while outperforming purely model-based methods in terms of final performance. Furthermore, the connection that our method elucidates between model-based and model-free learning may lead to a range of interesting future methods.\n\n2 PRELIMINARIES\nIn this section, we introduce the reinforcement learning (RL) formalism, temporal difference Qlearning methods, model-based RL methods, and goal-conditioned value functions. We will build on these components to develop temporal difference models (TDMs) in the next section. RL deals with decision making problems that consist of a state space S, action space A, transition dynamics P (s′ | s, a), and an initial state distribution p0. The goal of the learner is encapsulated by a reward function r(s, a, s′). Typically, long or infinite horizon tasks also employ a discount factor γ, and the standard objective is to find a policy π(a | s) that maximizes the expected discounted sum of rewards, Eπ[ ∑ t γ tr(st, at, st+1)], where s0 ∼ p0, at ∼ π(at|st), and st+1 ∼ P (s′ | s, a).\nQ-functions. We will focus on RL algorithms that learn a Q-function. The Q-function represents the expected total (discounted) reward that can be obtained by the optimal policy after taking action at in state st, and can be defined recursively as following:\nQ(st, at) = Ep(st+1|st,at)[r(st, at, st+1) + γmaxa Q(st+1, a)]. (1)\nThe optimal policy can then recovered according to π(at|st) = δ(at = argmaxaQ(st, a)). Qlearning algorithms (Watkins & Dayan, 1992; Riedmiller, 2005) learn the Q-function via an offpolicy stochastic gradient descent algorithm, estimating the expectation in the above equation with samples collected from the environment and computing its gradient. Q-learning methods can use transition tuples (st, at, st+1, rt) collected from any exploration policy, which generally makes them more efficient than direct policy search, though still less efficient than purely model-based methods.\nModel-based RL and optimal control. Model-based RL takes a different approach to maximize the expected reward. In model-based RL, the aim is to train a model of the form f(st, at) to predict the next state st+1. Once trained, this model can be used to choose actions, either by backpropagating reward gradients into a policy, or planning directly through the model. In the latter case, a particularly effective method for employing a learned model is model-predictive control (MPC), where a new action plan is generated at each time step, and the first action of that plan is executed, before replanning begins from scratch. MPC can be formalized as the following optimization problem:\nat = argmax at:t+T t+T∑ i=t r(si, ai) where si+1 = f(si, ai) ∀ i ∈ {t, ..., t+ T − 1}. (2)\nWe can also write the dynamics constraint in the above equation in terms of an implicit dynamics, according to\nat = argmax at:t+T ,st+1:t+T t+T∑ i=t r(si, ai) such that C(si, ai, si+1) = 0 ∀ i ∈ {t, ..., t+ T − 1}, (3)\nwhere C(si, ai, si+1) = 0 if and only if si+1 = f(si, ai). This implicit version will be important in understanding the connection between model-based and model-free RL.\nGoal-conditioned value functions. Q-functions trained for a specific reward are specific to the corresponding task, and learning a new task requires optimizing an entirely new Q-function. Goalconditioned value functions address this limitation by conditioning the Q-value on some task description vector sg ∈ G in a goal space G. This goal vector induces a parameterized reward r(st, at, st+1, sg), which in turn gives rise to parameterized Q-functions of the form Q(s, a, sg). A number of goal-conditioned value function methods have been proposed in the literature, such as universal value functions (Schaul et al., 2015) and Horde (Sutton et al., 2011). When the goal corresponds to an entire state, such goal-conditioned value functions usually predict how well an agent can reach a particular state, when it is trying to reach it. The knowledge contained in such a value function is intriguingly close to a model: knowing how well you can reach any state is closely related to understanding the physics of the environment. With Q-learning, these value functions can be learned for any goal sg using the same off-policy (st, at, st+1) tuples. Relabeling previously visited states with the reward for any goal leads to a natural data augmentation strategy, since each tuple can be replicated many times for many different goals without additional data collection. Andrychowicz et al. (2017) used this property to produce an effective curriculum for solving multi-goal task with delayed rewards. As we discuss below, relabeling past experience with different goals enables goal-conditioned value functions to learn much more quickly from the same amount of data.\n\n3 TEMPORAL DIFFERENCE MODEL LEARNING\nIn this section, we introduce a type of goal-conditioned value functions called temporal difference models (TDMs) that provide a direct connection to model-based RL. We will first motivate this connection by relating the model-based MPC optimizations in Equations (2) and (3) to goal-conditioned value functions, and then present our temporal difference model derivation, which extends this connection from a purely model-based setting into one that becomes increasingly model-free.\n\n3.1 FROM GOAL-CONDITIONED VALUE FUNCTIONS TO MODELS\nLet us consider the choice of reward function for the goal conditioned value function. Although a variety of options have been explored in the literature (Sutton et al., 2011; Schaul et al., 2015; Andrychowicz et al., 2017), a particularly intriguing connection to model-based RL emerges if we set G = S, such that g ∈ G corresponds to a goal state sg ∈ S , and we consider distance-based reward functions rd of the following form:\nrd(st, at, st+1, sg) = −D(st+1, sg),\nwhere D(st+1, sg) is a distance, such as the Euclidean distance D(st+1, sg) = ‖st+1 − sg‖2. If γ = 0, we have Q(st, at, sg) = −D(st+1, sg) at convergence of Q-learning, which means that\nQ(st, at, sg) = 0 implies that st+1 = sg . Plug this Q-function into the model-based planning optimization in Equation (3), denoting the task control reward as rc, such that the solution to\nat = argmax at:t+T ,st+1:t+T t+T∑ i=t rc(si, ai) such that Q(si, ai, si+1) = 0 ∀ i ∈ {t, ..., t+ T − 1} (4)\nyields a model-based plan. We have now derived a precise connection between model-free and model-based RL, in that model-free learning of goal-conditioned value functions can be used to directly produce an implicit model that can be used with MPC-based planning. However, this connection by itself is not very useful: the resulting implicit model is fully model-based, and does not provide any kind of long-horizon capability. In the next section, we show how to extend this connection into the long-horizon setting by introducing the temporal difference model (TDM).\n\n3.2 LONG-HORIZON LEARNING WITH TEMPORAL DIFFERENCE MODELS\nIf we consider the case where γ > 0, the optimization in Equation (4) no longer corresponds to any optimal control method. In fact, when γ = 0, Q-values have well-defined units: units of distance between states. For γ > 0, no such interpretation is possible. The key insight in temporal difference models is to introduce a different mechanism for aggregating long-horizon rewards. Instead of evaluating Q-values as discounted sums of rewards, we introduce an additional input τ , which represents the planning horizon, and define the Q-learning recursion as Q(st, at, sg, τ) = Ep(st+1|st,at)[−D(st+1, sg)1[τ=0] + maxa Q(st+1, a, sg, τ−1)1[τ 6=0]]. (5) The Q-function uses a reward of −D(st+1, sg) when τ = 0 (at which point the episode terminates), and decrements τ by one at every other step. Since this is still a well-defined Q-learning recursion, it can be optimized with off-policy data and, just as with goal-conditioned value functions, we can resample new goals sg and new horizons τ for each tuple (st, at, st+1), even ones that were not actually used when the data was collected. In this way, the TDM can be trained very efficiently, since every tuple provides supervision for every possible goal and every possible horizon.\nThe intuitive interpretation of the TDM is that it tells us how close the agent will get to a given goal state sg after τ time steps, when it is attempting to reach that state in τ steps. Alternatively, TDMs can be interpreted as Q-values in a finite-horizon MDP, where the horizon is determined by τ . For the case where τ = 0, TDMs effectively learn a model, allowing TDMs to be incorporated into a variety of planning and optimal control schemes at test time as in Equation (4). Thus, we can view TDM learning as an interpolation between model-based and model-free learning, where τ = 0 corresponds to the single-step prediction made in model-based learning and τ > 0 corresponds to the long-term prediction made by typical Q-functions. While the correspondence to models is not the same for τ > 0, if we only care about the reward at every K step, then we can recover a correspondence by replace Equation (4) with\nat = argmax at:K:t+T ,st+K:K:t+T ∑ i=t,t+K,...,t+T rc(si, ai)\nsuch that Q(si, ai, si+K ,K − 1) = 0 ∀ i ∈ {t, t+K, ..., t+ T −K}, (6)\nwhere we only optimize over every K th state and action. As the TDM becomes effective for longer horizons, we can increase K until K = T , and plan over only a single effective time step:\nat = argmax at,at+T ,st+T\nrc(st+T , at+T ) such that Q(st, at, st+T , T − 1) = 0. (7)\nThis formulation does result in some loss of generality, since we no longer optimize the reward at the intermediate steps. This limits the multi-step formulation to terminal reward problems, but does allow us to accommodate arbitrary reward functions on the terminal state st+T , which still describes a broad range of practically relevant tasks. In the next section, we describe how TDMs can be implemented and used in practice for continuous state and action spaces.\n\n4 TRAINING AND USING TEMPORAL DIFFERENCE MODELS\nThe TDM can be trained with any off-policy Q-learning algorithm, such as DQN (Mnih et al., 2015), DDPG (Lillicrap et al., 2015), NAF (Gu et al., 2016), and SDQN (Metz et al., 2017). During off-policy Q-learning, TDMs can benefit from arbitrary relabeling of the goal states g and the\nhorizon τ , given the same (st, at, st+1) tuples from the behavioral policy as done in (Andrychowicz et al., 2017). This relabeling enables simultaneous, data-efficient learning of short-horizon and long-horizon behaviors for arbitrary goal states, unlike previously proposed goal-conditioned value functions that only learn for a single time scale, typically determined by a discount factor (Schaul et al., 2015; Andrychowicz et al., 2017). In this section, we describe the design decisions needed to make practical a TDM algorithm.\n\n4.1 REWARD FUNCTION SPECIFICATION\nQ-learning typically optimizes scalar rewards, but TDMs enable us to increase the amount of supervision available to the Q-function by using a vector-valued reward. Specifically, if the distance D(s, sg) factors additively over the dimensions, we can train a vector-valued Q-function that predicts per-dimension distance, with the reward function for dimension j given by −Dj(sj , sg,j). We use the `1 norm in our implementation, which corresponds to absolute value reward −|sj − sg,j |. The resulting vector-valued Q-function can learn distances along each dimension separately, providing it with more supervision from each training point. Empirically, we found that this modifications provides a substantial boost in sample efficiency.\nWe can optionally make an improvement to TDMs if we know that the task reward rc depends only on some subset of the state or, more generally, state features. In that case, we can train the TDM to predict distances along only those dimensions or features that are used by rc, which in practice can substantially simplify the corresponding prediction problem. In our experiments, we illustrate this property by training TDMs for pushing tasks that predict distances from an end-effector and pushed object, without accounting for internal joints of the arm, and similarly for various locomotion tasks.\n\n4.2 POLICY EXTRACTION WITH TDMS\nWhile the TDM optimal control formulation Equation (7) drastically reduces the number of states and actions to be optimized for long-term planning, it requires solving a constrained optimization problem, which is more computationally expensive than unconstrained problems. We can remove the need for a constrained optimization through a specific architectural decision in the design of the function approximator for Q(s, a, sg, τ). We define the Q-function as Q(s, a, sg, τ) = −‖f(s, a, sg, τ)− sg‖, where f(s, a, sg, τ) outputs a state vector. By training the TDM with a standard Q-learning method, f(s, a, sg, τ) is trained to explicitly predict the state that will be reached by a policy attempting to reach sg in τ steps. This model can then be used to choose the action with fully explicit MPC as below, which also allows straightforward derivation of a multi-step version as in Equation (6).\nat = argmax at,at+T ,st+T\nrc(f(st, at, st+T , T − 1), at+T ) (8)\nIn the case where the task is to reach a goal state sg , a simpler approach to extract a policy is to use the TDM directly:\nat = argmax a Q(st, a, sg, T ) (9)\nIn our experiments, we use Equations (8) and (9) to extract a policy.\n\n4.3 ALGORITHM SUMMARY\nThe algorithm is summarized as Algorithm 1. A crucial difference from prior goal-conditioned value function methods (Schaul et al., 2015; Andrychowicz et al., 2017) is that our algorithm can be used to act according to an arbitrary terminal reward function rc, both during exploration and at test time. Like other off-policy algorithms (Mnih et al., 2015; Lillicrap et al., 2015), it consists of exploration and Q-function fitting. Noise is injected for exploration, and Q-function fitting uses standard Qlearning techniques, with target networks Q′ and experience replay (Mnih et al., 2015; Lillicrap et al., 2015). If we view the Q-function fitting as model fitting, the algorithm also resembles iterative model-based RL, which alternates between collecting data using the learned dynamics model for planning (Deisenroth & Rasmussen, 2011) and fitting the model. Since we focus on continuous tasks, we use DDPG (Lillicrap et al., 2015), though any Q-learning method could be used.\nThe computation cost of the algorithm is mostly determined by the number of updates to fit the Q-function per transition, I . In general, TDMs can benefit from substantially larger I than classic\nAlgorithm 1 Temporal Difference Model Learning Require: Task reward function rc(s, a), parameterized TDM Qw(s, a, sg, τ), replay buffer B 1: for n = 0, ..., N − 1 episodes do 2: s0 ∼ p(s0) 3: for t = 0, ..., T − 1 time steps do 4: a∗t = MPC(rc, st, Qw, T − t) // Eq. 6, Eq. 7, Eq. 8, or Eq. 9 5: at = AddNoise(a∗t ) // Noisy exploration 6: st+1 ∼ p(st, at), and store {st, at, st+1} in the replay buffer B // Step environment 7: for i = 0, I − 1 iterations do 8: Sample M transitions {sm, am, s′m} from the replay B. 9: Relabel time horizons and goal states τm, sg,m // Section A.1 10: ym = −‖s′m − sg,m‖1[τm = 0] + maxaQ′(s′m, a, sg,m, τm − 1)1[τm 6= 0] 11: L(w) = ∑ m(Qw(sm, am, sg,m, τm)− ym)\n2/M // Compute the loss 12: Minimize(w,L(w)) // Optimize 13: end for 14: end for 15: end for\nmodel-free methods such as DDPG due to relabeling increasing the amount of supervision signals. In real-world applications such as robotics where we care most of the sample efficiency (Gu et al., 2017), the learning is often bottlenecked by the data collection rather than the computation, and therefore large I values are usually not a significant problem and can continuously benefit from the acceleration in computation.\n\n5 RELATED WORK\nCombining model-based and model-free reinforcement learning techniques is a well-studied problem, though no single solution has demonstrated all of the benefits of model-based and model-free learning. Some methods first learn a model and use this model to simulate experience (Sutton, 1990; Gu et al., 2016) or compute better gradients for model-free updates (Heess et al., 2015; Nguyen & Widrow, 1990). Other methods use model-free algorithms to correct for the local errors made by the model (Chebotar et al., 2017; Bansal et al., 2017). While these prior methods focused on combining different model-based and model-free RL techniques, our method proposes an equivalence between these two branches of RL through a specific generalization of goal-conditioned value function. As a result, our approach achieves much better sample efficiency in practice on a variety of challenging reinforcement learning tasks than model-free alternatives, while exceeding the performance of purely model-based approaches.\nWe are not the first to study the connection between model-free and model-based methods, with Boyan (1999) and Parr et al. (2008) being two notable examples. Boyan (1999) shows that one can extract a model from a value function when using a tabular representation of the transition function. Parr et al. (2008) shows that, for linear function approximators, the model-free and model-based RL approaches produce the same value function at convergence. Our contribution differs substantially from these: we are not aiming to show that model-free RL performs similarly to model-based RL at convergence, but rather how we can achieve sample complexity comparable to model-based RL while retaining the favorable asymptotic performance of model-free RL in complex tasks with nonlinear function approximation.\nA central component of our method is to train goal-conditioned value functions. Many variants of goal-conditioned value functions have been proposed in the literature Foster & Dayan (2002); Sutton et al. (2011); Schaul et al. (2015); Dosovitskiy & Koltun (2016). Critically, unlike the works on contextual policies (Caruana, 1998; Da Silva et al., 2012; Kober et al., 2012) which require onpolicy trajectories with each new goal, the value function approaches such as Horde (Sutton et al., 2011) and UVF (Schaul et al., 2015) can reuse off-policy data to learn rich contextual value functions using the same data.\nTDMs condition on a policy trying to reach a goal and must predict τ steps into the future. This type of prediction is similar to the prediction made by prior work on multi-step models (Mishra et al., 2017; Venkatraman et al., 2016): predict the state after τ actions. An important difference is that\nmulti-step models do not condition on a policy reaching a goal, and so they require optimizing over a sequence of actions, making the input space grow linearly with the planning horizon.\nA particularly related UVF extension is hindsight experience replay (HER) Andrychowicz et al. (2017). Both HER and our method retroactively relabel past experience with goal states that are different from the goal aimed for during data collection. However, unlike our method, the standard UVF in HER uses a single temporal scale when learning, and does not explicitly provide for a connection between model-based and model-free learning. The practical result of these differences is that our approach empirically achieves substantially better sample complexity than HER on a wide range of complex continuous control tasks, while the theoretical connection between modelbased and model-free learning suggests a much more flexible use of the learned Q-function inside a planning or optimal control framework.\nLastly, our motivation is shared by other lines of work besides goal-conditioned value functions that aim to enhance supervision signals for model-free RL (Silver et al., 2017; Jaderberg et al., 2017; Bellemare et al., 2017). Predictions (Silver et al., 2017) augment classic RL with multi-step reward predictions, while UNREAL (Jaderberg et al., 2017) also augments it with pixel control as a secondary reward objective. These are substantially different methods from our work, but share the motivation to achieve efficient RL by increasing the amount of learning signals from finite data.\n\n6 EXPERIMENTS\nOur experiments examine how the sample efficiency and performance of TDMs compare to both model-based and model-free RL algorithms. We expect to have the efficiency of model-based RL but with less model bias. We also aim to study the importance of several key design decisions in TDMs, and evaluate the algorithm on a real-world robotic platform. For the model-free comparison, we compare to DDPG (Lillicrap et al., 2015), which typically achieves the best sample efficiency on benchmark tasks (Duan et al., 2016); HER, which uses goal-conditioned value functions (Andrychowicz et al., 2017); and DDPG with the same sparse rewards of HER. For the modelbased comparison, we compare to the model-based component in Nagabandi et al. (2017), a recent work that reports highly efficient learning with neural network dynamics models. Details of the baseline implementations are in the Appendix. We perform the comparison on five simulated tasks: (1) a 7 DoF arm reaching various random end-effector targets, (2) an arm pushing a puck to a target location, (3) a planar cheetah attempting to reach a goal velocity (either forward or backward), (4) a quadrupedal ant attempting to reach a goal position, and (5) an ant attempting to reach a goal position and velocity. The tasks are shown in Figure 1 and terminate when either the goal is reached or the time horizon is reached. The pushing task requires long-horizon reasoning to reach and push the puck. The cheetah and ant tasks require handling many contact discontinuities which is challenging for model-based methods, with the ant environment having particularly difficult dynamics given the larger state and action space. The ant position and velocity task presents a scenario where reward shaping as in traditional RL methods may not lead to optimal behavior, since one cannot maintain both a desired position and velocity. However, such a task can be very valuable in realistic settings. For example, if we want the ant to jump, we might instruct it to achieve a particular velocity at a particular location. We also tested TDMs on a real-world robot arm reaching end-effector positions, to study its applicability to real-world tasks.\nFor the simulated and real-world 7-DoF arm, our TDM is trained on all state components. For the pushing task, our TDM is trained on the hand and puck XY-position. For the half cheetah task, our TDM is trained on the velocity of the cheetah. For the ant tasks, our TDM is trained on either the position or the position and velocity for the respective task. Full details are in the Appendix.\n\n6.1 TDMS VS MODEL-FREE, MODE-BASED, AND DIRECT GOAL-CONDITIONED RL\nThe results are shown in Figure 2. When compared to the model-free baselines, the pure modelbased method learns learns much faster on all the tasks. However, on the harder cheetah and ant tasks, its final performance is worse due to model bias. TDMs learn as quickly or faster than the model-based method, but also always learn policies that are as good as if not better than the modelfree policies. Furthermore, TDMs requires fewer samples than the model-free baselines on ant tasks and drastically fewer samples on the other tasks. We also see that using HER does not lead to\nan improvement over DDPG. While we were initially surprised, we realized that a selling point of HER is that it can solve sparse tasks that would otherwise be unsolvable. In this paper, we were interested in improving the sample efficiency and not the feasibility of model-free reinforcement learning algorithms, and so we focused on tasks that DDPG could already solve. In these sorts of tasks, the advantage of HER over DDPG with a dense reward is not expected. To evaluate HER as a method to solve sparse tasks, we included the DDPG-Sparse baseline and we see that HER significantly outperforms it as expected. In summary, TDMs converge as fast or faster than modelbased learning (which learns faster than the model-free baselines), while achieving final performance that is as good or better that the model-free methods on all tasks.\nLastly, we ran the algorithm on a 7-DoF Sawyer robotic arm to learn a real-world analogue of the reaching task. Figure 2f shows that the algorithm outperforms and learns with fewer samples than DDPG, our model-free baseline. These results show that TDMs can scale to real-world tasks.\n\n6.2 ABLATION STUDIES\nIn this section, we discuss two key design choices for TDMs that provide substantially improved performance. First, Figure 3a examines the tradeoffs between the vectorized and scalar rewards. The results show that the vectorized formulation learns substantially faster than the naı̈ve scalar variant. Second, Figure 3b compares the learning speed for different horizon values τmax. Performance degrades when the horizon is too low, and learning becomes slower when the horizon is too high.\n\n7 CONCLUSION\nIn this paper, we derive a connection between model-based and model-free reinforcement learning, and present a novel RL algorithm that exploits this connection to greatly improve on the sample efficiency of state-of-the-art model-free deep RL algorithms. Our temporal difference models can be viewed both as goal-conditioned value functions and implicit dynamics models, which enables them to be trained efficiently on off-policy data while still minimizing the effects of model bias. As a result, they achieve asymptotic performance that compares favorably with model-free algorithms, but with a sample complexity that is comparable to purely model-based methods.\nWhile the experiments focus primarily on the new RL algorithm, the relationship between modelbased and model-free RL explored in this paper provides a number of avenues for future work. We demonstrated the use of TDMs with a very basic planning approach, but further exploring how TDMs can be incorporated into powerful constrained optimization methods for model-predictive control or trajectory optimization is an exciting avenue for future work. Another direction for future is to further explore how TDMs can be applied to complex state representations, such as images, where simple distance metrics may no longer be effective. Although direct application of TDMs to these domains is not straightforward, a number of works have studied how to construct metric embeddings of images that could in principle provide viable distance functions. We also note that while the presentation of TDMs have been in the context of deterministic environments, the extension to stochastic environments is straightforward: TDMs would learn to predict the expected distance between the future state and a goal state. Finally, the promise of enabling sample-efficient learning with the performance of model-free RL and the efficiency of model-based RL is to enable widespread RL application on real-world systems. Many applications in robotics, autonomous driving and flight, and other control domains could be explored in future work.\n\n8 ACKNOWLEDGMENT\nThis research was supported by the Office of Naval Research and the National Science Foundation through IIS-1614653 and IIS-1651843.\n",
    "approval": true,
    "rationale": "The paper universal value function type ideas to learn models of how long the current policy will take to reach various states (or state features), and then incorporates these into model-predictive control. This looks like a reasonable way to approach the problem of model-based RL in a way that avoids the covariate shift produced by rolling learned transition models forward in time. Empirical results show their method outperforming Hindsight Experience Replay (which looks quite bad in their experiments), DDPG, and more traditional model-based learning. It also outperforms DDPG quite a bit in terms of sample efficiency on a real robotic arm. They also show the impact of planning horizon on performance, demonstrating a nice trade-off.\n\nThere are however a couple of relevant existing papers that the authors miss referencing / discussing:\n- \"Reinforcement Learning with Unsupervised Auxiliary Tasks\" (Jarderberg et al, ICLR 2017) - uses predictions about auxiliary tasks, such as effecting maximum pixel change, to obtain much better sample efficiency.\n- \"The Predictron: End-To-End Learning and Planning\" (Silver et al, ICML 2017), which also provides a way of interpolating between model-based and model-free RL.\n\nI don't believe that these pieces of work subsume the current paper, however the authors do need to discuss the relationship their method has with them and what it brings.\n\n** UPDATE Jan 9: Updated my rating in light of authors' response and updated version. I recommend that the authors find a way to keep the info in Section 4.3 (Dynamic Goal and Horizon Resampling) in the paper though, unless I missed where it was moved to. **\n",
    "rating": 7,
    "type": "positive"
  },
  {
    "title": "LEARNING UNDERLYING PHYSICAL PROPERTIES FROM OBSERVATIONS FOR TRAJECTORY PREDIC-",
    "abstract": "In this work we present an approach that combines deep learning together with laws of Newton’s physics for accurate trajectory predictions in physical games. Our model learns to estimate physical properties and forces that generated given observations, learns the relationships between available player’s actions and estimated physical properties and uses these extracted forces for predictions. We show the advantages of using physical laws together with deep learning by evaluating it against two baseline models that automatically discover features from the data without such a knowledge. We evaluate our model abilities to extract physical properties and to generalize to unseen trajectories in two games with a shooting mechanism. We also evaluate our model capabilities to transfer learned knowledge from a 2D game for predictions in a 3D game with a similar physics. We show that by using physical laws together with deep learning we achieve a better human-interpretability of learned physical properties, transfer of knowledge to a game with similar physics and very accurate predictions for previously unseen data.",
    "text": "1 INTRODUCTION\nGames that follow Newton’s laws of physics despite being a relatively easy task for humans, remain to be a challenging task for artificially intelligent agents due to the requirements for an agent to understand underlying physical laws and relationships between available player’s actions and their effect in the environment. In order to predict the trajectory of a physical object that was shot using some shooting mechanism, one needs to understand the relationship between initial force that was applied to the object by a mechanism and its initial velocity, have a knowledge of hidden physical forces of the environment such as gravity and be able to use basic physical laws for the prediction. Humans, have the ability to quickly learn such physical laws and properties of objects in a given physical task from pure observations and experience with the environment. In addition, humans tend to use previously learned knowledge in similar tasks. As was found by researchers in human psychology, humans can transfer previously acquired abilities and knowledge to a new task if the domain of the original learning task overlaps with the novel one (Council, 2000).\nThe problem of learning properties of the physical environment and its objects directly from observations and the problem of using previously acquired knowledge in a similar task are important to solve in AI as this is one of the basic abilities of human intelligence that humans learn during infancy (Baillargeon, 1995). Solving these two problems can bring AI research one step closer to achieving human-like or superhuman results in physical games.\nIn this work we explore one of the possible approaches to these two problems by proposing a model that is able to learn underlying physical properties of objects and forces of the environment directly from observations and use the extracted physical properties in order to build a relationships between available in-game variables and related physical forces. Furthermore, our model then uses learned physical knowledge in order to accurately predict unseen objects trajectories in games that follow Newtonian physics and contain some shooting mechanism. We also explore the ability of our model\nto transfer learned knowledge by training a model in a 2D game and testing it in a 3D game that follows similar physics with no further training.\nOur approach combines modern deep learning techniques (LeCun et al., 2015) and well-known physics laws that were discovered by physicists hundreds of years ago. We show that our model automatically learns underlying physical forces directly from the small amount of observations, learns the relationships between learned physical forces with available in-game variables and uses them for prediction of unseen object’s trajectories. Moreover, we also show that our model allows us to easily transfer learned physical forces and knowledge to the game with similar task.\nIn order to evaluate our model abilities to infer physical properties from observations and to predict unseen trajectories, we use two different games that follow Newtonian Physics. The first game that we use as a testing environment for our model is Science Birds. Science Birds is a clone of Angry Birds - a popular video game where the objective is to destroy all green pigs by shooting birds from a slingshot. The game is proven to be difficult for artificially intelligent playing agents that use deep learning and many agents have failed to solve the game in the past (Renz et al., 2019). The second game that we are using as our testing environment is Basketball 3D shooter game. In this game the objective of a player is to shot a ball into a basket.\nIn order to test the abilities of our model to transfer knowledge to a different game we first train our model on a small amount of shot trajectories from Science Birds game and then test trained model for predictions of the ball trajectory in the Basketball 3D shooting game.\nWe compare the results of our proposed model that is augmented with physical laws against a two baseline models. The first baseline model learns to automatically extract features from observations without knowledge of physical laws, whereas the second baseline model learns to directly predict trajectories from the given in-game variables.\n\n2 RELATED WORK\nPrevious AI work in predicting future dynamics of objects has involved using deep learning approaches such as: graph neural networks for prediction of interactions between objects and their future dynamics ( Battaglia et al. (2016); Watters et al. (2017); Sanchez-Gonzalez et al. (2018)), Bidirectional LSTM and Mixture Density network for Basketball Trajectory Prediction ( Zhao et al. (2017)) and Neural Physics Engine (Chang et al. (2016)). Some of the researchers also tried to combine actual physical laws with deep learning. In one of such works, researchers propose a model that learns physical properties of the objects from observations (Wu et al. (2016)). Another work proposes to integrate a physics engine together with deep learning to infer physical properties (Wu et al. (2015))\nHowever, most of the work on predicting future objects dynamics is focused on learning physics from scratch or uses some known physical properties in order to train a model. This could be a problem as in most real-world physical games the underlying physical properties are not known to the player unless one has an access to the source code of a physics engine. Because of that, these properties have to be learned directly from experience with the environment without any supervision on the actual values of physical properties. Another important point, is that instead of learning physics from scratch we can use to our benefit already discovered and well-established laws of physics.\nIn this work we propose an approach that combines classical feedforward networks with well-known physical laws in order to guide our model learning process. By doing so, our model learns physical properties directly from observations without any direct supervision on actual values of these properties. Another contribution is that our model learns from a very small training dataset and generalizes well to the entire space. Furthermore, learned values can be easily interpreted by humans which allows us to use them in any other task in the presented test domains and can be easily transferred to other games with similar physics.\n\n3 APPROACH\n\n\n3.1 BASELINE MODELS\nIn order to measure the advantages of combining classical physical laws together with deep learning we are comparing our model against pure deep learning approaches with similar architectures.\n\n3.1.1 ENCODER BASELINE MODEL\nOur first baseline model is based on the idea of autoencoders (Rumelhart et al. (1986)). Contrary to the proposed model in section 3.2 this model learns to automatically discover features from observations. It takes a sequence of points T = {(x0, y0), (x1, y1), ..., (xn, yn)} as its input and encodes it to a latent space Tenc. The encoded trajectory is then used by decoder to reconstruct the trajectory. The second part of this baseline model consists of another MLP that learns to associate a relative position of a physical object that generated the trajectory with learned latent space Tenc. More formally, given trajectory T as input, encoder fencoder, and decoder fdecoder, our model reconstructs a trajectory T̂ from latent space as follows:\nT̂ = fdecoder(fencoder({(x0, y0), (x1, y1), ..., (xn, yn)})) (1) Once the trajectory is reconstructed, we compute the loss using Mean Squared Error and update the weights of our networks:\n1\nn n∑ i=1 (T − T̂ )2 (2)\nThe second part of this baseline model is a another MLP fassociate that learns to associate given initial relative position of a physical object (xr0 , yr0) with derived in a previous step encoded trajectory Tenc:\nˆTenc = fassociate((xr0 , yr0)) (3)\nIn order to update the weights of fassociate we compute the loss using Mean Squared Error between two derived encodings Tenc and ˆTenc.\nAfter that, we predict trajectory using derived ˆTenc as follows: T̂ = fdecoder( ˆTenc)\n\n3.1.2 SIMPLE BASELINE MODEL\nIn order to evaluate the advantages of using observations and an encoder-decoder scheme, we use the second baseline model that does not use encoder-decoder and directly learns to predict trajectory\nfrom the given in-game forces or relative position of a physical object. More formally, given relative position of a physical object (xr0 , yr0) and MLP fsimple, we compute the trajectory as follows:\nT̂ = fsimple((xr0 , yr0)) (4)\n\n3.2 PHYSICS AWARE MODEL\nSimilarly to the first baseline model presented in section 3.1.1, Physics Aware Network (PhysANet) consists of two parts: a neural network that discovers physical forces of the environment and action that generated given observations and a neural network that learns the relationship between the ingame actions or forces and predicted physical values. We further refer to these two parts as InferNet and RelateNet.\n\n3.2.1 INFERNET\nThe goal of InferNet is to extract physical forces that have generated a given trajectory {(x0, y0), (x1, y1)...(xn, yn)} using guidance from known physical equations. The discovered physical forces are then plugged to the projectile motion equation in order to calculate the trajectory.\nInferNet consists of two internal small MLPs that are trained together as shown on Figure 2 (Left). The first MLP takes in a batch of trajectories and predicts a single value of gravity force for all of them. The second MLP takes in a batch of trajectories and for each trajectory in a batch it predicts an initial velocity and angle of a shot. These predicted values are then inserted into a projectile motion equation in order to calculate the resulting trajectory. The projectile motion equation is defined as follows (Walker (2010)):\ny = h+ x tan (θ)− gx 2\n2V 20 cos(θ) 2\n(5)\nIn equation 5, h is the initial height, g is gravity, θ is the angle of a shot and V0 is initial velocity.\nOnce it had calculated the trajectory we compute the loss between observed trajectory T and predicted trajectory T̂ using Mean Squared Error in a similar way as was defined in equation 2.\n\n3.2.2 RELATENET\nThe goal of RelateNet that is shown on Figure 2 (Right) is to learn the relationship between ingame variables and physics forces predicted by InferNet. This network tries to predict extracted by InferNet forces directly from the given in-game values such as relative position of a bird. The in-game variables can be any variables with continuous or discrete domain that can be chosen by the playing agent in order to make a shot. As an example, in-game variables can be the initial forces of the shot or object’s relative position to the shooting mechanism. RelateNet consists of two internal MLPs where first MLP predicts initial velocities and second MLP predicts initial angles. In order to update the weights of both internal MLPs, we calculate the MSE between values predicted by InferNet and values predicted by RelateNet. More details on the architecture of the PhysANet can be found in the Appendix A.\n\n4 EXPERIMENTS\nIn our experiments we are using Science Birds (Ferreira, 2018) and Basketball 3D shooter game (Nagori, 2017) as a testing environments.\n\n4.1 SCIENCE BIRDS TRAJECTORY PREDICTION\nScience Birds is a clone of Angry Birds which has been one of the most popular video games for a period of several years. The main goal of the game is to kill all green pigs on the level together with applying as much damage as possible to the surrounding structures. The player is provided with a sequence of (sometimes) different birds to shoot from a slingshot. In Science Birds, similarly to Angry Birds all game objects are following the laws of Newton’s Physics (in 2D). In order to predict the outcomes of the shots, the player needs to understand basic physical laws that govern the game and be able to use them for a prediction of a bird’s trajectory.\nScience Birds contains a slingshot (Figure 3) which serves as a shooting mechanism that allows a player to shoot a bird. Player can select a strength and the angle of the shot by changing a relative position of the bird to the center of a slingshot. In order to accurately predict the trajectory of a bird, player needs to have an understanding of the relationship between relative position of the bird to the slingshot and resulting trajectory. The underlying physical properties of the game such as gravity, or initial velocity of a related shot or its angle is unknown to the player and has to be estimated from the observations and experiences.\nThe goal of our model in this experiment is to learn the relationship between relative position of a bird and initial velocity and angle of the resulting shot in order to predict trajectories of previously unseen shots. In this experiment we do not provide actual physical properties or forces to our model and it has to learn them directly from observations.\nIn order to train our model for this experiment, we have collected a small data set of 365 trajectories and relative positions of the bird that generated these trajectories from Science Birds. This data set was then split to train, test and validation data sets, resulting in a training data set that contains only 200 trajectories. We are using such small training data set in order to prevent a model to simply memorize all possible trajectories and instead learn to use extracted physical forces and properties to predict unseen trajectories.\nDuring the training of a model, PhysANet takes in a trajectory as input and estimates the angle θ, initial velocity V0 and gravity g that generated this trajectory. The predicted values are then inserted to projectile motion equation (5) in order to recreate the trajectory. As a second step, the relative\nposition of the bird to the slingshot (x0, y0) is fed as input to the RelateNet which learns to predict values θ and V0 predicted by InferNet.\nDuring the testing of a model, we only feed the relative position of the bird to RelateNet which predicts values θ and V0 that are plugged in to projectile motion equation in order to calculate the trajectory.\n\n4.2 BASKETBALL 3D SHOOTER GAME\nBasketball 3D shooter is a game where the player has to throw a ball to the basket in order to earn points. In this game the shooting mechanism is different compared to Science Birds and the initial force that is applied to the physical object does not depend on the relative position of that object.\nIn this experiment we are interested in the ability of our model to transfer the learned relationships and physical properties from one game with shooting mechanism to the other. In particular, we were interested in ability of our model to use learned knowledge from Science Birds for predictions of the ball’s trajectory in Basketball 3D Shooter. Because of the absence of the slingshot like mechanism in this game, we train our models to predict the trajectories based on the initial force that is applied to the bird or a ball by a shooting mechanism when launched.\nAs in the Science Birds experiment described in section 4.1, the input to the InferNet is a trajectory {(x0, y0), (x1, y1)...(xn, yn)} and an input to the RelateNet is initial force that is applied to the bird or a basketball ball by a shooting mechanism. We note here that the magnitude of the initial force applied to the ball in Basketball 3D Shooter game is higher than the magnitude of the initial force applied to the bird in Science Birds.\n\n4.3 TESTING DATASETS\nIn order to evaluate our model we are using three testing datasets: Test, Generalization and Basketball. Test dataset contains trajectories generated by previously unseen (by the model) initial forces or relative positions of a bird. Generalization dataset contains trajectories of the shots made to the opposite direction of shots in the training set. In particular, it contains trajectories and the related relative positions of the bird of shots to the left side of a slingshot. The basketball dataset contains trajectories and related to them initial forces that were applied to the ball in the Basketball 3D shooter game.\nThese three datasets are not exposed to our models during the training. In the Basketball experiment described in section 4.2, we do not train our models on the basketball dataset but only on the training dataset from Science Birds.\n\n5 RESULTS\n\n\n5.1 PREDICTION OF TRAJECTORIES IN SCIENCE BIRDS\nThe results of our first experiment show that PhysANet has the most accurate trajectory prediction out of all tested models. PhysANet was able to learn to associate a position of a bird relative to the slingshot with initial velocity and angle of a shot as shown on Figure 4. In particular Figure 4 shows two learned relationships between position of a bird before it is shot from a slingshot and initial parameters of the shot. The graph on the left side of Figure 4 shows how the initial angle between bird and center of a slingshot affects the initial angle of a trajectory. As an example from this graph we can see that bird positioned at angle of 220 degrees would result in trajectory that has initial angle of roughly 45 degrees which as we know from a trigonometry is close to the ”truth”. The graph on the right side of Figure 4 shows how initial velocity depends on the distance between initial bird position and center of the slingshot. In order to show this relationship we have fixed the bird at an angle of 225 relatively to the center of slingshot and only changed the distance. From this graph we can see that as one could expect the more we extend the slingshot the stronger our shot is going to be and the higher initial velocity will be.\nIn Figure 5 we present a few examples of the trajectory predicted by PhysANet and actual trajectories that bird took. The presented examples show three trajectories from a test dataset which the network\nhad never seen before and one trajectory of the shot to the left side of a slingshot. Despite the fact that our training dataset did not contain the shots to the left side of the slingshot, our model shows a good generalization ability and predicts the trajectory quite accurately. This shows that learned physical properties and relationships between position of a bird and slingshot can be used by PhysANet to accurately predict the trajectories.\nAs shown in Table 1, PhysANet showed significantly better overall accuracy on test and generalization datasets than our two baseline models.\n\n5.2 TRANSFER OF KNOWLEDGE TO BASKETBALL 3D\nThe results in this experiment show that learned physical properties and relationships in Science Birds can be transferred to the Basketball 3D shooter game. Due to the fact that model was trained on data from a 2D environment, we tested the predictions of 3D trajectories by separating prediction process into predictions in x,y and z,y planes. Surprisingly, despite being trained in a 2D environment, PhysANet showed good results of predicting the trajectories in both planes without any additional training in new 3D environment.\nThe Figure 6 shows the examples of predictions of trajectories from the Basketball dataset by PhysANet, BM1 and BM2. The first three pictures show trajectories predicted by PhysANet, fourth picture shows prediction made by BM1 and the last picture shows prediction made by BM2. The first two pictures show predictions for one trajectory in x,y and z,y planes. The rest of the pictures shown in Figure 6 show predictions in x,y plane only. As we can see from Figure 6, despite the fact that initial forces in Basketball 3D shooter game have higher magnitudes, the PhysANet was able to correctly handle it and still predict trajectories in a correct shape and have a relatively low accuracy error. This is a contrary to the two baseline models that could not handle new environment properly and predicted seemingly random trajectories despite being relatively accurate in the predictions in the first experiment described in section 4.\nThe results presented in Table 1 show the comparison of the observed Mean Squared Errors of all three models. These results show that PhysANet surpasses two baseline models in all testing datasets. Thus for example, error shown by PhysANet in Basketball dataset is nearly two times lower than error shown by Baseline Model 1 (BM1) which was described in section 3.1.1. Surprisingly, despite the fact that BM1 showed better results in Test and Generalization datasets than BM2, it showed higher prediction error in the Basketball 3D dataset than a simple baseline model (BM2). Despite the fact that PhysANet showed relatively good results in predicting trajectories in previously unseen game there still seems to be some loss in the accuracy of the predictions. We hypothesise that the loss of the accuracy can be caused by the differences in physics engines of the two games. One of the possible solutions of this problem is to improve our model abilities to adapt to the new environment after observing a few shots, however we leave such an improvement for a future research.\n\n6 DISCUSSION\nIn this work we have showed that combining known physical laws with deep learning can be beneficial for learning more human-interpretable and flexible physical properties directly from the small amount of the observations. We showed that a model that is augmented with physical laws achieved better generalization to unseen trajectories, was able to transfer learned knowledge to a different game with similar physics and made generally more accurate predictions of trajectories than models without such physical knowledge. Our results show that in some situations using already discovered physical laws and equations can be beneficial and can guide deep learning model to learn physical properties and relationships that lead to more accurate predictions than predictions made by models that learned features automatically without any supervision. Because learned values can be easily interpreted by a human these values can be used in potentially new tasks. As an example, learned gravity of the environment can be used for prediction of a physical behaviour of other physical objects in the environment. Another important quality of our approach is that learned knowledge can be transferred to a task with a similar physics as was shown by the Basketball experiment. Lastly, our model can be easily adopted to any other physical task by changing or adding more physical equations and learning different physical properties from the observations. This potentially could allow us to fully master the physics of a game and use learned knowledge as part of artificially intelligent agent in order to achieve human-like performance in physical games.\nOur results show that it could be beneficial to use physical laws that were discovered by physicists hundreds of years ago together with deep learning techniques in order to unleash their full predicting power and bring artificially intelligent agents a step closer to achieving human-like physical reasoning.\n",
    "approval": false,
    "rationale": "The problem addressed by this paper is the estimation of trajectories of moving objects thrown / launched by a user, in particular in computer games like angry birds or basketball simulation games. A deep neural network is trained on a small dataset of ~ 300 trajectories and estimates the underlying physical properties of the trajectory (initial position, direction and strength of initial force etc.). A new variant of deep network is introduced, which is based on an encoder-decoder model, the decoder being a fully handcrafted module using known physics (projectile motion).\n\nI have several objections, which can be summarized by the simplicity of the task (parabolic trajectories without any object/object or object/environment collisions / interactions), the interest of the task for the community (how does this generalize to other problems?), and the writing and structuring of the paper. I will detail these objections further in the rest of the review.\n\nLearning physical interactions is a problem which has received considerable attention in the computer vision and ML communities. The problem is certainly interesting, but I think we should be clear on what kind of scientific knowledge we want to gain by studying a certain problem and by proposing solutions. The tasks studied by the community are mostly quite complex physical phenomena including multiple objects of different shapes and properties and which interact with each other. All these phenomena can be simulated with almost arbitrary precision with physics engines, and these engines are mostly also used for generating the data. In other words, the simulation itself is solved and is not the goal of this body of work. The goal is to learn differentiable models, which can be used as inductive bias in larger models targeting more general tasks in AI.\n\nCompared to this goal, the proposed goal is far too easy: learning projectile motion is very easy, as these trajectories can be described by simple functions with a small number of parameters, which also have a clear and interpretable meaning. The simplicity of the task is also further corroborated by the small number of samples used to estimate these parameters (in the order of 300). A further indication is the fact, that the decoder in the model is fully hardcoded. No noise modelling was even necessary, which further corroborates that a very simple problems is addressed.\n\nIn order words, I am not really sure what kind of scientific problem is solved by this work, and how this knowledge can help us to solve other problems, harder problems.\n\nMy second objection is with the written form of the paper. The paper is not well enough structured and written, many things are left unsaid. First of all, the problem has never been formally introduced, we don’t know exactly what needs to be estimated. What are the inputs, outputs? Is computer vision used anywhere? How are the positions of the objects determined if not with computer vision? How are the user forces gathered? What are “in game variables” mentioned multiple times in the document? No notation has been introduced, no symbols have been introduced (or too late in the document). For instance, there is no notation for the latent space of the encoder-decoder model.\n\nThe figures are not very helpful, as the labelling of the blocks and labels is very fuzzy. As an example, For InferNet, inputs and trajectories are “Trajectories”, so what is the difference? Of course we can guess that (inputs are measured trajectories, outputs are reconstructed trajectories), but we should not guess things when reading papers.\n\nThe figure for encoder-decoder model is very confusing, as the different arrows have different functional meanings and we have no idea what they mean. The outputs of the encoder and the MLP both point to the latent space and at a first glance the reader might think that they are concatenated, which raises several questions. Reading the text, we infer that first a model is trained using on one of the arrows (the one coming from the encoder) and ignoring the other one, and then the MLP is learned to reconstruct the latent space using the other arrow (the one coming from the MLP), but this is absolutely impossible to understand looking at the figure, which does not make much sense. We can infer all this from the text around equations (1) to (3), which is itself quite fuzzy and difficult to understand, in spite of the simplicity of the underlying maths.\n\nThe relationship of RelateNet and InferNet is not clear. While the task of InferNet is clear, the role of InferNet in the underlying problem is not clear and it has not been described how it interacts with RelateNet.\n\nIt is unclear how the transfer between science birds and basketball has been performed and what exactly has been done there.\n\nAs mentioned above, the role of “in game variables” is unclear. What are those? I suggest to more clearly define their roles early in the document and use terms from well-defined fields like control (are they “control inputs”) or HCI (are they “user actions”?).\n\nIn the evaluation section, we have baseline models BM1 and BM2, but they have never been introduced. We need to guess which of the models described in the paper correspond to these.\n\nThe related work section is very short and mostly consists of an enumeration of references. The work should be properly described and related to the proposed work. How does the proposed work address topics which have not yet been solved by existing work?\n",
    "rating": 1,
    "type": "negative"
  },
  {
    "title": "GENERATIVE ADVERSARIAL INTERPOLATIVE AU- TOENCODING: ADVERSARIAL TRAINING ON LATENT SPACE INTERPOLATIONS ENCOURAGES CONVEX LA- TENT DISTRIBUTIONS",
    "abstract": "We present a neural network architecture based upon the Autoencoder (AE) and Generative Adversarial Network (GAN) that promotes a convex latent distribution by training adversarially on latent space interpolations. By using an AE as both the generator and discriminator of a GAN, we pass a pixel-wise error function across the discriminator, yielding an AE which produces sharp samples that match both highand low-level features of the original images. Samples generated from interpolations between data in latent space remain within the distribution of real data as trained by the discriminator, and therefore preserve realistic resemblances to the network inputs.",
    "text": "1 INTRODUCTION\nGenerative modeling has the potential to become an important tool for exploring the parallels between perceptual, physical, and physiological representations in fields such as psychology, linguistics, and neuroscience (e.g. Sainburg et al. 2018; Thielk et al. 2018; Zuidema et al. 2018). The ability to infer abstract and low-dimensional representations of data and to sample from these distributions allows one to quantitatively explore and vary complex stimuli in ways which typically require hand-designed feature tuning, for example varying formant frequencies of vowel phonemes, or the fundamental frequency of syllables of birdsong.\nSeveral classes of unsupervised neural networks such as the Autoencoder (AE; Hinton & Salakhutdinov 2006; Kingma & Welling 2013), Generative Adversarial Network (GAN; Goodfellow et al. 2014), autoregressive models (Hochreiter & Schmidhuber, 1997; Graves, 2013; Oord et al., 2016; Van Den Oord et al., 2016), and flow-based generative models (Kingma & Dhariwal, 2018; Dinh et al., 2014; Kingma et al., 2016; Dinh et al., 2016) are at present popularly used for learning latent representations that can be used to generate novel data samples. Unsupervised neural network approaches are ideal for data generation and exploration because they do not rely on hand-engineered features and thus can be applied to many different types of data. However, unsupervised neural networks often lack constraints that can be useful or important for psychophysical experimentation, such as pairwise relationships between data in neural network projections, or how well morphs between stimuli fit into the true data distribution.\nWe propose a novel AE that hybridizes features of an AE and a GAN. Our network is trained explicitly to control for the structure of latent representations and promotes convexity in latent space by adversarially constraining interpolations between data samples in latent space to produce realistic samples1.\n1Pairwise interpolations in between latent samples may only cover a subset of the convex hull of the latent distribution, as described in Figure 1.\n\n1.1 BACKGROUND ON GENERATIVE ADVERSARIAL NETWORKS (GANS) AND AUTOENCODERS (AES)\nAn AE is a form of neural network which takes as input xi (e.g. an image), and is trained to generate a reproduction of the input2 G(xi), by minimizing some error function between the input xi and output G(xi) (e.g. pixel-wise error). This translation is usually performed after compressing the representation xi into a low-dimensional representation zi. This low-dimensional representation is called a latent representation, and the layer corresponding to zi in the neural network is often called the latent layer. The first half of the network, which translates from xi to zi, is called the encoder; the second half of the network, which translates from zi to xi, is called the decoder. The combination of these two networks make the AE capable of both dimensionality reduction (X → Z); Hinton & Salakhutdinov 2006), and generativity (Z → X). Importantly, AE architectures are generative3, however they are not generative models (Bishop, 2006) because they do not model the joint probability of the observable and target variables P (X,Z). Variants such as the Variational Autoencoder (VAEs; Kingma & Welling 2013), which model P (X,Z) are generative models. AE latent spaces, therefore, cannot be sampled probabilistically, without modeling the joint probability as in VAEs. The AE architecture that we propose here does not model the joint probability of X and Z and thus is not a generative model, although the latent space of our network could be modeled probabilistically (e.g. with a VAE).\nGAN architectures are comprised of two networks, a generator, and a discriminator. The generator takes as input a latent sample, zi, drawn randomly from a distribution (e.g. uniform or normal), and is trained to produce a sampleGd(zi) in the data domainX . The discriminator takes as input both xi and Gd(zi), and is trained to differentiate between real xi, and generated Gd(zi) samples, typically by outputting either a 0 or 1 in a single-neuron output layer. The generator is trained to oppose the discriminator by ’tricking’ it into categorizing Gd(z) samples as x samples. Intuitively, this results in the generator producing Gd(Z) samples indistinguishable (at least to the discriminator) from those drawn from the distribution x. Thus the discriminator acts as a ’critic’ of the samples produced by a generator that is attempting to reproduce the distribution x. Because GANs sample directly from a predefined latent distribution, GANs are generative models, explicitly representing the joint probability, P (X,Z).\nOne common use for both GANs and AEs has been exploiting the semantically rich low-dimensional manifold, Z, on which data are either projected onto or sampled from (White, 2016; Hinton & Salakhutdinov, 2006). Operations performed in Z carry rich semantic features of data, and interpolations between points in Z produce semantically smooth interpolations in the original data space X (e.g. Radford et al. 2015; White 2016). However, samples generated by latent representations of both AEs and GANs are limited by the constraints provided by the algorithm. A significant amount of work has been done over the past several years in developing variants of AEs and GANs which add additional constraints and functionality to GAN and AE architectures, for example improving stability of GANs (e.g. Berthelot et al. 2017; Radford et al. 2015; Salimans et al. 2016), disentangling latent representations (e.g. Higgins et al. 2016; Chen et al. 2016; Bouchacourt et al. 2017), adding generative capacity to AEs (e.g. Kingma & Welling 2013; Kingma et al. 2016; Makhzani et al. 2015), and adding bidirectional inference to GANs (e.g. Larsen et al. 2015; Mescheder et al. 2017; Berthelot et al. 2017; Dumoulin et al. 2016; Ulyanov et al. 2017; Makhzani 2018).\nIn this work, we describe several limitations of GANs and Autoencoders, specifically as they relate to stimuli generation for psychophysical research, and propose a novel architecture, GAIA, that utilizes aspects of both the AE and GAN to negate shortcomings of each architecture independently. Our method provides a novel approach to increasing the stability of network training, increasing the convexity of latent space representations, preserving of high-dimensional structure in latent space representations, and bidirectionality from X → Z and Z → X .\n\n1.2 CONVEXITY OF LATENT SPACE\nGenerative latent-spaces enable the powerful capacity for smooth interpolations between real-world signals in a high-dimensional space. Linear interpolations in a low-dimensional latent space often\n2We denote G(xi) as being equivalent to Gd(Ge(xi)), or xi being passed through the encoder and decoder of the generator, G\n3having the power or function of generating, originating, producing, or reproducing (Webster, 2018)\nproduce comprehensible representations when projected back into high-dimensional space (e.g. Engel et al. 2017; Dosovitskiy et al. 2015). In the latent spaces of many network architectures such as AEs, however, linear interpolations are not necessarily justified, because the space between endpoints on an interpolation in Z is not explicitly trained to fall within the data distribution when translated back into X .\nA convex set of points is defined as a set in which the line-segment connecting any pair of points will fall within the rest of the set (Klee, 1971). For example, in Figure 1A, the purple distribution represents data projected into a two-dimensional latent space, and the surrounding whitespace represents regions of latent space that do not correspond to the data distribution. This distribution would be non-convex because a line connecting two points in the distribution (e.g. the black points in Figure 1A) could contain points outside the data distribution (the red region). In an AE, if the red region of the interpolation were sampled, projections back into the high-dimensional space may not necessarily correspond to realistic exemplars of x, because that region of Z does not belong to the true data distribution.\nOne approach to overcoming non-convexity in a latent space is to force the latent representation of the dataset into a pre-defined distribution (e.g. a normal distribution), as is performed by VAEs. By constraining the latent space of a VAE to fit a normal distribution, the latent space representations are encouraged to belong to a convex set. This method, however, pre-imposes a distribution in latent space that may be a suboptimal representation of the high-dimensional dataset. Standard GAN latent distributions are sampled directly, similarly allowing arbitrary convex distributions to be explicitly chosen for latent spaces. In both cases, hard-coding the distributional structure of the latent space may not respect the high-dimensional structure of the original distribution.\n\n1.3 PIXEL-WISE ERROR AND BIDIRECTIONALITY\nAEs that perform dimensionality reduction (in particular VAEs) can produce blurry images due to their pixel-wise loss functions (Goodfellow et al., 2014; Larsen et al., 2015), which minimize loss by smoothing the sharp contrasts (e.g. edges) present in real data. GANs do not suffer from this blurring problem, because they are not trained to reproduce input data. Instead, GANs are trained to generate data that could plausibly belong to the true distribution X . Thus, smoothing over uncertainty tends to be discouraged by the discriminator because it can use smoothed edges as a distinguishing feature between data sampled from X and G(X).\nProducing data that fits into the distribution of x, rather than reproducing individual instances of xi comes at a cost, however. While AEs learn both the translation from X to Z and Z to X , GANs only learn the latter (Z → X). In other words, the pixel-wise loss function of the AE produces smoothed data but is bidirectional, while the discriminator-based loss function of the GAN produces sharp images and is unidirectional.\n\n2 GENERATIVE ADVERSARIAL INTERPOLATIVE AUTOENCODING (GAIA)\nOur model, GAIA (Figure 1 left), is bidirectional but is trained on both a GAN loss function and a pixel-wise loss function, where the pixel-wise loss function is passed across the discriminator of the GAN to ensure that features such as blurriness are discriminated against. In full, GAIA is trained as a GAN in which both the generator and the discriminator are AEs. The discriminator is trained to minimize the pixel-wise loss (`1) between real data (xi) and their AE reproduction in the discriminator (D(xi)) while maximizing the AE loss between samples generated (G(xi)) by the generator and their reproduction in the discriminator (D(G(xi))):\n‖xi −D(xi)‖1 − ‖xi −D(G(xi))‖1 The generator is trained on the inverse, to minimize the pixel-wise loss between input (xi) and output (D(G(xi))) such that the discriminator reproduces the generated samples to be as close to the original data as possible: ‖xi −D(G(xi))‖1 Using an AE as a generator has been previously been used in the VAE-GAN (Larsen et al., 2015), and decreases blurring from the pixel-wise loss in AEs at the expense of exact-reproduction of data. Similarly, using an AE as a discriminator has been previously used in BEGAN (Berthelot\net al., 2017), which improves stability in GANs but remains unidirectional4. In GAIA, we combine these two architectures, allowing the generator to be trained on a pixel-wise loss that is passed across the discriminator, explicitly reproducing data as in an AE, while producing sharper samples characteristic of a GAN.\nIn addition, linear interpolations are taken between the latent-space representations:\nβ ← N (µ, σ2)\nzint. = zgeniβ + zgenj (1− β)\nWhere interpolations are Euclidean interpolations between pairs of points in Z, sampled from a 1-dimensional Gaussian distribution5 centered around the midpoint between zgeni and zgenj . The midpoints are then passed through the decoder of the discriminator, which are treated as generated samples by the GAN loss function:\n‖Gd(zint.)−D(Gd(zint.))‖1\nThe discriminator is trained to maximize this loss, and the generator is trained to minimize this loss.\n4Although it is possible to find the regions of Z most closely corresponding to xi 5σ = 0.25. We sample along the midpoint using a Gaussian rather than uniformly because we found that interpolations near to original samples required less training than interpolations to produce realistic interpolations.\nIn full, the loss of the discriminator, as in BEGAN, is to minimize pixel-wise loss of real data, and maximize pixel-wise loss of generated data (including interpolations):\nLDisc =‖xi −D(xi)‖1− ‖xi −D(G(xi))‖1− ‖Gd(zint.)−D(Gd(zint.))‖1\nThe loss of the generator is to minimize the error across the descriminator for the input data in the generator (D(G(xi))), along with the minimizing the error of the interpolations generated by the generator (D(Gd(zint.))).\nLGen =‖xi −D(G(xi))‖1+ ‖Gd(zint.)−D(Gd(zint.))‖1\nIn summary, the generator and discriminator are both AEs. As a result, reconstructions of x have the potential to resemble the input data (G(x)) at a pixel level, a feature non-existent in other GAN based inference methods (Figure 5). We also train the network on interpolations in the generator, to explicitly train the generator to produce interpolations (Gd(zint.)) which deceive the discriminator and are closer to the distribution in X than interpolations from an unconstrained AE.\n\n2.1 PRESERVATION OF LOCAL-STRUCTURE IN HIGH-DIMENSIONAL DATA\nVAEs and GANs force the latent distribution, z, into a pre-defined distribution, for example, a Gaussian or uniform distribution. This approach presents a number of advantages, such as ensuring latent space convexity and thus being better able to sample from the distribution. However, these benefits are gained at the loss of respecting the structure of the distribution of the original high dimensional dataset, x. Preserving high-dimensional structure in low dimensional embeddings is often the goal of dimensionality reduction, one of the functions of an autoencoder (Hinton & Salakhutdinov, 2006). To better respect the original high-dimensional structure of the dataset, we impose a regularization between the latent space representations of the data (z) and the original high dimensional dataset (x), motivated by Multidimensional Scaling (Kruskal, 1964).\nFor each minibatch presented to the network, we compute a loss for the distance between the log of the pairwise Euclidean distances of samples in X and Z space:\nLdist(x, z) = 1\nB B∑ i,j\n[ log2 ( 1 +\n(xi − xj)2 1 B ∑ i,j(xi − xj)2\n) − log2 ( 1 +\n(zi − zj)2 1 B ∑ i,j(zi − zj)2\n)]2\nWe then apply this error term to the generator to encourage the pairwise distances of the minibatch in latent space to be similar to the pairwise distances of the minibatch in the original high-dimensional space.\n\n3 EXPERIMENTS\nHere we apply out network architecture to two datasets: (1) five 2D distributions from Scikit-learn (Pedregosa et al., 2011) which allows us to visualize and quantify the behavior of GAIA in a lowdimensional space (Figure 2), and (2) the CELEBA-HQ dataset (Liu et al., 2015; Karras et al., 2017) which allows us to test the performance of GAIA on a more complex high dimensional dataset (Figure 3).\n\n3.1 2D DATASETS\nWe compared the performance of AE, VAE, and GAIA networks with the same architecture and training parameters on five 2D distributions from Scikit Learn (Pedregosa et al., 2011). We also compared the GAIA network with and without the distance loss term (Ldist = 0). We computed the learned latent representations (z) as well as reconstructions (Gd(z)) from of each of the networks (Figures 2, 6), and compared the these spaces on a number of metrics (Table 1).\nOur most salient observation can be found in the mesh-grids in Figure 2, where a clear boundary exists in the warping of high- and low-probability data in GAIA, as opposed to an autoencoder without adversarial regulation. A similar warping of low-probability data is seen in the VAE, although a smoother warping is seen at the boundaries.\nWe quantitatively analyzed the results of Figure 2 in Table 1. We found that interpolations in GAIA (Gd(zint.)) are the most likely to fall into the distribution of x (Figure 2 top; log(L(Gd(zint.)))). We also found that the distributions of both network reconstructions and interpolations in Z most highly match the input distribution (x) in the VAE network (measured by Kullback-Leibler divergence). This is likely due to the adversarial loss in GAIA. While VAEs are trained to match the distribution of x, GAIA’s generator is trained to find regions ofX which are sufficiently high-enough probability that the discriminator will not discriminate against it. Finally, we found that pairwise Euclidean distances in Z most highly resembled the original data distribution x (r(x, z)) in the GAIA network when the Ldist loss was imposed on the network. This leads us to conclude that GAIA can learn to map interpolations in latent-space onto the true data distribution in X in a similar manner as a VAE, while still respecting the original structure of the data.\n\n3.2 CELEBA-HQ\nTo observe the performance of our network on a more complex and high dimensional data, we use the CELEBA-HQ image dataset of aligned celebrity faces. We find that interpolations in Z produce smooth realistic morphs in X (Figure 3), and that complex features can be manipulated as linear vectors in the low-dimensional latent space of the network (Figure 4).\nTable 1: Comparison of GAIA, VAE, and AE on 2D datasets.\nModel r(x, z) ∗ log(L(Gd(zint.))) ‡ log(L(G(x))) DKL(x ‖ Gd(zint.)) † DKL(x ‖ G(x))\nAE 0.58 3207.39 3545.63 0.43 -0.57 VAE 0.64 3574.11 3588.50 -0.05 -0.64 GAIAll=0 0.38 3567.49 3564.27 0.19 -0.36 GAIAll=1 0.91 3593.84 3563.91 0.36 -0.32\nResults are intercepts from an OLS regression controlling for 2D dataset type, thus some values (such as KL divergence) can be negative. ∗Pearson correlation ‡Log-likelihood †Kullback-Leibler divergence\nFigure 3: Interpolations between autoencoded images in our network. The farthest left and right columns correspond to the input images, and the middle columns correspond to a linear interpolation in latent-space.\n\n3.2.1 FEATURE MANIPULATION\nFeature manipulation using generative models typically fall into two domains: (1) fully unsupervised approaches, where feature vectors are extracted and applied after learning (e.g. Radford et al. 2015; Larsen et al. 2015; Kingma & Dhariwal 2018; White 2016), and (2) supervised and partially supervised approaches, where high-level feature information is used during learning (e.g. Choi et al. 2017; Isola et al. 2017; Li et al. 2016; Perarnau et al. 2016; Zhu et al. 2017).\nWe find that, similar to the latter group of models, high-level features correspond to linear vectors in GAIA’s latent spaces (Figure 4). High-level feature representations are typically determined using the means of Z representations of images containing features (e.g Radford et al. 2015; Larsen et al. 2015). The mean of the latent representations of the faces in the dataset (here CELEBA-HQ) containing an attribute (zfeat) and not containing that attribute (znofeat) is subtracted (zfeat− znofeat) to acquire a high-level feature vector. The feature vector is then added to, or subtracted from, the latent representation of individual faces (zi), which is passed through the decoder of the generator, producing an image containing that high-level feature.\nSimilar to White (2016), we find that this approach is confounded by features being tangled together in the CELEBA-HQ dataset. For example, adding a latent vector to make images look older biases the image toward male, and making the image look more young biased the image toward female. This likely happens because the ratio of young males to older males is 0.28:1, whereas the ratio of young females to older females is much greater at 8.49:1 in the CELEBA-HQ dataset. As opposed to White (2016), who balance samples containing features in the training dataset, we use the coefficients of an ordinary least-squares regression trained to predict z representations from feature attributes on the full dataset as feature vectors. We find that these features (Figure 7 bottom) are less intertwined than subtracting means alone (Figure 7 top).\n\n3.3 RELATED WORK\nThis work builds primarily upon the GAN and AE. We used the AE as a discriminator motivated by Berthelot et al. (2017), and an AE as a generator motivated by Larsen et al. (2015), which in concert act as both an autoencoder and a GAN imparting bidirectionality on a GAN and imparting an adversarial loss on the autoencoder. A number of other adversarial network architectures (e.g. Larsen et al. 2015; Mescheder et al. 2017; Berthelot et al. 2017; Dumoulin et al. 2016; Ulyanov et al. 2017; Makhzani 2018) have been designed with a similar motivation in recent years. Our approach differs from these methods in that, by using an autoencoder as the discriminator, we are able to use a reconstruction loss which is passed across the discriminator, resulting in pixel-wise data reconstructions (Figure 5).\nSimilar motivations for better bidirectional inference-based methods have also been explored using flow-based generative models (Kingma & Dhariwal, 2018; Dinh et al., 2014; Kingma et al., 2016; Dinh et al., 2016), which do not rely on an adversarial loss. Due to their exact latent-variable inference (Kingma & Dhariwal, 2018), these architectures may also provide a useful direction for developing generative models to explore latent-spaces of data for generating datasets for psychophysical experiments.\nIn addition, the first revision of this work was published concurrently to ACAI network (Berthelot et al., 2018), which also uses an adversarial constraint on interpolations in the latent space of an autoencoder. Berthelot et al. find that adversarially constrained latent representations improve downstream tasks such as classification and clustering. At a high level, GAIA and ACAI networks perform the same functions, however, there are a few notable differences between the two networks. While ACAI uses an autoencoder as the discriminator of the adversarial network to improve pass the autoencoder error function across the discriminator, ACAI uses a traditional discriminator. As a result, the loss function is different between the two networks. Further comparisons are needed between the two architecture to compare network features such as training stability, reconstruction quality, latent feature representations, and downstream task performance.\n\n4 CONCLUSION\nWe propose a novel GAN-AE hybrid in which both the generator and discriminator of the GAN are AEs. In this architecture, a pixel-wise loss can be passed across the discriminator producing autoencoded data without smoothed reconstructions. Further, using the adversarial loss of the GAN, we train the generator’s AE explicitly on interpolations between samples projected into latent space, promoting a convex latent space representation. We find that in our 2D dataset examples, GAIA performs equivalently to a VAE in projecting interpolations in Z onto the true data distribution in X , while respecting the original structure in X . We conclude that our method more explicitly lends itself to interpolations between complex signals using a neural network latent space, while still respecting the high-dimensional structure of the input data.\nThe proposed architecture still leaves much to be accomplished, and modifications of this architecture may prove to be more useful, for example utilizing different encoder strategies such as progressively growing layers (Karras et al., 2017), interpolating across the entire minibatch rather than two-point interpolations, modeling the joint probability of X and Z, and exploring other methods to train more explicitly on a convex latent space. Further explorations are also needed to understand how interpolative sampling effects the structure of the latent space of GAIA in higher dimensions.\nOur network architecture furthers generative modeling by providing a novel solution to maintaining pixel-wise reconstruction over an adversarial architecture. Further, we take a step in the direction of convex latent space representations in a generative context. This architecture should prove useful both for current behavioral scientists interested in sampling from smooth and plausible stimuli spaces (e.g. Sainburg et al. 2018; Thielk et al. 2018; Zuidema et al. 2018), as well as providing motivation for future solutions to structured latent representations of data.\nOur network was trained using Tensorflow, and our full model, code, and high-resolution images, along with videos of the model will be made available when de-anonymized.\n\n5 APPENDIX\n\n\n5.1 NETWORK ARCHITECTURE\nIn principle, any form of AE network can be used in GAIA. In the experiments shown in this paper, we used two different types of networks. For the 2D dataset examples, we use 6 fully connected layers per network with 256 units per layer, and a latent layer with two neurons. For the CELEBAHQ dataset a modified version of network architecture advocated by Huang et al. (2018), which is comprised of a style and content AE using residual convolutional layers. Each layer of the decoder uses half as many filters as the encoder, and a linear latent layer is used in the encoder network but not the decoder network. The final number of latent neurons for the style and content networks are both 512 in the 128 × 128 pixel model shown here. The loss term for the pairwise-distance loss term is set at 2e-5. A Python/Tensorflow implementation of this network architecture is linked in the Conclusions section, and more details about the network architecture used are located in Huang et al. (2018).\n\n5.2 INSTABILITY IN ADVERSARIAL NETWORKS\nGANs are notoriously challenging to train, and refining techniques to balance and properly train GANs has been an area of active research since the conception of the GAN architecture (e.g. Berthelot et al. 2017; Salimans et al. 2016; Mescheder et al. 2017; Arjovsky et al. 2017). In traditional GANs, a balance needs to be found between training the generator and discriminator, otherwise one network will overpower the other and the generator will not learn a representation which fits the dataset. With GAIA, additional balances are required, such as between reproducing real images vs. discriminating against generated images, or balancing the generator of the network toward emphasizing autoencoding vs. producing high-quality latent-space interpolations.\nWe propose a novel, but simple, GAN balancing act which we find to be very effective. In our network, we balance the GAN’s loss using a sigmoid centered at zero:\nsigmoid(d) = 1\n1 + e−d∗b\nIn which b is a hyper-parameter representing the slope of the sigmoid6, and d is the difference between the two values being balanced in the network. For example, the balance in the learning rate of the discriminator and generator is based upon the loss of the real and generated images:\nδDisc ← sigmoid(‖xi −D(xi)‖1 − ‖xi −D(G(xi))‖1 + ‖Gd(zint.)−D(Gd(zint.))‖1/2)\nThe learning rate of the generator is then set as the inverse:\nδGen ← 1− δDisc\nThis allows each network to catch up to the other network when it is performing worse. The same principles are then used to balance the different losses within the generator and discriminator, which can be found in Algorithm 1. This balancing act allows the part of the network performing more poorly to be emphasized in the training regimen, resulting in more balanced and stable training.\n6kept at 20 for our networks\nAlgorithm 1 Training the GAIA Model 1: θGen, θDisc ← initialize network parameters 2: repeat 3: x← random mini-batch from dataset 4: # pass through network 5: z ← Ge(x) 6: zint. ← interpolate(z) 7: xgen ← Gd(z) 8: xint. ← Gd(zint.) 9: x̃← D(x) 10: x̃int. ← D(xint.) 11: x̃gen ← D(xgen) 12: # compute losses 13: Lx ← ‖x− x̃‖1 14: Lxgen ← ‖x− x̃gen‖1 15: Lxint. ← ‖xint. − x̃int.‖1 16: Ldistance ← pairwisedistance(x, zgen) 17: # balance losses 18: δDisc ← sigmoid(Lx −mean(Lxgen , Lxint.)) 19: δGen ← 1− δDisc 20: WGenint. ← sigmoid(Lxint. − Lxgen) 21: WGengen ← 1−WGenint. 22: WDiscfake ← sigmoid(mean(Lxgen , Lxint.) · γ − Lx) 23: # update parameters according to gradients\n",
    "approval": false,
    "rationale": "\nUpdate:\n\nI’d like to thank the authors for their thoroughness in responding to the issues I raised. I will echo my fellow reviewers in saying that I would encourage the authors to submit to another venue, given the substantial modifications made to the original submission.\n\nThe updated version provides a clearer context for the proposed approach (phychophysical experimentation) and avoids mischaracterizing GAIA as a generative model.\n\nDespite more emphasis being put on mentioning the existence of bidirectional variants of GANs, I still feel that the paper does not adequately address the following question: “What does GAIA offer that is not already achievable by models such as ALI, BiGAN, ALICE, and IAE, which equip GANs with an inference mechanism and can be used to perform interpolations between data points and produce sharp interpolates?” To be clear, I do think that the above models are inadequate for the paper’s intended use (because their reconstructions tend to be semantically similar but noticeably different perceptually), but I believe this is a question that is likely to be raised by many readers.\n\nTo answer the authors’ questions:\n\n- Flow-based generative models such as RealNVP relate to gaussian latent spaces in that they learn to map from the data distribution to a simple base distribution (usually a Gaussian distribution) in a way that is invertible (and which makes the computation of the Jacobian’s determinant tractable). The base distribution can be seen as a Gaussian latent space which has the same dimensionality as the data space.\n- Papers on building more flexible approximate posteriors in VAEs: in addition to the inverse autoregressive flow paper already cited in the submission, I would point the authors to Rezende and Mohamed’s “Variational Inference with Normalizing Flows”, Huang et al.’s “Neural Autoregressive Flows”, and van den Berg et al.’s “Sylvester Normalizing Flows for Variational Inference”.\n\n-----\n\nThe paper title summarizes the main claim of the paper: \"adversarial training on latent space interpolations encourage[s] convex latent distributions\". A convex latent space is defined as a space in which a linear interpolation between latent codes obtained by encoding a pair of points from some data distribution yields latent codes whose decoding also belongs to the same data distribution. The authors argue that current leading approaches fall short of producing convex latent spaces while preserving the \"high-dimensional structure of the original distribution\". They propose a GAN-AE hybrid, called GAIA, which they claim addresses this issue. The proposed approach turns the GAN generator and discriminator into autoencoders, and the adversarial game is framed in terms of minimizing/maximizing the discriminator’s reconstruction error. In addition to that, interpolations between pairs of data points are computed in the generator’s latent space, and the interpolations are decoded and treated as generator samples. A regularization term is introduced to encourage distances between pairs of data points to be mirrored by their representation in the generator’s latent space. The proposed approach is evaluated through qualitative inspection of latent space interpolations, attribute manipulations, attribute vectors, and generator reconstructions.\n\nOverall I feel like the problem presented in the paper is well-justified, but the paper itself does not build a sufficiently strong argument in favor of the proposed approach for me to recommend its acceptance. I do think there is a case to be made for a model which exhibits sharp reconstructions and which allows realistic latent space manipulations -- and this is in some ways put forward in the introduction -- but I don’t feel that the way in which the paper is currently cast highlights this very well. Here is a detailed breakdown of why, and where I think it should be improved, roughly ordered by importance:\n\n- The main reason for my reluctance to accept the paper is the fact that its main subject is convex latent spaces, yet I don’t see that reflected in the evaluation. The authors do not address how to evaluate (quantitatively or qualitatively) whether a certain model exhibits a convex latent space, and how to compare competing approaches with respect to latent space convexity. Figure 2 does present latent space interpolations which help get a sense of the extent to which interpolates also belong to the data distribution, however in the absence of a comparison to competing approaches it’s impossible for me to tell whether the proposed approach yields more convex latent spaces.\n- I don’t agree with the premise that current approaches are insufficient. The authors claim that autoencoders produce blurry reconstructions; while this may be true for factorized decoders, autoregressive decoders should alleviate this issue. They also claim that GANs lack bidirectionality but fail to mention the existing line of work in that direction (ALI, BiGAN, ALICE, and more recently Implicit Autoencoders). Finally, although flow-based generative models are mentioned later in the paper, they are not discussed in Section 1.2 when potential approaches to building convex latent spaces are enumerated and declared insufficient. As a result, the paper feels a little disconnected from the current state of the generative modeling literature.\n- The necessity for latent spaces to \"respect the high-dimensional structure of the [data] distribution\" is stated as a fact but not well-justified. How do we determine whether a marginal posterior is \"a suboptimal representation of the high-dimensional dataset\"? I think a more nuanced statement would be necessary. For instance, many recent approaches have been proposed to build more flexible approximate posteriors in VAEs; would that go some way towards embedding the data distribution in a more natural way?\n- I also question whether latent space convexity is a property that should always hold. In the case of face images a reasonable argument can be made, but in a dataset such as CIFAR10 how should we linearly interpolate between a horse and a car?\n- The proposed model is presented in the abstract as an \"AE which produces non-blurry samples\", but it’s not clear to me how one would sample from such a model. The generator is defined as a mapping from data points to their reconstruction; does this mean that the sampling procedure requires access to training examples? Alternatively one could fit a prior distribution on top of the latent codes and their interpolations, but as far as I can tell this is not discussed in the paper. I would like to see a more thorough discussion on the subject.\n- When comparing reconstructions with competing approaches there are several confounding factors, like the resolution at which the models were trained and the fact that they all reconstruct different inputs. Removing those confounding factors by comparing models trained at the same resolution and reconstructing the same inputs would help a great deal in comparing each approach.\n- The structure-preserving regularization term compares distances in X and Z space, but I doubt that pixelwise Euclidian distances are good at capturing an intuitive notion of distance: for example, if we translate an image by a few pixels the result is perceptually very similar but its Euclidian distance to the original image is likely to be high. As far as I can tell, the paper does not present evidence backing up the claim that the regularization term does indeed preserve local structure.\n- Figures 2 and 3 are never referenced in the main text, and I am left to draw my own conclusions as to what claim they are supporting. As far as I can tell they showcase the general capabilities of the proposed approach, but I would have liked to see a discussion of whether and how they improve on results that can be achieved by competing approaches.\n- The decision of making the discriminator an autoencoder is briefly justified when discussing related work; I would have liked to see a more upfront and explicit justification when first introducing the model architecture.\n- When discussing feature vectors it would be appropriate to also mention Tom White’s paper on Sampling Generative Networks.",
    "rating": 4,
    "type": "negative"
  },
  {
    "title": "IN A DISCRETE TARGET PROPAGATION SETTING",
    "abstract": "Learning deep neural networks with hard-threshold activation has recently become an important problem due to the proliferation of resource-constrained computing devices. In order to circumvent the inability to train with backpropagation in the present of hard-threshold activations, Friesen & Domingos (2018) introduced a discrete target propagation framework for training hard-threshold networks in a layer-by-layer fashion. Rather than using a gradient-based target heuristic, we explore the use of search methods for solving the target setting problem. Building on both traditional combinatorial optimization algorithms and gradient-based techniques, we develop a novel search algorithm Guided Random Local Search (GRLS). We demonstrate the effectiveness of our algorithm in training small networks on several datasets and evaluate our target-setting algorithm compared to simpler search methods and gradient-based techniques. Our results indicate that combinatorial optimization is a viable method for training hard-threshold networks that may have the potential to eventually surpass gradient-based methods in many settings.",
    "text": "1 INTRODUCTION\nInterest in network quantization has rapidly increased in recent years (Shan et al., 2018; Hubara et al., 2016; Friesen & Domingos, 2018). Network quantization reduces the complexity of a neural network by lowering the precision requirements of the activations and/or weights (Wu et al., 2018; Choi et al., 2016). As more network-based models are being deployed to mobile devices, computational power reduction and efficiency increase become critical issues. In order to train a deep neural network with full precision parameters efficiently, we depend upon powerful computational devices such as high-performance CPUs and GPUs. These devices use a lot of energy and have abundant memory, which mobile devices do not have. Learning networks with hard-threshold activations allows binary or low-precision inference and training which reduces computation time and energy demands. This seemingly innocuous change leads to a surprising challenge—networks with hard-threshold activations cannot be trained using the method of backpropagation since gradient descent requires a differentiable activation function. Unfortunately, the derivative of a hard-threshold function is 0 almost everywhere. The standard remedy is to use a gradient estimator known as the straight-through estimator (STE), first proposed by Hinton et al. (2012), and later analyzed by Bengio et al. (2013).\nFriesen & Domingos (2018) designed a novel discrete target propagation framework based on the idea of separating the network into a series of perception problems each with artificially set targets. They gave a simple gradient-based target setting heuristic using the sign of the next layer’s loss gradient. Instead, we propose the use of search methods for target setting. The application of combinatorial optimization methods to training neural networks remains largely unexplored. Discrete target propagation frameworks offer an opportunity to apply these methods in deep learning. We initiate a feasibility study on search methods for learning hard-threshold networks. This may open the door to significantly improving performance by leveraging state-of-the-art combinatorial optimization algorithms.\nBuilding on the convex-combinatorial optimization framework of Friesen & Domingos (2018), we present a search algorithm for target setting. We also develop a novel gradient-search hybrid target setting algorithm. We demonstrate the effectiveness of our method in training small networks on several data sets, with performance approaching the state of the art achieved by Friesen & Domingos (2018) for hard-threshold networks. We evaluate all target-setting algorithms mentioned in this paper, indicating significant room for improvements.\n\n1.1 RELATED WORK\nUsing the conventional backpropagation algorithm to train a neural network via gradient descent, the gradient of the loss is computed with respect to the output of each layer. This is propagated recursively from the output layer to the input layer using the chain rule. In our case, the activation function at each hidden layer would involve a non-linearity, meaning that the partial derivative will be zero, hence ruling out the use of backpropagation. In the case that we cannot use gradient descent, we use a gradient estimator. The straight-through estimator (STE) (Hinton et al., 2012; Bengio et al., 2013) backpropagates through the hard threshold function as if it were the identity function. When considering a single layer of neurons, the STE has the right sign, but once it is used through more hidden layers, this is not guaranteed.\nOur work builds on the idea of target propagation, first proposed by LeCun et al. (1989). The main idea is to compute targets rather than gradients, at each layer. Since then many different methods of target propagation have been proposed (Bengio & Frasconi, 1995; Courbariaux et al., 2015). In particular, Bengio (2014) proposed a method called “vanilla target propagation” to avoid the chain of derivatives through the network by considering an “approximate inverse”. Later, Hubara et al. (2016) improved this method, claiming that the imperfection of the inverse yields severe optimization problems. This led to their difference target propagation, which is a linearly corrected formula of Bengio (2014). They showed that this method of target propagation performs comparable to backpropagation methods.\nMost recently, Friesen & Domingos (2018) show that learning a deep hard-threshold network reduces to finding a feasible setting of its targets and then optimizing its weights in a layer-local fashion. Peng et al. (2018) introduce a new technique which they call SPIGOT: structured projection of intermediate gradients optimization technique. SPIGOT is a backpropagation technique for neural networks with hard-threshold predictions. It defines gradient-like quantities associated with intermediate non-differentiable operations, allowing backpropagation before and after them.\n\n2 FEASIBLE TARGET PROPAGATION\nWe begin by describing feasible target propagation, the method introduced by Friesen & Domingos (2018) for training deep hard-threshold networks upon which our work is based. Consider a dataset D = (X,T ) composed of an n ×m real-valued matrix X of vector-valued inputs {x(j)}mj=1 and a corresponding {+1,−1}-valued vector T of targets {t(j)}mj=1. The aim is to learn an `-layer neural network with hard-threshold activations\nf(x;W ) = g(W`g(W`−1 · · · g(W2g(W1x)))),\nwith input vector x ∈ Rn, weight matrices W = {Wi | Wi ∈ Rni×ni−1 , 1 6 i 6 `}, and activation function g(x) = sign(x), where n0 = n and sign(x) = 1 if x > 0 and −1 otherwise. For simplicity, the bias terms are incorporated into the weight matrices. Let Zi = Wig(Wi−1 · · · g(W1X)) and Hi = g(Zi) denote the full-batch pre-activation and postactivation matrices, and zi = (zi1, . . . , zini) and hi = (hi1, . . . , hini) denote the corresponding per-instance vectors. The learning goal is to find weights W of f that minimize the training loss L(H`, T ) = ∑m j=1 L(h (j) ` , t (j)) for some per-instance loss L(h`, t).\nBecause the derivative of the sign function is zero almost everywhere, backpropagation cannot be used to train the network f when ` > 1. Instead, Friesen & Domingos (2018) considered the decomposition f` ◦ f`−1 ◦ · · · ◦ f1 of f into perceptrons fi(hi) = hi+1 = g(Wihi). Given a matrix Ti of targets and a loss function Li for each perceptron fi(Hi), we can use the perceptron algorithm, or a gradient descent method, to set Wi such that fi produces these targets (if the dataset (Hi, Ti) is\nlinearly separable) or minimizes the loss Li(Zi, Ti). The problem then becomes setting the targets T = {T1, . . . , T`} in such a way as to minimize the output loss L(H`, T ). Towards that end, Friesen & Domingos (2018) proposed a recursive approach that involves setting the targets Ti of fi in order to minimize the next layer’s loss Li+1(Zi+1, Ti+1). More specifically, the method proceeds as follows. For each minibatch (Xb, Tb) from datasetD, initialize T` = Tb and T1, . . . , T`−1 as the activation of the corresponding layer in f(Xb;W ) and, starting with i = `, for all 1 6 i 6 `,\n1. assign the targets T̂i−1 for the next layer based on the current weights Wi and loss Li(fi(Ti−1), Ti),\n2. update Wi with respect to the loss Li(f(Ti−1), Ti), and\n3. set Ti−1 ← T̂i−1.\n\n2.1 RELATIONSHIP TO GRADIENT ESTIMATION\nIn their work, Friesen & Domingos (2018) used the heuristic\ntij = sign(− ∂\n∂hij Li+1(Zi+1, Ti+1)) (1)\nfor setting the jth component tij of the target vector ti at layer i, and the layer loss Li(zij , tij) = (tanh(−tijzij) + 1) ∣∣∣∣∂Li+1∂hij ∣∣∣∣ , (2) where tanh(−tz) + 1 is the soft hinge loss, so called because it is a smooth approximation of the saturated hinge loss max(0, 1−max(tz,−1)) commonly used for training classifier networks. This combination of target heuristic and layer loss function allows information to be transmitted from the output loss back through the network to every layer, enabling effective learning as empirically demonstrated by Friesen & Domingos (2018).\nThey briefly commented on the relationship between this form of feasible target propagation and the straight-through estimator, but we believe it is worth further consideration here. The gradient of the weighted loss Li given in (2) with respect to the pre-activation zij is\n∂Li(zij , tij)\n∂zij = − sign ( −∂Li+1 ∂hij ) sech2 ( − sign ( −∂Li+1 ∂hij ) zij ) ∣∣∣∣∂Li+1∂hij ∣∣∣∣\n= sech2(zij) ∂Li+1 ∂hij\nsince sech is an even function. This implies that the method of Friesen & Domingos (2018) is equivalent to a sech2 gradient estimator, that is, the straight-through estimator with the identity function replaced with sech2.\nThis is significant for two reasons. First, it suggests a simpler, independent justification for the performance improvements obtained by their method. Gradient estimation can be viewed as a way of propagating errors from the output layer by pretending the network has a different activation function. In particular, the saturated straight-through estimator corresponds to using a truncated identity function network as the error propagation proxy, while the sech2 gradient estimator corresponds to instead using a tanh network. When the activation function of the proxy network approximates the hard-threshold activation (in our case, the sign function) of the actual network (see Figure 1), we may expect the network to train effectively. From this perspective, the contributions to the state of the art by Friesen & Domingos (2018) reduce to finding an approximation to the sign function, the hyperbolic tangent, with more favorable properties for training; specifically, the derivative of tanh is nonzero outside the range [−1, 1], unlike the derivative of the truncated identity function. Although this justification and, moreover, the relationship between the straight-through estimator and their method were noted by Friesen & Domingos (2018), recognizing that this is the sole contributor to the obtained performance improvements clarifies the role of their method. It also leads into our second point, which is that this line of reasoning motivates investigating alternative implementations of the discrete target propagation framework, which may not bear any connection to gradient estimation.\n\n3 COMBINATORIAL SEARCH FOR TARGET SETTING\nWe propose an alternative to the target setting heuristic (1) used in Friesen & Domingos (2018). The target setting problem at layer i can be succinctly written as the following optimization problem\nmin Li(fi(Ti−1), Ti) s.t. Ti−1,j ∈ {−1, 1} ∀j (3) wherein the the subsequent candidate targets at layer Ti are treated as data. The feasible region is the vertices of a hypercube, and in general the objective function is non-convex. This justifies the use of heuristics, such as local search, for this problem.\n\n3.1 RANDOM LOCAL SEARCH\nThe procedure of random local search is highly flexible and many variations are possible. Over the course of training a neural network via feasible target propagation we have to solve very many large instances of problem (3). With that in mind we elected to use a very lightweight variation of random local search, presented in Algorithm 1.\n\n3.1.1 IMPROVEMENTS OVER NAIVE LOCAL SEARCH\nOur full method involves two improvements over the naive approach to local search. We improve 1) how the starting target is generated, and 2) how the probability of exploring a neighbour is chosen.\nGenerating a Seed Candidate As in the naive approach we begin with a uniform random seed candidate T then we generate a subset NT of the neighbourhood of T as in step 2(b) of our method using a uniform random probability distribution.\nNow we will choose our starting candidate using gradient information from each of these random sampled candidates. Our idea is to apply in the sign function to the average of the gradient evaluated at each point in N . In particular we choose (applying the sign function coordinate-wise) Ti = sign( 1|N | ∑ C∈N ∂Li+1 ∂Ti (C)).\nSetting the Probabilities Consider an entry h of Ti. Recall that the gradient ∇Li1 points in direction of local steepest increase. For intuition, imagine that Li+1 is a convex function. Then if we were to flip the sign of entry h when sign(∂Li+1∂h ) 6= sign(h) we would surely be increasing the value of Li+1. This guides us to the following heuristic approach: flip entries only when sign(∂Li+1∂h ) = sign(h) and do so in a manner proportional to | ∂Li+1 ∂h |. In particular we flip with probability min{|∂Li+1∂h | + offset, 1}) where offset is some positive constant. Typically offset = 3 5 works well in our experience, as we want a relatively high probability of following the direction indicated by the gradient.\n\n4 EXPERIMENTS\nTo evaluate our combinatorial search-based target setting methods on training hard-threshold networks via discrete target propagation, we begin by comparing the performance of models trained\nAlgorithm 1: Guided Random Local Search (GRLS) 1 function GRLS(i, α, a, β, γ, δ, ω);\nInput : i: layer of network to set targets (through which we access loss function L and dimensions for targets T ). α: probability any given entry of initial random candidate is 1. a: vector of probabilities for flipping sign of an entry of initial random candidate when generating neighbours. β: number of neighbours to average over in generating initial random seed. γ: number of rounds of local search. δ: number of random neighbours to check during local search. ω: additive offset to flip probability for gradient-guided random neighbour generation.\nOutput: Ti: targets for layer i. 2 Let T be a candidate such for each entry of h of T : Pr(h = 1) = α, and Pr(h = −1) = 1− α. 3 NT ← ∅ 4 for β iterations do 5 C ← T 6 for each entry h of C: flip sign of h with probability ah. 7 NT ← NT ∪ {C} 8 end 9 Ti ← sign( 1|NT | ∑ C∈NT ∂Li+1 ∂Ti (C))\n10 for γ rounds do 11 N ← ∅ 12 for δ iterations do 13 C ← Ti 14 for each entry h of C do 15 if sign(∂Li+1∂h ) 6= sign(h) then flip sign of h with probability ahph, where ph := min{|∂Li+1∂h |+ ω, 1} 16 end 17 N ← N ∪ {C} 18 end 19 Ti ← argmin{Li(fi(C), Ti+1) : C ∈ N} 20 end 21 return Ti\nwith our methods to models trained using gradient estimation, including the standard saturated straight-through estimator and the state-of-the-art tanh estimator of Friesen & Domingos (2018). We trained several different networks on three standard image classification datasets, MNIST (LeCun & Cortes (1998)), CIFAR-10 and CIFAR-100 (Krizhevsky (2009)), as well as a graph classification dataset. On MNIST, we trained a small 2-layer feedforward network and a simple 4-layer convolutional network. On CIFAR-10 and CIFAR-100, we trained the same 4-layer convolutional network, a simpler version of the 8-layer convolutional network of Zhou et al. (2016). This is the same convolutional network used by Friesen & Domingos (2018), allowing for direct comparison with their results.\nFor the purpose of testing our methods in a highly constrained setting, we assembled a small graph connectivity classification dataset. The dataset contains all graphs on six vertices labeled by their connectivity. There are 32768 graphs in total, 26704 connected graphs and 6064 disconnected graphs, and we randomly partitioned the dataset into training and test sets containing 29491 and 3277 graphs, respectively. We chose this dataset for its connection to discrete optimization, the basis of our methods, and with the inclination that the discrete nature of graph connectivity might demonstrate a potential advantage of our combinatorial search methods over gradient estimation in certain settings. On this dataset, denoted GRAPHS, we trained the small 2-layer feedforward network.\nFinal test accuracies for the 2-layer feedforward network and 4-layer convolutional network on each of the datasets are shown in Table 1. GRLS achieves a significantly higher test accuracy on\nGRAPHS than the SSTE and FTPROP, by a margin of 12.8% and 3.6% respectively; given that this is the only dataset on which it outperforms both FTPROP and SSTE, this suggests that our prior speculation may be correct and a highly discrete task such as graph connectivity classification is likely less amenable to gradient-based target setting methods and more suited to search-based techniques, potentially due to reduced smoothness in the loss landscape. As the network grows in size and the datasets become more complex, GRLS tends decrease in performance faster than the SSTE and FTPROP. While it is able to roughly match the performance of FTPROP on MNIST with either network, and exceed the accuracy of the SSTE on MNIST for both networks by 0.6% and 0.4%, respectively, the gap between GRLS and FTPROP widens by 9.9% on CIFAR-10. This begins to indicate that the higher dimensionality of the CIFAR-10 data manifold compared to MNIST may play a much larger role in inhibiting the performance of GRLS than increasing the size of the target search space. In fact, relative to FTPROP, GRLS achieves a 0.5% higher accuracy on MNIST with the 4-layer convolutional network than with the 2-layer feedforward network, suggesting that the convolutional layers ease the search problem or make it more suited to the specific search strategies of GRLS.\n\n4.1 METHOD VARIATIONS\nWe performed an ablation study measuring the validation accuracies for variations of the methods considered in this paper. For SSTE, FTPROP, and GRLS we considered the effect of dropping Loss Weighting (LW) from the methods. For GRLS we also considered removing Gradient Guiding (GG), Gradient Seeding (GS), and Criterion Weighting (CW) (weighting the target choosing criterion (loss) by the next layer’s loss gradient). We ran experiments on the 2-layer feedforward network and the 4-layer convolutional network, and tested two distinct datasets on each network. The results are summarized in Table 2.\nWhen removing GG and GS from GRLS we lose 12.3% accuracy on MNIST, and we lose a dramatic 43.8% on CIFAR-10. Even further, dropping either GG or GS leads to large decrease in accuracy for GRLS on CIFAR-10. This suggests the significant role gradient guiding and gradient seeding play in randomized local search; pure RLS has only a 30.2% accuracy on CIFAR-10. Another lesson from Table 2 is that Loss Gradient Weighting is necessary for good accuracy across all methods on the more complex datasets. Interestingly on the Graphs dataset, removing Loss Weighting leads to improvements for both SSTE and GRLS.\n\n4.2 SENSITIVITY TO HYPERPARAMETERS AND ROLE OF DIMENSIONALITY\nThere are two important hyperparameters to consider varying when studying GRLS: the number of iterations γ, and the number of candidates δ. Increasing these parameters leads to increasing the strength of local search; δ value increases the breadth, and larger γ increases the depth. We compared the accuracy of GRLS and RLS in training our 2-layer feedforward neural network on the MNIST dataset for various values of γ and δ. In one experiment we increase δ from 2 to 1024 in powers of 2, and the other we increase γ from 1 to 50. The results on shown in Figure 2, with the δ experiment on the left, and the γ experiment on the right.\nThe most striking phenomenon the data shows is a robustness of GRLS to these hyperparameter variations. In light of the performance variations on MNIST in Table 2, it seems that the Gradient Seeding technique is primarily responsible for this robustness, in particular it starts the search in a good position where further iterations do not yield significant improvement. As expected, RLS performs better as we increase δ, but curiously it seems GRLS is independent of δ. We believe this suggests the target setting problem exhibits a pervasiveness of local optima, each of which are close enough to the global optimum to be used effectively for target setting.\nWe investigate the effect of varying the number of hidden units on our training method. We test the accuracy of GRLS for training our 2-layer feedforward network with d hidden units for d ∈ {20, 50, 100, 200, 400, 800, 2000, 5000}, on the MNIST dataset. The results are presented in Table 3. We keep the parameters δ and γ constant as we vary d. For local search to be able to find good solutions δ and γ need to scale with the input size to the search problem, which d essentially corresponds to. Hence the the Loss value for the target found by RLS will increase with d. A decay in accuracy would then be evidence of the relationship between target setting quality and accuracy. Indeed the data shows such a decline in RLS accuracy for large d. Interestingly, GRLS seems to show a similar curve to FTPROP in terms of its accuracy on this dataset, lending credibility to the power of gradient-guided techniques to scale with d.\n\n4.3 METHOD SUBOPTIMALITY\nWe seek to compare the quality of target setting procedures considered in our study. We considered random instances of Problem (3) (i.e randomly generated weight matrix and output targets, with 10 possible output classes) and tasked GRLS, RLS, and FTPROP to set targets. We performed 1024 independent trials and collected the data in Table 3. We show there the loss mean and standard error of each given target setting method.\nThe losses for all three methods were comparable, but high as they were all over 2. Note that if a network is separable in the sense of Friesen & Domingos (2018), zero loss is possible. This indicates that there is considerable potential for improved target setting procedures that get a lower loss. Furthermore, GRLS exhibits a higher standard error than other methods, enough to potentially place the true empirical mean loss substantially lower than for FTPROP, indicating that it obtains notably better (and notably worse) targets in different trials.\n\n5 FUTURE WORK\nWe view our work as a preliminary investigation into combinatorial techniques for target setting in training hard-threshold networks. We have shown that there is a lot of potential for such approaches, and plenty of room for improvement. An important problem to solve in future work is to develop more computationally-tractable combinatorial search algorithms. The most significant barrier for our approach is the need to perform a forward pass through the network for each candidate target considered during the search. One may alleviate this problem by instead evaluating candidates by taking their scalar product with a loss gradient or previous candidates; in the latter case, choosing candidates with positive scalar product ensures that the loss cannot decrease. In a different direction, the locality of the receptive fields of a convolutional layer could be used to enable localized target evaluation and, thereby, effective splicing together of multiple candidates to create a significantly improved target.\nIn keeping with the wide open potential for this line of investigation we present some open questions. Computational tools such as BARON (Tawarmalani & Sahinidis, 2005; Sahinidis, 2017) exist, which can solve discrete non-linear optimization problems, including discrete target setting, to optimality. It would be interesting to investigate the effect such a solver would have on training accuracy if used to set targets instead of local search. Generalizing from this, the combinatorial optimization literature is vast, and there are a wealth of unexplored techniques which could be tested for solving Problem (3). Further, since combinatorial training techniques may have radically different behaviour from gradient-based continuous methods in general, it would be worthwhile to study the effect of changing network architectures on training in this context. It is conceivable that different network structures from those used in the traditional best-practices can be more amenable to this approach to training. Similarly, the GRAPHS dataset seems to indicate that for some datasets local search based training can be superior to continuous methods. It would be interesting to attempt to characterize those datasets for which this phenomenon holds.\n\n6 APPENDIX\n\n\n6.1 EXPERIMENT DETAILS\nIn all experiments involving our training method, we set the number of search iterations γ = 10, neighborhood sizes β = δ = 64, initial random candidate probability α = 1/2, and flip probability offset ω = 3/5. Because the number of hidden units per layer varies by network, the size of the search space varies nonlinearly between different networks and distinct layers within a given network. Consequently, we vary the perturbation probability a by network and layer; in particular, we set each ah = 5/32 for the 2-layer feedforward network and, for P = (0.0127, 0.0172, 0.031), set each ah = Pi for the 4-layer convolutional network. These correspond to an expectation of flipping approximately 15 candidate target entries at a time for the single hidden layer of the feedforward network, and 117, 70, and 31 entries for the three larger hidden layers of the 4-layer convolutional network.\nAs in the work of Friesen & Domingos (2018), we train all networks using the Adam optimizer (Kingma & Ba (2015)) (applied to each layer individually in the case of discrete target propagation) with a batch size of 64, weight decay coefficient 5×10−4, and learning rate 2.5×10−4—additional hyperparameter details can be found in the appendices. We used a single NVIDIA P100 GPU to train the convolutional and feedforward networks. For our search methods, at each training step we require a single forward pass through every layer for each candidate target generated during each iteration of the search, implying that the method requires approximately a factor of δγ/3 more computations than training a network with gradient estimation or, in the case of full-precision networks, backpropagation. This is somewhat alleviated by the parallel processing capabilities of GPUs, but does imply that training the small feedforward network and 4-layer convolutional network with our method is approximately ×3 and ×100 slower, respectively, than using gradient estimation. We view our work as an initial exploration of the use of combinatorial optimization methods in training artificial neural networks and leave this speed issue as a open problem for future study; note though that these issues do not effect the ability to perform fast and memory-efficient inference using the trained models, a key advantage of hard-threshold networks.\n",
    "approval": false,
    "rationale": "TargetProp\n\nThis paper addresses the problem of training neural networks with the sign activation function. A recent method for training such non-differentiable networks is target propagation: starting at the last layer, a target (-1 or +1) is assigned to each neuron in the layer; then, for each neuron in the layer, a separate optimization problem is solved, where the weights into that neuron are updated to achieve the target value. This procedure is iterated until convergence, as is typical for regular networks. Within the target propagation algorithm, the target assignment problem asks: how do we assign the targets at layer i, given fixed targets and weights at layer i+1? The FTPROP algorithm solves this problem by simply using sign of the corresponding gradient. Alternatively, this paper attempts to assign targets by solving a combinatorial problem. The authors propose a stochastic local search method which leverages gradient information for initialization and improvement steps, but is essentially combinatorial. Experimentally, the proposed algorithm, GRLS, is sometimes competitive with the original FTPROP, and is substantially better than the pure gradient approximation method that uses the straight-through estimator.\n\nOverall, I do like the paper and the general approach. However, I think the technical contribution is thin at the moment, and there is no dicussion or comparison with a number of methods from multiple papers. I look forward to discussing my concerns with the authors during the rebuttal period. However, I strongly believe that the authors should spend some time improving the method before submitting to the next major conference. I am confident they will have a strong paper if they do so.\n\nStrengths:\n- Clarity: a well-written paper, easy to read and clear w.r.t. the limitations of the proposed method.\n- Approach: I really like the combinatorial angle on this problem, and strongly believe this is the way forward for discrete neural nets.\n\nWeaknesses:\n- Algorithm: GRLS, in its current form, is quite basic. The Stochastic Local Search (SLS) literature (e.g. [1]) is quite rich and deep. Your algorithm can be seen as a first try, but it is really far from being a powerful, reliable algorithm for your problem. I do appreciate your analysis of the assignment rule in FTPROP, and how it is a very reasonable one. However, a proper combinatorial method should do better given a sufficient amount of time.\n- Related work: references [2-10] herein are all relevant to your work at different degrees. Overall, the FTPROP paper does not discuss or compare to any of these, which is really shocking. I urge the authors to implement some or all of these methods, and compare fairly against them. Even if your modified target assignment were to strictly improve over FTPROP, this would only be meaningful if the general target propagation procedure is actually better than [2-10] (or the most relevant subset).\n- Scalability: I realize that this is a huge challenge, but it is important to address it or at least show potential techniques for speeding up the algorithm. Please refer to classical SLS work [1] or other papers and try to get some guidance for the next iteration of your paper.\n\nGood luck!\n\n[1] Hoos, Holger H., and Thomas Stützle. Stochastic local search: Foundations and applications. Elsevier, 2004.\n[2] Stochastic local search for direct training of threshold networks\n[3] Training Neural Nets with the Reactive Tabu Search\n[4] Using random weights to train multilayer networks of hard-limiting units\n[5] Can threshold networks be trained directly?\n[6] The geometrical learning of binary neural networks\n[7] An iterative method for training multilayer networks with threshold functions\n[8] Backpropagation Learning for Systems with Discrete-Valued Functions\n[9] Training Multilayer Networks with Discrete Activation Functions\n[10] A Max-Sum algorithm for training discrete neural networks",
    "rating": 3,
    "type": "negative"
  },
  {
    "title": "GENERATIVE PARAGRAPH VECTOR",
    "abstract": "The recently introduced Paragraph Vector is an efficient method for learning highquality distributed representations for pieces of texts. However, an inherent limitation of Paragraph Vector is lack of ability to infer distributed representations for texts outside of the training set. To tackle this problem, we introduce a Generative Paragraph Vector, which can be viewed as a probabilistic extension of the Distributed Bag of Words version of Paragraph Vector with a complete generative process. With the ability to infer the distributed representations for unseen texts, we can further incorporate text labels into the model and turn it into a supervised version, namely Supervised Generative Paragraph Vector. In this way, we can leverage the labels paired with the texts to guide the representation learning, and employ the learned model for prediction tasks directly. Experiments on five text classification benchmark collections show that both model architectures can yield superior classification performance over the state-of-the-art counterparts.",
    "text": "1 INTRODUCTION\nA central problem in many text based applications, e.g., sentiment classification (Pang & Lee, 2008), question answering (Stefanie Tellex & Marton., 2003) and machine translation (I. Sutskever & Le, 2014), is how to capture the essential meaning of a piece of text in a fixed-length vector. Perhaps the most popular fixed-length vector representations for texts is the bag-of-words (or bag-of-ngrams) (Harris, 1954). Besides, probabilistic latent semantic indexing (PLSI) (Hofmann, 1999) and latent Dirichlet allocation (LDA) (Blei & Jordan, 2003) are two widely adopted alternatives.\nA recent paradigm in this direction is to use a distributed representation for texts (T. Mikolov & Dean, 2013a). In particular, Le and Mikolov (Quoc Le, 2014; Andrew M.Dai, 2014) show that their method, Paragraph Vector (PV), can capture text semantics in dense vectors and outperform many existing representation models. Although PV is an efficient method for learning high-quality distributed text representations, it suffers a similar problem as PLSI that it provides no model on text vectors: it is unclear how to infer the distributed representations for texts outside of the training set with the learned model (i.e., learned text and word vectors). Such a limitation largely restricts the usage of the PV model, especially in those prediction focused scenarios.\nInspired by the completion and improvement of LDA over PLSI, we first introduce the Generative Paragraph Vector (GPV) with a complete generation process for a corpus. Specifically, GPV can be viewed as a probabilistic extension of the Distributed Bag of Words version of Paragraph Vector (PVDBOW), where the text vector is viewed as a hidden variable sampled from some prior distributions, and the words within the text are then sampled from the softmax distribution given the text and word vectors. With a complete generative process, we are able to infer the distributed representations of new texts based on the learned model. Meanwhile, the prior distribution over text vectors also acts as a regularization factor from the view of optimization, thus can lead to higher-quality text representations.\nMore importantly, with the ability to infer the distributed representations for unseen texts, we now can directly incorporate labels paired with the texts into the model to guide the representation learning, and turn the model into a supervised version, namely Supervised Generative Paragraph Vector (SGPV). Note that supervision cannot be directly leveraged in the original PV model since it has no\ngeneralization ability on new texts. By learning the SGPV model, we can directly employ SGPV to predict labels for new texts. As we know, when the goal is prediction, fitting a supervised model would be a better choice than learning a general purpose representations of texts in an unsupervised way. We further show that SGPV can be easily extended to accommodate n-grams so that we can take into account word order information, which is important in learning semantics of texts.\nWe evaluated our proposed models on five text classification benchmark datasets. For the unsupervised GPV, we show that its superiority over the existing counterparts, such as bag-of-words, LDA, PV and FastSent (Felix Hill, 2016). For the SGPV model, we take into comparison both traditional supervised representation models, e.g. MNB (S. Wang, 2012), and a variety of state-of-the-art deep neural models for text classification (Kim, 2014; N. Kalchbrenner, 2014; Socher & Potts, 2013; Irsoy & Cardie, 2014). Again we show that the proposed SGPV can outperform the baseline methods by a substantial margin, demonstrating it is a simple yet effective model.\nThe rest of the paper is organized as follows. We first review the related work in section 2 and briefly describe PV in section 3. We then introduce the unsupervised generative model GPV and supervised generative model SGPV in section 4 and section 5 respectively. Experimental results are shown in section 6 and conclusions are made in section 7.\n\n2 RELATED WORK\nMany text based applications require the text input to be represented as a fixed-length feature vector. The most common fixed-length representation is bag-of-words (BoW) (Harris, 1954). For example, in the popular TF-IDF scheme (Salton & McGill, 1983), each document is represented by tfidf values of a set of selected feature-words. However, the BoW representation often suffers from data sparsity and high dimension. Meanwhile, due to the independent assumption between words, BoW representation has very little sense about the semantics of the words.\nTo address this shortcoming, several dimensionality reduction methods have been proposed, such as latent semantic indexing (LSI) (S. Deerwester & Harshman, 1990), Probabilistic latent semantic indexing (PLSI) (Hofmann, 1999) and latent Dirichlet allocation (LDA) (Blei & Jordan, 2003). Both PLSI and LDA have a good statistical foundation and proper generative model of the documents, as compared with LSI which relies on a singular value decomposition over the term-document cooccurrence matrix. In PLSI, each word is generated from a single topic, and different words in a document may be generated from different topics. While PLSI makes great effect on probabilistic modeling of documents, it is not clear how to assign probability to a document outside of the training set with the learned model. To address this issue, LDA is proposed by introducing a complete generative process over the documents, and demonstrated as a state-of-the-art document representation method. To further tackle the prediction task, Supervised LDA (David M.Blei, 2007) is developed by jointly modeling the documents and the labels.\nRecently, distributed models have been demonstrated as efficient methods to acquire semantic representations of texts. A representative method is Word2Vec (Tomas Mikolov & Dean, 2013b), which can learn meaningful word representations in an unsupervised way from large scale corpus. To represent sentences or documents, a simple approach is then using a weighted average of all the words. A more sophisticated approach is combing the word vectors in an order given by a parse tree (Richard Socher & Ng, 2012). Later, Paragraph Vector (PV) (Quoc Le, 2014) is introduced to directly learn the distributed representations of sentences and documents. There are two variants in PV, namely the Distributed Memory Model of Paragraph Vector (PV-DM) and the Distributed Bag of Words version of Paragraph Vector (PV-DBOW), based on two different model architectures. Although PV is a simple yet effective distributed model on sentences and documents, it suffers a similar problem as PLSI that it provides no model on text vectors: it is unclear how to infer the distributed representations for texts outside of the training set with the learned model.\nBesides these unsupervised representation learning methods, there have been many supervised deep models with directly learn sentence or document representations for the prediction tasks. Recursive Neural Network (RecursiveNN) (Richard Socher & Ng, 2012) has been proven to be efficient in terms of constructing sentence representations. Recurrent Neural Network (RNN) (Ilya Sutskever & Hinton, 2011) can be viewed as an extremely deep neural network with weight sharing across time. Convolution Neural Network (CNN) (Kim, 2014) can fairly determine discriminative phrases in a\ntext with a max-pooling layer. However, these deep models are usually quite complex and thus the training would be time-consuming on large corpus.\n\n3 PARAGRAPH VECTOR\nSince our model can be viewed as a probabilistic extension of the PV-DBOW model with a complete generative process, we first briefly review the PV-DBOW model for reference.\nIn PV-DBOW, each text is mapped to a unique paragraph vector and each word is mapped to a unique word vector in a continuous space. The paragraph vector is used to predict target words randomly sampled from the paragraph as shown in Figure 1. More formally, Let D={d1, . . . ,dN} denote a corpus of N texts, where each text dn = (wn1 , w n 2 , . . . , w n ln\n), n ∈ 1, 2, . . . , N is an lnlength word sequence over the word vocabulary V of size M . Each text d ∈ D and each word w ∈ V is associated with a vector ~d ∈ RK and ~w ∈ RK , respectively, where K is the embedding dimensionality. The predictive objective of the PV-DBOW for each word wnl ∈ dn is defined by the softmax function\np(wni |dn) = exp(~wni · ~dn)∑ w′∈V exp(~w ′ · ~dn) (1)\nThe PV-DBOW model can be efficiently trained using the stochastic gradient descent (Rumelhart & Williams, 1986) with negative sampling (T. Mikolov & Dean, 2013a).\nAs compared with traditional topic models, e.g. PLSI and LDA, PV-DBOW conveys the following merits. Firstly, PV-DBOW using negative sampling can be interpretated as a matrix factorization over the words-by-texts co-occurrence matrix with shifted-PMI values (Omer Levy & Ramat-Gan, 2015). In this way, more discriminative information (i.e., PMI) can be modeled in PV as compared with the generative topic models which learn over the words-by-texts co-occurrence matrix with raw frequency values. Secondly, PV-DBOW does not have the explicit “topic” layer and allows words automatically clustered according to their co-occurrence patterns during the learning process. In this way, PV-DBOW can potentially learn much finer topics than traditional topic models given the same hidden dimensionality of texts. However, a major problem with PV-DBOW is that it provides no model on text vectors: it is unclear how to infer the distributed representations for unseen texts.\n\n4 GENERATIVE PARAGRAPH VECTOR\nIn this section, we introduce the GPV model in detail. Overall, GPV is a generative probabilistic model for a corpus. We assume that for each text, a latent paragraph vector is first sampled from some prior distributions, and the words within the text are then generated from the normalized exponential (i.e. softmax) distribution given the paragraph vector and word vectors. In our work, multivariate normal distribution is employed as the prior distribution for paragraph vectors. It could\nbe replaced by other prior distributions and we will leave this as our future work. The specific generative process is as follows:\nFor each text dn ∈D, n = 1, 2, . . . , N : (a) Draw paragraph vector ~dn ∼ N (µ,Σ) (b) For each word wni ∈ dn, i = 1, 2, . . . , ln :\nDraw word wni ∼ softmax(~dn ·W )i\nwhere W denotes a k ×M word embedding matrix with W∗j = ~wj , and softmax(~dn ·W )i is the softmax function defined the same as in Equation (1). Figure 2 (Left) provides the graphical model of this generative process. Note that GPV differs from PV-DBOW in that the paragraph vector is a hidden variable generated from some prior distribution, which allows us to infer the paragraph vector over future texts given the learned model. Based on the above generative process, the probability of the whole corpus can be written as follows:\np(D)= N∏ n=1 ∫ p(~dn|µ,Σ) ∏ wni ∈dn p(wni |W, ~dn)d~dn\nTo learn the model, direct maximum likelihood estimation is not tractable due to non-closed form of the integral. We approximate this learning problem by using MAP estimates for ~dn, which can be formulated as follows:\n(µ∗,Σ∗,W ∗) = arg max µ,Σ,W\n∏ p(d̂n|µ,Σ) ∏ wni ∈dn p(wni |W, d̂n)\nwhere d̂n denotes the MAP estimate of ~dn for dn, (µ∗,Σ∗,W ∗) denotes the optimal solution. Note that for computational simplicity, in this work we fixed µ as a zero vector and Σ as a identity matrix. In this way, all the free parameters to be learned in our model are word embedding matrix W . By taking the logarithm and applying the negative sampling idea to approximate the softmax function, we obtain the final learning problem\nL= N∑ n=1 ( −1 2 ||d̂n||2+ ∑ wni ∈dn ( log σ(~wni ·d̂n)+k·Ew′∼Pnw log σ(− ~w′ · d̂n) )) where σ(x) = 1/(1 + exp(−x)), k is the number of “negative” samples, w′ denotes the sampled word and Pnw denotes the distribution of negative word samples. As we can see from the final objective function, the prior distribution over paragraph vectors actually act as a regularization term. From the view of optimization, such regularization term could constrain the learning space and usually produces better paragraph vectors.\nFor optimization, we use coordinate ascent, which first optimizes the word vectors W while leaving the MAP estimates (d̂) fixed. Then we find the new MAP estimate for each document while leaving the word vectors fixed, and continue this process until convergence. To accelerate the learning, we adopt a similar stochastic learning framework as in PV which iteratively updates W and estimates ~d by randomly sampling text and word pairs.\nAt prediction time, given a new text, we perform an inference step to compute the paragraph vector for the input text. In this step, we freeze the vector representations of each word, and apply the same MAP estimation process of ~d as in the learning phase. With the inferred paragraph vector of the test text, we can feed it to other prediction models for different applications.\n\n5 SUPERVISED GENERATIVE PARAGRAPH VECTOR\nWith the ability to infer the distributed representations for unseen texts, we now can incorporate the labels paired with the texts into the model to guide the representation learning, and turn the model into a more powerful supervised version directly towards prediction tasks. Specifically, we introduce an additional label generation process into GPV to accommodate text labels, and obtain the Supervised Generative Paragraph Vector (SGPV) model. Formally, in SGPV, the n-th text dn and the corresponding class label yn ∈ {1, 2, . . . , C} arise from the following generative process:\nFor each text dn ∈D, n = 1, 2, . . . , N : (a) Draw paragraph vector ~dn ∼ N (µ,Σ) (b) For each word wni ∈ dn, i = 1, 2, . . . , ln :\nDraw word wni ∼ softmax(~dn ·W )i (c) Draw label yn|~dn, U, b ∼ softmax(U · ~dn+b)\nwhere U is a C ×K matrix for a dataset with C output labels, and b is a bias term. The graphical model of the above generative process is depicted in Figure 2 (Right). SGPV defines the probability of the whole corpus as follows\np(D)= N∏ n=1 ∫ p(~dn|µ,Σ) ( ∏ wni ∈dn p(wni |W, ~dn) ) p(yn|~dn, U, b)d~dn\nWe adopt a similar learning process as GPV to estimate the model parameters. Since the SGPV includes the complete generative process of both paragraphs and labels, we can directly leverage it to predict the labels of new texts. Specifically, at prediction time, given all the learned model parameters, we conduct an inference step to infer the paragraph vector as well as the label using MAP estimate over the test text.\nThe above SGPV may have limited modeling ability on text representation since it mainly relies on uni-grams. As we know, word order information is often critical in capturing the meaning of texts. For example, “machine learning” and “learning machine” are totally different in meaning with the same words. There has been a variety of deep models using complex architectures such as convolution layers or recurrent structures to help capture such order information at the expense of large computational cost.\nHere we propose to extend SGPV by introducing an additional generative process for n-grams, so that we can incorporate the word order information into the model and meanwhile keep its simplicity in learning. We name this extension as SGPV-ngram. Here we take the generative process of SGPVbigram as an example.\nFor each text dn ∈D, n = 1, 2, . . . , N : (a) Draw paragraph vector ~dn ∼ N (µ,Σ) (b) For each word wni ∈ dn, i = 1, 2, . . . , ln :\nDraw word wni ∼ softmax(~dn ·W )i\n(c) For each bigram gni ∈ dn, i = 1, 2, . . . , sn : Draw bigram gni ∼ softmax(~dn ·G)i\n(d) Draw label yn|~dn, U, b ∼ softmax(U · ~dn+b)\nwhere G denotes a K × S bigram embedding matrix with G∗j = ~gj , and S denotes the size of bigram vocabulary. The joint probability over the whole corpus is then defined as\np(D)= N∏ n=1 ∫ p(~dn|µ,Σ) ( ∏ wni ∈dn p(wni |W, ~dn) )( ∏ gni ∈dn p(gni |G, ~dn) ) p(yn|~dn, U, b)d~dn\n\n6 EXPERIMENTS\nIn this section, we introduce the experimental settings and empirical results on a set of text classification tasks.\n\n6.1 DATASET AND EXPERIMENTAL SETUP\nWe made use of five publicly available benchmark datasets in comparison.\nTREC: The TREC Question Classification dataset (Li & Roth, 2002)1 which consists of 5, 452 train questions and 500 test questions. The goal is to classify a question into 6 different types depending on the answer they seek for.\nSubj: Subjectivity dataset (Pang & Lee, 2004) which contains 5, 000 subjective instances and 5, 000 objective instances. The task is to classify a sentence as being subjective or objective.\nMR: Movie reviews (Pang & Lee, 2005) 2 with one sentence per review. There are 5, 331 positive sentences and 5, 331 negative sentences. The objective is to classify each review into positive or negative category.\nSST-1: Stanford Sentiment Treebank (Socher & Potts, 2013) 3. SST-1 is provided with train/dev/test splits of size 8, 544/1, 101/2, 210. It is a fine-grained classification over five classes: very negative, negative, neutral, positive, and very positive.\nSST-2: SST-2 is the same as SST-1 but with neutral reviews removed. We use the standard train/dev/test splits of size 6, 920/872/1, 821 for the binary classification task.\nPreprocessing steps were applied to all datasets: words were lowercased, non-English characters and stop words occurrence in the training set are removed. For fair comparison with other published results, we use the default train/test split for TREC, SST-1 and SST-2 datasets. Since explicit split of train/test is not provided by subj and MR datasets, we use 10-fold cross-validation instead.\nIn our model, text and word vectors are randomly initialized with values uniformly distributed in the range of [-0.5, +0.5]. Following the practice in (Tomas Mikolov & Dean, 2013b) , we set the noise distributions for context and words as pnw(w) ∝ #(w)0.75. We adopt the same linear learning rate strategy where the initial learning rate of our models is 0.025. For unsupervised methods, we use support vector machines (SVM) 4 as the classifier.\n\n6.2 BASELINES\nWe adopted both unsupervised and supervised methods on text representation as baselines.\n\n6.2.1 UNSUPERVISED BASELINES\nBag-of-word-TFIDF and Bag-of-bigram-TFIDF. In the bag-of-word-TFIDF scheme (Salton & McGill, 1983) , each text is represented as the tf-idf value of chosen feature-words. The bag-of-\n1http://cogcomp.cs.illinois.edu/Data/QA/QC/ 2https://www.cs.cornell.edu/people/pabo/movie-review-data/ 3http://nlp.stanford.edu/sentiment/ 4http://www.csie.ntu.edu.tw/˜cjlin/libsvm/\nbigram-TFIDF model is constructed by selecting the most frequent unigrams and bigrams from the training subset. We use the vanilla TFIDF in the gensim library5.\nLSI (S. Deerwester & Harshman, 1990) and LDA (Blei & Jordan, 2003). LSI maps both texts and words to lower-dimensional representations in a so-called latent semantic space using SVD decomposition. In LDA, each word within a text is modeled as a finite mixture over an underlying set of topics. We use the vanilla LSI and LDA in the gensim library with topic number set as 100.\ncBow (Tomas Mikolov & Dean, 2013b). Continuous Bag-Of-Words model. We use average pooling as the global pooling mechanism to compose a sentence vector from a set of word vectors.\nPV (Quoc Le, 2014). Paragraph Vector is an unsupervised model to learn distributed representations of words and paragraphs.\nFastSent (Felix Hill, 2016). In FastSent, given a simple representation of some sentence in context, the model attempts to predict adjacent sentences.\nNote that unlike LDA and GPV, LSI, cBow, and FastSent cannot infer the representations of unseen texts. Therefore, these four models need to fold-in all the test data to learn representations together with training data, which makes it not efficient in practice.\n\n6.2.2 SUPERVISED BASELINES\nNBSVM and MNB (S. Wang, 2012). Naive Bayes SVM and Multinomial Naive Bayes with unigrams and bi-grams.\nDAN (Mohit Iyyer & III, 2015). Deep averaging network uses average word vectors as the input and applies multiple neural layers to learn text representation under supervision.\nCNN-multichannel (Kim, 2014). CNN-multichannel employs convolutional neural network for sentence modeling.\nDCNN (N. Kalchbrenner, 2014). DCNN uses a convolutional architecture that replaces wide convolutional layers with dynamic pooling layers.\nMV-RNN (Richard Socher & Ng, 2012). Matrix-Vector RNN represents every word and longer phrase in a parse tree as both a vector and a matrix.\nDRNN (Irsoy & Cardie, 2014). Deep Recursive Neural Networks is constructed by stacking multiple recursive layers.\nDependency Tree-LSTM (Kai Sheng Tai & Manning, 2015). The Dependency Tree-LSTM based on LSTM structure uses dependency parses of each sentence.\n\n6.3 PERFORMANCE OF GENERATIVE PARAGRAPH VECTOR\nWe first evaluate the GPV model by comparing with the unsupervised baselines on the TREC, Subj and MR datasets. As shown in table 1, GPV works better than PV over the three tasks. It demonstrates the benefits of introducing a prior distribution (i.e., regularization) over the paragraph vectors. Moreover, GPV can also outperform almost all the baselines on three tasks except Bow-TFIDF and Bigram-TFIDF on the TREC collection. The results show that for unsupervised text representation, bag-of-words representation is quite simple yet powerful which can beat many embedding models. Meanwhile, by using a complete generative process to infer the paragraph vectors, our model can achieve the state-of-the-art performance among the embedding based models.\n\n6.4 PERFORMANCE OF SUPERVISED GENERATIVE PARAGRAPH VECTOR\nWe compare SGPV model to supervised baselines on all the five classification tasks. Empirical results are shown in Table 2. We can see that SGPV achieves comparable performance against other deep learning models. Note that SGPV is much simpler than these deep models with significantly less parameters and no complex structures. Moreover, deep models with convolutional layers or recurrent structures can potentially capture compositional semantics (e.g., phrases), while SGPV only\n5http://radimrehurek.com/gensim/\nrelies on uni-gram. In this sense, SGPV is quite effective in learning text representation. Meanwhile, if we take Table 1 into consideration, it is not surprising to see that SGPV can consistently outperform GPV on all the three classification tasks. This also demonstrates that it is more effective to directly fit supervised representation models than to learn a general purpose representation in prediction scenarios.\nBy introducing bi-grams, SGPV-bigram can outperform all the other deep models on four tasks. In particular, the improvements of SGPV-bigram over other baselines are significant on SST-1 and SST-2. These results again demonstrated the effectiveness of our proposed SGPV model on text representations. It also shows the importance of word order information in modeling text semantics.\n\n7 CONCLUSIONS\nIn this paper, we introduce GPV and SGPV for learning distributed representations for pieces of texts. With a complete generative process, our models are able to infer vector representations as well as labels over unseen texts. Our models keep as simple as PV models, and thus can be efficiently learned over large scale text corpus. Even with such simple structures, both GPV and SGPV can produce state-of-the-art results as compared with existing baselines, especially those complex deep models. For future work, we may consider other probabilistic distributions for both paragraph vectors and word vectors.\n",
    "approval": false,
    "rationale": "This work reframes paragraph vectors from a generative point of view and in so doing, motivates the existing method of inferring paragraph vectors as well as applying a L2 regularizer on the paragraph embeddings. The work also motivates joint learning of a classifier on the paragraph vectors to perform text classification.\n\nThe paper has numerous citation issues both in formatting within the text and the formatting of the bibliography, e.g. on some occasions including first names, on others not. I suggest the authors use a software package like BibTex to have a more consistent bibliography. There seems to be little novelty in this work. \n\nThe authors claim that there is no proposed method for inferring unseen documents for paragraph vectors. This is untrue. In the original paragraph vector paper, the authors show that to get a new vector, the rest of the model parameters are held fixed and gradient descent is performed on the new paragraph vector. This means the original dataset is not needed when inferring a paragraph vector for new text. This work seems to be essentially doing the same thing when finding the MAP estimate for a new vector. Thus the only contribution from the generative paragraph vector framing is the regularization on the embedding matrix.\n\nThe supervised generative paragraph vector amounts to jointly training a linear classifier on the paragraph vectors, while inference for the paragraph vector is unchanged. For the n-gram based approach, the authors should cite Li et al., 2015.\n\nIn the experiments, table 1 and 2 are badly formatted with .0 being truncated. The authors also do not state the size of the paragraph vector. Finally the SGPV results are actually worse than that reported in the original paragraph vector paper where SST-1 got 48.7 and SST-2 got 86.3.\n\nBofang Li, Tao Liu, Xiaoyong Du, Deyuan Zhang, Zhe Zhao, Learning Document Embeddings by Predicting N-grams for Sentiment Classification of Long Movie Reviews, 2015.",
    "rating": 2,
    "type": "negative"
  },
  {
    "title": "MONTE CARLO DEEP NEURAL NETWORK ARITH-",
    "abstract": "Quantization is a crucial technique for achieving low-power, low latency and high throughput hardware implementations of Deep Neural Networks. Quantized floating point representations have received recent interest due to their hardware efficiency benefits and ability to represent a higher dynamic range than fixed point representations, leading to improvements in accuracy. We present a novel technique, Monte Carlo Deep Neural Network Arithmetic (MCDA), for determining the sensitivity of Deep Neural Networks to quantization in floating point arithmetic. We do this by applying Monte Carlo Arithmetic to the inference computation and analyzing the relative standard deviation of the neural network loss. The method makes no assumptions regarding the underlying parameter distributions. We evaluate our method on pre-trained image classification models on the CIFAR-10 and ImageNet datasets. For the same network topology and dataset, we demonstrate the ability to gain the equivalent of bits of precision by simply choosing weight parameter sets which demonstrate a lower loss of significance from the Monte Carlo trials. Additionally, we can apply MCDA to compare the sensitivity of different network topologies to quantization effects.1",
    "text": "1 INTRODUCTION\nDeep Neural Networks have achieved state-of-the-art performances in many machine learning tasks such as such as speech recognition (Collobert et al., 2011), machine translation (Bahdanau et al., 2014), object detection (Ren et al., 2015) and image classification (Krizhevsky et al., 2012). However, excellent performance comes at the cost of significantly high computational and memory complexity, typically requiring teraops of computation during inference and Gigabytes of storage. To overcome these complexities, compression methods have been utilized, aiming to exploit the inherent resilience of DNNs to noise. These engender representations which maintain algorithm performance but significantly improve latency, throughput and power consumption of hardware implementations. In particular, exploiting reduced numerical precision for data representations through quantization has been emphatically promising, whereby on customizable hardware, efficiency scales quadratically with each bit of precision.\nQuantization of fixed-point arithmetic (Q-FX) for DNN inference has been extensively studied, and more recently there has been increasing interest in quantized floating point (Q-FP) arithmetic for both DNN inference and training (Wang et al., 2018). Q-FP has the advantage of higher dynamic range compared to equivalent Q-FX representations and reduced hardware cost over single-precision floating point (FP). This has influenced application specific integrated circuits (ASICs) such as Google’s tensor processing unit (TPU), which supports 16-bit floating point (16-FP) and soft processors such as Microsoft’s Project Brainwave which utilizes 8-FP.\nTo illustrate these hardware benefits, we synthesized arithmetic logic units (ALUs) in different formats and different precision on an FPGA and present performance estimates in operations per second (OPs) and area estimates in Look-up Tables (LUTs) per operation (LUTs/Op) in Figure 1. As shown, 8-bit fixed point (8-FX) achieves improved performance and area over 8-FP. However, 7-FP is a significant improvement over 8 or 12-FX and 8 or 9-FP. These examples demonstrate substantial performance and area benefits from reducing FP precision by only 1 to 2-bits. Thus, if we\n1Source code will be available if the paper is accepted\ncan design networks which not only achieve high accuracy but are robust to quantization, higher performing hardware solutions are possible.\nSince in Q-FP, we are trying to represent the infinite set of real numbers using a finite number of bits, quantization and rounding artefacts will be introduced, with inaccuracies being cascaded along the computation graph (IEEE, 1985; Goldberg, 1991; Higham, 2002). This paper proposes Monte Carlo Deep Neural Network Arithmetic (MCDA), a novel way to apply Monte Carlo Arithmetic (MCA) (Parker et al., 2000) for determining the sensitivity of Deep Neural Networks to Q-FP representations. It allows hardware-software designers to quantify the impact of quantization, enabling more efficient systems to be discovered. We do this by exploiting Monte Carlo simulations under which rounding effects are randomized. This, in turn, allows one to infer the sensitivity of executing a computation graph to quantization effects.\nOur MCDA technique is highly sensitive, allowing very small differences in quantization behaviour to be detected. The technique makes no assumptions regarding data distributions and directly measures the effects of quantization on the problem under study. This allows us to provide insights into the precision requirements of any inference network for any given dataset. Additionally we can use the technique to select weight parameters which are more robust to floating point rounding. The theoretical and practical contributions of this work can be summarized as follows:\n• We introduce a novel and rigorous analysis technique, Monte Carlo Deep Neural Network Arithmetic (MCDA), which measures the sensitivity of Deep Neural Network inference computation to floating point rounding error.\n• When applied to neural network inference, we show MCDA can determine the precision requirements of different networks, rank them, and detect small differences between different neural network topologies and weight sets.\n• We demonstrate that while a network with the same topology but different weights may have the same loss and validation accuracy, their sensitivity to quantization can be vastly different. Using the CIFAR-10 and ImageNet datasets, we introduce a method to choose weights which are more robust to rounding error, resulting in a greatly improved accuracyarea tradeoff over state-of-the-art methods.\nIt is worth noting that although we consider convolutional neural networks for image classification, this method could be applied for any neural network model architectures and applications. Moreover, while the experiments in this paper are limited to inference, it may be possible to apply the same idea to analyze training algorithms.\n\n2 RELATED WORK\nLow-precision representations of deep learning have been extensively studied. Many training methods have been developed to design representations for fixed point inference (Jacob et al., 2018; Faraone et al., 2018; Zhou et al., 2016) and training (Wu et al., 2018b; Yang et al., 2019; Gupta et al., 2015; Sakr & Shanbhag, 2019). Other methods have also utilized Q-FP arithmetic for inference and training whilst maintaining single-precision accuracy. (Micikevicius et al., 2018) implemented 16- FP arithmetic training whilst storing a 32-FP master copy for the weight updates. Additionally,\n(Wang et al., 2018; Mellempudi et al., 2019) with 8-FP arithmetic whilst using a 16-FP copy of the weights and 16/32-bits for the accumulator. Techniques for determining per-layer sensitivity to quantization have also been studied (Choi et al., 2016; Sakr & Shanbhag, 2018). Further, other studies have successfully determined the minimum fixed point precision requirements for a given DNN accuracy threshold (Sakr et al., 2017). The accuracy and stability of various numerical algorithms in finite precision arithmetic has been studied in (Higham, 2002; Wilkinson, 1994). This has led to techniques for tracking information lost from finite precision arithmetic using random perturbation such as Monte Carlo Arithmetic (Parker et al., 2000; Frechtling & Leong, 2015). Monte Carlo methods have also been used in Bayesian Neural Networks (Buchholz et al., 2018; Blier & Ollivier, 2018). In particular, (Achterhold et al., 2018) introduced a quantizing prior to learn weights which are either close to a quantized representation or have high variance. (Louizos et al., 2017) used hierarchical priors to prune nodes and posterior uncertainties to determine the optimal fixed point precision. (Blundell et al., 2015) use a Monte Carlo approach to learn a probability distribution on the weights of a neural network. To the best of our knowledge, our work is the first to present a technique for directly determining the sensitivity of DNNs to floating point rounding and to explicitly compute precision bounds of a trained network. These ideas can be very usefully applied to extending the limits of low-precision representations in deep learning applications.\n\n3 BACKGROUND\nIn this section, we describe background theory upon which our technique for determining the sensitivity of DNNs to floating point rounding is based.\n\n3.1 FLOATING POINT ARITHMETIC\nThe IEEE-754 binary floating point format (IEEE, 1985) represents most real numbers x by a subset in normal form as: x̂ = (−1)sx(1 +mx)2ex (1) where sx ∈ {0, 1} is the sign bit, ex is an integer representing the exponent of x̂ and mx is the mantissa of x̂. Such number formats can be described as a (sx, ex,mx) tuple. In binary form the representation is (bs, be1, b e 2, ..., b e Bex , bm1 , b m 2 ..., b m Bmx\n) ∈ {0, 1}B , with Bex and Bmx being the number of exponent and mantissa bits, respectively. The infinite set of real numbers R is represented in a computer withB = 1+Bex+Bmx bits, and we define the finite set of real numbers representable in floating point format as exact values, F ⊂ R. Real numbers which aren’t representable are rounded to their nearest exact value. We call this set of numbers inexact values, I, where I∪F = R. The approximation x̂ = F(x) = x(1 + δ), given x ∈ I, introduces rounding error into the computation. The value of δ = ∥∥x−x̂ x\n∥∥, represents the relative error which is a function of the machine hardware precision, p, as δ ≤ , where = 2−p (IEEE, 1985; Goldberg, 1991; Higham, 2002). In general, inexactness can be caused by finite representations or errors propagating from earlier parts of the computation. Often the primary cause of error in floating point arithmetic is catastrophic cancellation which causes numerical inaccuracy. Catastrophic cancellation occurs when for example, two near equal FP numbers, sharing k significant digits, are subtracted from one another as shown in (2) (Higham, 2002).\n0. f1 f2 ... fk f(k+1) ... ft\n− 0. f1 f2 ... fk g(k+1) ... gt = 0. 0 0 .... 0 h(k+1) ... ht\n(2) 0. f1 f2 ... fk f(k+1) ... ft r(t+1) ... rp\n− 0. f1 f2 ... fk g(k+1) ... gt r̂(t+1) ... r̂p = 0. 0 0 .... 0 h(k+1) ... ht i(t+1) ... ip (3)\nIn normalized form, the leading zeros are removed by shifting the result to the left and adjusting the exponent accordingly. The result is 0.hk+1...hti1...ik which has only (t − k) accurate digits and digits i which are unknown. Additionally, the remaining accurate digits h are most likely affected by rounding error in previous computations. This can significantly magnify errors, especially in computing large computational graphs such as that of state-of-the-art DNNs.\nIf either operand in (2) is inexact, then the digits h are no more significant than any other sequence of digits. Yet, FP arithmetic has no mechanism of recording this loss of significance. By padding both our operands with random digits r and r̂ in (3), the resulting digits i are randomized. If k\ndigits are lost in the result, then k random digits will be in the normalized result and when computed over many random trials, the results will disagree on the trailing k digits. In this case, we are able to detect catastrophic cancellation because the randomization over many trials provides a statistical simulation of round-off errors. We can use techniques from numerical analysis such as Monte Carlo methods to appropriately insert precision-dependant randomization in this way.\n\n3.2 MONTE CARLO ARITHMETIC\nMonte Carlo methods can be used to analyze rounding by representing inexact values as random variables (Parker et al., 2000; Frechtling & Leong, 2015). The real value x, as represented in (1), can be modelled to t digits, using:\ninexact(x̂, t, δ) = x̂+ 2ex−tδ = (−1)sx(1 +mx + 2−tδ)2ex (4)\nwhere δ ∈ U(− 12 , 1 2 ) is a uniformly distributed random variable and t is a positive integer representing the virtual precision of concern. For the same input x̂ in (4), we can run many Monte Carlo trials which will yield different values on each trial, where 0 < t < p so that the MCA can be run accurately on a computer with machine precision p. The ability to vary t is useful because it allows us to then evaluate the hardware precision requirements of a given system or computational graph for a given DNN.\nMCA is a method to model the effect of rounding on a computational graph by randomizing all arithmetic operations. The randomization is applied for both generating inexact operands and also in rounding. In each operation using MCA, ideally both catastrophic cancellation and rounding error can be detected. An operation using MCA is defined as:\nx ◦ y = round(inexact(inexact(x) ◦ inexact(y))) (5)\nwhere ◦ ∈ ( +,−,×,÷). By applying the inexact function to both operators we make it possible to detect catastrophic cancellation. Furthermore, applying the inexact function to the operation output and then rounding this value implements random rounding and hence is used to detect rounding error (Parker & Langley, 1997). Hence, for the same input into the system, each trial will yield different operands and output.\nAfter modifying the inexact and rounding operations as described, we use random sampling to simulate Monte Carlo trials. For each trial, we collect data on the resulting output of the system and compute summary statistics to quantify its behaviour (Parker et al., 2000). With sufficiently large number of Monte Carlo trials and virtual precision t, the expected value of the output from these trials will equal the value from using real arithmetic. As explained in the next section, we can determine the total number of digits lost to rounding error and the minimum precision required to avoid a total loss of significance.\n\n3.3 ANALYSIS\nThe relative error is bounded by δ ≤ 2−p from the design of IEEE FP arithmetic (Wilkinson, 1994; Goldberg, 1991). With this inequality, we can determine the expected number of significant binary digits available from a p-digit FP system as p ≤ −log2(δ). These definitions can be adapted for MCA by replacing the precision of the FP system, p, by the virtual precision, t, of an MCA operation. Thus, the relative error of an MCA operation, for virtual precision t, is δ ≤ 2−t and the expected number of significant binary digits in a t-digit MCA operation is at most t. Using this definition and the proof provided in (Parker & Langley, 1997), the total significant binary digits in a set of M trials is s′ = log2 µσ where µ is the mean and σ the standard deviation. The output of the system should be some scalar value so that we can perform such analysis. For experimentation, M trials are run for all of t ∈ {1, 2, 3, ..., tmax}. The total number of base-2 significant digits lost in a set of M trials is Kt, in (6):\nKt = t− s′ = t− log2( µ\nσ ) = log2 Θ + t (6)\nwhere Θ = σµ , for (µ 6= 0), is the relative standard deviation (RSD) of the MCA results. The virtual precision t controls the perturbation strength applied by the inexact function. For a given Kt, as we reduce t, the RSD should increase according to equation 6. At some point, an unexpected loss of\nsignificance (Frechtling & Leong, 2015) is encountered due to the nonlinear effects of quantization. The value at which this occurs is defined as tmin. The number of significant digits lost for the system being analyzed is then computed by averaging all Kt whereby t > tmin as shown in (7):\nK =\n{ 1 tmax−tmin ∑tmax t=tmin Kt where tmax > 1\nKt where tmax = 1 (7)\nFor DNN inference, we propose to use K as a sensitivity measure for the network to FP rounding. The method for implementing this is discussed in the next section.\n\n4 MONTE CARLO DEEP NEURAL NETWORK ARITHMETIC\nWe now describe MCDA, a methodology for applying MCA techniques to DNN computation, allowing us to understand the sensitivity of a given network and its weight representation to FP rounding.\n\n4.1 NETWORK MODEL\nWe consider a generalized non-linear L-layer neural network with an output vector yL, input data vector x and learnable weight parameter tensor w = ⋃ wl (l = 1, . . . , L), whereby yL = f(x;w). To compute y, several layers consisting of general matrix multiplication (GEMM) operations (such as convolutional and fully-connected layers) between the layer input xl and weight parameters wl, followed by a non-linear activation function, h, producing intermediate layer outputs xl, i.e. yl = h(xl ⊗wl). The output of a given layer becomes the input to the subsequent layer, i.e. xl+1 = yl, with x1 = x. A loss function is the objective function to minimize updating w via an optimizer such as stochastic gradient descent. For a given set of input data X , the total network loss during inference is calculated by applying a loss function loss(f(x;w), ŷ(x)) where ŷ(x) is the target ground truth output for x. The total loss for X is then a scalar output, such that:\nL(X;w) = 1 |X| ∑ x∈X loss(f(x;w), ŷ(x)) (8)\nUpdates of w are usually done in small batches over subsets of X .\nNaively applying MCA to each operation (fine-grained MCA) as described in equation (5) poses significant computational difficulties for DNN models. We observe two primary issues with employing fine-grained MCA to a DNN computational graph:\n• Firstly, the number of required trials for Monte Carlo experiments to generate robust results can typically be in the hundreds or thousands. As DNN inference of state-of-the-art networks typically consist of billions of operations, the computational requirements of applying MCA after each operation will be very large, making the technique impractical. • Using the accuracy as the system output for MCA experiments is problematic because it is\na discrete value. For high values of t, Monte Carlo results across different t then become indistinguishable and the standard deviation for a given t is potentially zero.\n\n4.2 MONTE CARLO NETWORK INFERENCE\nTo reduce the computational cost of Monte Carlo experiments, we employ MCDA, which is a coarsegrained approach to MCA for GEMM operations. Conveniently, these can be naturally implemented in modern machine learning frameworks such as PyTorch. Furthermore, to ensure the system output is a continuous value, the loss function output is used, rather than the accuracy. In this case, small perturbations in layer operands are more likely to produce observable changes in the output.\nMCDA applies a vector version of (5) to the DNN inference computational problem in (8). Our operands in this case are vectors and ◦ represents a neural network layer operation. For example, the output from performing a GEMM operation can thus be represented by:\nyl = round(inexact(inexact(xl)⊗ inexact(wl))) (9) Since the inexact function is applied to the inputs and outputs of a GEMM operation, an optimized implementation can be used. This is in contrast to full MCA which requires the application of (5)\nto every individual scalar operation. The vector form in (9) is applied to each edge of neural network computational graph where multiply (division) and/or add (subtract) operations are performed. Hence, it is not applied to operations such as MaxPool and ReLu. As an example, in Figure 2 we show where the inexact function is applied for a residual block with folded batch normalization, which is a repeating sequence of layers found in ResNet models (He et al., 2015). At the final output of the network, the loss is computed with (8) and the inexact function is applied to the outputs y and also the loss output scalar value L. From analyzing the behavior of the loss, we infer the sensitivity of the accuracy of the system to FP rounding. By using MCDA for the GEMM operations, we will not be able to detect all instances of catastrophic cancellation. However, we significantly reduce execution time over fine-grained MCA and show in the next section that we can still retrieve valuable information about our system. In fact, for one trial with one batch of 32 images on ImageNet running on a Nvidia Titan Xp GPU, the speed up of regular inference without MCDA is only 1.05× (for a single Monte Carlo trial). We also note that fine-grained MCA could be applied with a simple modification and would be possible given appropriate customized hardware support for parallel Monte Carlo computations (Yeung et al., 2011).\n\n5 EXPERIMENTAL RESULTS\nIn this section, we present experimental results for applying MCDA to exemplary convolutional neural networks. We use the CIFAR-10 and ImageNet image classification datasets to compare MobileNet-v2 (Sandler et al., 2018), EfficientNet (Tan & Le, 2019), AlexNet, ResNet (He et al., 2015), SqueezeNet (Iandola et al., 2016) and MnasNet (Tan et al., 2018). For CIFAR-10, we use a batch size of 128, whilst we use a batch size of 32 for ImageNet experiments. Cross-entropy is used as the loss function for both datasets. For a given network, dataset, weight representation and t, we apply MCDA with M = 1000 trials. The resulting loss from each trial is computed with the same single batch of images and hence additional data is not required. We compute Θt from our results for all t ∈ {1, 2, 3, ..., 16}. Following this, we run linear regression analysis using the MCALIB2 (Frechtling & Leong, 2015) R library, on our Θt values, to determine tmin and K. tmin is defined as the point of lowest t where the difference between the regression line and the equivalent Θt is less than half a binary digit, i.e. log10(2\n0.5). Further detail on the calculation of K and tmin from MCALIB can be found in Appendix A.1. We report Q-FP validation accuracy using the quantization function from (Wang et al., 2018) with stochastic rounding3 (See Appendix A.2).\n\n5.1 DISTINGUISHING WEIGHT PARAMETER REPRESENTATIONS\nAs discussed in Section 3.1, the inexactness in FP arithmetic largely depends on the numerical value of operands. Two instances of the same network and dataset, with the same validation accuracy, but vastly different weight representations, will likely produce differing sensitivities to FP rounding. We first train 8 instances of EfficientNet-b0 and MobileNet-V2 on CIFAR-10 from scratch with random initialization from (Glorot & Bengio, 2010), all achieving within 1% validation accuracy of one another. Using MCDA, we calculate the K values for each model (See Appendix A.3).\n2https://github.com/mfrechtling/mcalib 3https://github.com/Tiiiger/QPyTorch\nWe then test their percentage validation accuracy decrease from using post-training quantization (i.e. no finetuning) with varying Q-FP precisions. In Figure 3, we see that the models with higher K values typically experience a larger drop in Q-FP accuracy, indicating they are more sensitive to floating point rounding error. Notably, the model with lowest K for 7-bit MobileNet-v2 experiences a lower percentage validation accuracy drop than three of the 8-bit models. In this case, MCDA model selection enables the saving of a bit of precision while achieving smaller accuracy decrease than some of the trained 8-FP models.\n\n5.2 COMPARISON TO PREVIOUS WORK\nOne practical use case from the insights gained by MCDA is model selection for quantization. Typically when quantizing a given model trained on a given dataset, the model with highest validation accuracy is chosen and the sensitivity to quantization is assumed to be the same across models. As discussed, for Q-FP representations this is not necessarily the case. We can use K from MCDA to predict which models will be more robust to quantization. To demonstrate this, in Table 1 we compare post-training quantization results for model selection based on K from MCDA, against a baseline model chosen based on the highest single-precision validation accuracy. Evidently, even though the single-precision accuracy is initially as much as 0.9% higher, after quantizing the network to 8-5 bits, the accuracy of the network chosen by smallest K is always significantly higher.\n\n5.3 NETWORK COMPARISON\nModern DNNs consist of convolutional blocks with highly varying computational graphs (Wu et al., 2018a; Howard et al., 2017). Using MCDA we can also compute and compare their sensitivites to floating point rounding error to determine which networks will be robust to Q-FP representations.\nIn Figure 4 we show the Θt of pre-trained models, trained on the ImageNet dataset from PyTorch4 5, for differing values of t and run linear regression analysis over our data points. From here we can then assign aK value to each network and compare their loss of significance. At each t, the distance from the regression lines to the ideal line represents the values of Kt, as described in (6). From the MCDA results, AlexNet is the least sensitive to rounding and ResNet-50 is the most, with various models in between these two. Additionally we compare EfficientNet at two different model scales and evidently the larger model has much larger sensitivity. We then also compare the validation accuracy percentage decrease of all models at 10, 9 and 8-FP post-training in Figure 5. At 8-FP, besides MnasNet which experiences a large accuracy drop, K is able to predict validation accuracy degradation. Thus, MCDA provides very valuable information about Q-FP model design.\n\n6 CONCLUSION\nWe present a novel, highly sensitive, technique to quantify rounding error in DNNs. This is the first method to successfully compare the sensitivity of networks to floating point rounding error. Ultimately, this technique provides a tool for enabling the design of networks which perform higher when quantized. We do this by applying concepts from Monte Carlo Arithmetic theory to DNN computation. Furthermore, we show that by calculating the loss of significance metric K from MCDA, on the CIFAR-10 and ImageNet datasets, we can compare network sensitivities to floating point rounding error and gain valuable insights to potentially design better neural networks. This is an important contribution due to the increasing interest in low-precision floating point arithmetic for efficient DNN hardware systems. The theoretical and practical contributions of this paper will likely translate well to analyzing floating point rounding in backpropagation in future work.\n4https://github.com/pytorch/vision/tree/master/torchvision 5https://github.com/rwightman/pytorch-image-models\n",
    "approval": false,
    "rationale": "Summary:\n\nThe paper studies the sensitivity of a neural network with respect to quantizing its weights and activations. The idea is to use Monte Carlo Arithmetic (MCA) in order to calculate the number of significant bits in the training loss (e.g. cross entropy) that are lost due to floating-point arithmetic. The results show that the number of significant bits lost correlates with the reduction in classification accuracy when quantizing the weights and activations of the neural network.\n\nDecision:\n\nOverall, this is an interesting paper with interesting results. However, I think there is considerable room for improvement, and that more details are needed in order to assess the significance of the results, as I detail in the rest of my review. For these reasons, I recommend weak reject for now, but I encourage the authors to continue working on improving the paper and to provide more details in the updated version.\n\nContribution:\n\nThe paper considers an important problem, that of quantizing the weights and activations of a neural network in order to reduce computational and memory cost, while maintaining the machine-learning performance as high as possible. \n\nIn my opinion, the main contribution of the paper is the experimental findings, and in particular that the sensitivity of the training loss with respect to the precision of the weights and activations correlates with the accuracy of the network. It seems to me that these results may relate to work on Bayesian neural networks, sharp vs flat minima, and minimum-description length approaches to variational inference. Work on these areas has also shown that sensitivity of the training loss with respect to the precision of the weights (which intuitively happens when the network is at a \"sharp\" local minimum vs a \"flat\" one) is related to poor generalization performance, and vice versa. I would encourage the authors to explore the potential relationship of their work with these areas, and possibly discuss them in an updated version of the paper.\n\nOriginality:\n\nThe paper describes a method for assessing the sensitivity of a neural network with respect to the precision of the weights and activations. The method is a straightforward application of Monte Carlo Arithmetic (MCA) to neural networks. I believe that the application of MCA to neural networks for this particular purpose is novel, and that the results are original. However, the introduction of the paper gives the impression that the proposed method is brand new, and even uses the acronym MCA to refer to the proposed method, which can be confusing to readers. I would suggest to the authors to rewrite the introduction so as to reflect more accurately that the contribution is not a brand-new method, but rather the application of an existing method (MCA) in a novel way.\n\nWriting quality:\n\nThe paper is generally easy to read, but there is considerable room for improvement. There are mistakes, and often the writing is sloppy and imprecise. I give some more specific suggestions on what to improve later on.\n\nTechnical quality:\n\nThe method is well motivated and the experiments seem reasonable. However, there is very little detail on the experiments, which makes it hard to assess their correctness/significance. I would suggest to the authors to rewrite the experimental section with full detail, or put more details in an appendix. In particular:\n- Is each Monte Carlo trial done on the same batch of training images or a different one? If different, how are the trials averaged, and does that mean that the standard deviation over trials also includes a contribution due to different batches?\n- In sections 5.1 and 5.2, how were the results for different t combined/aggregated? Did you use linear-regression analysis as in section 5.3?\n- When you say \"accuracy\", do you mean accuracy on the training set, validation set, or test set? This is particularly important for assessing the significance of the results, and is something that is currently missing from the description of the experiments.\n- How was the quantization of the neural networks performed? It would be good to explain this at least on a high level, in addition to citing Wang et al., (2018).\n- In section 5.2, how was the model selection for each method performed exactly? In the baseline method, was the model to be quantized selected based on validation performance before quantization or after quantization?\n\nSpecific suggestions for improvement:\n\nThe citation format, i.e. (Smith et al., (2019)), is unusual and uses unnecessarily many parentheses. Use \\citep for (Smith et al., 2019), and \\citet for Smith et al. (2019).\n\nThe illustration of fig. 1 is not fully convincing as a motivation for floating-point arithmetic. Even though it makes the case that Float(7, 7) is more efficient than Float(8, 8) and Float (9, 9), the comparison between Float(7, 7) and Fixed(12, 12) is hard to interpret, as we can't conclude whether the efficiency gain is due to reducing the number of bits or to switching from fixed-point to floating-point arithmetic. A more convincing illustration would compare fixed-point with floating-point arithmetic using the same number of bits.\n\nIt would be better if fig. 1 were 2D, as 3D doesn't add anything but makes it harder to compare sizes visually.\n\nPlease avoid exaggerations, such as \"exquisitely sensitive\" or \"extremely sensitive\", when \"sensitive\" would suffice.\n\nSection 2 is grammatically sloppy:\n- arihtmetic --> arithmetic\n- Last line of page 2 seems to be missing a verb.\n- this has lead --> this has led\n\nThe related-work section is too short and in many cases it doesn't explain what previous work has actually done. For example, \"rounding of inexact values to their nearest FP approximation has been studied in several publications\" is vague: what exactly these publication have done? This lack of detail makes it hard to assess the originality of the current paper, and how it differs from existing work.\n\nSection 3 is often unclear with imprecise mathematical notation:\n- \"e is the base-2 exponent of x in binary floating point arithmetic\": surely, the exponent is represented as an integer?\n- (bs, be1, be2, ..., bex, bm1, bm2, ..., bmx) is sloppy, as it indicates that the indices run from 1 to x.\n- Bx = sx + ex + mx  is also sloppy; what is meant here is the number of bits to represent sx, ex, mx and not the values themselves.\n- F(x) = x(1 + θ), shouldn't θ be δ?\n- \"which is typically the cause of horrific numerical inaccuracy from numerical analysis literature\", the phrase \"from numerical analysis literature\" doesn't make much sense here.\n- In eq. (4), substituting the expression for x from eq. (1) doesn't yield the same result.\n- \"The number of trials is an important consideration because [...] it can produce adverse effects on results\". What is meant by \"adverse effects\"? Do you mean that with few trials Monte Carlo doesn't give accurate results? Please be more specific.\n- \"we can determine the expected number of significant binary digits available from a p-digit FP system as p ≥ −log2(δ)\". I'm unable to follow this statement, please explain further. Also, from applying logs to δ ≤ 2^{−p} one gets an inequality that doesn't match the one in p ≥ −log2(δ).\n- \"The relative error of an MCA operation is, for virtual precision t, is δ ≤ 2^-t\", \"is\" is used twice here.\n- \"the expected number of significant binary digits in a t-digit MCA operations is at least t\", operations --> operation. Also, shouldn't it be at most t, otherwise K becomes negative?\n-  is discussed in the section --> is discussed in the next section.\n\nSome mistakes in section 4.1:\n- y = (x; w) --> y = f(x; w)\n- Eq. (8) is sloppy, it uses X for both the set and its size. Use |X| or something similar for the size.\n- In the caption of fig. 2, baes --> base\n\nI'm not convinced by the second bullet point in section 4.1, that the averaging over many images used to obtain the accuracy is the reason why MCA doesn't work well. Surely, the training loss (cross entropy) is also an average over many images? To me it would seem more plausible that the main reason MCA works with training loss but not accuracy is because accuracy is discrete, whereas training loss is continuous.\n\nFig. 3 would be much easier to read if the axes were labelled, and if the axes had the same range (so that different plots can be compared visually).\n\nFig. 5 would be easier to read if the networks were sorted with respect to K.\n\nIn section 5, CIFAR-10 is sometimes written as CIFAR10.\n\nAppendix A is empty, so it should be removed.\n",
    "rating": 3,
    "type": "negative"
  }
]
