[
  {
    "title": "REGULARIZING CNNS WITH LOCALLY CONSTRAINED DECORRELATIONS",
    "abstract": "Regularization is key for deep learning since it allows training more complex models while keeping lower levels of overfitting. However, the most prevalent regularizations do not leverage all the capacity of the models since they rely on reducing the effective number of parameters. Feature decorrelation is an alternative for using the full capacity of the models but the overfitting reduction margins are too narrow given the overhead it introduces. In this paper, we show that regularizing negatively correlated features is an obstacle for effective decorrelation and present OrthoReg, a novel regularization technique that locally enforces feature orthogonality. As a result, imposing locality constraints in feature decorrelation removes interferences between negatively correlated feature weights, allowing the regularizer to reach higher decorrelation bounds, and reducing the overfitting more effectively. In particular, we show that the models regularized with OrthoReg have higher accuracy bounds even when batch normalization and dropout are present. Moreover, since our regularization is directly performed on the weights, it is especially suitable for fully convolutional neural networks, where the weight space is constant compared to the feature map space. As a result, we are able to reduce the overfitting of state-of-the-art CNNs on CIFAR-10, CIFAR-100, and SVHN.",
    "text": "1 INTRODUCTION\nNeural networks perform really well in numerous tasks even when initialized randomly and trained with Stochastic Gradient Descent (SGD) (see Krizhevsky et al. (2012)). Deeper models, like Googlenet (Szegedy et al. (2015)) and Deep Residual Networks (Szegedy et al. (2015); He et al. (2015a)) are released each year, providing impressive results and even surpassing human performances in well-known datasets such as the Imagenet (Russakovsky et al. (2015)). This would not have been possible without the help of regularization and initialization techniques which solve the overfitting and convergence problems that are usually caused by data scarcity and the growth of the architectures.\nFrom the literature, two different regularization strategies can be defined. The first ones consist in reducing the complexity of the model by (i) reducing the effective number of parameters with weight decay (Nowlan & Hinton (1992)), and (ii) randomly dropping activations with Dropout (Srivastava et al. (2014)) or dropping weights with DropConnect (Wan et al. (2013)) so as to prevent feature co-adaptation. Due to their nature, although this set of strategies have proved to be very effective, they do not leverage all the capacity of the models they regularize.\nThe second group of regularizations is those which improve the effectiveness and generality of the trained model without reducing its capacity. In this second group, the most relevant approaches decorrelate the weights or feature maps, e.g. Bengio & Bergstra (2009) introduced a new criterion so as to learn slow decorrelated features while pre-training models. In the same line Bao et al. (2013) presented ”incoherent training”, a regularizer for reducing the decorrelation of the network activations or feature maps in the context of speech recognition. Although regularizations in the second group are promising and have already been used to reduce the overfitting in different tasks, even with the presence of Dropout (as shown by Cogswell et al. (2016)), they are seldom used in the large scale image recognition domain because of the small improvement margins they provide together with the computational overhead they introduce.\nAlthough they are not directly presented as regularizers, there are other strategies to reduce the overfitting such as Batch Normalization (Ioffe & Szegedy (2015)), which decreases the overfitting by reducing the internal covariance shift. In the same line, initialization strategies such as ”Xavier” (Glorot & Bengio (2010)) or ”He” (He et al. (2015b)), also keep the same variance at both input and output of the layers in order to preserve propagated signals in deep neural networks. Orthogonal initialization techniques are another family which set the weights in a decorrelated initial state so as to condition the network training to converge into better representations. For instance, Mishkin & Matas (2016) propose to initialize the network with decorrelated features using orthonormal initialization (Saxe et al. (2013)) while normalizing the variance of the outputs as well.\nIn this work we hypothesize that regularizing negatively correlated features is an obstacle for achieving better results and we introduce OrhoReg, a novel regularization technique that addresses the performance margin issue by only regularizing positively correlated feature weights. Moreover, OrthoReg is computationally efficient since it only regularizes the feature weights, which makes it very suitable for the latest CNN models. We verify our hypothesis through a series of experiments: first using MNIST as a proof of concept, secondly we regularize wide residual networks on CIFAR-10, CIFAR-100, and SVHN (Netzer et al. (2011)) achieving the lowest error rates in the dataset to the best of our knowledge.\n\n2 DEALING WITH WEIGHT REDUNDANCIES\nDeep Neural Networks (DNN) are very expressive models which can usually have millions of parameters. However, with limited data, they tend to overfit. There is an abundant number of techniques in order to deal with this problem, from L1 and L2 regularizations (Nowlan & Hinton (1992)), early-stopping, Dropout or DropConnect. Models presenting high levels of overfitting usually have a lot of redundancy in their feature weights, capturing similar patterns with slight differences which usually correspond to noise in the training data. A particular case where this is evident is in AlexNet (Krizhevsky et al. (2012)), which presents very similar convolution filters and even ”dead” ones, as it was remarked by Zeiler & Fergus (2014).\nIn fact, given a set of parameters θI,j connecting a set of inputs I = {i1, i2, . . . , in} to a neuron hj , two neurons {hj , hk}, j 6= k will be positively correlated, and thus fire always together if θI,j = θI,k and negatively correlated if θI,j = −θI,k. In other words, two neurons with the same or slightly different weights will produce very similar outputs. In order to reduce the redundancy present in the network parameters, one should maximize the amount of information encoded by each neuron. From an information theory point of view, this means one should not be able to predict the output of a neuron given the output of the rest of the neurons of the layer. However, this measure requires batch statistics, huge joint probability tables, and it would have a high computational cost.\nIn this paper, we will focus on the weights correlation rather than activation independence since it still is an open problem in many neural network models and it can be addressed without introducing too much overhead, see Table 1. Then, we show that models generalize better when different feature detectors are enforced to be dissimilar. Although it might seem contradictory, CNNs can benefit from having repeated filter weights with different biases, as shown by Li et al. (2016). However, those repeated filters must be shared copies and adding too many unshared filter weights to CNNs increases overfitting and the need for stronger regularization (Zagoruyko & Komodakis (May 2016)). Thus, our proposed method and multi-bias neural networks are complementary since they jointly increase the representation power of the network with fewer parameters.\nIn order to find a good target to optimize so as to reduce the correlation between weights, it is first required to find a metric to measure it. In this paper, we propose to use the cosine similarity between feature detectors to express how strong is their relationship. Note that the cosine similarity is equivalent to the Pearson correlation for mean-centered normalized vectors, but we will use the term correlation for the sake of clarity.\n\n2.1 ORTHOGONAL WEIGHT REGULARIZATION\nThis section introduces the orthogonal weight regularization, a regularization technique that aims to reduce feature detector correlation enforcing local orthogonality between all pairs of weight vectors. In order to keep the magnitudes of the detectors unaffected, we have chosen the cosine similarity between the vector pairs in order to solely focus on the vectors angle β ∈ [−π, π]. Then, given any pair of feature vectors of the same size θ1, θ2 the cosine of their relative angle is:\ncos(θ1, θ2) = 〈θ1, θ2〉 ||θ1||||θ2||\n(1)\nWhere 〈θ1, θ2〉 denotes the inner product between θ1 and θ2. We then square the cosine similarity in order to define a regularization cost function for steepest descent that has its local minima when vectors are orthogonal:\nC(θ) = 1\n2 n∑ i=1 n∑ j=1,j 6=i cos2(θi, θj) = 1 2 n∑ i=1 n∑ j=1,j 6=i ( 〈θi, θj〉 ||θi||||θj || )2 (2)\nWhere θi are the weights connecting the output of the layer l − 1 to the neuron i of the layer l, which has n hidden units. Interestingly, minimizing this cost function relates to the minimization of the Frobenius norm of the cross-covariance matrix without the diagonal. This cost will be added to the global cost of the model J(θ;X, y), where X are the inputs and y are the labels or targets, obtaining J̃(θ;X, y) = J(θ;X, y) + γC(θ). Note that γ is an hyperparameter that weights the relative contribution of the regularization term. We can now define the gradient with respect to the parameters:\nδ\nδθ(i,j) C(θ) = n∑ k=1,k 6=i θ(k,j)〈θi, θk〉 〈θi, θi〉〈θk, θk〉 − θ(i,j)〈θi, θk〉2 〈θi, θi〉2〈θk, θk〉 (3)\nThe second term is introduced by the magnitude normalization. As magnitudes are not relevant for the vector angle problem, this equation can be simplified just by assuming normalized feature detectors:\nδ\nδθ(i,j) C(θ) = n∑ k=1,k 6=i θ(k,j)〈θi, θk〉 (4)\nWe then add eq. 4 to the backpropagation gradient:\n∆θ(i,j) = −α ( ∇Jθ(i,j) + γ n∑ k=1,k 6=i θ(k,j)〈θi, θk〉 )\n(5)\nWhere α is the global learning rate coefficient, J any target loss function for the backpropagation algorithm.\nAlthough this update can be done sequentially for each feature-detector pair, it can be vectorized to speedup computations. Let Θ be a matrix where each row is a feature detector θ(I,j) corresponding to the normalized weights connecting the whole input I of the layer to the neuron j. Then, ΘΘt contains the inner product of each pair of vectors i and j in each position i, j. Subsequently, we\nAlgorithm 1 Orthogonal Regularization Step.\nRequire: Layer parameter matrices Θl, regularization coefficient γ, global learning rate α. 1: for each layer l to regularize do 2: η1 = norm rows(Θl) . Keep norm of the rows of Θl. 3: Θl1 = div rows(Θ\nl, η1) . Keep a Θl1 with normalized rows. 4: innerProdMat = Θl1transpose(Θ l 1) 5: ∇Θl1 = γ(innerProdMat− diag(innerProdMat))Θl1 . Second term in eq. 6 6: ∆Θl = −α(∇JΘl + γ∇Θl1) . Complete eq. 6. 7: end for\n(a) Global loss plot (eq. 2) (b) Local loss plot (eq. 7)\n(c) Direction of gradients for the loss in (a). (d) Direction of gradients for loss in (b).\nFigure 1: Comparison between the two loss functions represented by eq.2 and 7. (a) is the original loss, (b) is the new loss that discards negative correlations given for different λ values. It can be seen λ = 10 reaches a plateau when approximating to π2 . (c) and (d) shows the directions of the gradients for the two loss functions above. For instance, a red arrow coming from a green ball represents the gradient of the loss between the red and green balls with respect to the green one. In (d) most of the arrows disappear since the loss in (b) only applies to angles smaller than π2 .\nsubtract the diagonal so as to ignore the angle from each feature with respect to itself and multiply by Θ to compute the final value corresponding to the sum in eq. 5:\n∆Θ = −α ( ∇JΘ + γ(ΘΘt − diag(ΘΘt))Θ ) (6)\nWhere the second term is∇CΘ. Algorithm 1 summarizes the steps in order to apply OrthoReg.\n\n2.2 NEGATIVE CORRELATIONS\nNote that the presented algorithm, based on the cosine similarity, penalizes any kind of correlation between all pairs of feature detectors, i.e. the positive and the negative correlations, see Figure 1a. However, negative correlations are related to inhibitory connections, competitive learning, and self-organization. In fact, there is evidence that negative correlations can help a neural population to increase the signal-to-noise ratio (Chelaru & Dragoi (2016)) in the V1. In order to find out\nthe advantages of keeping negative correlations, we propose to use an exponential to squash the gradients for angles greater than π2 (orthogonal):\nC(θ) = n∑ i=1 n∑ j=1,j 6=i log(1 + eλ(cos(θi,θj)−1)) = log(1 + eλ(〈θi,θj〉−1)), ||θi|| = ||θj || = 1 (7)\nWhere λ is a coefficient that controls the minimum angle-of-influence of the regularizer, i.e. the minimum angle between two feature weights so that there exists a gradient pushing them apart, see Figure 1b. We empirically found that the regularizer worked well for λ = 10, see Figure 2b. Note that when λ ' 10 the loss and the gradients approximate to zero when vectors are at more than π2 (orthogonal). As a result of incorporating the squashing function on the cosine similarity, negatively correlated feature weights will not be regularized. This is different from all previous approaches and the loss presented in eq. 2, where all pairs of weight vectors influence each other. Thus, from now on, the loss in eq. 2 is named as global loss and the loss in eq. 7 is named as local loss.\nThe derivative of eq. 7 is:\nδ\nδθ(i,j) C(θ) = n∑ k=1,k 6=i λ eλ〈θi,θk〉θ(k,j) eλ〈θi,θk〉 + eλ (8)\nThen, given the element-wise exponential operator exp, we define the following expression in order to simplify the formulas:\nΘ̂ = exp(λ(ΘΘt)) (9)\nand thus, the ∆ in vectorial form can be formulated as:\n∇CΘ = λ (Θ̂− diag(Θ̂))Θ\nΘ̂− diag(Θ̂) + eλ (10)\nIn order to provide a visual example, we have created a 2D toy dataset and used the previous equations for positive and negative γ values, see Figure 2. As expected, it can be seen that the angle between all pairs of adjacent feature weights becomes more uniform after regularization. Note that Figure 2b shows that regularization with the global loss (eq. 2) results in less uniform angles than using the local loss as shown in 2c (which corresponds to the local loss presented in eq. 7) because vectors in opposite quadrants still influence each other. This is why in Figure 2d, it can be seen that the mean nearest neighbor angle using the global loss (b) is more unstable than the local loss (c). As a proof of concept, we also performed gradient ascent, which minimizes the angle between the vectors. Thus, in Figures 2e and 2f, it can be seen that the locality introduced by the local loss reaches a stable configuration where feature weights with angle π2 are too far to attract each other.\nThe effects of global and local regularizations on Alexnet, VGG-16 and a 50-layer ResNet are shown on Figure 3. As it can be seen, OrthoReg reaches higher decorrelation bounds. Lower decorrelation peaks are still observed when the input dimensionality of the layers is smaller than the output since all vectors cannot be orthogonal at the same time. In this case, local regularization largely outperforms global regularization since it removes interferences caused by negatively correlated feature weights. This suggests why increasing fully connected layers’ size has not improved networks performance.\n\n3 EXPERIMENTS\nIn this section we provide a set of experiments that verify that (i) training with the proposed regularization increases the performance of naive unregularized models, (ii) negatively correlated feature weights are useful, and (iii) the proposed regularization improves the performance of state-of-the-art models.\n\n3.1 VERIFICATION EXPERIMENTS\nAs a sanity check, we first train a three-hidden-layer Multi-Layer Perceptron (MLP) with ReLU non-liniarities on the MNIST dataset (LeCun et al. (1998)). Our code is based in the train-a-digit-classifier example included in torch/demos1, which uses an upsampled version of the dataset (32×32). The only pre-processing applied to the data is a global standardization. The model is trained with SGD and a batch size of 200 during 200 epochs. No momentum neither weight decay was applied. By default, the magnitude of the weights of this experiments is recovered after each regularization step in order to prove the regularization only affects their angle.\nSensitivity to hyperparameters. We train a three-hidden-layer MLP with 1024 hidden units, and different γ and λ values so as to verify how they affect the performance of the model. Figure 4a\n1https://github.com/torch/demos\nshows that the model effectively achieves the best error rate for the highest gamma value (γ = 1), thus proving the advantages of the regularization. On Figure 4b, we verify that higher regularization rates produce more general models. Figure 5a depicts the sensitivity of the model to λ. As expected, the best value is found when lambda corresponds to Orthogonality (λ ' 10). Negative Correlations. Figure 5b highlights the difference between regularizing with the global or the local regularizer. Although both regularizations reach better error rates than the unregularized counterpart, the local regularization is better than the global. This confirms the hypothesis that negative correlations are useful and thus, performance decreases when we reduce them.\nCompatibility with initialization and dropout. To demonstrate the proposed regularization can help even when other regularizations are present, we trained a CNN with (i) dropout (c32-c64-l512-d0.5-l10)2 or (ii) LSUV initialization (Mishkin & Matas (2016)). In Table 2, we show that best results are obtained when orthogonal regularization is present. The results are consistent with the hypothesis that OrthoReg, as well as Dropout and LSUV, focuses on reducing the model redundancy. Thus, when one of them is present, the margin of improvement for the others is reduced.\n2cxx = convolution with xx filters. lxx = fully-connected with xx units. dxx = dropout with prob xx.\n\n3.2 REGULARIZATION ON CIFAR-10 AND CIFAR-100\nWe show that the proposed OrthoReg can help to improve the performance of state-of-the-art models such as deep residual networks (He et al. (2015a)). In order to show the regularization is suitable for deep CNNs, we successfuly regularize a 110-layer ResNet3 on CIFAR-10, decreasing its error from 6.55% to 6.29% without data augmentation.\nIn order to compare with the most recent state-of-the-art, we train a wide residual network (Zagoruyko & Komodakis (November 2016)) on CIFAR-10 and CIFAR-100. The experiment is based on a torch implementation of the 28-layer and 10th width factor wide deep residual model, for which the median error rate on CIFAR-10 is 3.89% and 18.85% on CIFAR-1004. As it can be seen in Figure 6, regularizing with OrthoReg yields the best test error rates compared to the baselines.\nThe regularization coefficient γ was chosen using grid search although similar values were found for all the experiments, specially if regularization gradients are normalized before adding them to the weights. The regularization was equally applied to all the convolution layers of the (wide) ResNet. We found that, although the regularized models were already using weight decay, dropout, and batch normalization, best error rates were always achieved with OrthoReg.\nTable 3 compares the performance of the regularized models with other state-of-the-art results. As it can be seen the regularized model surpasses the state of the art, with a 5.1% relative error improvement on CIFAR-10, and a 1.5% relative error improvement on CIFAR-100.\n\n3.3 REGULARIZATION ON SVHN\nFor SVHN we follow the procedure depicted in Zagoruyko & Komodakis (May 2016), training a wide residual network of depth=28, width=4, and dropout. Results are shown in Table 4. As it\n3https://github.com/gcr/torch-residual-networks 4https://github.com/szagoruyko/wide-residual-networks\ncan be seen, we reduce the error rate from 1.64% to 1.54%, which is the lowest value reported on this dataset to the best of our knowledge.\n\n4 DISCUSSION\nRegularization by feature decorrelation can reduce Neural Networks overfitting even in the presence of other kinds of regularizations. However, especially when the number of feature detectors is higher than the input dimensionality, its decorrelation capacity is limited due to the effects of negatively correlated features. We showed that imposing locality constraints in feature decorrelation removes interferences between negatively correlated feature weights, allowing regularizers to reach higher decorrelation bounds, and reducing the overfitting more effectively.\nIn particular, we show that the models regularized with the constrained regularization present lower overfitting even when batch normalization and dropout are present. Moreover, since our regularization is directly performed on the weights, it is especially suitable for fully convolutional neural networks, where the weight space is constant compared to the feature map space. As a result, we are able to reduce the overfitting of 110-layer ResNets and wide ResNets on CIFAR-10, CIFAR-100, and SVHN improving their performance. Note that despite OrthoReg consistently improves state of the art ReLU networks, the choice of the activation function could affect regularizers like the one presented in this work. In this sense, the effect of asymmetrical activations on feature correlations and regularizers should be further investigated in the future.\n",
    "rationale": "The paper proposes a new regulariser for CNNs that penalises positive correlations between feature weights, but does not affect negative correlations. An alternative version which penalises all correlations regardless of sign is also considered. The paper refers to these as \"local\" and \"global\" respectively, which I find a bit confusing as these are very general terms that can mean a plethora of things.\n\nThe experimental validation is quite rigorous. Several experiments are conducted on benchmark datasets (MNIST, CIFAR-10, CIFAR-100, SVHN) and improvements are demonstrated in most cases. While these improvements may seem modest, the baselines are already very competitive as the authors pointed out. In some cases it does raise some questions about statistical significance though. More results with the global regulariser (i.e. not just on MNIST) would have been interesting, as the main novelty in the paper seems to be leaving the negative correlations alone, so it would be interesting to see exactly how much of a difference this makes.\n\nOne of my main concerns is ambiguity stemming from the fact that the paper sometimes discusses activations and sometimes filter weights, but refers to both as \"features\". However, the authors have already said they will address this.\n\nThe paper somewhat ignores interactions with the choice of nonlinearity, which seems like it could be very important; especially because the goal is to obtain feature activations that are uncorrelated, and this is done only by applying a penalty to the weights (i.e. in a data-agnostic way and also ignoring any nonlinearity). I believe the authors already mentioned in their responses to reviewer questions that this would be addressed, but I think this important and it definitely needs to be discussed.\n\nIn response to the authors' answer to my question about the role of biases: as they point out, it is perfectly possible to combine their proposed technique with the \"multi-bias\" approach, but this was not really my point. Rather, the latter is an example that challenges the idea that features should not be positively correlated / redundant, which seems to be the assumption that this work is built upon. My current intuition is that it's okay to have correlated features, as long as you're not wasting model capacity on them. This is the case for \"multi-bias\", seeing as the weights are shared across sets of correlated features.\n\nThe dichotomy between regularisation methods that reduce capacity and those that don't which is described in the introduction seems a bit arbitrary to me, especially considering that weight decay is counted among the former and the proposed method is counted among the latter. I think this very much depends on ones definition of model capacity (clearly weight decay does not actually reduce the number of parameters in a model).\n\nOverall, the work is perhaps a bit incremental, but it seems to be well-executed. The results are convincing, even if they aren't particularly ground-breaking.",
    "rating": 4
  },
  {
    "title": "OTHER MODIFICATIONS",
    "abstract": "PixelCNNs are a recently proposed class of powerful generative models with tractable likelihood. Here we discuss our implementation of PixelCNNs which we make available at https://github.com/openai/pixel-cnn. Our implementation contains a number of modifications to the original model that both simplify its structure and improve its performance. 1) We use a discretized logistic mixture likelihood on the pixels, rather than a 256-way softmax, which we find to speed up training. 2) We condition on whole pixels, rather than R/G/B sub-pixels, simplifying the model structure. 3) We use downsampling to efficiently capture structure at multiple resolutions. 4) We introduce additional short-cut connections to further speed up optimization. 5) We regularize the model using dropout. Finally, we present state-of-the-art log likelihood results on CIFAR-10 to demonstrate the usefulness of these modifications.",
    "text": "1 INTRODUCTION\nThe PixelCNN, introduced by van den Oord et al. (2016b), is a generative model of images with a tractable likelihood. The model fully factorizes the probability density function on an image x over all its sub-pixels (color channels in a pixel) as p(x) = ∏ i p(xi|x<i). The conditional distributions p(xi|x<i) are parameterized by convolutional neural networks and all share parameters. The PixelCNN is a powerful model as the functional form of these conditionals is very flexible. In addition it is computationally efficient as all conditionals can be evaluated in parallel on a GPU for an observed image x. Thanks to these properties, the PixelCNN represents the current state-of-the-art in generative modeling when evaluated in terms of log-likelihood. Besides being used for modeling images, the PixelCNN model was recently extended to model audio (van den Oord et al., 2016a), video (Kalchbrenner et al., 2016b) and text (Kalchbrenner et al., 2016a).\nFor use in our research, we developed our own internal implementation of PixelCNN and made a number of modifications to the base model to simplify its structure and improve its performance. We now release our implementation at https://github.com/openai/pixel-cnn, hoping that it will be useful to the broader community. Our modifications are discussed in Section 2, and evaluated experimentally in Section 3. State-of-the-art log-likelihood results confirm their usefulness.\n\n2 MODIFICATIONS TO PIXELCNN\nWe now describe the most important modifications we have made to the PixelCNN model architecure as described by van den Oord et al. (2016c). For complete details see our code release at https://github.com/openai/pixel-cnn.\n\n2.1 DISCRETIZED LOGISTIC MIXTURE LIKELIHOOD\nThe standard PixelCNN model specifies the conditional distribution of a sub-pixel, or color channel of a pixel, as a full 256-way softmax. This gives the model a lot of flexibility, but it is also very costly in terms of memory. Moreover, it can make the gradients with respect to the network parameters\nvery sparse, especially early in training. With the standard parameterization, the model does not know that a value of 128 is close to a value of 127 or 129, and this relationship first has to be learned before the model can move on to higher level structures. In the extreme case where a particular sub-pixel value is never observed, the model will learn to assign it zero probability. This would be especially problematic for data with higher accuracy on the observed pixels than the usual 8 bits: In the extreme case where very high precision values are observed, the PixelCNN, in its current form, would require a prohibitive amount of memory and computation, while learning very slowly. We therefore propose a different mechanism for computing the conditional probability of the observed discretized pixel values. In our model, like in the VAE of Kingma et al. (2016), we assume there is a latent color intensity ν with a continuous distribution, which is then rounded to its nearest 8-bit representation to give the observed sub-pixel value x. By choosing a simple continuous distribution for modeling ν (like the logistic distribution as done by Kingma et al. (2016)) we obtain a smooth and memory efficient predictive distribution for x. Here, we take this continuous univariate distribution to be a mixture of logistic distributions which allows us to easily calculate the probability on the observed discretized value x, as shown in equation (2). For all sub-pixel values x excepting the edge cases 0 and 255 we have:\nν ∼ K∑ i=1 πilogistic(µi, si) (1)\nP (x|π, µ, s) = K∑ i=1 πi [σ((x+ 0.5− µi)/si)− σ((x− 0.5− µi)/si)] , (2)\nwhere σ() is the logistic sigmoid function. For the edge case of 0, replace x − 0.5 by −∞, and for 255 replace x + 0.5 by +∞. Our provided code contains a numerically stable implementation for calculating the log of the probability in equation 2.\nOur approach follows earlier work using continuous mixture models (Domke et al., 2008; Theis et al., 2012; Uria et al., 2013; Theis & Bethge, 2015), but avoids allocating probability mass to values outside the valid range of [0, 255] by explicitly modeling the rounding of ν to x. In addition, we naturally assign higher probability to the edge values 0 and 255 than to their neighboring values, which corresponds well with the observed data distribution as shown in Figure 1. Experimentally, we find that only a relatively small number of mixture components, say 5, is needed to accurately model the conditional distributions of the pixels. The output of our network is thus of much lower dimension, yielding much denser gradients of the loss with respect to our parameters. In our experiments this greatly sped up convergence during optimization, especially early on in training. However, due to the other changes in our architecture compared to that of van den Oord et al. (2016c) we cannot say with certainty that this would also apply to the original PixelCNN model.\n\n2.2 CONDITIONING ON WHOLE PIXELS\nThe pixels in a color image consist of three real numbers, giving the intensities of the red, blue and green colors. The original PixelCNN factorizes the generative model over these 3 sub-pixels. This allows for very general dependency structure, but it also complicates the model: besides keeping track of the spatial location of feature maps, we now have to separate out all feature maps in 3 groups depending on whether or not they can see the R/G/B sub-pixel of the current location. This added complexity seems to be unnecessary as the dependencies between the color channels of a pixel are likely to be relatively simple and do not require a deep network to model. Therefore, we instead condition only on whole pixels up and to the left in an image, and output joint predictive distributions over all 3 channels of a predicted pixel. The predictive distribution on a pixel itself can be interpreted as a simple factorized model: We first predict the red channel using a discretized mixture of logistics as described in section 2.1. Next, we predict the green channel using a predictive distribution of the same form. Here we allow the means of the mixture components to linearly depend on the value of the red sub-pixel. Finally, we model the blue channel in the same way, where we again only allow linear dependency on the red and green channels. For the pixel (ri,j , gi,j , bi,j) at location (i, j) in our image, the distribution conditional on the context Ci,j , consisting of the mixture indicator and the previous pixels, is thus\np(ri,j , gi,j , bi,j |Ci,j) = P (ri,j |µr(Ci,j), sr(Ci,j))× P (gi,j |µg(Ci,j , ri,j), sg(Ci,j)) ×P (bi,j |µb(Ci,j , ri,j , gi,j), sb(Ci,j))\nµg(Ci,j , ri,j) = µg(Ci,j) + α(Ci,j)ri,j\nµb(Ci,j , ri,j , gi,j) = µb(Ci,j) + β(Ci,j)ri,j + γ(Ci,j)bi,j , (3)\nwith α, β, γ scalar coefficients depending on the mixture component and previous pixels.\nThe mixture indicator is shared across all 3 channels; i.e. our generative model first samples a mixture indicator for a pixel, and then samples the color channels one-by-one from the corresponding mixture component. Had we used a discretized mixture of univariate Gaussians for the sub-pixels, instead of logistics, this would have been exactly equivalent to predicting the complete pixel using a (discretized) mixture of 3-dimensional Gaussians with full covariance. The logistic and Gaussian distributions are very similar, so this is indeed very close to what we end up doing. For full implementation details we refer to our code at https://github.com/openai/pixel-cnn.\n\n2.3 DOWNSAMPLING VERSUS DILATED CONVOLUTION\nThe original PixelCNN only uses convolutions with small receptive field. Such convolutions are good at capturing local dependencies, but not necessarily at modeling long range structure. Although we find that capturing these short range dependencies is often enough for obtaining very good log-likelihood scores (see Table 2), explicitly encouraging the model to capture long range dependencies can improve the perceptual quality of generated images (compare Figure 3 and Figure 5). One way of allowing the network to model structure at multiple resolutions is to introduce dilated convolutions into the model, as proposed by van den Oord et al. (2016a) and Kalchbrenner et al. (2016b). Here, we instead propose to use downsampling by using convolutions of stride 2. Downsampling accomplishes the same multi-resolution processing afforded by dilated convolutions, but at a reduced computational cost: where dilated convolutions operate on input of ever increasing size (due to zero padding), downsampling reduces the input size by a factor of 4 (for stride of 2 in 2 dimensions) at every downsampling. The downside of using downsampling is that it loses information, but we can compensate for this by introducing additional short-cut connections into the network as explained in the next section. With these additional short-cut connections, we found the performance of downsampling to be the same as for dilated convolution.\n\n2.4 ADDING SHORT-CUT CONNECTIONS\nFor input of size 32 × 32 our suggested model consists of 6 blocks of 5 ResNet layers. In between the first and second block, as well as the second and third block, we perform subsampling by strided convolution. In between the fourth and fifth block, as well as the fifth and sixth block, we perform upsampling by transposed strided convolution. This subsampling and upsampling process loses information, and we therefore introduce additional short-cut connections into the model to recover\nthis information from lower layers in the model. The short-cut connections run from the ResNet layers in the first block to the corresponding layers in the sixth block, and similarly between blocks two and five, and blocks three and four. This structure resembles the VAE model with top down inference used by Kingma et al. (2016), as well as the U-net used by Ronneberger et al. (2015) for image segmentation. Figure 2 shows our model structure graphically.\n\n2.5 REGULARIZATION USING DROPOUT\nThe PixelCNN model is powerful enough to overfit on training data. Moreover, rather than just reproducing the training images, we find that overfitted models generate images of low perceptual quality, as shown in Figure 8. One effective way of regularizing neural networks is dropout (Srivastava et al., 2014). For our model, we apply standard binary dropout on the residual path after the first convolution. This is similar to how dropout is applied in the wide residual networks of Zagoruyko & Komodakis (2016). Using dropout allows us to successfully train high capacity models while avoiding overfitting and producing high quality generations (compare figure 8 and figure 3).\n\n3 EXPERIMENTS\nWe apply our model to modeling natural images in the CIFAR-10 data set. We achieve state-of-theart results in terms of log-likelihood, and generate images with coherent global structure.\n\n3.1 UNCONDITIONAL GENERATION ON CIFAR-10\nWe apply our PixelCNN model, with the modifications as described above, to generative modeling of the images in the CIFAR-10 data set. For the encoding part of the PixelCNN, the model uses 3 Resnet blocks consisting of 5 residual layers, with 2× 2 downsampling in between. The same architecture is used for the decoding part of the model, but with upsampling instead of downsampling in between blocks. All residual layers use 192 feature maps and a dropout rate of 0.5. Table 1 shows the stateof-the-art test log-likelihood obtained by our model. Figure 3 shows some samples generated by the model.\n\n3.2 CLASS-CONDITIONAL GENERATION\nNext, we follow van den Oord et al. (2016c) in making our generative model conditional on the class-label of the CIFAR-10 images. This is done by linearly projecting a one-hot encoding of the class-label into a separate class-dependent bias vector for each convolutional unit in our network. We find that making the model class-conditional makes it harder to avoid overfitting on the training data: our best test log-likelihood is 2.94 in this case. Figure 4 shows samples from the class-conditional model, with columns 1-10 corresponding the 10 classes in CIFAR-10. The images clearly look qualitatively different across the columns and for a number of them we can clearly identify their class label.\n\n3.3 EXAMINING NETWORK DEPTH AND FIELD OF VIEW SIZE\nIt is hypothesized that the size of the receptive field and additionally the removal of blind spots in the receptive field are important for PixelCNN’s performance (van den Oord et al., 2016b). Indeed van den Oord et al. (2016c) specifically introduced an improvement over the previous PixelCNN model to remove the blind spot in the receptive field that was present in their earlier model.\nHere we present the surprising finding that in fact a PixelCNN with rather small receptive field can attain competitive generative modelling performance on CIFAR-10 as long as it has enough capacity. Specifically, we experimented with our proposed PixelCNN++ model without downsampling blocks and reduce the number of layers to limit the receptive field size. We investigate two receptive field sizes: 11x5 and 15x8, and a receptive field size of 11x5, for example, means that the conditional distribution of a pixel can depends on a rectangle above the pixel of size 11x5 as well as 11−12 = 5x1 block to the left of the pixel.\nAs we limit the size of the receptive field, the capacity of the network also drops significantly since it contains many fewer layers than a normal PixelCNN. We call the type of PixelCNN that’s simply limited in depth “Plain” Small PixelCNN. Interestingly, this model already has better performance than the original PixelCNN in van den Oord et al. (2016b) which had a blind spot. To increase capacity, we introduced two simple variants that make Small PixelCNN more expressive without growing the receptive field:\n• NIN (Network in Network): insert additional gated ResNet blocks with 1x1 convolution between regular convolution blocks that grow receptive field. In this experiment, we inserted 3 NIN blocks between every other layer. • Autoregressive Channel: skip connections between sets of channels via 1x1 convolution\ngated ResNet block.\nBoth modifications increase the capacity of the network, resulting in improved log-likelihood as shown in Table 2. Although the model with small receptive field already achieves an impressive likelihood score, its samples do lack global structure, as seen in Figure 5.\n\n3.4 ABLATION EXPERIMENTS\nIn order to test the effect of our modifications to PixelCNN, we run a number of ablation experiments where for each experiment we remove a specific modification.\n\n3.4.1 SOFTMAX LIKELIHOOD INSTEAD OF DISCRETIZED LOGISTIC MIXTURE\nIn order to test the contribution of our logistic mixture likelihood, we re-run our CIFAR-10 experiment with the 256-way softmax as the output distribution instead. We allow the 256 logits for each sub-pixel to linearly depend on the observed value of previous sub-pixels, with coefficients that are given as output by the model. Our model with softmax likelihood is thus strictly more flexible than our model with logistic mixture likelihood, although the parameterization is quite different from that used by van den Oord et al. (2016c). The model now outputs 1536 numbers per pixel, describing the logits on the 256 potential values for each sub-pixel, as well as the coefficients for the dependencies between the sub-pixels. Figure 6 shows that this model trains more slowly than our original model. In addition, the running time per epoch is significantly longer for our tensorflow implementation. For our architecture, the logistic mixture model thus clearly performs better. Since our architecture differs from that of van den Oord et al. (2016c) in other ways as well, we cannot say whether this would also apply to their model.\n\n3.4.2 CONTINUOUS MIXTURE LIKELIHOOD INSTEAD OF DISCRETIZATION\nInstead of directly modeling the discrete pixel values in an image, it is also possible to de-quantize them by adding noise from the standard uniform distribution, as used by Uria et al. (2013) and others, and modeling the data as being continuous. The resulting model can be interpreted as a variational autoencoder (Kingma & Welling, 2013; Rezende et al., 2014), where the dequantized pixels z form a latent code whose prior distribution is captured by our model. Since the original discrete pixels x can be perfectly reconstructed from z under this model, the usual reconstruction term vanishes from\nthe variational lower bound. The entropy of the standard uniform distribution is zero, so the term that remains is the log likelihood of the dequantized pixels, which thus gives us a variational lower bound on the log likelihood of our original data.\nWe re-run our model for CIFAR-10 using the same model settings as those used for the 2.92 bits per dimension result in Table 1, but now we remove the discretization in our likelihood model and instead add standard uniform noise to the image data. The resulting model is a continuous mixture model in the same class as that used by Theis et al. (2012); Uria et al. (2013); Theis & Bethge (2015) and others. After optimization, this model gives a variational lower bound on the data log likelihood of 3.11 bits per dimension. The difference with the reported 2.92 bits per dimension shows the benefit of using discretization in the likelihood model.\n\n3.4.3 NO SHORT-CUT CONNECTIONS\nNext, we test the importance of the additional parallel short-cut connections in our model, indicated by the dotted lines in Figure 2. We re-run our unconditional CIFAR-10 experiment, but remove the short-cut connections from the model. As seen in Figure 7, the model fails to train without these connections. The reason for needing these extra short-cuts is likely to be our use of sub-sampling, which discards information that otherwise cannot easily be recovered,\n\n3.4.4 NO DROPOUT\nWe re-run our CIFAR-10 model without dropout regularization. The log-likelihood we achieve on the training set is below 2.0 bits per sub-pixel, but the final test log-likelihood is above 6.0 bits per\nsub-pixel. At no point during training does the unregularized model get a test-set log-likelihood below 3.0 bits per sub-pixel. Contrary to what we might naively expect, the perceptual quality of the generated images by the overfitted model is not great, as shown in Figure 8.\n\n4 CONCLUSION\nWe presented PixelCNN++, a modification of PixelCNN using a discretized logistic mixture likelihood on the pixels among other modifications. We demonstrated the usefulness of these modifications with state-of-the-art results on CIFAR-10. Our code is made available at https: //github.com/openai/pixel-cnn and can easily be adapted for use on other data sets.\n",
    "rationale": "# Review\nThis paper proposes five modifications to improve PixelCNN, a generative model with tractable likelihood. The authors empirically showed the impact of each of their proposed modifications using a series of ablation experiments. They also reported a new state-of-the-art result on CIFAR-10.\nImproving generative models, especially for images, is an active research area and this paper definitely contributes to it.\n\n\n# Pros\nThe authors motivate each modification well they proposed. They also used ablation experiments to show each of them is important.\n\nThe authors use a discretized mixture of logistic distributions to model the conditional distribution of a sub-pixel instead of a 256-way softmax. This allows to have a lower output dimension and to be better suited at learning ordinal relationships between sub-pixel values. The authors also mentioned it speeded up training time (less computation) as well as the convergence during the optimization of the model (as shown in Fig.6).\n\nThe authors make an interesting remark about how the dependencies between the color channels of a pixel are likely to be relatively simple and do not require a deep network to model. This allows them to have a simplified architecture where you don't have to separate out all feature maps in 3 groups depending on whether or not they can see the R/G/B sub-pixel of the current location.\n\n\n# Cons\nIt is not clear to me what the predictive distribution for the green channel (and the blue) looks like. More precisely, how are the means of the mixture components linearly depending on the value of the red sub-pixel? I would have liked to see the equations for them.\n\n\n# Minor Comments\nIn Fig.2 it is written \"Sequence of 6 layers\" but in the text (Section 2.4) it says 6 blocks of 5 ResNet layers. What is the remaining layer?\nIn Fig.2 what does the first \"green square -> blue square\" which isn't in the white rectangle represents?\nIs there any reason why the mixture indicator is shared across all three channels?",
    "rating": 4
  },
  {
    "title": "Handling Cold-Start Problem in Review Spam Detection by Jointly Embedding Texts and Behaviors",
    "abstract": "Solving cold-start problem in review spam detection is an urgent and significant task. It can help the on-line review websites to relieve the damage of spammers in time, but has never been investigated by previous work. This paper proposes a novel neural network model to detect review spam for cold-start problem, by learning to represent the new reviewers’ review with jointly embedded textual and behavioral information. Experimental results prove the proposed model achieves an effective performance and possesses preferable domain-adaptability. It is also applicable to a large scale dataset in an unsupervised way.",
    "text": "1 Introduction\nWith the rapid growth of products reviews at the web, it has become common for people to read reviews before making purchase decision. The reviews usually contain abundant consumers’ personal experiences. It has led to a significant influence on financial gains and fame for businesses. Existing studies have shown that an extra halfstar rating on Yelp causes restaurants to sell out 19% points more frequently (Anderson and Magruder, 2012), and a one-star increase in Yelp rating leads to a 5-9 % increase in revenue (Luca, 2011). This, unfortunately, gives strong incentives for imposters (called spammers) to game the system. They post fake reviews or opinions (called review spam) to promote or to discredit some targeted products and services. The news from BBC has shown that around 25% of Yelp reviews could be fake.1 Therefore, it is urgent to detect review s-\n1http://www.bbc.com/news/technology-24299742\npam, to ensure that the online review continues to be trusted.\nJindal and Liu (2008) make the first step to detect review spam. Most efforts are devoted to explore effective linguistic and behavioral features by subsequent work to distinguish such spam from the real reviews. However, to notice such patterns or form behavioral features, developers should take long time to observe the data, because the features are based on statistics. For instance, the feature activity window proposed by Mukherjee et al. (2013c) is to measure the activity freshness of reviewers. It usually takes several months to count the difference of timestamps between the last and first reviews for reviewers. When the features show themselves finally, some major damages might have already been done. Thus, it is important to design algorithms that can detect review spam as soon as possible, ideally, right after they are posted by the new reviewers. It is a coldstart problem which is the focus of this paper.\nIn this paper, we assume that we must identify fake reviews immediately when a new reviewer posts just one review. Unfortunately, it is very difficult because the available information for detecting fake reviews is very poor. Traditional behavioral features based on the statistics can only work well on users’ abundant behaviors. The more behavioral information obtained, the more effective the traditional behavioral features are (see experiments in Section 3 ). In the scenario of cold-start, a new reviewer only has a behavior: post a review. As a result, we can not get effective behavioral features from the data. Although, the linguistic features of reviews do not need to take much time to form, Mukherjee et al. (2013c) have proved that the linguistic features are not effective enough in detecting real-life fake reviews from the commercial websites, where we also obtain the same observation (the details are shown in Section 3).\n2\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n112\n113\n114\n115\n116\n117\n118\n119\n120\n121\n122\n123\n124\n125\n126\n127\n128\n129\n130\n131\n132\n133\n134\n135\n136\n137\n138\n139\n140\n141\n142\n143\n144\n145\n146\n147\n148\n149\n156\n157\n158\n159\n160\n161\n162\n163\n164\n165\n166\n167\n168\n169\n170\n171\n172\n173\n174\n175\n176\n177\n178\n179\n180\n181\n182\n183\n184\n185\n186\n187\n188\n189\n190\n191\n192\n193\n194\n195\n196\n197\n198\n199\nTherefore, the main difficulty of the cold-start spam problem is that there is no sufficient behaviors of the new reviewers for constructing effective behavioral features. Nevertheless, there are ample textual and behavioral information contained in the abundant reviews posted by the existing reviewers (Figure 1). We could employ behavioral information of existing similar reviewers to a new reviewer to approximate his behavioral features. We argue that a reviewer’s individual characteristics such as background information, motivation and interactive behavior style have a great influence on a reviewer’s textual and behavioral information. So the textual information and the behavioral information of a reviewer are correlated with each other (similar argument in Li et al. (2016)). For example, the students of college are likely to choose the youth hostel during summer vacation, and tend to comment the room price in their reviews. But the financial analysts on business trip may tend to choose the business hotel, the environment and service are what they care about in their reviews.\nTo augment the behavioral information of the new reviewers in the cold-start problem, we first try to find the textual information which is similar with that of the new reviewer, from the existing reviews. There are several ways to model the textual information of the review spam, such as Unigram (Mukherjee et al., 2013c), POS (Ott et al., 2011) and LIWC (Linguistic Inquiry and Word Count) (Newman et al., 2003). We employ the CNN (Convolutional Neural Network) to model the review text, which has been proved that it can capture complex global semantic information that is difficult to express using traditional discrete manual features (Ren and Zhang, 2016). Then we employ the behavioral information which is correlated with the found textual information to approximate the behavioral information of the new reviewer. An intuitive approach is to search the most similar existing review for the new review, then take the found reviewer’s behavioral features as the new reviewers’ features (detailed in Section 5.3). However, there are abundant behavioral information in the review graph (Figure 1), it is difficult for the traditional discrete manual behavioral features to record the global behavioral information (Wang et al., 2016). Moreover, the traditional features can not capture the reviewer’s individual characteristics, because there is no explicit characteristic tag available in the review system (experi-\nments in Section 5.3). So, we propose a neural network model to jointly encode the textual and behavioral information into the review embeddings for detecting the review spam in cold-start problem. By encoding the review graph structure (Figure 1), the proposed model can record the global footprints of the existing reviewers in an unsupervised way, and further record the reviewers’ latent characteristic information in the footprints. The jointly learnt review embeddings can model the correlation of the reviewers’ textual and behavioral information. When a new reviewer posts a review, the proposed model can represent the review with the similar textual information and the correlated behavioral information encoded in the word embeddings. Finally, the embeddings of the new review are fed into a classifier to identify whether it is spam or not.\nIn summary, our major contributions include: • To our best knowledge, this is the first work\nthat explores the cold-start problem in review spam detection. We qualitatively and quantitatively prove that the traditional linguistic and behavioral features are not effective enough in detecting review spam for the coldstart task. • We propose a neural network model to joint-\nly encode the textual and behavioral information into the review embeddings for the cold-start spam detection task. It is an unsupervised distributional representation model which can learn from large scale unlabeled review data. • Experimental results on two domains (hotel\nand restaurant ) give good confidence that the proposed model performs effectively in the cold-start spam detection task.\n\n2 Related Work\nJindal and Liu (2008) make the first step to detect review spam. Subsequent work devoted most efforts to explore effective features and spammerlike clues.\n3\n201\n202\n203\n204\n205\n206\n207\n208\n209\n210\n211\n212\n213\n214\n215\n216\n217\n218\n219\n220\n221\n222\n223\n224\n225\n226\n227\n228\n229\n230\n231\n232\n233\n234\n235\n236\n237\n238\n239\n240\n241\n242\n243\n244\n245\n246\n247\n248\n249\n261\n262\n263\n264\n265\n266\n267\n268\n269\n270\n271\n272\n273\n274\n275\n276\n277\n278\n279\n280\n281\n282\n283\n284\n285\n286\n287\n288\n289\n290\n291\n292\n293\n294\n295\n296\n297\n298\n299\nLinguistic features: Ott et al. (2011) applied psychological and linguistic clues to identify review spam; Harris (2012) explored several writing style features. Syntactic stylometry for review spam detection was investigated in Feng et al. (2012a); Xu and Zhao (2012) using deep linguistic features for finding deceptive opinion spam; Li et al. (2013) studied the topics in the review spam; Li et al. (2014b) further analyzed the general difference of language usage. Fornaciari and Poesio (2014) proved the effectiveness of the N-grams in detecting deceptive Amazon book reviews. The effectiveness of the N-grams was also explored in Cagnina and Rosso (2015). Li et al. (2014a) proposed a positive-unlabeled learning method based on unigrams and bigrams; Kim et al. (2015) carried out a frame-based deep semantic analysis. Hai et al. (2016) exploited the relatedness of multiple review spam detection tasks and available unlabeled data to address the scarcity of labeled opinion spam data by using linguistic features. Besides, (Ren and Zhang, 2016) proved that the CNN model is more effective than the RNN and the traditional discrete manual linguistic features. Hovy (2016) used N-gram generative models to produce reviews and evaluated their effectiveness.\nBehavioral features: Lim et al. (2010) analyzed reviewers’ rating behavioral features; Jindal et al. (2010) identified unusual review patterns which can represent suspicious behaviors of reviews; Li et al. (2011) proposed a two-view semisupervised co-training method base on behavioral features. Feng et al. (2012b) study the distributions of individual spammers’ behaviors. The group spammers’ behavioral features were studied in Mukherjee et al. (2012). Temporal patterns of spammers were investigated by Xie et al. (2012), Fei et al. (2013); Li et al. (2015) explored the temporal and spatial patterns. The review graph was analyzed by Wang et al. (2011), Akoglu et al. (2013); Mukherjee et al. (2013a) studied the spamicity of reviewers. Mukherjee et al. (2013c), Mukherjee et al. (2013b) proved that reviewers’ behavioral features are more effective than reviews’ linguistic features for detecting review spam. Based on this conclusion, recently, researchers (Rayana and Akoglu, 2015; KC and Mukherjee, 2016) have put more efforts in employing reviewers’ behavioral features for detecting review spam, the intuition behind which is to capture the reviewers’ actions and supposes that\nthose reviews written with spammer-like behaviors would be spam. Wang et al. (2016) explored a method to learn the review representation with global behavioral information.\n\n3 Whether Traditional Features are Effective\nAs a new reviewer posted just one review and we have to identify it immediately, the major challenge of the cold-start task is that, the available informations about the new reviewer are very poor. The new reviewer only provide us with one review record. For most traditional features based on the statistics, they can not form themselves or make no sense, such as the percentage of reviews written at weekends (Li et al., 2015), the entropy of rating distribution of user’s review (Rayana and Akoglu, 2015). To investigate whether traditional features are effective in the cold-start task, we conducted experiments on the Yelp dataset in Mukherjee et al. (2013c). We trained SVM models with different features on the existing reviews posted before January 1, 2012, and tested on the new reviews which just posted by the new reviewers after January 1, 2012. Results are shown in Table 1.\n\n3.1 Linguistic Features’ Poor Performance\nThe linguistic features need not to take much time to form. But Mukherjee et al. (2013c) have proved that the linguistic features are not effective enough in detecting real-life fake reviews from the commercial websites, compared with the performances on the crowd source datasets (Ott et al.,\n4\n301\n302\n303\n304\n305\n306\n307\n308\n309\n310\n311\n312\n313\n314\n315\n316\n317\n318\n319\n320\n321\n322\n323\n324\n325\n326\n327\n328\n329\n330\n331\n332\n333\n334\n335\n336\n337\n338\n339\n340\n341\n342\n343\n344\n345\n346\n347\n348\n349\n350\n351\n352\n353\n354\n355\n356\n357\n358\n359\n360\n361\n362\n363\n364\n365\n366\n367\n368\n369\n370\n391\n392\n393\n394\n395\n396\n397\n398\n399\n2011). They showed that the word bigrams perform better than the other linguistic features, such as LIWC (Newman et al., 2003; Pennebaker et al., 2007), part-of-speech sequence patterns (Mukherjee and Liu, 2010), deep syntax (Feng et al., 2012a), information gain (Mukherjee et al., 2013c) and so on. So, we conduct experiments with the word bigrams feature. As shown in Table 1 (a, b) row 1, the word bigrams result in only around 55% in accuracy in both the hotel and restaurant domains. It indicates that the most effective traditional linguistic feature (i.e., the word bigrams) can’t detect the review spam effectively in the cold start task.\n\n3.2 Behavioral Features only Work Well with\nAbundant Information\nBecause there is not enough available information about the new reviewer, for most traditional behavioral features based on the statistical mechanism, they couldn’t form themselves or make no sense. We investigated the previous work and found that there are three behavioral features can be applied to the cold-start task. They are proposed by Mukherjee et al. (2013b), i.e., 1.Review length (RL) : the length of the new review posted by the new reviewer; 2.Reviewer deviation (RD): the absolute rating deviation of the new reviewer’s review from other reviews on the same business; 3.Maximum content similarity (MCS) : the maximum content similarity (using cosine similarity) between the new reviewer’s review with other reviews on the same business.\nTable 1 (a, b) row 2 shows the experiment results by the combinations of the bigrams feature and the three behavioral features described above. The behavioral features make around 5% improvement in accuracy in the hotel domain (2.7% in the restaurant domain) as compared with only using bigrams. The accuracy is improved but it is just near 60% in average. It indicates that the traditional features are not effective enough with poor behavioral information. What’s more, the behavioral features cause around 4.6% decrease in F1score and around 19% decrease in Recall in both hotel and restaurant domains. It is obvious that there is more false-positive review spam caused by the behavioral features as compared to only using bigrams. It further indicates that the traditional behavioral features’ discrimination for review spam gets to be weakened by the poor behavioral infor-\nmation. To go a step further, we carried experiments with the three behavioral features which are formed on abundant behavioral information. When the new reviewers continue to post more reviews in after weeks, their behavioral information gets to be more. Then the review system could obtain more sufficient data to extract behavior features as compared to the poor information in the cold-start period. So the behavioral features with abundant information make an obvious improvement in accuracy (6.4%) in the hotel domain (Table 1 (a) row 3) as compared with the results in Table 1 (a) row 2. But it is only 0.6% in the restaurant domain. By statistics on the datasets, we found that the new reviewers posted about 54.4 reviews in average after their first post in the hotel domain, but it is only 10 reviews in average for the new reviewers in the restaurant domain. The added behavioral information in the hotel domain is richer than that in the restaurant domain. It indicates that:\n\n4 The Proposed Model\nThe difficulty of detecting review spam in the cold-start task is that the available behavioral information of new reviewers is very poor. The new reviewer just posted one review and we have to filter it out immediately, there is not any historical reviews provided to us. As we argued, the textual in-\n5\n401\n402\n403\n404\n405\n406\n407\n408\n409\n410\n411\n412\n413\n414\n415\n416\n417\n418\n419\n420\n421\n422\n423\n424\n425\n426\n427\n428\n429\n430\n431\n432\n433\n434\n435\n436\n437\n438\n439\n440\n441\n442\n443\n444\n445\n446\n447\n448\n449\n450\n451\n452\n453\n454\n455\n456\n457\n458\n459\n460\n461\n462\n463\n464\n465\n466\n467\n468\n469\n470\n471\n472\n473\n474\n475\n476\n477\n478\n479\n480\n481\n482\n483\n484\n485\n486\n487\n488\n489\n490\n491\n492\n493\n494\n495\n496\n497\n498\n499\nformation and the behavioral information of a reviewer are correlated with each other. So, to augment the behavioral information of new reviewers, we try to find the textual information which is similar with that of the new reviewer, from existing reviews. Then we take the behavioral information which is correlated with the found textual information as the most possible behavioral information of the new reviewer. For this purpose, we propose a neural network model to jointly encode the textual and behavioral information into the review embeddings for detecting the review spam in the cold-start problem (shown in Figure 2). When a new reviewer posts a review, the neural network can represent the review with the similar textual information and the correlated behavioral information encoded in the word embeddings. Finally, embeddings of the new review are fed into a classifier to identify whether it is spam or not.\n\n4.1 Behavioral Information Encoding\nIn Figure 1, there is a part of review graph which is simplified from the Yelp website. As it shows, the review graph contains the global behavioral information (footprints) of the existing reviewers. Because the motivations of the spammers and the real reviewers are totally different, the distributions of the behavioral information of them are different (Mukherjee et al., 2013a). There are businesses (even highly reputable ones) paying people to write fake reviews for them to promote their products/services and/or to discredit their competitors (Liu, 2015). So the behavioral footprints of the spammers are decided by the demands of the businesses. But the real reviewers only post reviews to the product or services they have actually experienced. Their behavioral footprints are influenced by their own characteristics. Previous work extracts behavioral features for reviewers from these behavioral information. But it is impractical to the new reviewers in the cold-start task. Moreover, the traditional discrete features can not effectively record the global behavioral information (Wang et al., 2016). Besides, there is no explicit characteristic tag available in the review system, and we need to find a way to record the reviewers’ latent characters information in footprints.\nTherefore we encode these behavioral information into our model by utilizing a embedding learning model which is similar with TransE (Bordes et al., 2013). TransE is a model which can encode the graph structure, and represent the nodes\nand edges (head, translation/relation, tail) in low dimension vector space. TransE has been proved that it is good at describing the global information of the graph structure by the work about distributional representation for knowledge base (Guu et al., 2015). We consider that each reviewer in review graph describes the product in his/her own view and writes the review. When we represent the product, reviewer and review in low dimension vector space, the reviewer embeddings can be taken as a translation vector, which has translated the product embeddings to the review embeddings. So, as shown in Figure 2, we take the products (hotels/restaurants) as the head part of the TransE network in our model, take the reviewers as the translation (relation) part and take the review as the tail part. By learning from the existing large scale unlabeled reviews of the review graph, we can encode the global behavioral information into our model without extracting any traditional behavioral feature, and record reviewers’ latent characteristics information.\nMore formally, we minimize a margin-based criterion over the training set:\nL = ∑\n(β,α,τ )∈S ∑ (β′,α,τ ′)∈S′ max\n{0, 1 + d(β +α, τ )− d(β′ +α, τ ′)} (1)\nS denotes the training set of triples (β,α, τ ) composed product β (β ∈ B, products set (head part)), reviewer α (α ∈ A, reviewers set (translation part)) and review text embeddings learnt by the CNN τ (τ ∈ T , review texts set (tail part)).\nS′ = {(β′,α, τ )|β′ ∈ B} ∪ {(β,α, τ ′)|τ ′ ∈ T} (2)\nThe set of corrupted triplets S′ (Equation (2)), is composed of training triplets with either the product or review text replaced by a random chosen one (but not both at the same time).\nd(β +α, τ ) = ‖β +α− τ‖22 , s.t. ‖β‖22 = ‖α‖ 2 2 = ‖τ‖ 2 2 = 1\n(3)\nd(β + α, τ ) is the dissimilarity function with the squared euclidean distance.\n\n4.2 Textual Information Encoding\nTo encode the textual information into our model, we adopt a convolutional neural network (CNN) to learn to represent the existing reviews. By statistics, we find that a review usually refers to several aspects of the products or services. For example, a hotel review may comment the room price, the free\n6\n501\n502\n503\n504\n505\n506\n507\n508\n509\n510\n511\n512\n513\n514\n515\n516\n517\n518\n519\n520\n521\n522\n523\n524\n525\n526\n527\n528\n529\n530\n531\n532\n533\n534\n535\n536\n537\n538\n539\n540\n541\n542\n543\n544\n545\n546\n547\n548\n549\n556\n557\n564\n565\n566\n567\n568\n569\n570\n571\n572\n573\n574\n575\n576\n577\n578\n579\n580\n581\n582\n583\n584\n585\n586\n587\n588\n589\n590\n591\n592\n593\n594\n595\n596\n597\n598\n599\nWiFi and the bathroom at the same time. Compared with the recurrent neural network (RNN), the CNN can do a better job of modeling the different aspects of a review. Ren and Zhang (2016) have proved that the CNN can capture complex global semantic information and detect review spam more effectively, compared with traditional discrete manual features and the RNN model. As shown in Figure 2, we take the learnt embeddings τ of reviews by the CNN as the tail part.\nSpecifically, we denote the review text consisting of n words as {w1, w2, ..., wn}, the word embeddings e(wi) ∈ RD, D is the word vector dimension. We take the concatenation of the word embeddings in a fixed length window size Z as the input of the linear layer, which is denoted as Ii ∈ RD×Z . So the output of the linear layer Hi is calculated by Hk,i = Wk · Ii + bi, where Wk ∈ RD×Z is the weight matrix of filter k. We utilize a max pooling layer to get the output of each filter. Then we take tanh as the activation function and concatenate the outputs as the final review embeddings, which is denoted as τi.\n\n4.3 Jointly Information Encoding\nTo model the correlation of the textual and behavioral information, we employ the jointly information encoding. By jointly learning from the global review graph, the textual and behavioral information of existing spammers and real reviewers are embedded into the word embeddings.\nIn addition, the rating usually represents the sentiment polarity of a review, e.g., five star means ‘like’ and one star means ‘dislike’. The spammers often review their target products with low rating for discredited purpose, and with high rating for promoted purpose. To encode the semantics of the sentiment polarity into the review embeddings, we learn the embeddings of 1-5 stars rating in our model at the same time. They are taken as the constraints of the review embeddings during the joint learning. They are calculated as:\nC = ∑\n(τ ,γ)∈Γ ∑ (τ ,γ′)∈Γ′ max{0, 1+ g(τ ,γ)− g(τ ,γ′)} (4)\nThe set of corrupted tuples Γ′ is composed of training tuples Γ with the rating of review replaced by its opposite rating (i.e., 1 by 5, 2 by 4, 3 by 1 or 5). g(τ ,γ) = ‖τ − γ‖22, norm constraints: ‖γ‖22 = 1.\nThe final joint loss function is as follows: LJ = (1− θ)L+ θC (5)\nwhere θ is a hyper-parameter.\n\n5 Experiments\n\n\n5.1 Datasets and Evaluation Metrics\nDatasets: To evaluate the proposed method, we conducted experiments on Yelp dataset that was used in (Mukherjee et al., 2013b,c; Rayana and Akoglu, 2015). The statistics of the Yelp dataset are listed in Table 2 and Table 3. The reviewed product here refers to a hotel or restaurant. We take the existing reviews posted before January 1, 2012 as the training datasets, and take the first new reviews which just posted by the new reviewers after January 1, 2012 as the test datasets. Evaluation Metrics: We select precision (P), recall (R), F1-Score (F1), accuracy (A) as metrics.\n\n5.2 Our Model v.s. the Traditional Features\nTo illustrate the effectiveness of our model, we conduct experiments on the public datasets, and make comparison with the most effective traditional linguistic features, e.g., bigrams, and the three practicable traditional behavioral features (RL, RD, MCS (Mukherjee et al., 2013b)) referred in Section 3.2. The results are shown in Table 4. For our model, we set the dimension of embeddings to 100, the number of CNN filters to 100, θ to 0.1, Z to 2. The hyper-parameters are tuned by grid search on the development dataset. The product and reviewer embeddings are randomly initialized from a uniform distribution (Socher et al., 2013). The word embeddings are initialized with 100-dimensions vectors pre-trained by the CBOW model (Word2Vec) (Mikolov et al., 2013).As Table 4 showed, our model observably performs better in detecting review spam for the cold-start task in both hotel and restaurant domains.\n7\n601\n602\n603\n604\n605\n606\n607\n608\n609\n610\n611\n612\n613\n614\n615\n616\n617\n618\n619\n620\n621\n622\n623\n624\n625\n626\n627\n628\n629\n630\n631\n632\n633\n634\n635\n636\n637\n638\n639\n640\n641\n642\n643\n644\n645\n646\n647\n648\n649\n650\n651\n652\n653\n654\n655\n656\n657\n658\n659\n660\n661\n662\n663\n664\n665\n666\n667\n668\n669\n670\n671\n672\n673\n674\n675\n676\n677\n678\n679\n680\n681\n682\n683\n684\n685\n686\n687\n688\n689\n690\n691\n692\n693\n694\n695\n696\n697\n698\n699\nFeatures P R F1 A LF 54.5 71.1 61.7 55.9 1\nLF+BF 63.4 52.6 57.5 61.1 2 BF EditSim+LF 55.3 69.7 61.6 56.6 3 BF W2Vsim+W2V 58.4 65.9 61.9 59.5 4 Ours RE 62.1 68.3 65.1 63.3 5 Ours RE+RRE+PRE 63.6 71.2 67.2 65.4 6 (a) Hotel\nP R F1 A 53.8 80.8 64.6 55.8 1 58.1 61.2 59.6 58.5 2 53.9 82.2 65.1 56.0 3 56.3 73.4 63.7 58.2 4 58.4 75.1 65.7 60.8 5 59.0 78.8 67.5 62.0 6\n(b) Restaurant\nTable 4: SVM classification results across linguistic features (LF, bigrams here (Mukherjee et al., 2013b)), behavioral features (BF: RL, RD, MCS (Mukherjee et al., 2013b)); the SVM classification results by the intuitive method that finding the most similar existing review by edit distance ratio and take the found reviewers’ behavioral features as approximation (BF EditSim+LF), and results by the intuitive method that finding the most similar existing review by averaged pre-trained word embeddings (using Word2Vec) (BF W2Vsim+W2V); and the SVM classification results across the learnt review embeddings (RE), the learnt review’s rating embeddings (RRE), the learnt product’s average rating embeddings (PRE) by our model. Improvements of our model are statistically significant with p<0.005 based on paired t-test.\nReview Embeddings Compared with the traditional linguistic features, e.g., bigrams, using the review embeddings learnt by our model, results in around 3.4% improvement in F1 and around 7.4% improvement in A in the hotel domain (1.1% in F1 and 5.0% in A for the restaurant domain, shown in Tabel 4 (a,b) rows 1, 5). Compared with the combination of the bigrams and the traditional behavioral features, using the review embeddings learnt by our model, results in around 7.6% improvement in F1 and around 2.2% improvement in A in the hotel domain (6.1% in F1 and 2.3% in A for the restaurant domain, shown in Tabel 4 (a,b) rows 2, 5). The F1-Score (F1) of the classification under the balance distribution reflects the ability of detecting the review spam. The accuracy (A) of the classification under the balance distribution reflects the ability of identifying both the review spam and the real review. The experiment results indicate that our model performs significantly better than the traditional methods in F1 and A at the same time. The learnt review embeddings with encoded linguistic and behavioral information are more effective in detecting review spam for the cold-start task. Rating Embeddings As we referred in Section 4.3, the rating of a review usually means the sentiment polarity of a real reviewer or the motivation of a spammer. As shown in Table 4 (a,b) rows 6, adding the rating embeddings of the products (hotel/restaurant) and reviews renders even higher F1 and A. We suppose that different rating embeddings are encoded with different semantic meanings. They reflect the semantic divergences be-\ntween the average rating of the product and the review rating. In results, using RE+RRE+PRE which makes the best performance of our model, results in around 5.5% improvement in F1 and around 9.5% improvement in A in the hotel domain (2.9% in F1 and 6.2% in A for the restaurant domain, shown in Tabel 4 (a,b) rows 1, 6), compared with the LF. Using RE+RRE+PRE results in around 9.7% improvement in F1 and around 4.3% improvement in A in the hotel domain (7.9% in F1 and 3.5% in A for the restaurant domain, shown in Tabel 4 (a,b) rows 2, 6), compared with the LF+BF.\nThe experiment results proves that our model is effective. The improvements in both the F1 and A prove that our model performs well in both detecting the review spam and identifying the real review. Furthermore, the improvements in both the hotel and restaurant domains prove that our model possesses preferable domain-adaptability 2. It can learn to represent the reviews with global linguistic and behavioral information from large scale unlabeled existing reviews.\n\n5.3 Our Jointly Embeddings v.s. the Intuitive Methods\nAs mentioned in Section 1, to approximate the behavioral information of the new reviewers, there are other intuitive methods. So we conduct experiments with two intuitive methods as com-\n2The improvements in hotel domain are greater than that in restaurant domain. The possible reason is the proportion of the available training data in hotel domain is higher than that in restaurant domain (99.01% vs. 97.40% in Table 2).\n8\n701\n702\n703\n704\n705\n706\n707\n708\n709\n710\n711\n712\n713\n714\n715\n716\n717\n718\n719\n720\n721\n722\n723\n724\n725\n726\n727\n728\n729\n730\n731\n732\n733\n734\n735\n736\n737\n738\n739\n740\n741\n742\n743\n744\n745\n746\n747\n748\n749\n750\n751\n752\n753\n754\n755\n756\n757\n758\n759\n760\n761\n762\n763\n764\n765\n766\n767\n768\n769\n770\n771\n772\n773\n774\n775\n776\n777\n778\n779\n780\n781\n782\n783\n784\n785\n786\n787\n788\n789\n790\n791\n792\n793\n794\n795\n796\n797\n798\n799\nFeatures P R F1 A LF 54.5 71.1 61.7 55.9 1 Ours CNN 61.2 51.7 56.1 59.5 2 Ours RE 62.1 68.3 65.1 63.3 3\n(a) Hotel\nP R F1 A 53.8 80.8 64.6 55.8 1 56.9 58.8 57.8 57.1 2 58.4 75.1 65.7 60.8 3\n(b) Restaurant\nTable 5: SVM classification results across linguistic features (LF, bigrams here (Mukherjee et al., 2013b)), the learnt review embeddings (RE) ; and the classification results by only using our CNN. Both training and testing use balanced data (50:50). Improvements of our model are statistically significant with p<0.005 based on paired t-test.\nparison. One is finding the most similar existing review by edit distance ratio and taking the found reviewers’ behavioral features as approximation, and then training the classifier on the behavioral features and bigrams (BF EditSim+LF). The other is finding the most similar existing review by cosine similarity of review embeddings which is the average of the pre-trained word embeddings (using Word2Vec), and then training the classifier on the behavioral features and review embeddings (BF W2Vsim+W2V). As shown in Table 4, our jointly embeddings (Ours RE and Ours RE+RRE+PRE) obviously perform better than the intuitive methods, such as the Ours RE is 3.8% (Accuracy) and 3.2% (F1) better than BF W2Vsim+W2V in the hotel domain. The experiments indicate that, our jointly embeddings do a better job in capturing the reviewer’s characteristics and modeling the correlation of textual and behavioral information.\n\n5.4 The Effectiveness of Encoding the Global\nBehavioral Information\nTo further evaluate the effectiveness of encoding the global behavioral information in our model, we build an independent supervised convolutional neural network which has the same structure and parameter settings with the CNN part of our model. There is not any review graphic or behavioral information in this independent supervised CNN (Tabel 5 (a,b) row 2). As shown in Tabel 5 (a,b) rows 2, 3, compared with the review embeddings learnt by the independent supervised CNN, using the review embeddings learnt by our model results in around 9.0% improvement in F1 and around 3.8% improvement in A in the hotel domain (7.9% in F1 and 3.7% in A for the restaurant domain. The results show that our model can represent the new reviews posted by the new reviewers with the correlated behavioral information encoded in the word embeddings. The transE part of our model has effectively recorded the behavioral informa-\ntion of the review graph. Thus, our model is more effective by jointly embedding the textual and behavioral informations, it helps to augment the possible behavioral information of the new reviewer.\n\n5.5 The Effectiveness of CNN\nCompared with the the most effective linguistic features, e.g., bigrams, our independent supervised convolutional neural network performs better in A than F1 (shown in Tabel 4 (a,b) rows 1, 2). It indicates that the CNN do a better job in identifying the real review than the review spam. We suppose that the possible reason is that the CNN is good at modeling the different semantic aspects of a review. And the real reviewers usually tend to describe different aspects of a hotel or restaurant according to their real personal experiences, but the spammers can only forge fake reviews with their own infinite imagination. Mukherjee et al. (2013b) also proved that different psychological states of the minds of the spammers and non-spammers, lead to significant linguistic differences between review spam and non-spam.\n\n6 Conclusion and Future Work\nThis paper analyzes the importance and difficulty of the cold-start challenge in review spam combat. We propose a neural network model that jointly embeds the existing textual and behavioral information for detecting review spam in the coldstart task. It can learn to represent the new review of the new reviewer with the similar textual information and the correlated behavioral information in an unsupervised way. Then, a classifier is applied to detect the review spam. Experimental results prove the proposed model achieves an effective performance and possesses preferable domain-adaptability. It is also applicable to a large scale dataset in an unsupervised way. To our best knowledge, this is the first work to handle the coldstart problem in review spam detection. We are going to explore more effective models in future.\n9\n801\n802\n803\n804\n805\n806\n807\n808\n809\n810\n811\n812\n813\n814\n815\n816\n817\n818\n819\n820\n821\n822\n823\n824\n825\n826\n827\n828\n829\n830\n831\n832\n833\n834\n835\n836\n837\n838\n839\n840\n841\n842\n843\n844\n845\n846\n847\n848\n849\n850\n851\n852\n853\n854\n855\n856\n857\n858\n859\n860\n861\n862\n863\n864\n865\n866\n867\n868\n869\n870\n871\n872\n873\n874\n875\n876\n877\n878\n879\n880\n881\n882\n883\n884\n885\n886\n887\n888\n889\n890\n891\n892\n893\n894\n895\n896\n897\n898\n899\n",
    "rationale": "This paper investigates the cold-start problem in review spam detection. The\nauthors first qualitatively and quantitatively analyze the cold-start problem.\nThey observe that there is no enough prior data from a new user in this\nrealistic scenario. The traditional features fail to help to identify review\nspam. Instead, they turn to rely on the abundant textual and behavioral\ninformation of the existing reviewer to augment the information of a new user.\nIn specific, they propose a neural network to represent the review of the new\nreviewer with the learnt word embedding and jointly encoded behavioral\ninformation. In the experiments, the authors make comparisons with traditional\nmethods, and show the effectiveness of their model.\n\n- Strengths:\n\nThe paper is well organized and clearly written. The idea of jointly encoding\ntexts and behaviors is interesting. The cold-start problem is actually an\nurgent problem to several online review analysis applications. In my knowledge,\nthe previous work has not yet attempted to tackle this problem. This paper is\nmeaningful and presents a reasonable analysis. And the results of the proposed\nmodel can also be available for downstream detection models.\n\n- Weaknesses:\n\nIn experiments, the author set the window width of the filters in the CNN\nmodule to 2. Did the author try other window widths, for example width `1' to\nextract unigram features, `3' to trigram, or use them together? \nThe authors may add more details about the previous work in the related work\nsection. More specifically description would help the readers to understand the\ntask clearly.\n\nThere are also some typos to be corrected:\nSec 1: ``...making purchase decision...'' should be ``making a/the purchase\ndecision''\nSec 1: ``...are devoted to explore... '' should be `` are devoted to\nexploring''\nSec 1: ``...there is on sufficient behaviors...'' should be “there are no\nsufficient behaviors''\nSec 1: ``...on business trip...'' should be ``on a business trip''\nSec 1: ``...there are abundant behavior information...'' should be ``there is\nabundant behavior''\nSec 3: ``The new reviewer only provide us...'' should be ``...The new reviewer\nonly provides us...''\nSec 3: ``...features need not to take much...'' should be ``...features need\nnot take much...''\nSec 4: ``...there is not any historical reviews...'' should be ``...there are\nnot any historical reviews...''\nSec 4: ``...utilizing a embedding learning model...'' should be ``...utilizing\nan embedding learning model...''\nSec 5.2 ``...The experiment results proves...'' should be ``...The experiment\nresults prove...''\n\n- General Discussion:\n\nIt is a good paper and should be accepted by ACL.",
    "rating": 5
  },
  {
    "title": "Creating Training Corpora for NLG Micro-Planning",
    "abstract": "In this paper, we focus on how to create data-to-text corpora which can support the learning of wide-coverage microplanners i.e., generation systems that handle lexicalisation, aggregation, surface realisation, sentence segmentation and referring expression generation. We start by reviewing common practice in designing training benchmarks for Natural Language Generation. We then present a novel framework for semi-automatically creating linguistically challenging NLG corpora from existing Knowledge Bases. We apply our framework to DBpedia data and compare the resulting dataset with (Wen et al., 2016)’s dataset. We show that while (Wen et al., 2016)’s dataset is more than twice larger than ours, it is less diverse both in terms of input and in terms of text. We thus propose our corpus generation framework as a novel method for creating challenging data sets from which NLG models can be learned which are capable of generating text from KB data.",
    "text": "1 Introduction\nTo train Natural Language Generation (NLG) systems, various input-text corpora have been developed which associate (numerical, formal, linguistic) input with text. As discussed in detail in Section 2, these corpora can be classified into three main types namely, (i) domain specific corpora, (ii) benchmarks constructed from “Expert” Linguistic Annotations and (iii) crowdsourced benchmarks1.\n1We ignore here (Lebret et al., 2016)’s dataset which was created fully automatically from Wikipedia by associating infoboxes with text because this dataset fails to ensure an\nIn this paper, we focus on how to create datato-text corpora which can support the learning of wide-coverage micro-planners i.e., generation systems that handles such NLG subtasks as lexicalisation (mapping data to words), aggregation (exploiting linguistic constructs such as ellipsis and coordination to avoid repetition), surface realisation (using the appropriate syntactic constructs to build sentences), sentence segmentation and referring expression generation.\nWe start by reviewing the main existing types of NLG benchmarks and we argue for a crowdsourcing approach where data units are automatically built from an existing knowledge base and where text is crowdsourced from the data (Section 2).\nWe then propose a generic framework for semiautomatically creating training corpora for NLG (Section 3) from existing Knowledge Bases. In Section 4, we apply this framework to DBpedia data and we compare the resulting dataset with (Wen et al., 2016)’s using various metrics to evaluate the linguistic and computational adequacy of both datasets. By applying these metrics, we show that while (Wen et al., 2016)’s dataset is more than twice larger than ours, it is less diverse both in terms of input and in terms of text. We also compare the performance of a sequence-to-sequence model (Vinyals et al., 2015) on both datasets to estimate the complexity of the learning task induced by each dataset. We show that the performance of this neural model is much lower on the new data set than on the existing ones. We thus propose our corpus generation framework as a novel method for creating challenging data sets from which NLG\nadequate match between data and text. We manually examined 50 input/output pairs randomly extracted from this dataset and did not find a single example where data and text matched. As such, this dataset is ill-suited for training microplanners. Moreover, since its texts contain both missing and additional information, it cannot be used to train joint models for content selection and micro-planning either.\nmodels can be learned which are capable of generating complex texts from KB data.\n\n2 NLG Benchmarks\nDomain Specific Benchmarks. Several domain specific data-text corpora have been built by researchers to train and evaluate NLG systems. In the sports domain, Chen and Mooney (2008) constructed a dataset mapping soccer games events to text which consists of 1,539 data-text pairs and a vocabulary of 214 words. For weather forecast generation, (Liang et al., 2009)’s dataset includes 29,528 data-text pairs with a vocabulary of 345 words. For the air travel domain, Ratnaparkhi (2000) created a dataset consisting of 5,426 datatext pairs with a richer vocabulary (927 words) and in the biology domain, the KBGen shared task (Banik et al., 2013) made available 284 data-text pairs where the data was extracted from an existing knowledge base and the text was authored by biology experts.\nAn important limitation of these datasets is that, because they are domain specific, systems learned from them are restricted to generating domain specific, often strongly stereotyped text (e.g., weather forecast or soccer game commentator reports). Arguably, training corpora for NLG should support the learning of wide-coverage generators. By nature however, domain specific corpora restrict the lexical and often the syntactic coverage of the texts to be produced and thereby indirectly limit the expressivity of the generators trained on them.\nBenchmarks Constructed from “Expert” Linguistic Annotations. NLG benchmarks have also been proposed where the input data is either derived from dependency parse trees (SR’11 task, (Belz et al., 2011)) or constructed through manual annotation (AMR Corpus (Banarescu et al., 2012)). Contrary to the domain-specific data sets just mentioned, these corpora have a wider coverage and are large enough for training systems that can generate linguistically sophisticated text.\nOne main drawback of these benchmarks however is that their construction required massive manual annotation of text with complex linguistic structures (parse trees for the SR task and Abstract Meaning Representation for the AMR corpus). Moreover because these structures are complex, the annotation must be done by experts. It cannot be delegated to the crowd. In short, the creation of such benchmark is costly both in terms\nof time and in terms of expertise. Another drawback is that, because the input representation derived from a text is relatively close to its surface form2, the NLG task is mostly restricted to surface realisation (mapping input to sentences). That is, these benchmarks give very limited support for learning models that can handle micro-planning NLG subtasks such as lexicalisation, aggregation, sentence segmentation and referring expression generation.\nCrowdsourced Benchmarks. More recently, data-to-text benchmarks have also been created by associating data units with text using crowdsourcing.\nWen et al. (2016) first created data by enumerating all possible combinations of 14 dialog act types (e.g., request, inform) and attribute-value pairs present in four small-size, hand-written ontologies about TVs, laptops, restaurants and hotels. They then use crowdsourcing to associate each data unit with a text. The resulting dataset is both large and varied (4 domains) and was successfully exploited to train neural and imitation learning data-to-text generator (Wen et al., 2016; Lampouras and Vlachos, 2016). Similarly, Novikova and Rieser (2016) described a framework for collecting data-text pairs using automatic quality control measures and evaluating how the type of the input representations (text vs pictures) impacts the quality of crowdsourced text.\nThe crowdsourcing approach to creating inputtext corpora has several advantages.\nFirst, it is low cost in that the data is produced automatically and the text is authored by a crowdworker. This is in stark contrast with the previous approach where expert linguists are required to align text with data.\nSecond, because the text is crowd-sourced from the data (rather than the other way round), there is an adequate match between text and data both semantically (the text expresses the information contained in the data) and computationally (the data is sufficiently different from the text to require the learning of complex generation operations such as sentence segmentation, aggregation and referring expression generation).\n2For instance, the input structures made available by the shallow track of the SR task contain all the lemmas present in the corresponding text. In this case, the generation task is limited to determining (i) the linear ordering and (ii) the full form of the word in the input.\nThird, by exploiting small hand-written ontologies to quickly construct meaningful artificial data, the crowdsourcing approach allows for the easy creation of a large dataset with data units of various size and bearing on different domains. This, in turn, allows for better linguistic coverage and for NLG tasks of various complexity since typically, inputs of larger size increases the need for complex microplanning operations.\n\n3 A Framework for Creating Data-to-Text, Micro-Planning Benchmarks\nWhile as just noted, the crowdsourcing approach presented in (Wen et al., 2016) has several advantages, it also has a number of shortcomings.\nOne important drawback is that it builds on artificial rather than “real” data i.e., data that would be extracted from an existing knowledge base. As a result, the training corpora built using this method cannot be used to train KB verbalisers i.e., generation systems that can verbalise KB fragments.\nAnother limitation concerns the shape of the input data. (Wen et al., 2016)’s data can be viewed as trees of depth one (a set of attributes-value pairs describing a single entity e.g., a restaurant or a laptop). As illustrated in Figure 1 however, there is a strong correlation between the shape of the input and the syntactic structure of the corresponding sentence. The path structure T1 where B is shared by two predicates (mission and operator) will favour the use of a participial or a passive subject relative clause. In contrast, the branching structure T2 will favour the use of a new clause with a pronominal subject or a coordinated VP. More generally, allowing for trees of deeper depth is necessary to indirectly promote the introduction in the benchmark of a more varied set of syntactic constructs to be learned by generators.\nTo address these issues, we introduce a novel method for creating data-to-text corpora from large knowledge bases such as DBPedia. Our method combines (i) a content selection module designed to extract varied, relevant and coherent data units from DBPedia with (ii) a crowdsourcing process for associating data units with human authored texts that correctly capture their meaning. Example 1 shows a data/text unit created by our method using DBPedia as input KB.\n(1) a. (John E Blaha birthDate 1942 08 26) (John E Blaha birthPlace San Antonio) (John E Blaha occupation Fighter pilot)\nb. John E Blaha, born in San Antonio on 1942-08-26, worked as a fighter pilot\nOur method has the following features. First, it can be used to create a data-to-text corpus from any knowledge base where entities are categorised and there is a large number of entities belonging to the same category. As noted above, this means that the resulting corpus can be used to train KB verbalisers i.e., generators that are able to verbalise fragments of existing knowledge bases. It could be used for instance, to verbalise fragments of e.g., MusicBrainz3, FOAF4 or LinkedGeoData5.\nSecond, as crowdworkers are required to enter text that matches the data and a majority vote validation process is used to eliminate mis-matched pairs, there is a direct match between text and data. This allows for a clear focus on the non content selection part of generation known as microplanning.\nThird, because data of increasing size is matched with texts ranging from simple clauses to short texts consisting of several sentences, the resulting benchmark is appropriate for exercising the main subtasks of microplanning. For instance, in Example (1) above, given the input shown in (1a), generating (1b) involves lexicalising the occupation property as the phrase worked as (lexicalisation); using PP coordination (born in San Antonio on 1942-08-26) to avoid repeating the word born (aggregation); and verbalising the three triples using a single complex sentence including an apposition, a PP coordination and a transitive verb construction (sentence segmentation and surface realisation).\n3https://musicbrainz.org/ 4http://www.foaf-project.org/ 5http://linkedgeodata.org/\n\n3.1 DBPedia\nTo illustrate the functioning of our benchmark creation framework, we apply it to DBPedia. DBPedia is a multilingual knowledge base that was built from various kinds of structured information contained in Wikipedia (Mendes et al., 2012). This data is stored as RDF (Resource Description Format) triples of the form (subject, property, object) where the subject is a URI (Uniform Resource Identifier), the property is a binary relation and the object is either a URI or a literal value such as a string, a date or a number. We use an English version of the DBPedia knowledge base which encompasses 6.2M entities, 739 classes, 1,099 properties with reference values and 1,596 properties with typed literal values.6\n\n3.2 Selecting Content\nTo create data units, we follow the procedure outlined in (Perez-Beltrachini et al., 2016) and sketched in Figure 2. This method can be summarised as follows.\nFirst, DBPedia category graphs are extracted from DBPedia by retrieving up to 500 entity graphs for entities of the same category7. For example, we build a category graph for the Astronaut category by collecting, graphs of depth five for 500 entities of types astronaut.\nNext, category graphs are used to learn bigram models of DBPedia properties which specify the probability of two properties co-occuring together. Three types of bi-gram models are extracted from category graphs using the SRILM toolkit: one model (S-Model) for bigrams occurring in sibling triples (triples with a shared subject); one model (C-Model) for bigrams occurring in chained triples (the object of one triple is the subject of the other); and one model (M-Model) which is a linear interpolation of the sibling and the chain model. The intuition is that these sibling and chain models capture different types of coherence, namely, topic-based coherence for the S-Model and discourse-based coherence for the CModel.\nFinally, the content selection task is formulated as an Integer Linear Programming (ILP) problem to select, for a given entity of category C and its\n6http://wiki.dbpedia.org/ dbpedia-dataset-version-2015-10\n7An entity graph for some entity e is a graph obtained by traversing the DBPedia graph starting in e and stopping at depth five.\nentity graph Ge, subtrees of Ge with maximal bigram probability and varying size (between 1 and 7 RDF triples).\nWe applied this content selection procedure to the DBPedia categories Astronaut (A), Building (B), Monument (M), University (U), Sports team (S) and Written work (W), using the three bi-gram models (S-Model, C-Model, M-Model) and making the number of triples required by the ILP constraint to occur in the output solutions vary between 1 and 7. The results are shown in Table 1. An input is a set of triples produced by the content selection module. The number of input is thus the number of distinct sets of triples produced by this module. In contrast, input patterns are inputs where subject and object have been abstracted over. That is, the number of input patterns is the number of distinct sets of properties present in the set of inputs. The number of properties is the number of distinct RDF properties occurring in the dataset. Similarly, the number of entities is the number of distinct RDF subjects and objects occurring in each given dataset.\n\n3.3 Associating Content with Text\nWe associate data with text using the Crowdflower platform8. We do this in four main steps as follows.\n1. Clarifying Properties. One difficulty when collecting texts verbalising sets of DBPedia triples\n8http://www.crowdflower.com\nis that the meaning of DBPedia properties may be unclear. We therefore first manually clarified for each category being worked on, those properties which have no obvious lexicalisations (e.g., crew1up was replaced by commander).\n2. Getting Verbalisations for Single Triples. Next, we collected three verbalisations for data units of size one, i.e. single triples consisting of a subject, a property and an object. For each such input, crowdworkers were asked to produce a sentence verbalising its content. We used both a priori automatic checks to prevent spamming and a posteriori manual checks to remove incorrect verbalisations. We also monitored crowdworkers as they entered their input and banned those who tried to circumvent our instructions and validators. The automatic checks comprise 12 custom javascript validators implemented in the CrowdFlower platform to block contributor answers which fail to meet requirements such as the minimal time a contributor should stay on page, the minimal length of the text produced, the minimal match of tokens between a triple and its verbalisation and various format restrictions used to detect invalid input. The exact match between a triple and its verbalisation was also prohibited. In addition, after data collection was completed, we manually checked each data-text pair and eliminated from the data set any pair where the text either did not match the information conveyed by the triple or was not a well-formed English sentence.\n3. Getting Verbalisations for Input containing more than one Triple. The verbalisations collected for single triples were used to construct input with bigger size. Thus, for input with a number of triples more than one, the crowd was asked to merge the sentences corresponding to each triple (obtained in step 2) into a natural sounding text. In such a way, we diminish the risk of having misinterpretations of the original semantics of a data unit. Contributors were also encouraged to change the order, and the wording of sentences, while writing their texts. For each data unit, we collected three verbalisations.\n4. Verifying the Quality of the Collected Texts. The verbalisations obtained in Step 3 were verified through crowdsourcing. Each verbalisation collected in Step 3 was displayed to CrowdFlower contributors together with the corresponding set of triples. Then the crowd was asked to assess its\nfluency, semantic adequacy, and grammaticality. Those criteria were checked by asking the following three questions: Does the text sound fluent and natural?, Does the text contain all and only the information from the data?, Is the text good English (no spelling or grammatical mistakes)?. We collected five answers per verbalisation. A verbalisation was considered as bad, if it received three negative answers in at least one criterion. After the verification step, the total corpus loss was of 8.7%.\nTable 2 shows some statistics about the text obtained using our crowdsourcing procedure.\n\n4 Comparing Benchmarks\nWe now compare a dataset created using our dataset creation framework (henceforth DBPNLG) with (Wen et al., 2016)’s dataset9 (henceforth, RNNLG). Example 2 shows a sample data-text pair taken from the RNNLG dataset. The DBPNLG dataset has been uploaded with this submission. (2) Dialog Moves\nrecommend(name=caerus 33;type=television; screensizerange=medium;family=t5;hasusbport=true) The caerus 33 is a medium television in the T5 family that’s USB-enabled\nAs should be clear from the discussion in Section 2 and 3, both datasets are similar in that, in both cases, data is built from ontological information and text is crowdsourced from the data. An important difference between the two datasets is that, while the RNNLG data was constructed by enumerating possible combinations of dialog act types and attribute-value pairs, the DBPNLG data is created using a sophisticated content selection procedure geared at producing sets of data units that are relevant for a given ontological category and that are varied in terms of size, shape and content (Perez-Beltrachini et al., 2016). We now investigate the impact of this difference on the two datasets (DBPNLG and RNNLG). To assess the degree to which both datasets support the generation of linguistically varied text requiring complex microplanning operations, we examine a number of data and text related metrics. We also compare the results of an out-of-the-box sequence-to-sequence model as a way to estimate the complexity of the learning task induced by each dataset.\n\n4.1 Data Comparison\nTerminology. The attributes in (Wen et al., 2016)’s dataset can be viewed as binary relations between\n9https://github.com/shawnwun/RNNLG\nthe object talked about (a restaurant, a laptop, a TV or a hotel) and a value. Similarly, in DBPNLG, DBpedia RDF properties relate a subject entity to an object which can be either an entity or a datatype value. In what follows, we refer to both as attributes.\nTable 3 shows several statistics which indicate that, while the RNNLG dataset is larger than DBPNLG, DBPNLG is much more diverse in terms of attributes, input patterns and input shapes.\nNumber of attributes. As illustrated in Example (3) below, different attributes can be lexicalised using different parts of speech. A dataset with a larger number of attributes is therefore more likely to induce texts with greater syntactic variety.\n(3) Verb: X title Y / X served as Y Relational noun: X nationality Y / X’s nationality is Y Preposition: X country Y / X is in Y Adjective: X nationality USA / X is American\nAs shown in Table 3, DBPNLG has a more diverse attribute set than RNNLG both in absolute (172 attributes in DBPNLG against 108 in RNNLG) and in relative terms (RNNLG is a little more than twice as large as DBPNLG).\nNumber of Input Patterns. Since attributes may give rise to lexicalisation with different parts of speech, the sets of attributes present in an input (input pattern10) indirectly determine the syntactic realisation of the corresponding text. Hence a higher number of input patterns will favour a higher number of syntactic realisations. This is exemplified in Example (4) where two inputs with the same number of attributes give rise to texts with different syntactic forms. While in Example (4a), the attribute set { country, location, startDate } is realised by a passive (is located), an apposition (Australia) and a deverbal nominal (its construction), in Example (4b), the attribute set { almaMater, birthPlace, selection } induced a passive (was born) and two VP coordinations (graduated and joined).\n10Recall from section 3 that input patterns are inputs where subjects and objects have been remove thus, in essence, an input pattern is the set of all the attributes occurring in a given input.\n(4) a. (‘108 St Georges Terrace location Perth’, ‘Perth country Australia’, ‘108 St Georges Terrace startDate 1981’) country, location, startDate 108 St. Georges Terrace is located in Perth, Australia. Its construction began in 1981. passive, apposition, deverbal nominal\nb. (‘William Anders selection 1963’, ‘William Anders birthPlace British Hong Kong’, ‘William Anders almaMater ”AFIT, M.S. 1962”’) almaMater, birthPlace, selection William Anders was born in British Hong Kong, graduated from AFIT in 1962, and joined NASA in 1963. passive, VP coordination, VP coordination\nAgain, despite the much larger size of the RNNLG dataset, the number of input patterns in both datasets is almost the same. That is, the relative variety in input patterns is higher in DBPNLG.\nNumber of Input / Number of Input Patterns. The ratio between number of inputs and the number of input patterns has an important impact both in terms of linguistic diversity and in terms of learning complexity. A large ratio indicates a “repetitive dataset” where the same pattern is instantiated a high number of times. While this facilitates learning, this also reduces linguistic coverage (less combinations of structures can be learned) and may induce overfitting. Note that because datasets are typically delexicalised when training NLG models (cf. e.g., (Wen et al., 2015; Lampouras and Vlachos, 2016)) , at training time, different instantiations of the same input pattern reduce to identical input.\nThe two datasets markedly differ on this ratio which is five times lower in DBPNLG. While in DBPNLG, the same pattern is instantiated in average 2.40 times, it is instantiated 10.31 times in average in RNNLG. From a learning perspective, this means that the RNNLG dataset facilitates learning but also makes it harder to assess how well systems trained on it can generalise to handle unseen input.\nInput Shape. As mentioned in Section 3, in the RNNLG dataset, all inputs can be viewed as trees of depth one while in the DBPNLG dataset, input may have various shapes. As a result, RNNLG texts will be restricted to syntactic forms which permit expressing such multiple predications of the same\nentity e.g., subject relative clause, VP and sentence coordination etc. In contrast, the trees extracted by the DBPNLG content selection procedure may be of depth five and therefore allow for further syntactic constructs such as object relative clause and passive participials (cf. Figure 1).\nWe can show this empirically as well that DBPNLG is far more diverse than RNNLG in terms of input shapes. The RNNLG dataset has only 6 distinct shapes and all of them are of depth 1, i.e., all (attribute, value) pairs in an input are siblings to each other. In contrast, the DBPNLG dataset has 58 distinct shapes, out of which only 7 shapes are with depth 1, all others have depth more than 1 and they cover 49.6% of all inputs.\n\n4.2 Text Comparison\nTable 4 gives some statistics about the texts contained in each dataset.\n(5) a. (Alan Bean birthDate “1932-03-15”) Alan Bean was born on March 15, 1932\n(6) a. (‘Alan Bean nationality United States’, ‘Alan Bean birthDate “1932-03-15”’, ‘Alan Bean almaMater “UT Austin, B.S. 1955”’, ‘Alan Bean birthPlace Wheeler, Texas’, ‘Alan Bean selection 1963’) Alan Bean was an American astronaut, born on March 15, 1932 in Wheeler, Texas. He received a Bachelor of Science degree at the University of Texas at Austin in 1955 and was chosen by NASA in 1963.\nAs illustrated by the contrast between example 5 and 6 above, text length (number of tokens per text) and the number of sentences per text are strong indicators of the complexity of the generation task. We use the Stanford Part-Of-Speech Tagger and Parser version 3.5.2 (date 2015-04-20) to tokenize and to perform sentence segmentation on text. As shown in Table 4, DBPNLG’s texts are longer both in terms of tokens and in terms of number of sentences per text. Another difference between the two datasets is that DBPNLG contains a\nhigher number of text per input thereby providing a better basis for learning paraphrases.\nThe size and the content of the vocabulary is another important factor in ensuring the learning of wide coverage generators. While a large vocabulary makes the learning problem harder, it also allows for larger coverage. DBPNLG exhibits a higher corrected type-token ratio (CTTR), which indicates greater lexical variety, and higher lexical sophistication (LS). Lexical sophistication measures the proportion of relatively unusual or advanced word types in the text. In practice, LS is the proportion of lexical word types (lemma) which are not in the list of 2,000 most frequent words generated from the British National Corpus11. Typetoken ratio (TTR) is a measure of diversity defined as the ratio of the number of word types to the number of words in a text. To address the fact that this ratio tends to decrease with the size of the corpus, corrected TTR can be used to control for corpus size. It is defined as T/ √ 2N , where T is the number of types and N the number of tokens. Overall, the results shown in Table 4 indicate that DBPNLG texts are both lexically more diverse (higher corrected type/token ratio) and more sophisticated (higher proportion of unfrequent words) than RNNLG’s. They also show a proportionately larger vocabulary for DBPNLG (2992 types for 290479 tokens in DBPNLG against 3524 types for 531871 tokens in RNNLG).\n\n4.3 Neural Generation\nRicher and more varied datasets are harder to learn from. As a proof-of-concept study of the comparative difficulty of the two datasets with respect to machine learning, we compare the performance of a sequence-to-sequence model for generation on both datasets. In particular, we use a multilayered sequence-to-sequence model with an at-\n11We compute LS and CTTR using the Lexical Complexity Analyzer developed by Lu (2012).\ntention mechanism (Vinyals et al., 2015).12 The model was trained with 3 layers of 512 units each. To allow for a fair comparison, we use a similar amount of data (13K data-text pairs) for both datasets. As RNNLG is bigger in size than DBPNLG, we constructed a balanced sample of RNNLG which included equal number of instances per category (tv, laptop, etc). We use a 3:1:1 ratio for training, developement and testing. The training was done in two delexicalisation modes: fully and name only. In case of fully delexicalisation, all entities were replaced by their generic terms, whereas in name only mode only subjects were modified in that way. For instance, the triple FC Köln manager Peter Stöger was delexicalised as SportsTeam manager Manager in the first mode, and as SportsTeam manager Peter Stöger in the second mode. The delexicalisation in sentences was done using the exact match between entities and tokens.\nTable 5 shows the perplexity results. In both modes, RNNLG yielded lower scores than DBPNLG. This is inline with the observations made above concerning the higher data diversity, larger vocabulary and more complex texts of DBPNLG. Similary, the BLEU score of the generated sentences (Papineni et al., 2002) is lower for DBPNLG suggesting again a dataset that is more complex and therefore more difficult to learn from.\n\n5 Conclusion\nWe presented a framework for building NLG datato-text training corpora from existing knowledge bases.\nOne feature of our framework is that datasets created using this framework can be used for training and testing KB verbalisers an in particular,\n12We used the TensorFlow code available at https://github.com/tensorflow/models/ tree/master/tutorials/rnn/translate. Alternatively, we could have used (Wen et al., 2016)’s implementation which is optimised for generation. However the code is geared toward dialog acts and modifying it to handle RDF triples is non trivial. Since the comparison aims at examining the relative performance of the same neural network on the two datasets, we used the tensor flow implementation instead.\nverbalisers for RDF knowledge bases. Following the development of the semantic web, many large scale datasets are encoded in the RDF language (e.g., MusicBrainz, FOAF, LinkedGeoData) and official institutions13 increasingly publish their data in this format. In this context, our framework is useful both for creating training data from RDF KB verbalisers and to increase the number of datasets available for training and testing NLG.\nAnother important feature of our framework is that it permits creating semantically and linguistically diverse datasets which should support the learning of lexically and syntactically, wide coverage micro-planners. We applied our framework to DBpedia data and showed that although twice smaller than the largest corpora currently available for training data-to-text microplanners, the resulting dataset is more semantically and linguistically diverse. Despite the disparity in size, the number of attributes is comparable in the two datasets. The ratio between input and input patterns is five times lower in our dataset thereby making learning harder but also diminishing the risk of overfitting and providing for wider linguistic coverage. Conversely, the ratio of text per input is twice higher thereby providing better support for learning paraphrases.\nWe are currently working on further extending the DBPNLG dataset and once completed, will make it available as part of a shared task for evaluating data-to-text micro-planners. While we only report on a dataset developed using 6 DBpedia categories, we have collected content for 14 further categories using the content selection procedure described in Section 3 and will collect the corresponding texts using our selective crowdsourcing procedure.\nRecently, several sequence-to-sequence models have been proposed for generation. Our experiments suggest that these are not optimal when it comes to generate linguistically complex texts from rich data. More generally, they indicate that the data-to-text corpora built by our framework are challenging for such models. We hope that the DBPNLG dataset which we will make available in the shared task will drive the deep learning community to take up this new challenge and work on the development of neural generators that can handle the generation of linguistically rich texts.\n13See http://museum-api.pbworks.com for examples.\n",
    "rationale": "- Strengths:\n\nThis paper presents a step in the direction of developing more challenging\ncorpora for training sentence planners in data-to-text NLG, which is an\nimportant and timely direction. \n\n- Weaknesses:\n\nIt is unclear whether the work reported in this paper represents a substantial\nadvance over Perez-Beltrachini et al.'s (2016) method for selecting content. \nThe authors do not directly compare the present paper to that one. It appears\nthat the main novelty of this paper is the additional analysis, which is\nhowever rather superficial.\n\nIt is good that the authors report a comparison of how an NNLG baseline fares\non this corpus in comparison to that of Wen et al. (2016).  However, the\nBLEU scores in Wen et al.'s paper appear to be much much higher, suggesting\nthat this NNLG baseline is not sufficient for an informative comparison.\n\n- General Discussion:\n\nThe authors need to more clearly articulate why this paper should count as a\nsubstantial advance over what has been published already by Perez-Beltrachini\net al, and why the NNLG baseline should be taken seriously.  In contrast to\nLREC, it is not so common for ACL to publish a main session paper on a corpus\ndevelopment methodology in the absence of some new results of a system making\nuse of the corpus.\n\nThe paper would also be stronger if it included an analysis of the syntactic\nconstructions in the two corpora, thereby more directly bolstering the case\nthat the new corpus is more complex.  The exact details of how the number of\ndifferent path shapes are determined should also be included, and ideally\nassociated with the syntactic constructions.\n\nFinally, the authors should note the limitation that their method does nothing\nto include richer discourse relations such as Contrast, Consequence,\nBackground, etc., which have long been central to NLG. In this respect, the\ncorpora described by Walker et al. JAIR-2007 and Isard LREC-2016 are more\ninteresting and should be discussed in comparison to the method here.\n\nReferences\n\nMarilyn Walker, Amanda Stent, François Mairesse, and\nRashmi Prasad. 2007. Individual and domain adaptation\nin sentence planning for dialogue. Journal of\nArtificial Intelligence Research (JAIR), 30:413–456.\n\nAmy Isard, 2016. “The Methodius Corpus of Rhetorical Discourse\nStructures and Generated Texts” , Proceedings of the Tenth Conference\non Language Resources and Evaluation (LREC 2016), Portorož, Slovenia,\nMay 2016.\n\n---\nAddendum following author response:\n\nThank you for the informative response.  As the response offers crucial\nclarifications, I have raised my overall rating.  Re the comparison to\nPerez-Beltrachini et al.: While this is perhaps more important to the PC than\nto the eventual readers of the paper, it still seems to this reviewer that the\nadvance over this paper could've been made much clearer.  While it is true that\nPerez-Beltrachini et al. \"just\" cover content selection, this is the key to how\nthis dataset differs from that of Wen et al.  There doesn't really seem to be\nmuch to the \"complete methodology\" of constructing the data-to-text dataset\nbeyond obvious crowd-sourcing steps; to the extent these steps are innovative\nor especially crucial, this should be highlighted.  Here it is interesting that\n8.7% of the crowd-sourced texts were rejected during the verification step;\nrelated to Reviewer 1's concerns, it would be interesting to see some examples\nof what was rejected, and to what extent this indicates higher-quality texts\nthan those in Wen et al.'s dataset.  Beyond that, the main point is really that\ncollecting the crowd-sourced texts makes it possible to make the comparisons\nwith the Wen et al. corpus at both the data and text levels (which this\nreviewer can see is crucial to the whole picture).\n\nRe the NNLG baseline, the issue is that the relative difference between the\nperformance of this baseline on the two corpora could disappear if Wen et al.'s\nsubstantially higher-scoring method were employed.  The assumption that this\nrelative difference would remain even with fancier methods should be made\nexplicit, e.g. by acknowledging the issue in a footnote.  Even with this\nlimitation, the comparison does still strike this reviewer as a useful\ncomponent of the overall comparison between the datasets.\n\nRe whether a paper about dataset creation should be able to get into ACL\nwithout system results:  though this indeed not unprecedented, the key issue is\nperhaps how novel and important the dataset is likely to be, and here this\nreviewer acknowledges the importance of the dataset in comparison to existing\nones (even if the key advance is in the already published content selection\nwork).\n\nFinally, this reviewer concurs with Reviewer 1 about the need to clarify the\nrole of domain dependence and what it means to be \"wide coverage\" in the final\nversion of the paper, if accepted.",
    "rating": 5
  },
  {
    "title": "INTROSPECTION:ACCELERATING NEURAL NETWORK TRAINING BY LEARNING WEIGHT EVOLUTION",
    "abstract": "Neural Networks are function approximators that have achieved state-of-the-art accuracy in numerous machine learning tasks. In spite of their great success in terms of accuracy, their large training time makes it difficult to use them for various tasks. In this paper, we explore the idea of learning weight evolution pattern from a simple network for accelerating training of novel neural networks. We use a neural network to learn the training pattern from MNIST classification and utilize it to accelerate training of neural networks used for CIFAR-10 and ImageNet classification. Our method has a low memory footprint and is computationally efficient. This method can also be used with other optimizers to give faster convergence. The results indicate a general trend in the weight evolution during training of neural networks.",
    "text": "1 INTRODUCTION\nDeep neural networks have been very successful in modeling high-level abstractions in data. However, training a deep neural network for any AI task is a time-consuming process. This is because a large number of parameters need to be learnt using training examples. Most of the deeper networks can take days to get trained even on GPU thus making it a major bottleneck in the large-scale application of deep networks. Reduction of training time through an efficient optimizer is essential for fast design and testing of deep neural nets.\nIn the context of neural networks, an optimization algorithm iteratively updates the parameters (weights) of a network based on a batch of training examples, to minimize an objective function. The most widely used optimization algorithm is Stochastic Gradient Descent. Even with the advent of newer and faster optimization algorithms like Adagrad, Adadelta, RMSProp and Adam there is still a need for achieving faster convergence.\nIn this work we apply neural network to predict weights of other in-training neural networks to accelerate their convergence. Our method has a very low memory footprint and is computationally efficient. Another aspect of this method is that we can update the weights of all the layers in parallel.\n∗This work was done as part of an internship at Adobe Systems, Noida\n\n2 RELATED WORK\nSeveral extensions of Stochastic Gradient Descent have been proposed for faster training of neural networks. Some of them are Momentum (Rumelhart et al., 1986), AdaGrad (Duchy et al., 2011), AdaDelta (Zeiler, 2012), RMSProp (Hinton et al., 2012) and Adam (Kingma & Ba, 2014). All of them reduce the convergence time by suitably altering the learning rate during training. Our method can be used along with any of the above-mentioned methods to further improve convergence time.\nIn the above approaches, the weight update is always a product of the gradient and the modified/unmodified learning rate. More recent approaches (Andrychowicz et al., 2016) have tried to learn the function that takes as input the gradient and outputs the appropriate weight update. This exhibited a faster convergence compared to a simpler multiplication operation between the learning rate and gradient. Our approach is different from this, because our forecasting Network does not use the current gradient for weight update, but rather uses the weight history to predict its future value many time steps ahead where network would exhibit better convergence. Our approach generalizes better between different architectures and datasets without additional retraining. Further our approach has far lesser memory footprint as compared to (Andrychowicz et al., 2016). Also our approach need not be involved at every weight update and hence can be invoked asynchronously which makes it computationally efficient.\nAnother recent approach, called Q-gradient descent (Fu et al., 2016), uses a reinforcement learning framework to tune the hyperparameters of the optimization algorithm as the training progresses. The Deep-Q Network used for tuning the hyperparameters itself needs to be trained with data from any specific network N to be able to optimize the training of N . Our approach is different because we use a pre-trained forecasting Network that can optimize any network N without training itself by data from N .\nFinally the recent approach by (Jaderberg et al., 2016) to predict synthetic gradients is similar to our work, in the sense that the weights are updates independently, but it still relies on an estimation of the gradient, while our update method does not.\nOur method is distinct from all the above approaches because it uses information obtained from the training process of existing neural nets to accelerate the training of novel neural nets.\n\n3 PATTERNS IN WEIGHT EVOLUTION\nThe evolution of weights of neural networks being trained on different classification tasks such as on MNIST and CIFAR-10 datasets and over different network architectures (weights from different layers of fully connected as well as convolutional architectures) as well as different optimization rules were analyzed. It was observed that the evolution followed a general trend independent of the task the model was performing or the layer to which the parameters belonged to. A major proportion of the weights did not undergo any significant change. Two metrics were used to quantify weight changes:\n• Difference between the final and initial values of a weight scalar: This is a measure of how much a weight scalar has deviated from its initial value after training.In figure 4 we show the frequency histogram plot of the weight changes in a convolutional network trained for MNIST image classification task, which indicates that most of the weight values do not undergo a significant change in magnitude. Similar plots for a fully connected network trained on MNIST dataset ( figure 6 ) and a convolutional network trained on CIFAR-10 dataset (figure 8 ) present similar observations.\n• Square root of 2nd moment of the values a weight scalar takes during training: Through this measure we wish to quantify the oscillation of weight values. This moment has been taken about the initial value of the weight. In figure 5, we show the frequency histogram plot of the second moment of weight changes in a convolutional network trained for the MNIST digit classification task, which indicates that most of the weight values do not undergo a significant oscillations in value during the training. Similar plots for a fully\nconnected network trained on MNIST (figure 7 ) and a convolutional network trained on CIFAR-10 ( figure 9) dataset present similar observations.\nA very small subset of the all the weights undergo massive changes compared to the rest.\nThe few that did change significantly were observed to be following a predictable trend, where they would keep on increasing or decreasing with the progress of training in a predictable fashion. In figures 1, 2 and 3 we show the evolution history of a few weights randomly sampled from the weight change histogram bins of figures 4,6 and 8 respectively, which illustrates our observation.\n0 20000 40000 60000 80000 100000 Training steps\n−0.8\n−0.6\n−0.4\n−0.2\n0.0\n0.2\n0.4\n0.6\n0.8\nDi ffe\nre nc e of w ei gh\nt al ue\nfr om\nin iti al iz ed\nal ue\nDe iation of weight alue from initialization with training fully connected network on MNIST\nFigure 2: Deviation of weight values from initialized values as a fully-connected network gets trained on MNIST dataset using Adam optimizer..\n0 10000 20000 30000 40000 50000 Training s eps\n−0.20\n−0.15\n−0.10\n−0.05\n0.00\n0.05\n0.10\nDi ffe\nre nc\ne of\nw ei\ngh v\nal ue\nfr om\nin i i\nal iz\ned v\nal ue\nDevia ion of weigh values from ini ialized values when raining a convolu ional ne work on CIFAR-10\nFigure 3: Deviation of weight values from initialized values as CNN gets trained on CIFAR-10 dataset using SGD optimizer.\n\n3.1 WEIGHT PREDICTION\nWe collect the weight evolution trends of a network that is being trained and use the collected data to train a neural network I to forecast the future values of each weight based on its values in the previous time steps. The trained network I is then used to predict the weight values of an unseen network N during its training which move N to a state that enables a faster convergence. The time taken for the forecast is significantly smaller compared to the time a standard optimizer (e.g. SGD) would have taken to achieve the same accuracy. This leads to a reduction in the total training\ntime. The predictor I that is used for forecasting weights is a comparatively smaller neural network, whose inference time is negligible compared to the training time of the network that needs to be trained(N). We call this predictor I Introspection network because it looks at the weight evolution during training.\nThe forecasting network I is a simple 1-layered feedforward neuralnet. The input layer consists of four neurons that take four samples from the training history of a weight. The hidden layer consists of 40 neurons, fully connected to the input layer, with ReLU activation. The output layer is a single neuron that outputs the predicted future value of the weight. In our experiments four was minimum numbers of samples for which the training of Introspection Network I converged.\nThe figure 10 below shows a comparison of the weight evolution for a single scalar weight value with and without using the introspection network I . The vertical green bars indicate the points at which the introspection network was used to predict the future values. Post prediction, the network continues to get trained normally by SGD, until the introspection network I is used once again to jump to a new weight value.\n\n4 EXPERIMENTS\n\n\n4.1 TRAINING OF INTROSPECTION NETWORK\nThe introspection network I is trained on the training history of the weights of a network N0 which was trained on MNIST dataset.The network N0 consisted of 3 convolutional layers and two fully connected layers, with ReLU activation and deploying Adam optimiser. Max pooling(2X2 pool size and a 2X2 stride) was applied after the conv layers along with dropout applied after the first fc layer. The shapes of the conv layer filters were [5, 5, 1, 8] , [5, 5, 8, 16] and [5, 5, 16, 32] respectively whereas of the fc layer weight were [512, 1024] and [1024, 10] respectively.The network N0 was trained with a learning rate of 1e − 4 and batch size of 50. The training set of I is prepared as follows. A random training step t is selected for each weight of N0 selected as a training sample and the following 4 values are given as inputs for training I:\n1. value of the weight at step t\n2. value of the weight at step 7t/10\n3. value of the weight at step 4t/10\n4. at step 0 (i.e. the initialized value)\nSince a large proportion of weights remain nearly constant throughout the training, a preprocessing step is done before getting the training data for I. The large number of weight histories collected are sorted in decreasing order on the basis of their variations in values from time step 0 to time step t. We choose 50% of the training data from the top 50th percentile of the sorted weights, 25% from the next 25th percentile(between 50 to 75th percentile of the sorted weights) and the remaining 25% from the rest (75th to 100th percentile). Approximately 0.8 million examples of weight history are used to train I . As the weight values are very small fractions they are further multiplied by 1000 before being input to the network I. The expected output of I , which is used for training I using backpropagation, is a single scalar the value of the same weight at step 2t. This is an empirical choice. For example, any step kt with k > 1 can be chosen instead of 2t. In our experiments with varying the value of k, we found that the value of k = 2.2 reached a slightly better validation accuracy than k = 2.0 on MNIST dataset (see figure 15 ) but, on the whole the value of k = 2.0 was a lot more consistent in its out-performance at various points in its history. All the results reported here are with respect to the I trained to predict weight values at 2t.\nAdam optimizer was used for the training of the introspection network with a mini-batch size of 20.The training was carried out for 30k steps. The learning rate used was 5e-4 which decreased gradually after every 8k training steps. L1- error was used as the loss function for training . We experimented with both L2 error and percentage error but found that L1 error gave the best result over the validation set. The final training loss obtained was 3.1 and the validation loss of the final trained model was 3.4. These correspond to average L1 weight prediction error of 0.0031 and 0.0034 in the training and validation set respectively as the weight values are multiplied by 1000 before they are input to I .\n\n4.2 USING PRE-TRAINED INTROSPECTION NETWORK TO TRAIN UNSEEN NETWORKS\nThe introspection network once trained can be then used to guide the training of other networks. We illustrate our method by using it to accelerate the training of several deep neural nets with varying architectures on 3 different datasets, namely MNIST, CIFAR-10 and ImageNet. We note that the same introspection network I , trained on the weight evolutions of the MNIST network N0 was used in all these different cases.\nAll the networks have been trained using either Stochastic Gradient Descent, or ADAM and the network I is used at a few intermediate steps to propel the network to a state with higher accuracy.We refer to the time step at which the introspection network I is applied to update all the weights as a ”jump point”.\nThe selection of the steps at which I is to be used is dependent on the distribution of the training step t used for training I . We show the effect of varying the timing of the initial jump and the time interval between jump points in section 4.2.2. It has been observed that I gives a better increase in accuracy when it is used in later training steps rather than in the earlier ones.\nAll the networks trained using I required comparatively less time to reach the same accuracy as normal SGD training. Also, when the same network was trained for the same time with and without updates by I , the former is observed to have better accuracy. These results show that there is a remarkable similarity in the weight evolution trajectories across network architectures,tasks and datasets.\n\n4.2.1 MNIST\nFour different neural networks were trained using I on MNIST dataset:\n1. A convolutional neural network MNIST1 with 2 convolutional layer and 2 fully connected layers(dropout layer after 1st fc layer is also present)with ReLU acitvations for\nclassification task on MNIST image dataset.Max pooling(2X2 pool size and a 2X2 stride) was applied after every conv layer. The CNN layer weights were of shape [5, 5, 1, 8] and [5, 5, 32, 64] respectively and the fc layer were of sizes [3136, 1024] and [1024, 10].The weights were initialised from a truncated normal distribution with a mean of 0 and std of 0.01. The network was trained using SGD with a learning rate of 1e−2 and batch size of 50. It takes approximately 20,000 steps for convergence via SGD optimiser. For MNIST1, I was used to update all weights at training step 3000, 4000, and 5000.\n2. A convolutional network MNIST2 with 2 convolutional layer and 2 fully connected layers with ReLU acitvations. Max pooling(2X2 pool size and a 2X2 stride) was applied after every conv layer. The two fc layer were of sizes [800, 500] and [500, 10] whereas the two conv layers were of shape [5, 5, 1, 20] and [5, 5, 20, 50] respectively. The weight initialisations were done via xavier intialisation. The initial learning rate was 0.01 which was decayed via the inv policy with gamma and power being 1e − 4 and 0.75 respectively. Batch size of 64 was used for the training.It takes approximately 10,000 steps for convergence . The network I was used to update weights at training step 2500 and 3000.\n3. A fully connected network MNIST3 with 2 hidden layers each consisting of 256 hidden units and having ReLU acitvations. The network was trained using SGD with a learning rate of 5e − 3 and a batch size of 100. The initial weights were drawn out from a normal distribution having mean 0 and std as 1.0. For this network the weight updations were carried out at steps 6000, 8000 and 10000.\n4. A RNN MNIST4 used to classify MNIST having a LSTM cell of hidden size of 128 followed by a fc layer of shape [128, 10] for classification. The RNN was trained on Adam optimizer with a learning rate of 5e− 4 and a batch size of 128. The weight updations for this network were done at steps 2000,3000 and 4000. Since the LSTM cell uses sigmoid and tanh activations, the RNN MNIST4 allows us to explore if the introspection network, trained on ReLU can generalize to networks using different activation functions.\nA comparison of the validation accuracy with and without updates by I is shown in figures 11, 12 ,13 and 14. The green lines indicate the steps at which the introspection network I is used. For the MNIST1 network with the application of the introspection network I at three points, we found that it took 251 seconds and 20000 SGD steps to reach a validation accuracy of 98.22%. In the same number of SGD steps, normal training was able to reach a validation accuracy of only 97.22%. In the same amount of time (251 seconds), normal training only reached 97.92%. Hence the gain in accuracy with the application of introspection network translates to real gains in training times.\nFor the MNIST2 network, the figure 12 shows that to reach an accuracy of 99.11%, the number of iterations required by normal SGD was 6000, whereas with the application of the introspection network I , the number of iterations needed was only 3500, which represents a significant savings in time and computational effort.\ntraining steps\nThe initial drop in accuracy seen after a jump in MNIST2 figure 12 can be attributed to the fact that each weight scalar is predicted independently, and the interrelationship between the weight scalars in a layer or across different layers is not taken into consideration. This interrelationship is soon reestablished after few SGD steps. This phenomenon is noticed in the CIFAR and ImageNet cases too.\ntraining steps\ntraining steps\nFor MNIST3 after 15000 steps of training,the max accuracy achieved by normal training of network via Adam optimizer was 95.71% whereas with introspection network applied the max accuracy was 96.89%. To reach the max accuracy reached by normal training , the modified network(weights updated by I) took only 8300 steps.\nFor MNIST4 after 7000 steps of training, the max accuracy achieved by normal training of network was 98.65% achieved after 6500 steps whereas after modification by I it was 98.85% achieved after 5300 steps. The modified network(weights updated by I) reached the max accuracy achieved by normal network after only 4200 steps. It is notable that the introspection network I trained on weight evolutions with ReLU activations was able to help accelerate the convergence of an RNN network which uses sigmoid and tanh activations.\n\n4.2.2 CIFAR-10\nWe applied our introspection network I on a CNN CIFAR1 for classifying images in the CIFAR10 (Krizhevsky, 2009) dataset. It has 2 convolutional layers, 2 fully connected layer and a final softmax layer with ReLU activation function. Max pooling (3X3 pool size and a 2X2 stride) and batch normalization has been applied after each convolutional layer. The two conv layer filter weights were of shape [5, 5, 3, 64] and [5, 5, 64, 64] respectively whereas the two fc layers and final softmax layer were of shape [2304, 384],[384, 192] and [192, 10] respectively. The weights were initialized from a zero mean normal distribution with std of 1e − 4 for conv layers,0.04 for the two fc layers and 1/192.0 for the final layer. The initial learning rate used is 0.1 which is decayed by a factor of 0.1 after every 350 epochs. Batch size of 128 was used for training of the model which was trained via the SGD optimizer. It takes approximately 40,000 steps for convergence. The experiments on\nCIFAR1 were done to investigate two issues. The first was to investigate if the introspection network trained on MNIST weight evolutions is able to generalize to a different network and different dataset. The second was to investigate the effect of varying the timing of the initial jump, the interval between successive jumps and the number of jumps. To investigate these issues, four separate training instances were performed with 4 different set of jump points:\n1. Set1 : Weight updates were carried out at training steps 12000 and 17000. 2. Set2 : Weight updates at steps 15000 and 18000 . 3. Set3 : Weight updates at steps 12000 , 15000 and 19000 . 4. Set4 : Weight updates at steps 14000 , 17000 and 20000 .\nWe observed that for the CIFAR1 network that in order to reach a validation accuracy of 85.7%, we need 40,000 iterations with normal SGD without any intervention with the introspection network I . In all the four sets where the introspection network was used, the target accuracy of 85.7% was reached in approximately 28,000 steps. This shows that the introspection network is able to successfully generalize to a new dataset and new architecture and show significant gains in training time.\nOn CIFAR1, the time taken by I for prediction is negligible compared to the time required for SGD. So the training times in the above cases on CIFAR1 can be assumed to be proportional to the number of SGD steps required.\nA comparison of the validation accuracy with and without updates by I at the four different sets of jump points are shown in figures 16, 17, 18 and 19. The results show that the while choice of jump points have some effect on the final result, the effects are not very huge. In general, we notice that better accuracy is reached when the jumps take place in later training steps.\n\n4.2.3 IMAGENET\nTo investigate the practical feasibility and generalization ability of our introspection network, we applied it in training AlexNet(Krizhevsky et al., 2012) (AlexNet1) on the ImageNet (Russakovsky\net al., 2015) dataset. It has 5 conv layers and 3 fully connected layers . Max pooling and local response normalization have been used after the two starting conv layers and the pooling layer is there after the fifth conv layer as well. We use SGD with momentum of 0.9 to train this network, starting from a learning rate of 0.01. The learning rate was decreased by one tenth every 100, 000 iterations. The mini-batch size was 128. It takes approximately 300,000 steps for convergence. The weight updates were carried out at training steps 120, 000 , 130, 000 , 144, 000 and 160, 000 .\nWe find that in order to achieve a top-5 accuracy of 72%, the number of iterations required in the normal case was 196,000. When the introspection network was used, number of iterations required to reach the same accuracy was 179,000. Again the time taken by I for prediction is negligible compared to the time required for SGD. A comparison of the validation accuracy with and without updates by I is shown in figure 20. The green lines indicate the steps at which the introspection network I is used. The corresponding plot of loss function against training steps has been shown in figure 21.\n1.1 1.2 1.3 1.4 1.5 1.6 1.7 1.8 1.9 2\nTraining steps ×10 5\n0.6\n0.62\n0.64\n0.66\n0.68\n0.7\n0.72\n0.74\nA c c u ra\nc y\nPlot of accuracy vs training steps for imageNet\nnormal training Introspection network applied\nFigure 20: Validation accuracy plot for AlexNet1 on ImageNet\n1.1 1.2 1.3 1.4 1.5 1.6 1.7 1.8\nx 10 5\n1.6\n1.8\n2\n2.2\n2.4\n2.6\n2.8\n3\n3.2\nTraining steps\nT ra\nin in\ng l o\ns s\nnormal training Introspection network applied\nFigure 21: Plot of loss function vs training steps for AlexNet1 on ImageNet\nThe results on Alexnet1 show that our approach has a small memory footprint and computationally efficient to be able to scale to training practical large scale networks.\n\n4.3 COMPARISON WITH BASELINE TECHNIQUES\nIn this section we provide a comparison with other optimizers and simple heuristics which can be used to update the weights at different training steps instead of updations by introspection network.\n\n4.4 COMPARISON WITH ADAM OPTIMIZER\nWe applied the introspection network on MNIST1 and MNIST3 networks being trained with Adam optimizer with learning rates of 1e − 4 and 1e − 3. The results in figure 22 and figure 23 show that while Adam outperforms normal SGD and SGD with introspection, we were able to successfully apply the introspection network on Adam optimizer and accelerate it.\nFor MNIST1 the max accuracy achieved by Adam with introspection was 99.34%, by normal Adam was 99.3%, by SGD with introspection was 99.21% and by normal SGD was 99.08% . With introspection applied on Adam the model reaches the max accuracy as achieved by normal Adam after only 7200 steps whereas the normal training required 10000 steps.\nFor MNIST3 the max accuracy achieved by Adam with introspection was 96.9%, by normal Adam was 95.7%, by SGD with introspection was 94.47% and by normal SGD was 93.39% . With introspection applied on Adam the model reaches the max accuracy as achieved by normal Adam after only 8800 steps whereas the normal training required 15000 steps.\ntraining steps\ntraining steps\n\n4.4.1 FITTING QUADRATIC CURVE\nA separate quadratic curve was fit to each of the weight values of the model on the basis of the 4 past weight values chosen from history.The weight values chosen from history were at the same steps as they were for updations by I . The new updated weight would be the value of the quadratic curve at some future time step.For MNIST1 , experiments were performed by updating the weights to the value predicted by the quadratic function at a future timestep which was one of 1.25,1.3 or 1.4 times the current time step. For other higher jump ratios the updates would cause the model to diverge, and lower jump ratios did not show much improvement in performance. The plot showing the comparison in validation accuracy have been shown below in figure 24.\ntraining steps\nThe max accuracy achieved with introspection applied was 99.21% whereas with quadratic fit it was 99.19%. We note that even though the best performing quadratic fit eventually almost reaches the same max accuracy than that achieved with introspection network, it required considerable experimentation to find the right jump ratio.A unique observation for the quadratic fit baseline was that it would take the accuracy down dramatically, upto 9.8%, from which the training often never recovers. Sometimes,the optimizers (SGD or Adam) would recover the accuracy, as seen in figure 24. Moreover, the quadratic fit baseline was not able to generalize to other datasets and tasks. The best performing jump ratio of 1.25 was not able to outperform Introspection on the CIFAR-10 dataset, as seen in figure 25.\nIn the CIFAR-10 case, The maximum accuracy achieved via updations by introspection was 85.6 which was achieved after 25500 steps, whereas with updations by quadratic fit, the max accuracy of 85.45 was achieved after 27200 steps.\nFor the normal training via SGD without any updations after 30000 steps of training, the max accuracy of 85.29 was achieved after 26500 steps, whereas the same accuracy was achieved by introspection after only 21200 steps and after 27000 steps via updation by quadratic.\n\n4.4.2 FITTING LINEAR CURVE\nInstead of fitting a quadratic curve to each of the weights we tried fitting a linear curve. Experiments were performed on MNIST1 for jump ratios of 1.1 and 1.075 as the higher ratios would cause the model to diverge after 2 or 3 jumps.The result has been shown below in figure 26.\ntraining steps\ntraining steps\nAs no significant improvement in performance was observed the experiment was not repeated over cifar.\n\n4.5 LINEAR INTROSPECTION NETWORK\nWe removed the ReLU nonlinearity from the introspection network and used the same training procedure of the normal introspection network to predict the future values at 2t. We then used this linear network on the MNIST1 network. We found that it gave some advantage over normal SGD, but was not as good as the introspection network as shown in figure 27. Hence we did not explore this baseline for other datasets and networks.\n\n4.5.1 ADDING NOISE\nThe weight values were updated by adding small gaussian random zero mean noise values . The experiment was performed over MNIST1 for two different std. value, the results of which have been shown below in figure 28.\nSince no significant improvement was observed for the weight updations via noise for MNIST, the experiment was not performed over cifar-10.\n\n5 LIMITATIONS AND OPEN QUESTIONS\nSome of the open questions to be investigated relate to determination of the optimal jump points and investigations regarding the generalization capacity of the introspection network to speed up training\nin RNNs and non-image tasks. Also, we noticed that applying the jumps in very early training steps while training AlexNet1 tended to degrade the final outcomes. This may be due to the fact that our introspection network is extremely simple and has been trained only on weight evolution data from MNIST. A combination of a more powerful network and training data derived from a diverse set may ameliorate this problem.\n\n6 CONCLUSION\nWe introduced a method to accelerate neural network training. For this purpose, we used a neural network I that learns a general trend in weight evolution of all neural networks. After learning the trend from one neural network training, I is used to update weights of many deep neural nets on 3 different tasks - MNIST, CIFAR-10, and ImageNet, with varying network architectures, activations, optimizers, and normalizing strategies(batch norm,lrn). Using the introspection network I led to faster convergence compared to existing methods in all the cases. Our method has a small memory footprint, is computationally efficient and is usable in practical settings. Our method is different from other existing methods in the aspect that it utilizes the knowledge obtained from weights of one neural network training to accelerate the training of several unseen networks on new tasks. The results reported here indicates the existence of a general underlying pattern in the weight evolution of any neural network.\n",
    "rationale": "In this paper, the authors use a separate introspection neural network to predict the future value of the weights directly from their past history. The introspection network is trained on the parameter progressions collected from training separate set of meta learning models using a typical optimizer, e.g. SGD.  \n\nPros:\n+ The organization is generally very clear\n+ Novel meta-learning approach that is different than the previous learning to learn approach\n\nCons: \n- The paper will benefit from more thorough experiments on other neural network architectures where the geometry of the parameter space are sufficiently different than CNNs such as fully connected and recurrent neural networks.  \n- Neither MNIST nor CIFAR experimental section explained the architectural details\n- Mini-batch size for the experiments were not included in the paper\n- Comparison with different baseline optimizer such as Adam would be a strong addition or at least explain how the hyper-parameters, such as learning rate and momentum, are chosen for the baseline SGD method. \n\nOverall, due to the omission of the experimental details in the current revision, it is hard to draw any conclusive insight about the proposed method. ",
    "rating": 3
  },
  {
    "title": "A New Formula for Vietnamese Text Readability Assessment",
    "abstract": "Text readability has an important role in text drafting and document selecting. Researches on the readability of the text have been made long ago for English and some common languages. There are few researches in Vietnamese text readability and most of them are performed from more than two decades ago on very small corpora. In this paper, we build a new and larger corpus and use it to create a newer formula to predict the difficulty of Vietnamese text. The experimental results show that the new formula can predict the readability of Vietnamese documents with over 80% accuracy.",
    "text": "1 Introduction\nText readability – as defined by Edgar Dale and Jeanne Chall (Dale and Chall, 1949) – is “the sum total (including all the interactions) of all those elements within a given piece of printed material that affect the success a group of readers has with it. The success is the extent to which they understand it, read it at an optimal speed, and find it interesting.” Text readability has a huge impact on the reading and comprehending a text. Base on the readability, readers can determine whether a text is suitable for their reading ability or not. The text author(s) can also use the readability of the draft to guide readers object or have some adjustments to make it fit the toward reader. Building a model to analyze text readability has meant a lot in the scientific and practical: help scientists writing research reports more readable; support educators drafting textbooks and curricula to suit each age of students; support publishers in shaping the audience; help governments drafting legal documents to suit the majority of citi-\nzens; or to assist manufacturers in preparing user guide for their products. . . In addition, text readability can effectively support in choosing appropriate curriculums when teaching language for foreigners.\nResearches on text readability have begun since the early of the 20th century, most of them are for English and some common languages. Most famous studies in text readability are creating linear functions to assess and grade documents like Dale-Chall formula (Dale and Chall, 1949), Flesch Reading Ease formula (Flesch, 1949), FleschKincaid formula (Kincaid et al., 1975), Gunning Fog formula (Robert, 1952), SMOG formula (Laughlin, 1969). . . In Vietnamese, there are only two studies on text readability of Liem Thanh Nguyen and Alan B. Henkin in 1982 and 1985. Both these two researches focus on examining relations between statistical characteristics at words level and at sentences level and text readability. However, these two works only evaluate on small corpora of 20 documents (Nguyen and Henkin, 1982) and 54 documents (Nguyen and Henkin, 1985). Since these studies, there is almost no other publication on Vietnamese text readability.\nIn this paper, we mainly focus on creating a new formula for Vietnamese text readability assessment base on a self-built corpus with a large number of documents. Following this, the remaining of this paper is presented as below: we first present some famous formulas in English text readability as related works in section 2; we then present our works on building a corpus and creating formulas to assess Vietnamese text readability along with some experiments and results in section 3; finally, section 4 presents our discussions and conclusions.\n2\n101\n102\n103\n104\n105\n106\n107\n108\n109\n110\n111\n112\n113\n114\n115\n116\n117\n118\n119\n120\n121\n122\n123\n124\n125\n126\n127\n128\n129\n130\n131\n132\n133\n134\n135\n136\n137\n138\n139\n140\n141\n142\n143\n144\n145\n146\n147\n148\n149\n150\n151\n152\n153\n154\n155\n156\n157\n158\n159\n160\n161\n162\n163\n164\n165\n166\n167\n168\n169\n170\n171\n172\n173\n174\n175\n176\n177\n178\n179\n180\n181\n182\n183\n184\n185\n186\n187\n188\n189\n190\n191\n192\n193\n194\n195\n196\n197\n198\n199\n\n2 Related works\nStudies on text readability have begun since the early of the 20th century. Up to now, there are thousands of works in this field. In this section, we will describe some famous formulas for English text readability assessment and two formulas for Vietnamese. First is the new Dale-Chall formula (Chall and Dale, 1995). This is a very famous readability formula that provides the comprehension difficulty score of an English document. This uses a list of 3000 words that students in fourth-grade could understand. Words that not in this list are considered as difficult words. The score is calculated as Equation 1:\nRS = 0.1579 ∗ (PDW ) + 0.0496 ∗ASL (1)\nwhere RS is the Reading Grade of a reader who can comprehend the text at 3rd grade or below, the higher RS, the more difficult the text; PDW is Percentage of Difficult Words; and ASL is Average Sentence Length in words. The list of 3.000 words that student in fourth-grade could understand was replaced by the list of 3.000 frequent Vietnamese words to experiment in Vietnamese corpus. The second formula is Flesh Reading Ease (Flesch, 1949). This is an easy formula for measuring English text readability and is used by many US government agencies like US Department of Defense. . . It is also integrated into the Microsoft Word - the most popular word processor - from version 2007 to help users checking and controlling the readability of the document. The formula is defined as Equation 2:\nRE = 206.835−(1.015 ∗ASL)−(84.6 ∗ASW ) (2) whereRE is the Readability Ease of the text;ASL is Average Sentence Length in words; and ASW is the average number of syllables per word. Like the name of the formula, the higherRE, the easier the document. Next is the Flesch-Kincaid grade level readability formula (Kincaid et al., 1975) for English. This formula is best suited for education and is also integrated into the Microsoft Word. It is defined as Equation 3:\nFKRA = (0.39xASL)+(11.8 ∗ASW )−15.59 (3)\nwhere FKRA is the Flesch-Kincaid Reading Age, indicates the grade-school level that student can read and comprehend; ASL is Average Sentence Length in words; andASW is the average number of syllables per word. Because this formula determines the grade-school level of the text so the higher FKRA, the more difficult the text. Continue is the Gunning Fog index formula (Robert, 1952). This formula is developed by Robert Gunning – an American textbook publisher. It is defined as Equation 4:\nGrade level = 0.4 (ASL+ PHW ) (4)\nwhere ASL is Average Sentence Length in words; and PHW is Percentage of Difficult Words - the percentage of words which have three or more syllables that are not proper nouns, combinations of easy words or hyphenated words, or two-syllables verbs made into three with -es and -ed endings. Similar to the Flesch-Kincaid grade level, the higher Gunning Fog index, the more difficult the text. Next is the SMOG formula (Laughlin, 1969). This formula estimates the years of education a person needs to understand a specific text and is widely used in checking health messages. The higher SMOG value, the more difficult the text. The formula is:\nSMOG grade = 3 + √ Polysyllable Count (5)\nIn Vietnamese, as mentioned in Section 1, there are only two researches on text readability of Liem Thanh Nguyen and Alan B. Henkin in 1982 and 1985 (Nguyen and Henkin, 1982, 1985). The first formula is defined as Equation 6\nRL = 2WL+ 0.2SL− 6 (6)\nwhere RL is Readability Level of the text; WL is Average Word Length in characters; and SL is Average Sentence Length in words. The second formula was revised from the first one with the additional role of the ratio of difficult words in the document:\nRL = 0.27WD + 0.13SL+ 1.74 (7)\nwhere RL is Readability Level of the text, the higher RL value, the more difficult the text; WD is Word Difficulty; and SL is Average Sentence Length in words.\n3\n201\n202\n203\n204\n205\n206\n207\n208\n209\n210\n211\n212\n213\n214\n215\n216\n217\n218\n219\n220\n221\n222\n223\n224\n225\n226\n227\n228\n229\n230\n231\n232\n233\n234\n235\n236\n237\n238\n239\n240\n241\n242\n243\n244\n245\n246\n247\n248\n249\n251\n252\n253\n254\n255\n256\n257\n258\n259\n260\n261\n262\n263\n264\n265\n266\n267\n268\n269\n270\n271\n272\n273\n274\n275\n276\n277\n278\n279\n280\n281\n282\n283\n284\n285\n286\n287\n288\n289\n290\n291\n292\n293\n294\n295\n296\n297\n298\n299\nHowever, these two works only evaluate on small corpora of 20 documents (Nguyen and Henkin, 1982) and 54 documents (Nguyen and Henkin, 1985). Since these studies, there is almost no other publication on Vietnamese text readability. Furthermore, as proposed by Professor Lucius Adelno Sherman, languages are still changing over time (Sherman, 1893), so researches on readability will still continue.\n\n3 Method\n\n\n3.1 Building corpus\nAs mentioned in sections 1 and 2, there are only two publications on Vietnamese text readability and both of them use only small corpora with 20 and 54 documents (Nguyen and Henkin, 1982, 1985). So in our research, we built another corpus with a larger amount of document for examining. 1,000 documents were collected into 3 categories of difficulty from various sources with the following criteria:\n1. Easy documents: including documents written for children or by children or just need people who are studying at primary schools or having maximum primary education to read and understand. These documents were mainly collected from primary school textbooks, primary sample essays, fairy-tales, stories for babies. . .\n2. Normal documents: they are documents written for middle and high school students, or documents which only need people with high school education to be readable and understandable. Most documents in this category were collected from textbooks and general newspapers.\n3. Difficult documents: including documents written for college students, specialized documents, scientific paper. . . which need high or specialized education to be readable and understandable. These documents were collected from university textbooks, specialized documents, political theory articles, language and literary articles, law and legal documents. . .\nTen experts were asked to evaluate collected documents. They are Vietnamese language specialists, current or former Vietnamese literature teacher - who has much knowledge and experiment in using and teaching Vietnamese. Each document was ensured to be evaluated by 3 experts with the following instruction: each person carefully reads each given document and gives a score for that document: a) If that document is easy enough for children and people with just primary school education to read and understand, give the Score = 1; b) Score = 2 if almost all adults and people with average education can read and understand; c) If documents need people with high or specialized education to be readable and understandable, give the Score = 3. The overall scores of each document are used to re-category that document:\n1. If the document got 3 equal scores: that document will be categorized according to the category of the score.\n2. If the document got 2 equal scores and one different: that document will be put to the category of the 2 equal scores.\n3. If the document got 3 different scores: that document will be filtered out of the corpus.\nEasy Normal Difficult Overall No. documents 235 413 348 996 No. sentences 4,006 13,772 24,646 42,424 Average sentence length in word 13.59 19.24 22.71 19.12 Average sentence length in syllable 16.48 25.04 33.66 26.03 Average sentence length in character 79.50 124.39 173.54 130.97 Average word length in syllable 1.21 1.30 1.48 1.34 Average word length in character 5.82 6.44 7.62 6.71\nTable 1: Statistics numbers of built corpus.\n4\n301\n302\n303\n304\n305\n306\n307\n308\n309\n310\n311\n312\n313\n314\n315\n316\n317\n318\n319\n320\n321\n322\n323\n324\n325\n326\n327\n328\n329\n330\n331\n332\n333\n334\n335\n336\n337\n338\n339\n340\n341\n342\n343\n344\n345\n346\n347\n348\n349\n351\n352\n353\n354\n355\n356\n357\n358\n359\n360\n361\n362\n363\n364\n365\n366\n367\n368\n369\n370\n371\n372\n373\n374\n375\n376\n377\n378\n379\n380\n381\n382\n383\n384\n385\n386\n387\n388\n389\n390\n391\n392\n393\n394\n395\n396\n397\n398\n399\nFinally, for some statistic later, we assign the readability value of 1 for all documents in the Easy category, 2 for the Normal and 3 for the Difficult category. Table 1 presents some statistic number of the final corpus. To ensure that our corpus is reliable, we calculated the Fleiss’ kappa score (Fleiss, 1971). This is a statistical measure for assessing the agreement between a fixed number of raters when classifying items. The Fleiss’ kappa measure is usually used in natural language processing to assess the reliability of agreement between references in the corpus or between manual annotation by raters/experts. The K value is 0.422, which demonstrates that there is a moderate agreement between annotators so the corpus is reliable.\n\n3.2 Features\nIn this part, we will describe some features that are commonly used in text readability assessment. Average sentence length: the average sentence length of a text is one of the simplest and common characteristic when measuring text readability. In this paper, we examined three types of average sentence length as Equation 8, 9 and 10: Average Sentence Length in Words (ASLW):\nASLW = word count\nsentence count (8)\nAverage Sentence Length in Syllables (ASLS):\nASLS = syllable count sentence count\n(9)\nAverage Sentence Length in Characters (ASLC):\nASLC = character count sentence count\n(10)\nAverage word length: two types of average word length were examined in this study as Equation 11 and 12: Average Word Length in Syllables (AWLS):\nAWLS = syllable count word count\n(11)\nAverage Word Length in Characters (AWLC):\nAWLC = character count word count\n(12)\nIn Vietnamese, word maybe monosyllabic or polysyllabic (compound word) with a whitespace\nbetween each syllable; each syllable is a combination of letters with or without tonal and word marks. For example: the word “nghe” (listen) is a monosyllable with four letters (n, g, h, and e); the word “chạy” (run) is a monosyllable with four letters (c, h, a, y) and a tonal mark (.); the word “thời gian” (time) is a polysyllable (2 syllables “thời” and “gian”) with eight letters (t, h, o, i, g, i, a, n), a word mark ( ’ ), a tonal mark ( ` ) and a whitespace.\nPercentage of difficult words: in many studies, the percentage of difficult words is an important feature when evaluating text readability. However, create the easy or difficult word list needs a lot of effort, so most researches used frequent word list as a replacement: if a word does not appear in the frequent list, it will be considered as a difficult word. In this study, we used two frequent lists: the first is top 1,000 frequent words extracted from the frequent word list of Dien and Hao (Dinh and Hao, 2015); and the second is 1,000 frequent words extracted from all easy documents of our built corpus. The percentage of difficult words is calculated as Equation 13:\nPDWi = difficult word count\nword count (13)\nIn this paper, PDW1 stands for a percentage of difficult words calculated using Dien and Hao’s list and PDW2 stands for a percentage of difficult words calculated using our easy list. Not only the percentage of difficult words, but also the percentage of difficult syllables was examined in our study. We also used two frequent lists: the top 1,000 frequent syllables extracted from the list of Dien and Hao (Dinh and Hao, 2015); and the 1,000 frequent syllables extracted from all easy documents of our corpus. The percentage of difficult syllables is calculated as Equation 14:\nPDWi = difficult syllable count\nsyllable count (14)\nSimilar to word, in this paper, PDS1 and PDS2 stand for a percentage of difficult words calculated using Dien and Hao’s list and our easy list respectively.\n\n3.3 Create formula\nThe first thing is finding which features are suitable for predicting text readability through corre-\n5\n401\n402\n403\n404\n405\n406\n407\n408\n409\n410\n411\n412\n413\n414\n415\n416\n417\n418\n419\n420\n421\n422\n423\n424\n425\n426\n427\n428\n429\n430\n431\n432\n433\n434\n435\n436\n437\n438\n439\n440\n441\n442\n443\n444\n445\n446\n447\n448\n449\n451\n452\n453\n454\n455\n456\n457\n458\n459\n460\n461\n462\n463\n464\n465\n466\n467\n468\n469\n470\n471\n472\n473\n474\n475\n476\n477\n478\n479\n480\n481\n482\n483\n484\n485\n486\n487\n488\n489\n490\n491\n492\n493\n494\n495\n496\n497\n498\n499\nTR ASLW ASLS ASLC AWLS AWLC PDS1 PDW1 PDS2 PDW2 TR 1 ASLW 0.567 1 ASLS 0.675 0.970 1 ASLC 0.695 0.955 0.997 1 AWLS 0.770 0.483 0.669 0.699 1 AWLC 0.774 0.497 0.674 0.716 0.979 1 PDS1 0.051 -0.116 -0.140 -0.132 -0.107 -0.073 1 PDW1 0.103 -0.069 -0.029 -0.018 0.156 0.156 0.680 1 PDS2 0.466 0.201 0.237 0.251 0.328 0.355 0.666 0.602 1 PDW2 0.786 0.438 0.562 0.589 0.785 0.793 0.340 0.467 0.782 1\nTable 2: Correlation between each feature with the text readability and between features together (TR is the Text Readability).\nFold 1 Fold 2 Fold 3 Fold 4 Fold 5 Average Coefficient values\nConst -0.8141 -0.6767 -0.6372 -0.7777 -0.7419 -0.7295 ASLC 0.0037 0.0040 0.0043 0.0040 0.0040 0.0040 AWLC 0.2105 0.1779 0.1699 0.2032 0.1911 0.1905 PDW2 2.6960 2.7974 2.7224 2.6370 2.7208 2.7147\nAccuracy Easy 0.7234 0.7021 0.8511 0.7234 0.8085 0.7660 Normal 0.8313 0.9036 0.8537 0.8537 0.8313 0.8571 Difficult 0.7681 0.7857 0.8000 0.8000 0.7826 0.7845 Overall 0.7839 0.8150 0.8342 0.8040 0.8090 0.8102\nTable 3: Coefficient values and accuracy of all formulas.\nlation analysis. Table 2 shows the correlation coefficients between each feature with the text readability and between features together performed on our corpus.\nWe can see that the ASLS, ASLC, AWLS, AWLC and PDW2 are high correlated with the text readability. However, they are also high correlated with some others. To choose which features to put in the formula, we select sequentially features from the highest correlation with text readability to the lowest and remove features have high correlation coefficient with selected features. Through some experiments, we decided to use the threshold 0.9 for removing features: if a feature has the correlation coefficient with selected features greater than or equal to 0.9, that feature will not be cho-\nsen. Following this, three features were selected: ASLC, AWLC, and PDW2. The features AWLS and ASLS are high correlated with AWLC and ASLC so they were not chosen. The selected features were used as predictors to perform multiple regression analysis with Text Readability as the criterion. The purpose is to find coefficient values of these features to form a formula for predicting text readability. The general formula is:\nReadability = A1ASLC +A2AWLC\n+A3PDW2 +A4 (15)\nWithA1 toA4 are coefficient values. In this study, we divided our corpus into five equal parts for cross-fold analyzing. In each fold, four part were\n6\n501\n502\n503\n504\n505\n506\n507\n508\n509\n510\n511\n512\n513\n514\n515\n516\n517\n518\n519\n520\n521\n522\n523\n524\n525\n526\n527\n528\n529\n530\n531\n532\n533\n534\n535\n536\n537\n538\n539\n540\n541\n542\n543\n544\n545\n546\n547\n548\n549\n551\n552\n553\n554\n555\n556\n557\n558\n559\n560\n561\n562\n563\n564\n565\n566\n567\n568\n569\n570\n571\n572\n573\n574\n575\n576\n577\n578\n579\n580\n581\n582\n583\n584\n585\n586\n587\n588\n589\n590\n591\n592\n593\n594\n595\n596\n597\n598\n599\nselected to perform regression analysis and the coefficient values of each fold will be used to predict the readability of the remaining part using Formula 15. The predicted values were rounded to the nearest unit and compared to the expert evaluated readability for accuracy assessment. Finally, the average coefficient value of each predictor was used to form the final text readability formula. The final formula is:\nReadability = 0.004ASLC + 0.1905AWLC\n+2.7147PDW2− 0.7295 (16)\nTable 3 presents coefficient values and accuracy of all formulas.\n\n4 Discussion and conclusion\nFrom the Table 2, we can see that word length, sentence length and the percentage of difficult words still play an important role in measuring the text readability. The Percentage of difficult syllables (PDS1) and words (PDW1) calculated on the lists extracted from Dien and Hao’s lists (Dinh and Hao, 2015) are low correlated with the Text readability of the built corpus. The main reason is the lists of Dien and Hao were statistically analyzed from the corpus mainly collected from newspapers, which are mostly texts with normal readability. So the Dien and Hao’s lists and other frequent lists which were collected from normal texts may not be good replacements for the easy word and syllable list. In the Table 3, we can see that the final text readability formula can accurately predict more than 76% of the easy texts, more than 85% of the normal documents and more than 78% of the difficult texts. Overall, the formula can predict the Vietnamese text readability with the accuracy up to 81%. This is a good result and can be applied in practice. In this paper, we have presented our work on creating a new large corpus for Vietnamese text readability assessing. We also used the corpus to create a new formula for predicting Vietnamese text readability. Experiments performed on the corpus using created formula shows that the formula can predict the readability of Vietnamese text with high accuracy. For the future works, other corpora will be built with more detailed levels of difficulty and\nfor more specific domains. Other deeper features like part-of-speech, sentence structure, discourse. . . will be examined to create more precise formula(s). Some machine learning methods will be examined to create some classifier for automatically Vietnamese text readability assessment.\n",
    "rationale": "- Strengths:\n New Dataset, \n NLP on Resource poor language\n\n- Weaknesses:\n Incomplete related work references, \n No comparison with recent methods and approaches, \n Lack of technical contribution, \n Weak experiments,\n\n- General Discussion:\n\nIn this paper the authors present a simple formula for readability assessment\nof Vietnamese Text. Using a combination of features such as word count,\nsentence length etc they train a simple regression model to estimate the\nreadability of the documents. \n\nOne of the major weaknesses of the paper its lack of technical contribution -\nwhile early work in readability assessment employed simple methods like the one\noutlined in this paper, recent work on predicting readability uses more robust\nmethods that rely on language models for instance (Eg :\nhttp://www.cl.cam.ac.uk/~mx223/readability_bea_2016.pdf,\nhttp://www-personal.umich.edu/~kevynct/pubs/ITL-readability-invited-article-v10\n-camera.pdf). A comparison with such methods could be a useful contribution and\nmake the paper stronger especially if simple methods such as those outlined in\nthis paper can compete with more complicated models. \n\nBaseline experiments with SMOG, Gunning Fog index etc should also be presented\nas well as the other Vietnamese metrics and datasets that the authors cite. \n\nAnother problem is that while previous readability indices were more selective\nand classified content into granular levels corresponding to grade levels (for\ninstance), the authors use a coarse classification scheme to label documents as\neasy, medium and hard which makes the metric uninteresting. (Also, why not use\na classifier?)\n\nThe work is probably a bit too pre-mature and suffers from significant\nweaknesses to be accepted at this stage. I would encourage the authors to\nincorporate suggested feedback to make it better. \n\nThe paper also has quite a few grammatical errors which should be addressed in\nany future submission.",
    "rating": 3
  },
  {
    "title": "METACONTROL FOR ADAPTIVE IMAGINATION-BASED OPTIMIZATION",
    "abstract": "Many machine learning systems are built to solve the hardest examples of a particular task, which often makes them large and expensive to run—especially with respect to the easier examples, which might require much less computation. For an agent with a limited computational budget, this “one-size-fits-all” approach may result in the agent wasting valuable computation on easy examples, while not spending enough on hard examples. Rather than learning a single, fixed policy for solving all instances of a task, we introduce a metacontroller which learns to optimize a sequence of “imagined” internal simulations over predictive models of the world in order to construct a more informed, and more economical, solution. The metacontroller component is a model-free reinforcement learning agent, which decides both how many iterations of the optimization procedure to run, as well as which model to consult on each iteration. The models (which we call “experts”) can be state transition models, action-value functions, or any other mechanism that provides information useful for solving the task, and can be learned on-policy or off-policy in parallel with the metacontroller. When the metacontroller, controller, and experts were trained with “interaction networks” (Battaglia et al., 2016) as expert models, our approach was able to solve a challenging decision-making problem under complex non-linear dynamics. The metacontroller learned to adapt the amount of computation it performed to the difficulty of the task, and learned how to choose which experts to consult by factoring in both their reliability and individual computational resource costs. This allowed the metacontroller to achieve a lower overall cost (task loss plus computational cost) than more traditional fixed policy approaches. These results demonstrate that our approach is a powerful framework for using rich forward models for efficient model-based reinforcement learning.",
    "text": "1 INTRODUCTION\nWhile there have been significant recent advances in deep reinforcement learning (Mnih et al., 2015; Silver et al., 2016) and control (Lillicrap et al., 2015; Levine et al., 2016), most efforts train a network that performs a fixed sequence of computations. Here we introduce an alternative in which an agent uses a metacontroller to choose which, and how many, computations to perform. It “imagines” the consequences of potential actions proposed by an actor module, and refines them internally, before executing them in the world. The metacontroller adaptively decides which expert models to use to evaluate candidate actions, and when it is time to stop imagining and act. The learned experts may be state transition models, action-value functions, or any other function that is relevant to the task, and can vary in their accuracy and computational costs. Our metacontroller’s learned policy can exploit the diversity of its pool of experts by trading off between their costs and reliability, allowing it to automatically identify which expert is most worthwhile.\nWe draw inspiration from research in cognitive science and neuroscience which has studied how people use a meta-level of reasoning in order to control the use of their internal models and allocation of their computational resources. Evidence suggests that humans rely on rich generative models of the world for planning (Gläscher et al., 2010), control (Wolpert & Kawato, 1998), and reasoning (Hegarty, 2004; Johnson-Laird, 2010; Battaglia et al., 2013), that they adapt the amount of computation they perform with their model to the demands of the task (Hamrick et al., 2015), and that they trade off between multiple strategies of varying quality (Lee et al., 2014; Lieder et al., 2014; Lieder & Griffiths, in revision; Kool et al., in press).\nOur imagination-based optimization approach is related to classic artificial intelligence research on bounded-rational metareasoning (Horvitz, 1988; Russell & Wefald, 1991; Hay et al., 2012), which formulates a meta-level MDP for selecting computations to perform, where the computations have a known cost. We also build on classic work by Schmidhuber (1990a;b), which used an RL controller with a recurrent neural network (RNN) world model to evaluate and improve upon candidate controls online.\nRecently Andrychowicz et al. (2016) used a fully differentiable deep network to learn to perform gradient descent optimization, and Tamar et al. (2016) used a convolutional neural network for performing value iteration online in a deep learning setting. In other similar work, Fragkiadaki et al. (2015) made use of “visual imaginations” for action planning. Our work is also related to recent notions of “conditional computation” (Bengio, 2013; Bengio et al., 2015), which adaptively modifies network structure online, and “adaptive computation time” (Graves, 2016) which allows for variable numbers of internal “pondering” iterations to optimize computational cost.\nOur work’s key contribution is a framework for learning to optimize via a metacontroller which manages an adaptive, imagination-based optimization loop. This represents a hybrid RL system where a model-free metacontroller constructs its decisions using an actor policy to manage model-free and model-based experts. Our experimental results demonstrate that a metacontroller can flexibly allocate its computational resources on a case-by-case basis to achieve greater performance than more rigid fixed policy approaches, using more computation when it is required by a more difficult task.\n\n2 MODEL\nWe consider a class of fully observed, one-shot decision-making tasks (i.e., continuous, contextual bandits). The performance objective is to find a control c ∈ C which, given an initial state x ∈ X , minimizes some loss function L between a known future goal state x∗ and the result of a forward process, f(x, c). The performance loss LP is the (negative) utility of executing the control in the world, and is related to the optimal solution c∗ ∈ C as follows:\nLP (x ∗, x, c) = L(x∗, f(x, c)), (1)\nc∗ = argmin c LP (x ∗, x, c). (2)\nHowever, (2) defines only the optimal solution—not how to achieve it.\n\n2.1 OPTIMIZING PERFORMANCE\nWe consider an iterative optimization procedure that takes x∗ and x as input and returns an approximation of c∗ in order to minimize (1). The optimization procedure consists of a controller, which iteratively proposes controls, and an expert, which evaluates how good those controls are. On the nth iteration, the controller πC : X × X ×H → C takes as input, x∗, x, and information about the history of previously proposed controls and evaluations hn−1 ∈ H, and returns a proposed control cn that aims to improve on previously proposed controls. An expert E : X × X × C → E takes the proposed control and provides some information en ∈ E about the quality of the control, which we call an opinion. This opinion is added to the history, which is passed back to the controller, and the loop continues for N steps, after which a final control cN is proposed.\nStandard optimization methods use principled heuristics for proposing controls. In gradient descent, for example, controls are proposed by adjusting cn in the direction of the gradient of the reward with respect to the control. In Bayesian optimization, controls are proposed based on selection criteria such as “probability of improvement”, or a meta-selection criterion for choosing among\nseveral basic selection criteria Hoffman et al. (2011); Shahriari et al. (2014). Rather than choosing one of several controllers, our work learns a single controller and instead focuses on selecting from multiple experts (see Sec. 2.2). In some cases f is known and inexpensive to compute, and thus the optimization procedure sets E ≡ f . However, in many real-world settings, f is expensive or non-stationary and so it can be advantageous to use an approximation of f (e.g., a state transition model), LP (e.g., an action-value function), or any other quantity that gives some information about f or LP .\n\n2.2 OPTIMIZING COMPUTATIONAL COST\nGiven a controller and one or more experts, there are two important decisions to be made. First, how many optimization iterations should be performed? The approximate solution usually improves\nwith more iterations, but each iteration costs computational resources. However, most traditional optimizers either ignore the cost of computation or select the number of iterations using simple heuristics. Because they do not balance the cost of computation against the performance loss, the overall effectiveness of these approaches is subject to the skill and preferences of the practitioners who use them. Second, which expert should be used on each step of the optimization? Some experts may be accurate but expensive to compute in terms of time, energy and/or money, while others may be crude, yet cheap. Moreover, the reliability of the experts may not be known a priori, further limiting the effectiveness of the optimization procedure. Our use of a metacontroller address these issues by jointly optimizing over the choices of how many steps to take and which experts to use.\nWe consider a family of optimizers which use the same controller, πC , but vary in their expert evaluators, {E1, . . . , EK}. Assuming that the controller and experts are deterministic functions, the number of iterationsN and the sequences of experts k = (k1, . . . , kN−1) exactly determine the final control and performance loss LP . This means we have transformed the performance optimization over c into an optimization over N and k: (N,k)∗ = argmink,n LP (x\n∗, x, c(N,k, x, x∗)), where the notation c(N,k, x, x∗) is used to emphasize that the control is a function N , k, x, and x∗.\nIf each optimizer has an associated computational cost τk, then N and k also exactly determine the computational resource loss of the optimization run, LR(N,k) = ∑N−1 n=1 τkn . The total loss is then the sum of LP and LR, each of which are functions of N and k, LT (x ∗, x,N,k) = LP (x ∗, x, c(N,k, x, x∗)) + LR(N,k) (3)\n= L(x∗, f(x, πC(x∗, x, hN−1))) + N−1∑ n=1 τkn , (4)\nand the optimal solution is defined as (N,k)∗ = argminN,k LT (x ∗, x,N,k). Optimizing LT is difficult because of the recursive dependency on the history, hN−1, and because the discrete choices of N and k mean LT is not differentiable.\nTo optimize LT we recast it as an RL problem where the objective is to jointly optimize task performance and computational cost. As shown in Figure 1a, the metacontroller agent aM is comprised of a controller πC , a pool of experts {E1, . . . , EK}, a manager πM , and a memory µ. The manager is a meta-level policy (Russell & Wefald, 1991; Hay et al., 2012) over actions indexed by k, which determine whether to terminate the optimization procedure (k = 0) or to perform another iteration of the optimization procedure with the kth expert. Specifically, on the nth iteration the controller produces a new control cn based on the history of controls, experts, and evaluations. The manager, also relying on this history, independently decides whether to end the optimization procedure (i.e., to execute the control in the world) or to perform another iteration and evaluate the proposed control with the kthn expert (i.e., to ponder, after Graves (2016)). The memory then updates the history hn by concatenating k, cn, and en with the previous history hn−1. Coming back to the notion of imagination-based optimization, we suggest that this iterative optimization process is analogous to imagining what will happen (using one or more approximate world models) before actually executing that action in the world. For further details, see Appendix A, and for an algorithmic illustration of the metacontroller agent, see Algorithm 1 in the appendix.\nWe also define two special cases of the metacontroller for baseline comparisons. The iterative agent aI does not have a manager and uses only a single expert. Its number of iterations are pre-set to a single N . The reactive agent, a0, is a special case of the iterative agent, where the number of iterations is fixed to N = 0. This implies that proposed controls are executed immediately in the world, and are not evaluated by an expert. For algorithmic illustrations of the iterative and reactive agents, see Algorithms 2 and 3 in the appendix.\n\n2.3 NEURAL NETWORK IMPLEMENTATION\nWe use standard deep learning building blocks, e.g., multi-layer perceptrons (MLPs), RNNs, etc., to implement the controller, experts, manager, and memory, because they are effective at approximating complex functions via gradient-based and reinforcement learning, but other approaches could be used as well. In particular, we constructed our implementation to be able to make control decisions in complex dynamical systems, such as controlling the movement of a spaceship (Figure 1b-c), though we note that our approach is not limited to such physical reasoning tasks. Here we used mean-squared error (MSE) for our L and Adam (Kingma & Ba, 2014) as the training optimizer.\nExperts We implemented the experts as MLPs and “interaction networks” (INs) (Battaglia et al., 2016), which are well-suited to predicting complex dynamical systems like those in our experiments below. Each expert has parameters θEk , i.e. en = Ek(x∗, x, cn; θEk), and may be trained either on-policy using the outputs of the controller (as is the case in this paper), or off-policy by any data that pairs states and controls with future states or reward outcomes. The objective LEk for each expert may be different depending on what the expert outputs. For example, the objective could be the loss between the goal and future states, LEk = L ( f(x, c), Ek(x ∗, x, c; θEk) ) , which is what we use in our experiments. Or, it could be the loss between LP and an action-value function that predicts LP directly, LEk = L ( LP (x ∗, x, c), Ek(x ∗, x, c; θEk) ) . See Appendix B.1 for details.\nController and Memory We implemented the controller as an MLP with parameters θC , i.e. cn = πC(x∗, x, hn−1; θ\nC), and we implemented the memory as a Long Short-Term Memory (LSTM) (Hochreiter & Schmidhuber, 1997) with parameters θµ. The memory embeds the history as a fixedlength vector, i.e. hn = µ(hn−1, kn, cn, Ekn(x ∗, x, cn); θ µ). The controller and memory were trained jointly to optimize (1). However, this objective includes f , which is often unknown or not differentiable. We overcame this by approximating LP with a differentiable critic analogous to those used in policy gradient methods (e.g. Silver et al., 2014; Lillicrap et al., 2015; Heess et al., 2015). See Appendices B.2 and B.3 for details.\nManager We implemented the manager as a stochastic policy that samples from a categorical distribution whose weights are produced by an MLP with parameters θM , i.e. kn ∼ Categorical(k;πM (x∗, x, hn−1; θ\nM )). We trained the manager to minimize (3) using REINFORCE (Williams, 1992), but other deep RL algorithms could be used instead. See Appendix B.4 for details.\n\n3 EXPERIMENTS\nTo evaluate our metacontroller agent, we measured its ability to learn to solve a class of physicsbased tasks that are surprisingly challenging. Each episode consisted of a scene which contained a spaceship and multiple planets (Figure 1b-c). The spaceship’s goal was to rendezvous with its mothership near the center of the system in exactly 11 time steps, but it only had enough fuel to fire its thrusters once. The planets were static but the gravitational force they exerted on the spacecraft induced complex non-linear dynamics on the motion over the 11 steps. The spacecraft’s action space was continuous, up to some maximum magnitude, and represented the instantaneous Cartesian velocity vector imparted by its thrusters. Further details are in Appendix C.\nWe trained the reactive, iterative, and metacontroller agents on five versions of the spaceship task involving different numbers of planets.1 The iterative agent was trained to take anywhere from zero (i.e., the reactive agent) to ten ponder steps. The metacontroller was allowed to take a maximum of ten ponder steps. We considered three different experts which were all differentiable: an MLP expert which used an MLP to predict the final location of the spaceship, an IN expert which used an interaction network (Battaglia et al., 2016) to predict the full trajectory of the spaceship, and a true simulation expert which was the same as the world model. In some conditions the metacontroller could use exactly one expert and in others it was allowed to select between the MLP and IN experts. For experiments with the true simulation expert, we used it to backpropagate gradients to the controller and memory. For experiments with an MLP as the only expert, we used a learned IN as the critic. For experiments with an IN as one of its experts, the critic was an IN with shared parameters. We trained the metacontroller on a range of different ponder costs, τk, for the different experts. Further details of the training procedure are available in Appendix D.\n\n3.1 REACTIVE AND ITERATIVE AGENTS\nFigure 2 shows the performance on the test set of the reactive and iterative agents for different numbers of ponder steps. The reactive agent performed poorly on the task, especially when the task was more difficult. With the five planets dataset, it was only able to achieve a performance loss of 0.583 on average (see Figure 1 for a depiction of the magnitude of the loss). In contrast, the iterative agent with the true simulation expert performed much better, reaching ceiling performance on the\n1Available from: https://www.github.com/deepmind/spaceship dataset\ndatasets with one and two planets, and achieving a performance loss of 0.0683 on the five planets dataset. The IN and MLP experts also improve over the reactive agent, with a minimum performance loss of 0.117 and 0.375 on the five planets dataset, respectively.\nFigure 2 also highlights how important the choice of expert is. When using the true simulation and IN experts, the iterative agent performs well. With the MLP expert, however, performance is substantially diminished. But despite the poor performance of the MLP expert, there is still some benefit of pondering with it. With even just a few steps, the MLP iterative agent outperforms its reactive counterpart. However comparing the reactive agent with the N = 1 iterative agent is somewhat unfair because the iterative agent has more parameters due to the expert and the memory. However, given that there tends to also be an increase in performance between one and two ponder steps (and beyond), it is clear that pondering—even with a highly inaccurate model—can still lead to better performance than a model-free reactive approach.\n\n3.2 METACONTROLLER WITH ONE EXPERT\nThough the iterative agents achieve impressive results, they expend more computation than necessary. For example, in the one and two planet conditions, the performances of the IN and true simulation iterative agents received little performance benefit from pondering more than two or three steps, while for the four and five planet conditions they required at least five to eight steps before their performance converged. When computational resources have no cost, the number of steps are of no concern, but when they have some cost it is important to be economical.\nBecause the metacontroller learns to choose its number of pondering steps, it can balance its performance loss against the cost of computation. Figure 3 (top row, middle and right subplots) shows that the IN and true simulation expert metacontroller take fewer ponder steps as τ increases, tracking closely the minimum of the iterative agent’s cost curve (i.e., the metacontroller points are always near the iterative agent curves’ minima). This adaptive behavior emerges automatically from the manager’s learned policy, and avoids the need to perform a hyperparameter search to find the best number of iterations for a given τ .\nThe metacontroller does not simply choose an average number of ponder steps to take per episode: it actually tailors this choice to the difficulty of each episode. Figure 4 shows how the number of ponder steps the IN metacontroller chooses in each episode depends on that episode’s difficulty, as measured by the episode’s loss under the reactive agent. For more difficult episodes, the metacontroller tends to take more ponder steps, as indicated by the positive slopes of the best fit lines, and this proportionality persists across the different levels of τ in each subplot.\nThe ability to adapt its choice of number of ponder steps on a per-episode basis is very valuable because it allows the metacontroller to spend additional computation only on those episodes which require it. The total costs of the IN and true simulation metacontrollers’ are 11% and 15% lower (median) than the best achievable costs of their corresponding iterative agents, respectively, across the range of τ values we tested (see Figure 7 in the Appendix for details).\nThere can even be a benefit to using a metacontroller when there are no computational resource costs. Consider the rightmost points in Figure 3 (bottom row, middle and right subplots), which show the performance loss for the IN and true simulation metacontrollers when τ is low. Remarkably, these points still outperform the best achievable iterative agents. This suggests that there can be an advantage to stopping pondering once a good solution is found, and more generally demonstrates that the metacontroller’s learning process can lead to strategies that are superior to those available to less flexible agents.\nThe metacontroller with the MLP expert had very poor average performance and high variance on the five planet condition (Figure 3, top left subplot), which is why we restricted our focus in this section to how the metacontrollers with IN and true simulation experts behaved. The MLP’s poor performance is crucial, however, for the following section (3.3) which analyzes how a multipleexpert metacontroller manages experts which vary greater in their reliability.\n\n3.3 METACONTROLLER WITH TWO EXPERTS\nWhen we allow the manager to additionally choose between two experts, rather than only relying on a single expert, we find a similar pattern of results in terms of the number of ponder steps (Figure 5, left). Additionally, the metacontroller is successfully able to identify the more reliable IN network and consequently uses it a majority of the time, except in a few cases where the cost of the IN network is extremely high relative to the cost of the MLP network (Figure 5, right). This pattern of results makes sense given the good performance (described in the previous section) of the metacontroller with the IN expert compared to the poor performance of the metacontroller with the MLP expert. The manager should not generally rely on the MLP expert because it is simply not a reliable source of information.\nHowever, the metacontroller has more difficulty finding an optimal balance between the two experts on a step-by-step basis: the addition of a second expert did not yield much of an improvement over the single-expert metacontroller, with only 9% of the different versions (trained with different τ values for the two experts) achieving a lower loss than the best iterative controller. We believe the mixed performance of the metacontroller with multiple experts is partially due to an entropy term which we used to encourage the manager’s policy to be non-deterministic (see Appendix B.4). In particular, for high values of τ , the optimal thing to do is to always execute immediately without pondering. However, because of the entropy term, the manager is encourage to have a non-deterministic policy and therefore is likely to ponder more than it should—and to use experts that are more unreliable— even when this is suboptimal in terms of the total loss (3).\nDespite the fact that the metacontroller with multiple experts does not result in a substantial improvement over that which uses a single expert, we emphasize that the manager is able to identify and use the more reliable expert the majority of the time. And, it is still able to choose a variable number of steps according to how difficult the task is (Figure 5, left). This, in and of itself, is an improvement over more traditional optimization methods which would require that the expert is hand-picked ahead of time and that the number of steps are determined heuristically.\n\n4 DISCUSSION\nIn this paper, we have presented an approach to adaptive, imagination-based optimization in neural networks. Our approach is able to flexibly choose which computations to perform as well as how many computations need to be performed, approximately solving a speed-accuracy trade-off that depends on the difficulty of the task. In this way, our approach learns to rely on whatever source of information is most useful and most efficient. Additionally, by consulting the experts on-the-fly, our approach allows agents to test out actions to ensure that their consequences are not disastrous before actually executing them.\nWhile the experiments in this paper involve a one-shot decision task, our approach lays a foundation that can be built upon to support more complex situations. For example, rather than applying a force only on the first time step, we could turn the problem into one of trajectory optimization for continuous control by asking the controller to produce a sequence of forces. In the case of planning, our approach could potentially be combined with methods like Monte Carlo Tree-Search (MCTS) (Coulom, 2006), where our experts would be akin to having several different rollout policies to choose from, and our controller would be akin to the tree policy. While most MCTS implementations will run rollouts until a fixed amount of time has passed, our approach would allow the manager to adaptively choose the number of rollouts to perform and which policies to perform the rollouts with. Our method could also be used to naturally augment existing model-free approaches such as DQN (Mnih et al., 2015) with online model-based optimization by using the model-free policy as a controller and adding additional experts in the form of state-transition models. An interesting extension would be to compare our metacontroller architecture with a naı̈ve model-based controller that performs gradient-based optimization to produce the final control. We expect our metacontroller architecture might require fewer model evaluations and to be more robust to model inaccuracies compared to the gradient-based method, because our method has access to the full history of proposed controls and evaluations whereas traditional gradient-based methods do not.\nAlthough we rely on differentiable experts in our metacontroller architecture, we do not utilize the gradient information from these experts. An interesting extension to our work would be to pass this gradient information through to the manager and controller (as in Andrychowicz et al. (2016)), which would likely improve performance further, especially in the more complex situations discussed here. Another possibility is to train some or all of the experts inline with the controller and metacontroller, rather than independently, which could allow their learned functionality to be more tightly integrated with the rest of the optimization loop, at the expense of their generality and ability to be repurposed for other uses.\nTo conclude, we have demonstrated how neural network-based agents can use metareasoning to adaptively choose what to think about, how to think about it, and for how long to think for. Our\nmethod is directly inspired by human cognition and suggests a way to make agents much more flexible and adaptive than they currently are, both in decision making tasks such as the one described here, as well as in planning and control settings more broadly.\n",
    "rationale": "This paper introduces an approach to reinforcement learning and control wherein, rather than training a single controller to perform a task, a metacontroller with access to a base-level controller and a number of accessory « experts » is utilized. The job of the metacontroller is to decide how many times to call the controller and the experts, and which expert to invoke at which iteration. (The controller is a bit special in that in addition to being provided the current state, it is given a summary of the history of previous calls to itself and previous experts.) The sequence of controls and expert advice is embedded into a fixed-size vector through an LSTM. The method is tested on an N-body  control task, where it is shown that there are benefits to multiple iterations (« pondering ») even for simple experts, and that the metacontroller can deliver accuracy and computational cost benefits over fixed-iteration controls.\n\nThe paper is in general well written, and reasonably easy to follow. As the authors note, the topic of metareasoning has been studied to some extent in AI, but its use as a differentiable and fully trainable component within an RL system appears new. At this stage, it is difficult to evaluate the impact of this kind of approach: the overall model architecture is intriguing and probably merits publication, but whether and how this will scale to other domains remains the subject of future work. The experimental validation is interesting and well carried out, but remains of limited scope. Moreover, given such a complex architecture, there should be a discussion of the training difficulties and convergence issues, if any.\n\nHere are a few specific comments, questions and suggestions:\n\n1) in Figure 1A, the meaning of the graphical language should be explained. For instance, there are arrows of different thickness and line style — do these mean different things? \n\n2) in Figure 3, the caption should better explain the contents of the figure. For example, what do the colours of the different lines refer to? Also, in the top row, there are dots and error bars that are given, but this is explained only in the « bottom row » part. This makes understanding this figure difficult.\n\n3) in Figure 4, the shaded area represents a 95% confidence interval on the regression line; in addition, it would be helpful to give a standard error on the regression slope (to verify that it excludes zero, i.e. the slope is significant), as well as a fraction of explained variance (R^2). \n\n4) in Figure 5, the fraction of samples using the MLP expert does not appear to decrease monotonically with the increasing cost of the MLP expert (i.e. the bottom left part of the right plot, with a few red-shaded boxes). Why is that? Is there lots of variance in these fractions from experiment to experiment?\n\n5) the supplementary materials are very helpful. Thank you for all these details.\n",
    "rating": 2
  },
  {
    "title": "LIE-ACCESS NEURAL TURING MACHINES",
    "abstract": "External neural memory structures have recently become a popular tool for algorithmic deep learning (Graves et al., 2014; Weston et al., 2014). These models generally utilize differentiable versions of traditional discrete memory-access structures (random access, stacks, tapes) to provide the storage necessary for computational tasks. In this work, we argue that these neural memory systems lack specific structure important for relative indexing, and propose an alternative model, Lieaccess memory, that is explicitly designed for the neural setting. In this paradigm, memory is accessed using a continuous head in a key-space manifold. The head is moved via Lie group actions, such as shifts or rotations, generated by a controller, and memory access is performed by linear smoothing in key space. We argue that Lie groups provide a natural generalization of discrete memory structures, such as Turing machines, as they provide inverse and identity operators while maintaining differentiability. To experiment with this approach, we implement a simplified Lie-access neural Turing machine (LANTM) with different Lie groups. We find that this approach is able to perform well on a range of algorithmic tasks.",
    "text": "1 INTRODUCTION\nRecent work on neural Turing machines (NTMs) (Graves et al., 2014; 2016) and memory networks (MemNNs) (Weston et al., 2014) has repopularized the use of explicit external memory in neural networks and demonstrated that these networks can be effectively trained in an end-to-end fashion. These methods have been successfully applied to question answering (Weston et al., 2014; Sukhbaatar et al., 2015; Kumar et al., 2015), algorithm learning (Graves et al., 2014; Kalchbrenner et al., 2015; Kaiser & Sutskever, 2015; Kurach et al., 2015; Zaremba & Sutskever, 2015; Grefenstette et al., 2015; Joulin & Mikolov, 2015), machine translation (Kalchbrenner et al., 2015), and other tasks. This methodology has the potential to extend deep networks in a general-purpose way beyond the limitations of fixed-length encodings such as standard recurrent neural networks (RNNs).\nA shared theme in many of these works (and earlier exploration of neural memory) is to re-frame traditional memory access paradigms to be continuous and possibly differentiable to allow for backpropagation. In MemNNs, traditional random-access memory is replaced with a ranking approach that finds the most likely memory. In the work of Grefenstette et al. (2015), classical stack-, queue-, and deque-based memories are replaced by soft-differentiable stack, queue, and deque datastructures. In NTMs, sequential local-access memory is simulated by an explicit tape data structure.\nThis work questions the assumption that neural memory should mimic the structure of traditional discrete memory. We argue that a neural memory should provide the following: (A) differentiability for end-to-end training and (B) robust relative indexing (perhaps in addition to random-access). Surprisingly many neural memory systems fail one of these conditions, either lacking Criterion B, discussed below, or employing extensions like REINFORCE to work around lack of differentiability (Zaremba & Sutskever, 2015).\nWe propose instead a class of memory access techniques based around Lie groups, i.e. groups with differentiable operations, which provide a natural structure for neural memory access. By definition, their differentiability satisfies the concerns of Criterion A. Additionally the group axioms provide identity, invertibility, and associativity, all of which are desirable properties for a relative indexing scheme (Criterion B), and all of which are satisfied by standard Turing machines. Notably though,\nsimple group properties like invertibility are not satisfied by neural Turing machines, differentiable neural computers, or even by simple soft-tape machines. In short, in our method, we construct memory systems with keys placed on a manifold, and where relative access operations are provided by Lie groups.\nTo experiment with this approach, we implement a neural Turing machine with an LSTM controller and several versions of Lie-access memory, which we call Lie-access neural Turing machines (LANTM). The details of these models are exhibited in Section 4.1 Our main experimental results are presented in Section 5. The LANTM model is able to learn non-trivial algorithmic tasks such as copying and permutating sequences with higher accuracy than more traditional memory-based approaches, and significantly better than fixed memory LSTM models. The memory structures and key transformation learned by the model resemble interesting continuous space representations of traditional discrete memory data structures.\n\n2 BACKGROUND: RECURRENT NEURAL NETWORKS WITH MEMORY\nThis work focuses particularly on recurrent neural network (RNN) controllers of abstract neural memories. Formally, an RNN is a differentiable function RNN : X × H → H, where X is an arbitrary input space and H is the hidden state space. On input (x(1), . . . , x(T )) ∈ X T and with initial state h(0) ∈ H, the RNN produces states h(1), . . . , h(T ) based on the recurrence,\nh(t) := RNN(x(t), h(t−1)).\nThese states can be used for downstream tasks, for example sequence prediction which produces outputs (y(1), . . . , y(T )) based on an additional transformation and prediction layer y(t) = F (h(t)) such as a linear-layer followed by a softmax. RNNs can be trained end-to-end by backpropagationthrough-time (BPTT) (Werbos, 1990). In practice, we use long short-term memory (LSTM) RNNs (Hochreiter & Schmidhuber, 1997). LSTM’s hidden state consists of two variables (c(t), h(t)), where h(t) is also the output to the external world; we however use the above notation for simplicity.\nAn RNN can also serve as the controller for an external memory system (Graves et al., 2014; Grefenstette et al., 2015; Zaremba & Sutskever, 2015), which enables: (1) the entire system to carry state over time from both the RNN and the external memory, and (2) the RNN controller to collect readings from and compute additional instructions to the external memory. Formally, we extend the recurrence to,\nh(t) := RNN([x(t); ρ(t−1)], h(t−1)),\nΣ(t), ρ(t) := RW(Σ(t−1), h(t)),\nwhere Σ is the abstract memory state, and ρ(t) is the value read from memory, and h is used as an abstract controller command to a read/write function RW. Writing occurs in the mutation of Σ at each time step. Throughout this work, Σ will take the form of an ordered set {(ki, vi, si)}i where ki ∈ K is an arbitrary key, vi ∈ Rm is a memory value, and si ∈ R+ is a memory strength. In order for the model to be trainable with backpropagation, the memory function RW must also be differentiable. Several forms of differentiable memory have been proposed in the literature. We begin by describing two simple forms: (neural) random-access memory and (neural) tape-based memory. For this section, we focus on the read step and assume Σ is fixed.\nRandom-Access Memory Random-access memory consists of using a now standard attentionmechanism or MemNN to read a memory (our description follows Miller et al. (2016)). The controller hidden state is used to output a random-access pointer, q′(h) that determines a weighting of memory vectors via dot products with the corresponding keys. This weighting in turn determines the read values via linear smoothing based on a function w,\nwi(q,Σ) := si exp 〈q, ki〉∑\nj sj exp 〈q, kj〉 ρ := ∑ i wi(q ′(h),Σ)vi.\nThe final read memory is based on how “close” the read pointer was to each of the keys, where closeness in key space is determined by w.\n1Our implementations are available at https://github.com/harvardnlp/lie-access-memory\nTape-Based Memory Neural memories can also be extended to support relative access by maintaining read state. Following notation from Turing machines, we call this state the head, q. In the simplest case the recurrence now has the form,\nΣ′, q′, ρ = RW(Σ, q, h),\nand this can be extended to support multiple heads.\nIn the simplest case of soft tape-based memory (a naive version of the much more complicated neural Turing machine), the keys ki indicate one-hot positions along a tape with ki = δi. The head q is a probability distribution over tape positions. It determines the read value by directly specifying the weights. The controller can only “shift” the head by outputting a kernel K(h) = (K−1,K0,K+1) in the probability simplex ∆2 and applying convolution.\nq′(q, h) := q ∗K(h), i.e. q′j = qj−1K+1 + qjK0 + qj+1K−1\nWe can view this as the soft version of a single-step discrete Turing machine where the kernel can softly shift the “head” of the machine one to the left, one to the right, or remain in the same location. The value returned can then be computed with linear smoothing as above,\nwi(q,Σ) := si〈q, ki〉∑\nj sj〈q, kj〉 ρ := ∑ i wi(q ′(q, h),Σ)vi.\n\n3 LIE GROUPS FOR MEMORY\nLet us now take a brief digression and consider the standard (non-neural) Turing machine (TM) and the movement of its head over a tape. A TM has a head q ∈ Z indicating the position on a tape. Between reads, the head can move any number of steps left or right. Moving a + b steps and then c steps eventually puts the head at the same location as moving a steps and then b + c steps — i.e. the head movement is associative. In addition, the machine should be able to reverse a head shift, for example, in a stack simulation algorithm, going from push to pop — i.e. each head movement should also have a corresponding inverse. Finally, the head should also be allowed to stay put, for example, to read a single data item and use it for multiple time points, an identity.\nThese movements correspond directly to group actions: the possible head movements should be associative, and contain inverse and identity elements. This group acts on the set of possible head locations. In a TM, the set of Z-valued head movement acts on the set of locations on the Z-indexed infinite tape. By our reasoning above, if a Turing machine is to store data contents at points in a general space K (instead of an infinite Z-indexed tape), then its head movements should form a group and act on K via group actions. For a neural memory system, we desire the network to be (almost everywhere) differentiable. The notion of “differentiable” groups is well-studied in mathematics, where they are known as Lie groups, and “differentiable group actions” are correspondingly called Lie group actions. In our case, using Lie group actions as generalized head movements on a general key space (more accurately, manifolds) would most importantly mean that we can take derivatives of these movements and perform the usual backpropagation algorithm.\n\n4 LIE-ACCESS NEURAL TURING MACHINES\nThese properties motivate us to propose Lie access as an alternative formalism to popular neural memory systems, such as probabilistic tapes, which surprisingly do not satisfy invertibility and often do not provide an identity.2 Our Lie-access memory will consist of a set of points in a manifold K.\n2The Markov kernel convolutional soft head shift mechanism proposed in Graves et al. (2014) and sketched in Section 2 does not in general have inverses. Indeed, the authors reported problems with the soft head losing “sharpness” over time, which they dealt with by sharpening coefficients. In the followup work, Graves et al. (2016) utilize a temporal memory link matrix for actions. They note, “the operation Lw smoothly shifts the focus forwards to the locations written ... whereas L>w shifts the focus backwards” but do not enforce this as a true inverse. They also explicitly do not include an identity, noting “Self-links are excluded (the diagonal of the link matrix is always 0)”; however, they could ignore the link matrix with an interpolation gate, which in effect acts as the identity.\nWe replace the discrete head with a continuous head q ∈ K. The head moves based on a set of Lie group actions a ∈ A generated by the controller. To read memories, we will rely on a distance measure in this space, d : K × K → R≥0.3 Together these properties describe a general class of possible neural memory architectures.\nFormally a Lie-access neural Turing machine (LANTM) computes the following function,\nΣ′, q′, q′(w), ρ := RW(Σ, q, q(w), h)\nwhere q, q(w) ∈ K are resp. read and write heads, and Σ is the memory itself. We implement Σ, as above, as a weighted dictionary Σ = {(ki, vi, si)}i.\n\n4.1 ADDRESSING PROCEDURE\nThe LANTM maintains a read head q which at every step is first updated to q′ and then used to read from the memory table. This update occurs by selecting a Lie group action from A which then acts smoothly on the key space K. We parametrize the action transformation, a : H 7→ A by the hidden state to produce the Lie action, a(h) ∈ A. In the simplest case, the head is then updated based on this action (here · denotes group action): q′ := a(h) · q. For instance, consider two possible Lie groups:\n(1) A shift group R2 acting additively on R2. This means that A = R2 so that a(h) = (α, β) acts upon a head q = (x, y) by,\na(h) · q = (α, β) + (x, y) = (x+ α, y + β).\n(2) A rotation group SO(3) acting on the sphere S2 = {v ∈ R3 : ‖v‖ = 1}. Each rotation can be described by its axis ξ (a unit vector) and angle θ. An action (ξ, θ) · q is just the appropriate rotation of the point q, and is given by Rodrigues’ rotation formula,\na(h) · q = (ξ, θ) · q = q cos θ + (ξ × q) sin θ + ξ〈ξ, q〉(1− cos θ). Here × denotes cross product.\n\n4.2 READING AND WRITING MEMORIES\nRecall that memories are stored in Σ, each with a key, ki, memory vector, vi, and strength, si, and that memories are read using linear smoothing over vectors based on a key weighting function w, ρ := ∑ i wi(q\n′,Σ)vi . While there are many possible weighting schemes, we use one based on the distance of each memory address from the head in key-space assuming a metric d on K. We consider two different weighting functions (1) inverse-square and (2) softmax. There first uses the polynomial law and the second an annealed softmax of the squared distances:\nw (1) i (q,Σ) :=\nsid(q, ki) −2∑\nj sjd(q, kj) −2 w\n(2) i (q,Σ, T ) := si exp(−d(q, ki)2/T )∑ j sj exp(−d(q, kj)2/T ) ,\nwhere we use the convention that it takes the limit value when q → ki and T is a temperature that represents the certainty of its reading, i.e. higher T creates more uniform w.\nThe writing procedure is similar to reading. The LANTM maintains a separate write head q(w) that moves analogously to the read head, i.e. with action function a(w)(h) and updated value q′(w) . At each call to RW, a new memory is automatically appended to Σ with k = q′(w). The corresponding\n3This metric should satisfy a compatibility relation with the Lie group action. When points x, y ∈ X are simultaneously moved by the same Lie group action v, their distance should stay the same (One possible mathematical formalization is thatX should be a Riemannian manifold and the Lie group should be a subgroup of X’s isometry group.): d(vx, vy) = d(x, y). This condition ensures that if the machine writes a sequence of data along a “straight line” at points x, vx, v2x, . . . , vkx, then it can read the same sequence by emitting a read location y close to x and then follow the “v-trail” y, vy, v2y, . . . , vky.\nmemory v and strength s are created by MLP’s v(h) ∈ Rm and s(h) ∈ [0, 1] taking h as input. After writing, the new memory set is,\nΣ′ := Σ ∪ {(q′(w), v(h), s(h))}.\nNo explicit erase mechanism is provided, but to erase a memory (k, v, s), the controller may in theory write (k,−v, s).\n\n4.3 COMBINING WITH RANDOM ACCESS\nFinally we combine this relative addressing procedure with direct random-access to give the model the ability for absolute address access. We do this by outputting an absolute address each step and simply interpolating with our current head. Write t(h) ∈ [0, 1] for the interpolation gate and q̃(h) ∈ K for our proposed random-access layer. For key space manifolds K like Rn, 4 there’s a well defined straight-line interpolation between two points, so we can set\nq′ := a · (tq + (1− t)q̃)\nwhere we have omitted the implied dependence on h. For other manifolds like the spheres Sn that have well-behaved projection functions π : Rn → Sn, we can just project the straight-line interpolation to the sphere:\nq′ := a · π(tq + (1− t)q̃).\nIn the case of a sphere Sn, π is just L2-normalization.5\n\n5 EXPERIMENTS\nWe experiment with Lie-access memory on a variety of algorithmic learning tasks. We are particularly interested in: (a) how Lie-access memory can be trained, (b) whether it can be effectively utilized for algorithmic learning, and (c) what internal structures the model learns compared to systems based directly on soft discrete memory. In particular Lie access is not equipped with an explicit stack or tape, so it would need to learn continuous patterns that capture these properties.\nSetup. Our experiments utilize an LSTM controller in a version of the encoder-decoder setup (Sutskever et al., 2014), i.e. an encoding input pass followed by a decoding output pass. The encoder reads and writes memories at each step; the decoder only reads memories. The encoder is given 〈s〉,\n4Or in general, manifolds with convex embeddings in Rn. 5Technically, in the sphere case, domπ = Rd − {0}. But in practice one almost never gets 0 from a\nstraight-line interpolation, so computationally this makes little difference.\nfollowed by an the input sequence, and then 〈/s〉 to terminate input. The decoder is not re-fed its output or the correct symbol, i.e. we do not use teacher forcing, so x(t) is a fixed placeholder input symbol. The decoder must correctly emit an end-of-output symbol 〈/e〉 to terminate. Models and Baselines. We implement three main baseline models including: (a) a standard LSTM encoder-decoder, without explicit external memory, (b) a random access memory network, RAM using the key-value formulation as described in the background, roughly analogous to an attentionbased encoder-decoder, and (c) an interpolation of a RAM/Tape-based memory network as described in the background, i.e. a highly simplified version of a true NTM (Graves et al., 2014) with a sharpening parameter. Our models include four versions of Lie-access memory. The main model, LANTM, has an LSTM controller, with a shift group A = R2 acting additively on key space K = R2. We also consider a model SLANTM with spherical memory, utilizing a rotation group A = SO(3) acting on keys in the sphere K = S2. For both of the models, the distance function d is the Euclidean (L2) distance, and we experiment with smoothing using inverse-square (default) and with an annealed softmax.6\nModel Setup. For all tasks, the LSTM baseline has 1 to 4 layers, each with 256 cells. Each of the other models has a single-layer, 50-cell LSTM controller, with memory width (i.e. the size of each memory vector) 20. Other parameters such as learning rate, decay, and intialization are found through grid search. Further hyperparameter details are give in the appendix.\nTasks. Our experiments are on a series of algorithmic tasks shown in Table 1a. The COPY, REVERSE, and BIGRAM FLIP tasks are based on Grefenstette et al. (2015); the DOUBLE and INTERLEAVED ADD tasks are designed in a similar vein. Additionally we also include three harder tasks: ODD FIRST, REPEAT COPY, and PRIORITY SORT. In ODD FIRST, the model must output the oddindexed elements first, followed by the even-indexed elements. In REPEAT COPY, each model must repeat a sequence of length 20, N times. In PRIORITY SORT, each item of the input sequence is given a priority, and the model must output them in priority order.\nWe train each model in two regimes, one with a small number of samples (16K) and one with a large number of samples (320K). In the former case, the samples are iterated through 20 times, while in the latter, the samples are iterated through only once. Thus in both regimes, the total training times are the same. Training is done by minimizing negative log likelihood with RMSProp.\nPrediction is performed via argmax/greedy prediction at each step. To evaluate the performance of the models, we compute the fraction of tokens correctly predicted and the fraction of all answers completely correctly predicted, respectively called fine and coarse scores. We assess the models on 3.2K randomly generated out-of-sample 2x length examples, i.e. with sequence lengths 2k (or repeat number 2N in the case of REPEAT COPY) to test the generalization of the system. More precisely, for all tasks other than repeat copy, during training, the length k is varied in the interval [lk, uk] (as shown in table 1ba). During test time, the length k is varied in the range [uk + 1, 2uk]. For repeat copy, the repetition number N is varied similarly, instead of k.\nResults. Main results comparing the different memory systems and read computations on a series of tasks are shown in Table 1b. Consistent with previous work the fixed-memory LSTM system fails consistently when required to generalize to the 2x samples, unable to solve any 2x problem correctly, and only able to predict at most ∼ 50% of the symbols for all tasks except interleaved addition, regardless of training regime. The RAM (attention-based) and the RAM/tape hybrid are much stronger baselines, answering more than 50% of the characters correctly for all but the 6-ODD FIRST task. Perhaps surprisingly, RAM and RAM/tape learned the 7-REPEAT COPY task with almost perfect generalization scores when trained in the large sample regime. In general, it does not seem that the simple tape memory confers much advantage to the RAM model, as the generalization performances of both models are similar for the most part, which motivates more advanced NTM enhancements beyond sharpening.\nThe last four columns illustrate the performance of the LANTM models. We found the inversesquare LANTM and SLANTM models to be the most effective, achieving > 90% generalization\n6Note that the read weight calculation of a SLANTM with softmax is essentially the same as the RAM model: For head q, exp(−d(q, ki)2/T ) = exp(−‖q − ki‖2/T ) = exp(−(2 − 2〈q, ki〉)/T ), where the last equality comes from ‖q‖ = ‖ki‖ = 1 (key-space is on the sphere). Therefore the weights wi =\nsi exp(−d(q,ki)2/T )∑ j sj exp(−d(q,kj)2/T ) = si exp(−2〈q,ki〉/T )∑ j sj exp(−2〈q,kj〉/T ) , which is the RAM weighting scheme.\nTask Input Output Size k |V| 1 - COPY a1a2a3 · · · ak a1a2a3 · · · ak [2, 64] 128 2 - REVERSE a1a2a3 · · · ak akak−1ak−2 · · · a1 [2, 64] 128 3 - BIGRAM FLIP a1a2a3a4 · · · a2k−1a2k a2a1a4a3 · · · a2ka2k−1 [1, 16] 128 4 - DOUBLE a1a2 · · · ak 2× |ak · · · a1| [2, 40] 10 5 - INTERLEAVED ADD a1a2a3a4 · · · a2k−1a2k |a2ka2k−2 · · · a2|+ |a2k−1 · · · a1| [2, 16] 10 6 - ODD FIRST a1a2a3a4 · · · a2k−1a2k a1a3 · · · a2k−1a2a4 · · · a2k [1, 16] 128 7 - REPEAT COPY Na1 · · · a20 a1 · · · a20 · · · a1 · · · a20 (N times) N ∈ [1, 5] 128 8 - PRIORITY SORT 5a52a29a9 · · · a1a2a3 · · · ak [2, 10] 128\n(a) Task descriptions and parameters. |ak · · · a1| means the decimal number repesented by decimal digits ak · · · a1. Arithmetic tasks have all numbers formatted with the least significant digits on the left and with zero padding. The DOUBLE task takes an integer x ∈ [0, 10k) padded to k digits and outputs 2x in k + 1 digits, zero padded to k+ 1 digits. The INTERLEAVED ADD task takes two integers x, y ∈ [0, 10k) padded to k digits and interleaved, forming a length 2k input sequence and outputs x + y zero padded to k + 1 digits. The last two tasks use numbers in unary format: N is the shorthand for a length N sequence of a special symbol @, encoding N in unary, e.g. 3 = @@@.\nBase Memory Lie LSTM RAM RAM/Tape LANTM LANTM-s SLANTM SLANTM-s\nS L S L S L S L S L S L S L\n1 16/0 21/0 61/0 61/1 70/2 70/1 ? ? ? ? ? ? ? ? 2 26/0 32/0 58/2 54/2 24/1 43/2 ? ? 97/44 98/88 99/96 ? ? ? 3 30/0 39/0 56/5 54/9 64/8 69/9 ? ? ? 99/94 99/99 97/67 93/60 90/43 4 44/0 47/0 72/8 74/15 70/12 71/6 ? ? ? ? ? ? ? ? 5 60/0 61/0 74/13 76/17 77/23 67/19 99/93 99/93 90/38 94/57 99/91 99/97 98/78 ? 6 29/0 42/0 31/5 46/4 43/8 62/8 99/91 99/95 90/29 50/0 49/7 56/8 74/15 76/16 7 24/0 37/0 98/56 99/98 71/18 99/93 67/0 70/0 17/0 48/0 99/91 99/78 96/41 99/51 8 46/0 53/0 60/5 80/22 78/15 66/9 87/35 98/72 99/95 99/99 ? 99/99 98/79 ?\n(b) Main results. Numbers represent the accuracy percentages on the fine/coarse evaluations on the out-ofsample 2× tasks. The S and L columns resp. indicate small and large sample training regimes. Symbol ? indicates exact 100% accuracy (Fine scores above 99.5 are not rounded up). Baselines are described in the body. LANTM and SLANTM use inverse-square while LANTM-s and SLANTM-s use softmax weighting scheme. The best scores, if not 100% (denoted by stars), are bolded for each of the small and large sample regimes.\naccuracy on most tasks, and together they solve all of the tasks here with > 90% coarse score. In particular, LANTM is able to solve the 6-ODD FIRST problem when no other model can correctly solve 20% of the 2x instances; SLANTM on the other hand is the only Lie access model able to solve the 7-REPEAT COPY problem.\nThe best Lie access model trained with the small sample regime beats or is competitive with any of the baseline trained under the large sample regime. In all tasks other than 7-REPEAT COPY, the gap in the coarse score between the best Lie access model in small sample regime and the best baseline in any sample regime is ≥ 70%. However, in most cases, training under the large sample regime does not improve much. For a few tasks, small sample regime actually produces a model with better generalization than large sample regime. We observed in these instances, the generalization error curve under a large sample regime reaches an optimum at around 2/3 to 3/4 of training time, and then increases almost monotonically from there. Thus, the model likely has found an algorithm that works only for the training sizes; in particular, this phenomenon does not seem to be due to lack of training time.\n\n6 DISCUSSION\nQualitative Analysis. We did further visual analysis of the different Lie-access techniques to see how the models were learning the underlying tasks, and to verify that they were using the relative addressing scheme. Figure 2 shows two diagrams of the LANTM model of the tasks of priority sort and repeat copy. Figure 3 shows two diagrams of the SLANTM model for the same two tasks. Fig-\nure 4 shows the memory access pattern of LANTM on 6-ODD FIRST task. Additionally, animations tracing the evolution of the memory access pattern of models over training time can be found at http://nlp.seas.harvard.edu/lantm. They demonstrate that the models indeed learn relative addressing and internally are constructing geometric data structures to solve these algorithmic tasks.\nUnbounded storage One possible criticism of the LANTM framework could be that the amount of information stored increases linearly with time, which limits the usefulness of this framework for long timescale tasks. This is indeed the case with our implementations, but need not be the case in general. There can be many ways of limiting physical memory usage. For example, a simple way is to discard the least recently used memory, as in the work of Graves et al. (2016) and Gulcehre et al. (2016). Another way is to approximate with fixed number of bits the read function that takes a head position and returns the read value. For example, noting that this function is a rational function on the head position, keys, and memory vectors, we can approximate the numerators and denominators with a fixed degree polynomial.\nContent address Our Lie-access framework is not mutually exclusive from content addressing methods. For example, in each of our implementations, we could have the controllers output both a position in the key space and a content addresser of the same size as memory vectors, and interpolated the read values from Lie-access and the read values from content addressing.\n\n7 CONCLUSION\nThis paper introduces Lie-access memory as an alternative neural memory access paradigm, and explored several different implementations of this approach. LANTMs follow similar axioms as discrete Turing machines while providing differentiability. Experiments show that simple models can learn algorithmic tasks. Internally these models naturally learn equivalence of standard data structures like stack and cyclic lists. In future work we hope to experiment with more groups and to scale these methods to more difficult reasoning tasks. For instance, we hope to build a general purpose encoder-decoder model for tasks like question answering and machine translation that makes use of differentiable relative-addressing schemes to replace RAM-style attention.\n",
    "rationale": "The Neural Turing Machine and related “external memory models” have demonstrated an ability to learn algorithmic solutions by utilizing differentiable analogues of conventional memory structures. In particular, the NTM, DNC and other approaches provide mechanisms for shifting a memory access head to linked memories from the current read position.\n\nThe NTM, which is the most relevant to this work, uses a differentiable version of a Turing machine tape. The controller outputs a kernel which “softly” shifts the head, allowing the machine to read and write sequences. Since this soft shift typically “smears” the focus of the head, the controller also outputs a sharpening parameter which compensates by refocusing the distribution.\n\nThe premise of this work is to notice that while the NTM emulates a differentiable version of a Turing tape, there is no particular reason that one is constrained to follow the topology of a Turing tape. Instead they propose memory stored at a set of points on a manifold and shift actions which form a Lie group. In this way, memory points can have have different relationships to one another, rather than being constrained to Z.\n\nThis is mathematically elegant and here they empirically test models with the shift group R^2 acting on R^2 and the rotation group acting on a sphere.\n\nOverall, the paper is well communicated and a novel idea.\n\nThe primary limitation of this paper is its limited impact. While this approach is certainly mathematically elegant, even likely beneficial for some specific problems where the problem structure matches the group structure, it is not clear that this significantly contributes to building models capable of more general program learning. Instead, it is likely to make an already complex and slow model such as the NTM even slower. In general, it would seem memory topology is problem specific and should therefore be learned rather than specified.\n\nThe baseline used for comparison is a very simple model, which does not even having the sharpening (the NTM approach to solving the problem of head distributions becoming ‘smeared’). There is also no comparison with the successor to the NTM, the DNC, which provides a more general approach to linking memories based on prior memory accesses.\n\nMinor issues:\nFootnote on page 3 is misleading regarding the DNC. While the linkage matrix explicitly excludes the identity, the controller can keep the head in the same position by gating the following of the link matrix.\nFigures on page 8 are difficult to follow.\n",
    "rating": 2
  },
  {
    "title": "THIRD-PERSON IMITATION LEARNING",
    "abstract": "Reinforcement learning (RL) makes it possible to train agents capable of achieving sophisticated goals in complex and uncertain environments. A key difficulty in reinforcement learning is specifying a reward function for the agent to optimize. Traditionally, imitation learning in RL has been used to overcome this problem. Unfortunately, hitherto imitation learning methods tend to require that demonstrations are supplied in the first-person: the agent is provided with a sequence of states and a specification of the actions that it should have taken. While powerful, this kind of imitation learning is limited by the relatively hard problem of collecting first-person demonstrations. Humans address this problem by learning from third-person demonstrations: they observe other humans perform tasks, infer the task, and accomplish the same task themselves. In this paper, we present a method for unsupervised third-person imitation learning. Here third-person refers to training an agent to correctly achieve a simple goal in a simple environment when it is provided a demonstration of a teacher achieving the same goal but from a different viewpoint; and unsupervised refers to the fact that the agent receives only these third-person demonstrations, and is not provided a correspondence between teacher states and student states. Our methods primary insight is that recent advances from domain confusion can be utilized to yield domain agnostic features which are crucial during the training process. To validate our approach, we report successful experiments on learning from third-person demonstrations in a pointmass domain, a reacher domain, and inverted pendulum.",
    "text": "1 INTRODUCTION\nReinforcement learning (RL) is a framework for training agents to maximize rewards in large, unknown, stochastic environments. In recent years, combining techniques from deep learning with reinforcement learning has yielded a string of successful applications in game playing and robotics Mnih et al. (2015; 2016); Schulman et al. (2015a); Levine et al. (2016). These successful applications, and the speed at which the abilities of RL algorithms have been increasing, makes it an exciting area of research with significant potential for future applications.\nOne of the major weaknesses of RL is the need to manually specify a reward function. For each task we wish our agent to accomplish, we must provide it with a reward function whose maximizer will precisely recover the desired behavior. This weakness is addressed by the field of Inverse Reinforcement Learning (IRL). Given a set of expert trajectories, IRL algorithms produce a reward function under which these the expert trajectories enjoy the property of optimality. Recently, there has been a significant amount of work on IRL, and current algorithms can infer a reward function from a very modest number of demonstrations (e.g,. Abbeel & Ng (2004); Ratliff et al. (2006); Ziebart et al. (2008); Levine et al. (2011); Ho & Ermon (2016); Finn et al. (2016)).\nWhile IRL algorithms are appealing, they impose the somewhat unrealistic requirement that the demonstrations should be provided from the first-person point of view with respect to the agent. Human beings learn to imitate entirely from third-person demonstrations – i.e., by observing other humans achieve goals. Indeed, in many situations, first-person demonstrations are outright impossible to obtain. Meanwhile, third-person demonstrations are often relatively easy to obtain.\nThe goal of this paper is to develop an algorithm for third-person imitation learning. Future advancements in this class of algorithms would significantly improve the state of robotics, because it will enable people to easily teach robots news skills and abilities. Importantly, we want our algorithm to be unsupervised: it should be able to observe another agent perform a task, infer that there is an underlying correspondence to itself, and find a way to accomplish the same task.\nWe offer an approach to this problem by borrowing ideas from domain confusion Tzeng et al. (2014) and generative adversarial networks (GANs) Goodfellow et al. (2014). The high-level idea is to introduce an optimizer under which we can recover both a domain-agnostic representation of the agent’s observations, and a cost function which utilizes this domain-agnostic representation to capture the essence of expert trajectories. We formulate this as a third-person RL-GAN problem, and our solution builds on the first-person RL-GAN formulation by Ho & Ermon (2016).\nSurprisingly, we find that this simple approach has been able to solve the problems that are presented in this paper (illustrated in Figure 1), even though the student’s observations are related in a complicated way to the teacher’s demonstrations (given that the observations and the demonstrations are pixel-level). As techniques for training GANs become more stable and capable, we expect our algorithm to be able to infer solve harder third-person imitation tasks without any direct supervision.\n\n2 RELATED WORK\nImitation learning (also learning from demonstrations or programming by demonstration) considers the problem of acquiring skills from observing demonstrations. Imitation learning has a long history, with several good survey articles, including (Schaal, 1999; Calinon, 2009; Argall et al., 2009). Two main lines of work within imitation learning are: 1) behavioral cloning, where the demonstrations are used to directly learn a mapping from observations to actions using supervised learning, potentially with interleaving learning and data collection (e.g., Pomerleau (1989); Ross et al. (2011)). 2) Inverse reinforcement learning (Ng et al., 2000), where a reward function is estimated that explains the demonstrations as (near) optimal behavior. This reward function could be represented as nearness to a trajectory (Calinon et al., 2007; Abbeel et al., 2010), as a weighted combination of features (Abbeel & Ng, 2004; Ratliff et al., 2006; Ramachandran & Amir, 2007; Ziebart et al., 2008; Boularias et al., 2011; Kalakrishnan et al., 2013; Doerr et al., 2015), or could also involve feature learning (Ratliff et al., 2007; Levine et al., 2011; Wulfmeier et al., 2015; Finn et al., 2016; Ho & Ermon, 2016).\nThis past work, however, is not directly applicable to the third person imitation learning setting. In third-person imitation learning, the observations and actions obtained from the demonstrations are not the same as what the imitator agent will be faced with. A typical scenario would be: the imitator agent watches a human perform a demonstration, and then has to execute that same task. As discussed in Nehaniv & Dautenhahn (2001) the ”what and how to imitate” questions become significantly more challenging in this setting. To directly apply existing behavioral cloning or inverse reinforcement learning techniques would require knowledge of a mapping between observations and actions in the demonstrator space to observations and actions in the imitator space. Such a mapping is often difficult to obtain, and it typically relies on providing feature representations that captures the invariance between both environments Carpenter et al. (2002); Shon et al. (2005); Calinon et al. (2007); Nehaniv (2007); Gioioso et al. (2013); Gupta et al. (2016). Contrary to prior work, we consider third-person imitation learning from raw sensory data, where no such features are made available.\nThe most closely related work to ours is by Finn et al. (2016); Ho & Ermon (2016); Wulfmeier et al. (2015), who also consider inverse reinforcement learning directly from raw sensory data. However, the applicability of their approaches is limited to the first-person setting. Indeed, matching raw sensory observations is impossible in the 3rd person setting.\nOur work also closely builds on advances in generative adversarial networks Goodfellow et al. (2014), which are very closely related to imitation learning as explained in Finn et al. (2016); Ho & Ermon (2016). In our optimization formulation, we apply the gradient flipping technique from Ganin & Lempitsky (2014).\nThe problem of adapting what is learned in one domain to another domain has been studied extensively in computer vision in the supervised learning setting Yang et al. (2007); Mansour et al. (2009); Kulis et al. (2011); Aytar & Zisserman (2011); Duan et al. (2012); Hoffman et al. (2013); Long & Wang (2015). It has also been shown that features trained in one domain can often be relevant to other domains Donahue et al. (2014). The work most closely related to ours is Tzeng et al. (2014; 2015), who also consider an explicit domain confusion loss, forcing trained classifiers to rely on features that don’t allow to distinguish between two domains. This work in turn relates to earlier work by Bromley et al. (1993); Chopra et al. (2005), which also considers supervised training of deep feature embeddings.\nOur approach to third-person imitation learning relies on reinforcement learning from raw sensory data in the imitator domain. Several recent advances in deep reinforcement learning have made this practical, including Deep Q-Networks (Mnih et al., 2015), Trust Region Policy Optimization (Schulman et al., 2015a), A3C Mnih et al. (2016), and Generalized Advantage Estimation (Schulman et al., 2015b). Our approach uses Trust Region Policy Optimization.\n\n3 BACKGROUND AND PRELIMINARIES\nA discrete-time finite-horizon discounted Markov decision process (MDP) is represented by a tuple M = (S,A,P, r, ρ0, γ, T ), in which S is a state set, A an action set, P : S × A × S → R+ a transition probability distribution, r : S × A → R a reward function, ρ0 : S → R+ an initial state distribution, γ ∈ [0, 1] a discount factor, and T the horizon. In the reinforcement learning setting, the goal is to find a policy πθ : S × A → R+ parametrized by θ that maximizes the expected discounted sum of rewards incurred, η(πθ) = Eπθ [ ∑T t=0 γ\ntc(st)], where s0 ∼ ρ0(s0), at ∼ πθ(at|st), and st+1 ∼ P(st+1|st, at). In the (first-person) imitation learning setting, we are not given the reward function. Instead we are given traces (i.e., sequences of states traversed) by an expert who acts according to an unknown policy πE . The goal is to find a policy πθ that performs as well as the expert against the unknown reward function. It was shown in Abbeel & Ng (2004) that this can be achieved through inverse reinforcement learning by finding a policy πθ that matches the expert’s empirical expectation over discounted sum of all features that might contribute to the reward function. The work by Ho & Ermon (2016) generalizes this to the setting when no features are provided as follows: Find a policy πθ that makes it impossible for a discriminator (in their work a deep neural net) to distinguish states visited by the expert from states visited by the imitator agent. This can be formalized as follows:\nmax πθ min DR − Eπθ [logDR(s)]− EπE [log(1−DR(s))] (1)\nHere, the expectations are over the states experienced by the policy of the imitator agent, πθ, and by the policy of the expert, πE , respectively. DR is the discriminator, which outputs the probability of a state having originated from a trace from the imitator policy πθ. If the discriminator is perfectly able to distinguish which policy originated state-action pairs, then DR will consistently output a probability of 1 in the first term, and a probability of 0 in the second term, making the objective its lowest possible value of zero. It is the role of the imitator agent πθ to find a policy that makes it difficult for the discriminator to make that distinction. The desired equilibrium has the imitator agent making it impractical for the discriminator to distinguish, hence forcing the discriminator to assign probability 0.5 in all cases. Ho & Ermon (2016) present a practical approach for solving this type of game when representing both πθ and DR as deep neural networks. Their approach repeatedly performs gradient updates on each of them. Concretely, for a current policy πθ traces can be collected, which together with the expert traces form a data-set on which DR can be trained with supervised learning minimizing the negative log-likelihood (in practice only performing a modest number of updates). For a fixed DR, this is a policy optimization problem where − logDR(s, a) is the reward, and policy gradients can be computed from those same traces. Their approach uses trust region policy optimization (Schulman et al., 2015a) to update the imitator policy πθ from those gradients.\nIn our work we will have more terms in the objective, so for compactness of notation, we will realize the discriminative minimization from Eqn. (1) as follows:\nmax πθ min DR LR = ∑ i CE(DR(si), c`i) (2)\nWhere si is state i, c`i is the correct class label (was the state si obtained from an expert vs. from a non-expert), and CE is the standard cross entropy loss.\n\n4 A FORMAL DEFINITION OF THE THIRD-PERSON IMITATION LEARNING PROBLEM\nFormally, the third-person imitation learning problem can be stated as follows. Suppose we are given two Markov Decision Processes MπE and Mπθ . Suppose further there exists a set of traces ρ = {(s1, . . . , sn)}ni=0 which were generated under a policy πE acting optimally under some unknown reward RπE . In third-person imitation learning, one attempts to recover by proxy through ρ a policy πθ = f(ρ) which acts optimally with respect to Rπθ .\n\n5 A THIRD-PERSON IMITATION LEARNING ALGORITHM\n\n\n5.1 GAME FORMULATION\nIn this section, we discuss a simple algorithm for third-person imitation learning. This algorithm is able to successfully discriminate between expert and novice policies, even when the policies are executed under different environments. Subsequently, this discrimination signal can be used to train expert policies in new domains via RL by training the novice policy to fool the discriminator, thus forcing it to match the expert policy.\nIn third-person learning, observations are more typically available rather than direct state access, so going forward we will work with observations ot instead of states st as representing the expert traces. The top row of Figure 8 illustrates what these observations are like in our experiments.\nWe begin by recalling that in the algorithm proposed by Ho & Ermon (2016) the loss in Equation 2 is utilized to train a discriminator DR capable of distinguishing expert vs non-expert policies. Unfortunately, (2) will likely fail in cases when the expert and non-expert act in different environments, since DR will quickly learn these differences and use them as a strong classification signal. To handle the third-person setting, where expert and novice are in different environments, we consider that DR works by first extracting features from ot, and then using these features to make a\nclassification. Suppose then that we partition DR into a feature extractor DF and the actual classifier which assigns probabilities to the outputs of DF . Overloading notation, we will refer to the classifier as DR going forward. For example, in case of a deep neural net representation, DF would correspond to the earlier layers, and DR to the later layers. The problem is then to ensure that DF contains no information regarding the rollout’s domain label d` (i.e., expert vs. novice domain). This can be realized as\nmax πθ minLR = ∑ i CE(DR(DF (oi)), c`i)\ns.t. MI(DF (oi); dl) = 0\nWhere MI is mutual information and hence we have abused notation by using DR, DF , and d` to mean the classifier, feature extractor, and the domain label respectively as well as distributions over these objects.\nThe mutual information term can be instantiated by introducing another classifier DD, which takes features produced by DF and outputs the probability that those features were produced by in the expert vs. non-expert environment. (See Bridle et al. (1992); Barber & Agakov (2005); Krause et al. (2010); Chen et al. (2016) for further discussion on instantiating the information term by introducing another classifier.) If σi = DF (oi), then the problem can be written as\nmax πθ min DR max DD LR + LD = ∑ i CE(DR(σi), c`i) + CE(DD(σi), d`i) (3)\nIn words, we wish to minimize class loss while maximizing domain confusion.\nOften, it can be difficult for even humans to judge a static image as expert vs. non-expert because it does not convey any information about the environmental change affected by the agent’s actions. For example, if a pointmass is attempting to move to a target location and starts far away from its goal state, it can be difficult to judge if the policy itself is bad or the initialization was simply unlucky. In response to this difficulty, we give DR access to not only the image at time t, but also at some future time t + n. Define σt = DF (ot) and σt+n = DF (ot+n). The classifier then makes a prediction DR(σt, σt+n) = ĉ`. This renders the following formulation:\nmax πθ min DR max DD LR + LD = ∑ i CE(DR(σi, σi+n), c`i) + CE(DD(σi), d`i) (4)\nNote we also want to optimize overDF , the feature extractor, but it feeds both intoDR and intoDD, which are competing (hidden under σ), which we will address now.\nTo deal with the competition over DF , we introduce a function G that acts as the identity when moving forward through a directed acyclic graph and flips the sign when backpropagating through the graph. This technique has enjoyed recent success in computer vision. See, for example, (Ganin & Lempitsky, 2014). With this trick, the problem reduces to its final form\nmax πθ min DR,DD,DF LR + LD = ∑ i CE(DR(σi, σi+n), c`i) + λ CE(DD(G(σi), d`i) (5)\nIn Equation (5), we flip the gradient’s sign during backpropagation ofDF with respect to the domain classification loss. This corresponds to stochastic gradient ascent away from features that are useful for domain classification, thus ensuring that DF produces domain agnostic features. Equation 5 can be solved efficiently with stochastic gradient descent. Here λ is a hyperparameter that determines the trade-off made between the objectives that are competing over DF . To ensure sufficient signal for discrimination between expert and non-expert, we collect third-person demonstrations in the expert domain from both an expert and from a non-expert.\nOur complete formulation is graphically summarized in Figure 2.\n\n5.2 ALGORITHM\nTo solve the game formulation in Equation (5), we perform alternating (partial) optimization over the policy πθ and the reward function and domain confusion encoded through DR,DD,DF . The optimization over DR,DD,DF is done through stochastic gradient descent with ADAM Kingma & Ba (2014).\nOur generator (πθ) step is similar to the generator step in the algorithm by (Ho & Ermon, 2016). We simply use − logDR as the reward. Using policy gradient methods (TRPO), we train the generator to minimize this cost and thus push the policy further towards replicating expert behavior. Once the generator step is done, we start again with the discriminator step. The entire process is summarized in algorithm 1.\n\n6 EXPERIMENTS\nWe seek to answer the following questions through experiments:\n1. Is it possible to solve the third-person imitation learning problem in simple settings? I.e., given a collection of expert image-based rollouts in one domain, is it possible to train a policy in a different domain that replicates the essence of the original behavior?\n2. Does the algorithm we propose benefit from both domain confusion and velocity?\n3. How sensitive is our proposed algorithm to the selection of hyper-parameters used in deployment?\n4. How sensitive is our proposed algorithm to changes in camera angle?\n5. How does our method compare against some reasonable baselines?\nAlgorithm 1 A third-person imitation learning algorithm. 1: Let CE be the standard cross entropy loss. 2: Let G be a function that flips the gradient sign during backpropogation and acts as the identity\nmap otherwise. 3: Initialize two domains, E and N for the expert and novice. 4: Initialize a memory bank Ω of expert success and of failure in domainE. Each trajectory ω ∈ Ω\ncomprises a rollout of images o = o1, . . . , ot, . . . on, a class label c`, and a domain label d`. 5: Initialize D = DF ,DR,DD, a domain invariant discriminator. 6: Initialize a novice policy πθ. 7: Initialize numiters, the number of inner policy optimization iterations we wish to run. 8: for iter in numiters do 9: Sample a set of successes and failures ωE from Ω.\n10: Collect on policy samples ωN 11: Set ω = ωE ∪ ωN . 12: Shuffle ω 13: for o, c`, d` in ω do 14: for ot in o do 15: σt = DF (ot) 16: σt+4 = DF (ot+4) 17: LR = CE(DR(σt, σt+4), c`) 18: Ld = CE(DD(G(σt)), d`) 19: L = λ · Ld + LR 1 20: minimize L with ADAM. 21: end for 22: end for 23: Collect on policy samples ωN from πθ. 24: for ω in ωN do 25: for ωt in ω do 26: σt = DF (ot) 27: σt+4 = DF (ot+4) 28: ĉ` = DR(σt, σt+4) 29: r = ĉ`[0], the probability that ot, ot+4 were generated via expert rollouts. 30: Use r to train πθ with via policy gradients (TRPO). 31: end for 32: end for 33: end for 34: return optimized policy πθ\n\n6.1 ENVIRONMENTS\nTo evaluate our algorithm, we consider three environments in the MuJoCo physics simulator. There are two different versions of each environment, an expert variant and a novice variant. Our goal is to train a cost function that is domain agnostic, and hence can be trained with images on the expert domain but nevertheless produce a reasonable cost on the novice domain. See Figure 1 for a visualization of the differences between expert and novice environments for the three tasks.\nPoint: A pointmass attempts to reach a point in a plane. The color of the target and the camera angle change between domains.\nReacher: A two DOF arm attempts to reach a designated point in the plane. The camera angle, the length of the arms, and the color of the target point are changed between domains. Note that changing the camera angle significantly alters the image background color from largely gray to roughly 30 percent black. This presents a significant challenge for our method.\nInverted Pendulum: A classic RL task wherein a pendulum must be made to balance via control. For this domain, We only change the color of the pendulum and not the camera angle. Since there is no target point, we found that changing the camera angle left the domain invariant representations with too little information and resulted in a failure case. In contrast to some traditional renderings\nof this problem, we do not terminate an episode when the agent falls but rather allow data collection to continue for a fixed horizon.\n\n6.2 EVALUATIONS\nIs it possible to solve the third-person imitation learning problem in simple settings? In Figure 3, we see that our proposed algorithm is indeed able to recover reasonable policies for all three tasks we examined. Initially, the training is quite unstable due to the domain confusion wreaking havoc on the learned cost. However, after several iterations the policies eventually head towards reasonable local minima and the standard deviation over the reward distribution shrinks substantially. Finally, we note that the extracted feature representations used to complete this task are in fact domain-agnostic, as seen in Figure 9. Hence, the learning is properly taking place from a third-person perspective.\nDoes the algorithm we propose benefit from both domain confusion and the multi-time step input? We answer this question with the experiments summarized in Figure 5. This experiment compares our approach with: (i) our approach without the domain confusion loss; (ii) our approach without the multi-time step input; (iii) our approach without the domain confusion loss and without the multitime step input (which is very similar to the approach in Ho & Ermon (2016)). We see that adding domain confusion is essential for getting strong performance in all three experiments. Meanwhile, adding multi-time step input marginally improves the results. See also Figure 7 for an analysis of the effects of multi-time step input on the final results.\nHow sensitive is our proposed algorithm to the selection of hyper-parameters used in deployment? Figure 6 shows the effect of the domain confusion coefficient λ, which trades off how much we should weight the domain confusion objective vs. the standard cost-recovery objective, on the final performance of the algorithm. Setting λ too low results in slower learning and features that are not domain-invariant. Setting λ too high results in an objective that is too quick to destroy information, which makes it impossible to recover an accurate cost.\nFor multi-time step input, one must choose the number of look-ahead frames that are utilized. If too small a window is chosen, the agent’s actions have not affected a large amount of change in the environment and it is difficult to discern any additional class signal over static images. If too large a time-frame passes, causality becomes difficult to interpolate and the agent does worse than simply being trained on static frames. Figure 7 illustrates that no number of look-ahead frames is consistently optimal across tasks. However, a value of 4 showed good performance over all tasks, and so this value was utilized in all other experiments.\nHow sensitive is our algorithm to changes in camera angle? We present graphs for the reacher and point experiments wherein we exam the final reward obtained by a policy trained with thirdperson imitation learning vs the camera angle difference between the first-person and third-person perspective. We omit the inverted double pendulum experiment, as the color and not the camera angle changes in that setting and we found the case of slowly transitioning the color to be the definition of uninteresting science.\nHow does our method compare against reasonable baselines? We consider the following baselines for comparisons against third-person imitation learning. 1) Standard reinforcement learning with using full state information and the true reward signal. This agent is trained via TRPO. 2)\nStandard GAIL (first-person imitation learning). Here, the agent receives first-person demonstration and attempts to imitate the correct behavior. This is an upper bound on how well we can expect to do, since we have the correct perspective. 3) Training a policy using first-person data and applying it to the third-person environment.\nWe compare all three of these baselines to third-person imitation learning. As we see in figure 9: 1) Standard RL, which (unlike the imitation learning approaches) has access to full state and true reward, helps calibrate performance of the other approaches. 2) First-person imitation learning is faced with a simpler imitation problem and accordingly outperforms third-person imitation, yet third-person imitation learning is nevertheless competitive. 3) Applying the first-person policy to the third-person agent fails miserably, illustrating that explicitly considering third-person imitation is important in these settings.\nSomewhat unfortunately, the different reward function scales make it difficult to capture information on the variance of each learning curve. Consequently, in Appendix A we have included the full learning curves for these experiments with variance bars, each plotted with an appropriate scale to examine the variance of the individual curves.\n\n7 DISCUSSION AND FUTURE WORK\nIn this paper, we presented the problem of third-person imitation learning. We argue that this problem will be important going forward, as techniques in reinforcement learning and generative adversarial learning improve and the cost of collecting first-person samples remains high. We presented an algorithm which builds on Generative Adversarial Imitation Learning and is capable of solving simple third-person imitation tasks.\nOne promising direction of future work in this area is to jointly train policy features and cost features at the pixel level, allowing the reuse of image features. Code to train a third person imitation learning agent on the domains from this paper is presented here: https://github.com/bstadie/ third_person_im\n\n8 APPENDIX A: LEARNING CURVES FOR BASELINES\nHere, we plot the learning curves for each of the baselines mentioned in the experiments section as a standalone plot. This allows one to better examine the variance of each individual learning curve.\n\n9 APPENDIX B: ARCHITECTURE PARAMETERS\nJoint Feature Extractor: Input is images are size 50 x 50 with 3 channels, RGB. Layers are 2 convolutional layers each followed by a max pooling layer of size 2. Layers use 5 filters of size 3 each.\nDomain Discriminator and the Class Discriminator: Input is domain agnostic output of convolutional layers. Layers are two feed forward layers of size 128 followed by a final feed forward layer of size 2 and a soft-max layer to get the log probabilities.\nADAM is used for discriminator training with a learning rate of 0.001. The RL generator uses the off-the-shelf TRPO implementation available in RLLab.\n",
    "rationale": "The paper extends the imitation learning paradigm to the case where the demonstrator and learner have different points of view. This is an important contribution, with several good applications.  The main insight is to use adversarial training to learn a policy that is robust to this difference in perspective.  This problem formulation is quite novel compared to the standard imitation learning literature (usually first-order perspective), though has close links to the literature on transfer learning (as explained in Sec.2).\n\nThe basic approach is clearly explained, and follows quite readily from recent literature on imitation learning and adversarial training.\n\nI would have expected to see comparison to the following methods added to Figure 3:\n1)  Standard 1st person imitation learning using agent A data, and apply the policy on agent A.  This is an upper-bound on how well you can expect to do, since you have the correct perspective.\n2)  Standard 1st person imitation learning using agent A data, then apply the policy on agent B.  Here, I expect it might do less well than 3rd person learning, but worth checking to be sure, and showing what is the gap in performance.\n3)  Reinforcement learning using agent A data, and apply the policy on agent A.  I expect this might do better than 3rd person imitation learning but it might depend on the scenario (e.g. difficulty of imitation vs exploration; how different are the points of view between the agents). I understand this is how the expert data is collected for the demonstrator, but I don’t see the performance results from just using this procedure on the learner (to compare to Fig.3 results).\n\nIncluding these results would in my view significantly enhance the impact of the paper.",
    "rating": 1
  },
  {
    "title": "THIRD-PERSON IMITATION LEARNING",
    "abstract": "Reinforcement learning (RL) makes it possible to train agents capable of achieving sophisticated goals in complex and uncertain environments. A key difficulty in reinforcement learning is specifying a reward function for the agent to optimize. Traditionally, imitation learning in RL has been used to overcome this problem. Unfortunately, hitherto imitation learning methods tend to require that demonstrations are supplied in the first-person: the agent is provided with a sequence of states and a specification of the actions that it should have taken. While powerful, this kind of imitation learning is limited by the relatively hard problem of collecting first-person demonstrations. Humans address this problem by learning from third-person demonstrations: they observe other humans perform tasks, infer the task, and accomplish the same task themselves. In this paper, we present a method for unsupervised third-person imitation learning. Here third-person refers to training an agent to correctly achieve a simple goal in a simple environment when it is provided a demonstration of a teacher achieving the same goal but from a different viewpoint; and unsupervised refers to the fact that the agent receives only these third-person demonstrations, and is not provided a correspondence between teacher states and student states. Our methods primary insight is that recent advances from domain confusion can be utilized to yield domain agnostic features which are crucial during the training process. To validate our approach, we report successful experiments on learning from third-person demonstrations in a pointmass domain, a reacher domain, and inverted pendulum.",
    "text": "1 INTRODUCTION\nReinforcement learning (RL) is a framework for training agents to maximize rewards in large, unknown, stochastic environments. In recent years, combining techniques from deep learning with reinforcement learning has yielded a string of successful applications in game playing and robotics Mnih et al. (2015; 2016); Schulman et al. (2015a); Levine et al. (2016). These successful applications, and the speed at which the abilities of RL algorithms have been increasing, makes it an exciting area of research with significant potential for future applications.\nOne of the major weaknesses of RL is the need to manually specify a reward function. For each task we wish our agent to accomplish, we must provide it with a reward function whose maximizer will precisely recover the desired behavior. This weakness is addressed by the field of Inverse Reinforcement Learning (IRL). Given a set of expert trajectories, IRL algorithms produce a reward function under which these the expert trajectories enjoy the property of optimality. Recently, there has been a significant amount of work on IRL, and current algorithms can infer a reward function from a very modest number of demonstrations (e.g,. Abbeel & Ng (2004); Ratliff et al. (2006); Ziebart et al. (2008); Levine et al. (2011); Ho & Ermon (2016); Finn et al. (2016)).\nWhile IRL algorithms are appealing, they impose the somewhat unrealistic requirement that the demonstrations should be provided from the first-person point of view with respect to the agent. Human beings learn to imitate entirely from third-person demonstrations – i.e., by observing other humans achieve goals. Indeed, in many situations, first-person demonstrations are outright impossible to obtain. Meanwhile, third-person demonstrations are often relatively easy to obtain.\nThe goal of this paper is to develop an algorithm for third-person imitation learning. Future advancements in this class of algorithms would significantly improve the state of robotics, because it will enable people to easily teach robots news skills and abilities. Importantly, we want our algorithm to be unsupervised: it should be able to observe another agent perform a task, infer that there is an underlying correspondence to itself, and find a way to accomplish the same task.\nWe offer an approach to this problem by borrowing ideas from domain confusion Tzeng et al. (2014) and generative adversarial networks (GANs) Goodfellow et al. (2014). The high-level idea is to introduce an optimizer under which we can recover both a domain-agnostic representation of the agent’s observations, and a cost function which utilizes this domain-agnostic representation to capture the essence of expert trajectories. We formulate this as a third-person RL-GAN problem, and our solution builds on the first-person RL-GAN formulation by Ho & Ermon (2016).\nSurprisingly, we find that this simple approach has been able to solve the problems that are presented in this paper (illustrated in Figure 1), even though the student’s observations are related in a complicated way to the teacher’s demonstrations (given that the observations and the demonstrations are pixel-level). As techniques for training GANs become more stable and capable, we expect our algorithm to be able to infer solve harder third-person imitation tasks without any direct supervision.\n\n2 RELATED WORK\nImitation learning (also learning from demonstrations or programming by demonstration) considers the problem of acquiring skills from observing demonstrations. Imitation learning has a long history, with several good survey articles, including (Schaal, 1999; Calinon, 2009; Argall et al., 2009). Two main lines of work within imitation learning are: 1) behavioral cloning, where the demonstrations are used to directly learn a mapping from observations to actions using supervised learning, potentially with interleaving learning and data collection (e.g., Pomerleau (1989); Ross et al. (2011)). 2) Inverse reinforcement learning (Ng et al., 2000), where a reward function is estimated that explains the demonstrations as (near) optimal behavior. This reward function could be represented as nearness to a trajectory (Calinon et al., 2007; Abbeel et al., 2010), as a weighted combination of features (Abbeel & Ng, 2004; Ratliff et al., 2006; Ramachandran & Amir, 2007; Ziebart et al., 2008; Boularias et al., 2011; Kalakrishnan et al., 2013; Doerr et al., 2015), or could also involve feature learning (Ratliff et al., 2007; Levine et al., 2011; Wulfmeier et al., 2015; Finn et al., 2016; Ho & Ermon, 2016).\nThis past work, however, is not directly applicable to the third person imitation learning setting. In third-person imitation learning, the observations and actions obtained from the demonstrations are not the same as what the imitator agent will be faced with. A typical scenario would be: the imitator agent watches a human perform a demonstration, and then has to execute that same task. As discussed in Nehaniv & Dautenhahn (2001) the ”what and how to imitate” questions become significantly more challenging in this setting. To directly apply existing behavioral cloning or inverse reinforcement learning techniques would require knowledge of a mapping between observations and actions in the demonstrator space to observations and actions in the imitator space. Such a mapping is often difficult to obtain, and it typically relies on providing feature representations that captures the invariance between both environments Carpenter et al. (2002); Shon et al. (2005); Calinon et al. (2007); Nehaniv (2007); Gioioso et al. (2013); Gupta et al. (2016). Contrary to prior work, we consider third-person imitation learning from raw sensory data, where no such features are made available.\nThe most closely related work to ours is by Finn et al. (2016); Ho & Ermon (2016); Wulfmeier et al. (2015), who also consider inverse reinforcement learning directly from raw sensory data. However, the applicability of their approaches is limited to the first-person setting. Indeed, matching raw sensory observations is impossible in the 3rd person setting.\nOur work also closely builds on advances in generative adversarial networks Goodfellow et al. (2014), which are very closely related to imitation learning as explained in Finn et al. (2016); Ho & Ermon (2016). In our optimization formulation, we apply the gradient flipping technique from Ganin & Lempitsky (2014).\nThe problem of adapting what is learned in one domain to another domain has been studied extensively in computer vision in the supervised learning setting Yang et al. (2007); Mansour et al. (2009); Kulis et al. (2011); Aytar & Zisserman (2011); Duan et al. (2012); Hoffman et al. (2013); Long & Wang (2015). It has also been shown that features trained in one domain can often be relevant to other domains Donahue et al. (2014). The work most closely related to ours is Tzeng et al. (2014; 2015), who also consider an explicit domain confusion loss, forcing trained classifiers to rely on features that don’t allow to distinguish between two domains. This work in turn relates to earlier work by Bromley et al. (1993); Chopra et al. (2005), which also considers supervised training of deep feature embeddings.\nOur approach to third-person imitation learning relies on reinforcement learning from raw sensory data in the imitator domain. Several recent advances in deep reinforcement learning have made this practical, including Deep Q-Networks (Mnih et al., 2015), Trust Region Policy Optimization (Schulman et al., 2015a), A3C Mnih et al. (2016), and Generalized Advantage Estimation (Schulman et al., 2015b). Our approach uses Trust Region Policy Optimization.\n\n3 BACKGROUND AND PRELIMINARIES\nA discrete-time finite-horizon discounted Markov decision process (MDP) is represented by a tuple M = (S,A,P, r, ρ0, γ, T ), in which S is a state set, A an action set, P : S × A × S → R+ a transition probability distribution, r : S × A → R a reward function, ρ0 : S → R+ an initial state distribution, γ ∈ [0, 1] a discount factor, and T the horizon. In the reinforcement learning setting, the goal is to find a policy πθ : S × A → R+ parametrized by θ that maximizes the expected discounted sum of rewards incurred, η(πθ) = Eπθ [ ∑T t=0 γ\ntc(st)], where s0 ∼ ρ0(s0), at ∼ πθ(at|st), and st+1 ∼ P(st+1|st, at). In the (first-person) imitation learning setting, we are not given the reward function. Instead we are given traces (i.e., sequences of states traversed) by an expert who acts according to an unknown policy πE . The goal is to find a policy πθ that performs as well as the expert against the unknown reward function. It was shown in Abbeel & Ng (2004) that this can be achieved through inverse reinforcement learning by finding a policy πθ that matches the expert’s empirical expectation over discounted sum of all features that might contribute to the reward function. The work by Ho & Ermon (2016) generalizes this to the setting when no features are provided as follows: Find a policy πθ that makes it impossible for a discriminator (in their work a deep neural net) to distinguish states visited by the expert from states visited by the imitator agent. This can be formalized as follows:\nmax πθ min DR − Eπθ [logDR(s)]− EπE [log(1−DR(s))] (1)\nHere, the expectations are over the states experienced by the policy of the imitator agent, πθ, and by the policy of the expert, πE , respectively. DR is the discriminator, which outputs the probability of a state having originated from a trace from the imitator policy πθ. If the discriminator is perfectly able to distinguish which policy originated state-action pairs, then DR will consistently output a probability of 1 in the first term, and a probability of 0 in the second term, making the objective its lowest possible value of zero. It is the role of the imitator agent πθ to find a policy that makes it difficult for the discriminator to make that distinction. The desired equilibrium has the imitator agent making it impractical for the discriminator to distinguish, hence forcing the discriminator to assign probability 0.5 in all cases. Ho & Ermon (2016) present a practical approach for solving this type of game when representing both πθ and DR as deep neural networks. Their approach repeatedly performs gradient updates on each of them. Concretely, for a current policy πθ traces can be collected, which together with the expert traces form a data-set on which DR can be trained with supervised learning minimizing the negative log-likelihood (in practice only performing a modest number of updates). For a fixed DR, this is a policy optimization problem where − logDR(s, a) is the reward, and policy gradients can be computed from those same traces. Their approach uses trust region policy optimization (Schulman et al., 2015a) to update the imitator policy πθ from those gradients.\nIn our work we will have more terms in the objective, so for compactness of notation, we will realize the discriminative minimization from Eqn. (1) as follows:\nmax πθ min DR LR = ∑ i CE(DR(si), c`i) (2)\nWhere si is state i, c`i is the correct class label (was the state si obtained from an expert vs. from a non-expert), and CE is the standard cross entropy loss.\n\n4 A FORMAL DEFINITION OF THE THIRD-PERSON IMITATION LEARNING PROBLEM\nFormally, the third-person imitation learning problem can be stated as follows. Suppose we are given two Markov Decision Processes MπE and Mπθ . Suppose further there exists a set of traces ρ = {(s1, . . . , sn)}ni=0 which were generated under a policy πE acting optimally under some unknown reward RπE . In third-person imitation learning, one attempts to recover by proxy through ρ a policy πθ = f(ρ) which acts optimally with respect to Rπθ .\n\n5 A THIRD-PERSON IMITATION LEARNING ALGORITHM\n\n\n5.1 GAME FORMULATION\nIn this section, we discuss a simple algorithm for third-person imitation learning. This algorithm is able to successfully discriminate between expert and novice policies, even when the policies are executed under different environments. Subsequently, this discrimination signal can be used to train expert policies in new domains via RL by training the novice policy to fool the discriminator, thus forcing it to match the expert policy.\nIn third-person learning, observations are more typically available rather than direct state access, so going forward we will work with observations ot instead of states st as representing the expert traces. The top row of Figure 8 illustrates what these observations are like in our experiments.\nWe begin by recalling that in the algorithm proposed by Ho & Ermon (2016) the loss in Equation 2 is utilized to train a discriminator DR capable of distinguishing expert vs non-expert policies. Unfortunately, (2) will likely fail in cases when the expert and non-expert act in different environments, since DR will quickly learn these differences and use them as a strong classification signal. To handle the third-person setting, where expert and novice are in different environments, we consider that DR works by first extracting features from ot, and then using these features to make a\nclassification. Suppose then that we partition DR into a feature extractor DF and the actual classifier which assigns probabilities to the outputs of DF . Overloading notation, we will refer to the classifier as DR going forward. For example, in case of a deep neural net representation, DF would correspond to the earlier layers, and DR to the later layers. The problem is then to ensure that DF contains no information regarding the rollout’s domain label d` (i.e., expert vs. novice domain). This can be realized as\nmax πθ minLR = ∑ i CE(DR(DF (oi)), c`i)\ns.t. MI(DF (oi); dl) = 0\nWhere MI is mutual information and hence we have abused notation by using DR, DF , and d` to mean the classifier, feature extractor, and the domain label respectively as well as distributions over these objects.\nThe mutual information term can be instantiated by introducing another classifier DD, which takes features produced by DF and outputs the probability that those features were produced by in the expert vs. non-expert environment. (See Bridle et al. (1992); Barber & Agakov (2005); Krause et al. (2010); Chen et al. (2016) for further discussion on instantiating the information term by introducing another classifier.) If σi = DF (oi), then the problem can be written as\nmax πθ min DR max DD LR + LD = ∑ i CE(DR(σi), c`i) + CE(DD(σi), d`i) (3)\nIn words, we wish to minimize class loss while maximizing domain confusion.\nOften, it can be difficult for even humans to judge a static image as expert vs. non-expert because it does not convey any information about the environmental change affected by the agent’s actions. For example, if a pointmass is attempting to move to a target location and starts far away from its goal state, it can be difficult to judge if the policy itself is bad or the initialization was simply unlucky. In response to this difficulty, we give DR access to not only the image at time t, but also at some future time t + n. Define σt = DF (ot) and σt+n = DF (ot+n). The classifier then makes a prediction DR(σt, σt+n) = ĉ`. This renders the following formulation:\nmax πθ min DR max DD LR + LD = ∑ i CE(DR(σi, σi+n), c`i) + CE(DD(σi), d`i) (4)\nNote we also want to optimize overDF , the feature extractor, but it feeds both intoDR and intoDD, which are competing (hidden under σ), which we will address now.\nTo deal with the competition over DF , we introduce a function G that acts as the identity when moving forward through a directed acyclic graph and flips the sign when backpropagating through the graph. This technique has enjoyed recent success in computer vision. See, for example, (Ganin & Lempitsky, 2014). With this trick, the problem reduces to its final form\nmax πθ min DR,DD,DF LR + LD = ∑ i CE(DR(σi, σi+n), c`i) + λ CE(DD(G(σi), d`i) (5)\nIn Equation (5), we flip the gradient’s sign during backpropagation ofDF with respect to the domain classification loss. This corresponds to stochastic gradient ascent away from features that are useful for domain classification, thus ensuring that DF produces domain agnostic features. Equation 5 can be solved efficiently with stochastic gradient descent. Here λ is a hyperparameter that determines the trade-off made between the objectives that are competing over DF . To ensure sufficient signal for discrimination between expert and non-expert, we collect third-person demonstrations in the expert domain from both an expert and from a non-expert.\nOur complete formulation is graphically summarized in Figure 2.\n\n5.2 ALGORITHM\nTo solve the game formulation in Equation (5), we perform alternating (partial) optimization over the policy πθ and the reward function and domain confusion encoded through DR,DD,DF . The optimization over DR,DD,DF is done through stochastic gradient descent with ADAM Kingma & Ba (2014).\nOur generator (πθ) step is similar to the generator step in the algorithm by (Ho & Ermon, 2016). We simply use − logDR as the reward. Using policy gradient methods (TRPO), we train the generator to minimize this cost and thus push the policy further towards replicating expert behavior. Once the generator step is done, we start again with the discriminator step. The entire process is summarized in algorithm 1.\n\n6 EXPERIMENTS\nWe seek to answer the following questions through experiments:\n1. Is it possible to solve the third-person imitation learning problem in simple settings? I.e., given a collection of expert image-based rollouts in one domain, is it possible to train a policy in a different domain that replicates the essence of the original behavior?\n2. Does the algorithm we propose benefit from both domain confusion and velocity?\n3. How sensitive is our proposed algorithm to the selection of hyper-parameters used in deployment?\n4. How sensitive is our proposed algorithm to changes in camera angle?\n5. How does our method compare against some reasonable baselines?\nAlgorithm 1 A third-person imitation learning algorithm. 1: Let CE be the standard cross entropy loss. 2: Let G be a function that flips the gradient sign during backpropogation and acts as the identity\nmap otherwise. 3: Initialize two domains, E and N for the expert and novice. 4: Initialize a memory bank Ω of expert success and of failure in domainE. Each trajectory ω ∈ Ω\ncomprises a rollout of images o = o1, . . . , ot, . . . on, a class label c`, and a domain label d`. 5: Initialize D = DF ,DR,DD, a domain invariant discriminator. 6: Initialize a novice policy πθ. 7: Initialize numiters, the number of inner policy optimization iterations we wish to run. 8: for iter in numiters do 9: Sample a set of successes and failures ωE from Ω.\n10: Collect on policy samples ωN 11: Set ω = ωE ∪ ωN . 12: Shuffle ω 13: for o, c`, d` in ω do 14: for ot in o do 15: σt = DF (ot) 16: σt+4 = DF (ot+4) 17: LR = CE(DR(σt, σt+4), c`) 18: Ld = CE(DD(G(σt)), d`) 19: L = λ · Ld + LR 1 20: minimize L with ADAM. 21: end for 22: end for 23: Collect on policy samples ωN from πθ. 24: for ω in ωN do 25: for ωt in ω do 26: σt = DF (ot) 27: σt+4 = DF (ot+4) 28: ĉ` = DR(σt, σt+4) 29: r = ĉ`[0], the probability that ot, ot+4 were generated via expert rollouts. 30: Use r to train πθ with via policy gradients (TRPO). 31: end for 32: end for 33: end for 34: return optimized policy πθ\n\n6.1 ENVIRONMENTS\nTo evaluate our algorithm, we consider three environments in the MuJoCo physics simulator. There are two different versions of each environment, an expert variant and a novice variant. Our goal is to train a cost function that is domain agnostic, and hence can be trained with images on the expert domain but nevertheless produce a reasonable cost on the novice domain. See Figure 1 for a visualization of the differences between expert and novice environments for the three tasks.\nPoint: A pointmass attempts to reach a point in a plane. The color of the target and the camera angle change between domains.\nReacher: A two DOF arm attempts to reach a designated point in the plane. The camera angle, the length of the arms, and the color of the target point are changed between domains. Note that changing the camera angle significantly alters the image background color from largely gray to roughly 30 percent black. This presents a significant challenge for our method.\nInverted Pendulum: A classic RL task wherein a pendulum must be made to balance via control. For this domain, We only change the color of the pendulum and not the camera angle. Since there is no target point, we found that changing the camera angle left the domain invariant representations with too little information and resulted in a failure case. In contrast to some traditional renderings\nof this problem, we do not terminate an episode when the agent falls but rather allow data collection to continue for a fixed horizon.\n\n6.2 EVALUATIONS\nIs it possible to solve the third-person imitation learning problem in simple settings? In Figure 3, we see that our proposed algorithm is indeed able to recover reasonable policies for all three tasks we examined. Initially, the training is quite unstable due to the domain confusion wreaking havoc on the learned cost. However, after several iterations the policies eventually head towards reasonable local minima and the standard deviation over the reward distribution shrinks substantially. Finally, we note that the extracted feature representations used to complete this task are in fact domain-agnostic, as seen in Figure 9. Hence, the learning is properly taking place from a third-person perspective.\nDoes the algorithm we propose benefit from both domain confusion and the multi-time step input? We answer this question with the experiments summarized in Figure 5. This experiment compares our approach with: (i) our approach without the domain confusion loss; (ii) our approach without the multi-time step input; (iii) our approach without the domain confusion loss and without the multitime step input (which is very similar to the approach in Ho & Ermon (2016)). We see that adding domain confusion is essential for getting strong performance in all three experiments. Meanwhile, adding multi-time step input marginally improves the results. See also Figure 7 for an analysis of the effects of multi-time step input on the final results.\nHow sensitive is our proposed algorithm to the selection of hyper-parameters used in deployment? Figure 6 shows the effect of the domain confusion coefficient λ, which trades off how much we should weight the domain confusion objective vs. the standard cost-recovery objective, on the final performance of the algorithm. Setting λ too low results in slower learning and features that are not domain-invariant. Setting λ too high results in an objective that is too quick to destroy information, which makes it impossible to recover an accurate cost.\nFor multi-time step input, one must choose the number of look-ahead frames that are utilized. If too small a window is chosen, the agent’s actions have not affected a large amount of change in the environment and it is difficult to discern any additional class signal over static images. If too large a time-frame passes, causality becomes difficult to interpolate and the agent does worse than simply being trained on static frames. Figure 7 illustrates that no number of look-ahead frames is consistently optimal across tasks. However, a value of 4 showed good performance over all tasks, and so this value was utilized in all other experiments.\nHow sensitive is our algorithm to changes in camera angle? We present graphs for the reacher and point experiments wherein we exam the final reward obtained by a policy trained with thirdperson imitation learning vs the camera angle difference between the first-person and third-person perspective. We omit the inverted double pendulum experiment, as the color and not the camera angle changes in that setting and we found the case of slowly transitioning the color to be the definition of uninteresting science.\nHow does our method compare against reasonable baselines? We consider the following baselines for comparisons against third-person imitation learning. 1) Standard reinforcement learning with using full state information and the true reward signal. This agent is trained via TRPO. 2)\nStandard GAIL (first-person imitation learning). Here, the agent receives first-person demonstration and attempts to imitate the correct behavior. This is an upper bound on how well we can expect to do, since we have the correct perspective. 3) Training a policy using first-person data and applying it to the third-person environment.\nWe compare all three of these baselines to third-person imitation learning. As we see in figure 9: 1) Standard RL, which (unlike the imitation learning approaches) has access to full state and true reward, helps calibrate performance of the other approaches. 2) First-person imitation learning is faced with a simpler imitation problem and accordingly outperforms third-person imitation, yet third-person imitation learning is nevertheless competitive. 3) Applying the first-person policy to the third-person agent fails miserably, illustrating that explicitly considering third-person imitation is important in these settings.\nSomewhat unfortunately, the different reward function scales make it difficult to capture information on the variance of each learning curve. Consequently, in Appendix A we have included the full learning curves for these experiments with variance bars, each plotted with an appropriate scale to examine the variance of the individual curves.\n\n7 DISCUSSION AND FUTURE WORK\nIn this paper, we presented the problem of third-person imitation learning. We argue that this problem will be important going forward, as techniques in reinforcement learning and generative adversarial learning improve and the cost of collecting first-person samples remains high. We presented an algorithm which builds on Generative Adversarial Imitation Learning and is capable of solving simple third-person imitation tasks.\nOne promising direction of future work in this area is to jointly train policy features and cost features at the pixel level, allowing the reuse of image features. Code to train a third person imitation learning agent on the domains from this paper is presented here: https://github.com/bstadie/ third_person_im\n\n8 APPENDIX A: LEARNING CURVES FOR BASELINES\nHere, we plot the learning curves for each of the baselines mentioned in the experiments section as a standalone plot. This allows one to better examine the variance of each individual learning curve.\n\n9 APPENDIX B: ARCHITECTURE PARAMETERS\nJoint Feature Extractor: Input is images are size 50 x 50 with 3 channels, RGB. Layers are 2 convolutional layers each followed by a max pooling layer of size 2. Layers use 5 filters of size 3 each.\nDomain Discriminator and the Class Discriminator: Input is domain agnostic output of convolutional layers. Layers are two feed forward layers of size 128 followed by a final feed forward layer of size 2 and a soft-max layer to get the log probabilities.\nADAM is used for discriminator training with a learning rate of 0.001. The RL generator uses the off-the-shelf TRPO implementation available in RLLab.\n",
    "rationale": "The paper extends the imitation learning paradigm to the case where the demonstrator and learner have different points of view. This is an important contribution, with several good applications.  The main insight is to use adversarial training to learn a policy that is robust to this difference in perspective.  This problem formulation is quite novel compared to the standard imitation learning literature (usually first-order perspective), though has close links to the literature on transfer learning (as explained in Sec.2).\n\nThe basic approach is clearly explained, and follows quite readily from recent literature on imitation learning and adversarial training.\n\nI would have expected to see comparison to the following methods added to Figure 3:\n1)  Standard 1st person imitation learning using agent A data, and apply the policy on agent A.  This is an upper-bound on how well you can expect to do, since you have the correct perspective.\n2)  Standard 1st person imitation learning using agent A data, then apply the policy on agent B.  Here, I expect it might do less well than 3rd person learning, but worth checking to be sure, and showing what is the gap in performance.\n3)  Reinforcement learning using agent A data, and apply the policy on agent A.  I expect this might do better than 3rd person imitation learning but it might depend on the scenario (e.g. difficulty of imitation vs exploration; how different are the points of view between the agents). I understand this is how the expert data is collected for the demonstrator, but I don’t see the performance results from just using this procedure on the learner (to compare to Fig.3 results).\n\nIncluding these results would in my view significantly enhance the impact of the paper.",
    "rating": 1
  }
]