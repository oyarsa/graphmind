[
  {
    "title": "DETERMINISTIC PAC-BAYESIAN GENERALIZATION BOUNDS FOR DEEP NETWORKS VIA GENERALIZING NOISE-RESILIENCE",
    "abstract": "The ability of overparameterized deep networks to generalize well has been linked to the fact that stochastic gradient descent (SGD) finds solutions that lie in flat, wide minima in the training loss – minima where the output of the network is resilient to small random noise added to its parameters. So far this observation has been used to provide generalization guarantees only for neural networks whose parameters are either stochastic or compressed. In this work, we present a general PAC-Bayesian framework that leverages this observation to provide a bound on the original network learned – a network that is deterministic and uncompressed. What enables us to do this is a key novelty in our approach: our framework allows us to show that if on training data, the interactions between the weight matrices satisfy certain conditions that imply a wide training loss minimum, these conditions themselves generalize to the interactions between the matrices on test data, thereby implying a wide test loss minimum. We then apply our general framework in a setup where we assume that the pre-activation values of the network are not too small (although we assume this only on the training data). In this setup, we provide a generalization guarantee for the original (deterministic, uncompressed) network, that does not scale with product of the spectral norms of the weight matrices – a guarantee that would not have been possible with prior approaches.",
    "text": "1 INTRODUCTION\nModern deep neural networks contain millions of parameters and are trained on relatively few samples. Conventional wisdom in machine learning suggests that such models should massively overfit on the training data, as these models have the capacity to memorize even a randomly labeled dataset of similar size (Zhang et al., 2017; Neyshabur et al., 2015). Yet these models have achieved state-ofthe-art generalization error on many real-world tasks. This observation has spurred an active line of research (Soudry et al., 2018; Brutzkus et al., 2018; Li & Liang, 2018) that has tried to understand what properties are possessed by stochastic gradient descent (SGD) training of deep networks that allows these networks to generalize well.\nOne particularly promising line of work in this area (Neyshabur et al., 2017; Arora et al., 2018) has been bounds that utilize the noise-resilience of deep networks on training data i.e., how much the training loss of the network changes with noise injected into the parameters, or roughly, how wide is the training loss minimum. While these have yielded generalization bounds that do not have a severe exponential dependence on depth (unlike other bounds that grow with the product of spectral norms of the weight matrices), these bounds are quite limited: they either apply to a stochastic version of the classifier (where the parameters are drawn from a distribution) or a compressed version of the classifier (where the parameters are modified and represented using fewer bits).\nIn this paper, we revisit the PAC-Bayesian analysis of deep networks in Neyshabur et al. (2017; 2018) and provide a general framework that allows one to use noise-resilience of the deep network\non training data to provide a bound on the original deterministic and uncompressed network. We achieve this by arguing that if on the training data, the interaction between the ‘activated weight matrices’ (weight matrices where the weights incoming from/outgoing to inactive units are zeroed out) satisfy certain conditions which results in a wide training loss minimum, these conditions themselves generalize to the weight matrix interactions on the test data.\nAfter presenting this general PAC-Bayesian framework, we specialize it to the case of deep ReLU networks, showing that we can provide a generalization bound that accomplishes two goals simultaneously: i) it applies to the original network and ii) it does not scale exponentially with depth in terms of the products of the spectral norms of the weight matrices; instead our bound scales with more meaningful terms that capture the interactions between the weight matrices and do not have such a severe dependence on depth in practice. We note that all but one of these terms are indeed quite small on networks in practice. However, one particularly (empirically) large term that we use is the reciprocal of the magnitude of the network pre-activations on the training data (and so our bound would be small only in the scenario where the pre-activations are not too small). We emphasize that this drawback is more of a limitation in how we characterize noise-resilience through the specific conditions we chose for the ReLU network, rather than a drawback in our PAC-Bayesian framework itself. Our hope is that, since our technique is quite general and flexible, by carefully identifying the right set of conditions, in the future, one might be able to derive a similar generalization guarantee that is smaller in practice.\nTo the best of our knowledge, our approach of generalizing noise-resilience of deep networks from training data to test data in order to derive a bound on the original network that does not scale with products of spectral norms, has neither been considered nor accomplished so far, even in limited situations.\n\n2 BACKGROUND AND RELATED WORK\nOne of the most important aspects of the generalization puzzle that has been studied is that of the flatness/width of the training loss at the minimum found by SGD. The general understanding is that flatter minima are correlated with better generalization behavior, and this should somehow help explain the generalization behavior (Hochreiter & Schmidhuber, 1997; Hinton & van Camp, 1993; Keskar et al., 2017). Flatness of the training loss minimum is also correlated with the observation that on training data, adding noise to the parameters of the network results only in little change in the output of the network – or in other words, the network is noise-resilient. Deep networks are known to be similarly resilient to noise injected into the inputs (Novak et al., 2018); but note that our theoretical analysis relies on resilience to parameter perturbations.\nWhile some progress has been made in understanding the convergence and generalization behavior of SGD training of simple models like two-layered hidden neural networks under simple data distributions (Neyshabur et al., 2015; Soudry et al., 2018; Brutzkus et al., 2018; Li & Liang, 2018), all known generalization guarantees for SGD on deeper networks – through analyses that do not use noise-resilience properties of the networks – have strong exponential dependence on depth. In particular, these bounds scale either with the product of the spectral norms of the weight matrices (Neyshabur et al., 2018; Bartlett et al., 2017) or their Frobenius norms (Golowich et al., 2018). In practice, the weight matrices have a spectral norm that is as large as 2 or 3, and an even larger Frobenius norm that scales with √ H where H is the width of the network i.e., maximum number of hidden units per layer. 1 Thus, the generalization bound scales as say, 2D or HD/2, where D is the depth of the network.\nAt a high level, the reason these bounds suffer from such an exponential dependence on depth is that they effectively perform a worst case approximation of how the weight matrices interact with each other. For example, the product of the spectral norms arises from a naive approximation of the Lipschitz constant of the neural network, which would hold only when the singular values of the\n1To understand why these values are of this order in magnitude, consider the initial matrix that is randomly initialized with independent entries with variance 1/ √ H . It can be shown that the spectral norm of this matrix, with high probability, lies near its expected value, near 2 and the Frobenius norm near its expected value which is √ H . Since SGD is observed not to move too far away from the initialization regardless of H (Nagarajan & Kolter, 2017), these values are more or less preserved for the final weight matrices.\nweight matrices all align with each other. However, in practice, for most inputs to the network, the interactions between the activated weight matrices are not as adverse.\nBy using noise-resilience of the networks, prior approaches (Arora et al., 2018; Neyshabur et al., 2017) have been able to derive bounds that replace the above worst-case approximation with smaller terms that realistically capture these interactions. However, these works are limited in critical ways. Arora et al. (2018) use noise-resilience of the network to modify and “compress” the parameter representation of the network, and derive a generalization bound on the compressed network. While this bound enjoys a better dependence on depth because its applies to a compressed network, the main drawback of this bound is that it does not apply on the original network. On the other hand, Neyshabur et al. (2017) take advantage of noise-resilience on training data by incorporating it within a PAC-Bayesian generalization bound (McAllester, 1999a). However, their final guarantee is only a bound on the expected test loss of a stochastic network.\nIn this work, we revisit the idea in Neyshabur et al. (2017), by pursuing the PAC-Bayesian framework (McAllester, 1999a) to answer this question. The standard PAC-Bayesian framework provides generalization bounds for the expected loss of a stochastic classifier, where the stochasticity typically corresponds to Gaussian noise injected into the parameters output by the learning algorithm. However, if the classifier is noise-resilient on both training and test data, one could extend the PAC-Bayesian bound to a standard generalization guarantee on the deterministic classifier.\nOther works have used PAC-Bayesian bounds in different ways in the context of neural networks. Langford & Caruana (2001); Dziugaite & Roy (2017) optimize the stochasticity and/or the weights of the network in order to numerically compute good (i.e., non-vacuous) generalization bounds on the stochastic network. Neyshabur et al. (2018) derive generalization bounds on the original, deterministic network by working from the PAC-Bayesian bound on the stochastic network. However, as stated earlier, their work does not make use of noise resilience in the networks learned by SGD.\nOUR CONTRIBUTIONS The key contribution in our work is a general PAC-Bayesian framework for deriving generalization bounds while leveraging the noise resilience of a deep network. While our approach is applied to deep networks, we note that it is general enough to be applied to other classifiers.\nIn our framework, we consider a set of conditions that when satisfied by the network, makes the output of the network noise-resilient at a particular input datapoint. For example, these conditions could characterize the interactions between the activated weight matrices at a particular input. To provide a generalization guarantee, we assume that the learning algorithm has found weights such that these conditions hold for the weight interactions in the network on training data (which effectively implies a wide training loss minimum). Then, as a key step, we generalize these conditions over to the weight interactions on test data (which effectively implies a wide test loss minimum) 2. Thus, with the guarantee that the classifier is noise-resilient both on training and test data, we derive a generalization bound on the test loss of the original network.\nFinally, we apply our framework to a specific set up of ReLU based feedforward networks. In particular, we first instantiate the above abstract framework with a set of specific conditions, and then use the above framework to derive a bound on the original network. While very similar conditions have already been identified in prior work (Arora et al., 2018; Neyshabur et al., 2017) (see Appendix G for an extensive discussion of this), our contribution here is in showing how these conditions generalize from training to test data. Crucially, like these works, our bound does not have severe exponential dependence on depth in terms of products of spectral norms.\nWe note that in reality, all but one of our conditions on the network do hold on training data as necessitated by the framework. The strong, non-realistic condition we make is that the pre-activation values of the network are sufficiently large, although only on training data; however, in practice a small proportion of the pre-activation values can be arbitrarily small. Our generalization bound scales inversely with the smallest absolute value of the pre-activations on the training data, and hence in practice, our bound would be large.\n2Note that we can not directly assume these conditions to hold on test data, as that would be ‘cheating’ from the perspective of a generalization guarantee.\nIntuitively, we make this assumption to ensure that under sufficiently small parameter perturbations, the activation states of the units are guaranteed not to flip. It is worth noting that Arora et al. (2018); Neyshabur et al. (2017) too require similar, but more realistic assumptions about pre-activation values that effectively assume only a small proportion of units flip under noise. However, even under our stronger condition that no such units exist, it is not apparent how these approaches would yield a similar bound on the deterministic, uncompressed network without generalizing their conditions to test data. We hope that in the future our work could be developed further to accommodate the more realistic conditions from Arora et al. (2018); Neyshabur et al. (2017).\n\n3 A GENERAL PAC-BAYESIAN FRAMEWORK\nIn this section, we present our general PAC-Bayesian framework that uses noise-resilience of the network to convert a PAC-Bayesian generalization bound on the stochastic classifier to a generalization bound on the deterministic classifier.\nNOTATION. Let KL(⋅∥⋅) denote the KL-divergence. Let ∥⋅∥ , ∥⋅∥∞ denote the `2 norm and the `∞ norms of a vector, respectively. Let ∥⋅∥2 , ∥⋅∥F , ∥⋅∥2,∞ denote the spectral norm, Frobenius norm and maximum row `2 norm of a matrix, respectively. Consider a K-class learning task where the labeled datapoints (x, y) are drawn from an underlying distribution D over X × {1,2,⋯,K} where X ∈ RN . We consider a classifier parametrized by weightsW . For a given input x and class k, we denote the output of the classifier by f (x;W) [k]. In our PAC-Bayesian analysis, we will use U ∼ N (0, σ2) to denote parameters whose entries are sampled independently from a Gaussian, andW + U to denote the entrywise addition of the two sets of parameters. We use ∥W∥2F to denote ∑ D d=1 ∥Wd∥ 2 F . Given a training set S of m samples, we let (x, y) ∼ S to denote uniform sampling from the set. Finally, for any γ > 0, let Lγ(f (x;W) , y) denote a margin-based loss such that the loss is 0 only when f (x;W) [y] ≥ maxj≠y f (x;W) [j]+γ, and 1 otherwise. Note that L0 corresponds to 0-1 error. See Appendix A for more notations.\nTRADITIONAL PAC-BAYESIAN BOUNDS. The PAC-Bayesian framework (McAllester, 1999a;b) allows us to derive generalization bounds for a stochastic classifier. Specifically, let W̃ be a random variable in the parameter space whose distribution is learned based on training data S. Let P be a prior distribution in the parameter space chosen independent of the training data. The PAC-Bayesian framework yields the following generalization bound on the 0-1 error of the stochastic classifier that holds with probability 1 − δ over the draw of the training set S of m samples3:\nEW̃[E(x,y)∼D[L0(f (x;W̃) , y)]] ≤ EW̃[E(x,y)∼S[L0(f (x;W̃) , y)]] + Õ ( √ KL(W̃∥P )/m)\nTypically, and in the rest of this discussion, W̃ is a Gaussian with covariance σ2I for some σ > 0 centered at the weightsW learned based on the training data. Furthermore, we will set P to be a Gaussian with covariance σ2I centered at the random initialization of the network like in Dziugaite & Roy (2017), instead of at the origin, like in Neyshabur et al. (2018). This is because the resulting KL-divergence – which depends on the distance between the means of the prior and the posterior – is known to be smaller, and to save a √ H factor in the bound (Nagarajan & Kolter, 2017).\n\n3.1 OUR FRAMEWORK\nTo extend the above PAC-Bayesian bound to a standard generalization bound on a deterministic classifierW , we need to replace the training and the test loss of the stochastic classifier with that of the original, deterministic classifier. However, in doing so, we will have to introduce extra terms in the upper bound to account for the perturbation suffered by the train and test loss under the Gaussian perturbation of the parameters. To tightly bound these two terms, we need that the network is noise-resilient on training and test data respectively. Our hope is that if the learning algorithm has found weights such that the network is noise-resilient on the training data, we can then generalize this noise-resilience over to test data as well, allowing us to better bound the excess terms.\n3We use Õ(⋅) to hide logarithmic factors.\nWe now discuss how noise-resilience is formalized in our framework through certain conditions on the weight matrices. Much of our discussion below is dedicated to how these conditions must be designed, as these details carry the key ideas behind how noise-resilience can be generalized from training to test data. We then present our main generalization bound and some intuition about our proof technique.\nINPUT-DEPENDENT PROPERTIES OF WEIGHTS Recall that, at a high level, the noise-resilience of a network corresponds to how little the network reacts to random parameter perturbations. Naturally, this would vary depending on the input. Hence, in our framework, we will analyze the noise-resilience of the network as a function of a given input. Specifically, we will characterize noise-resilience through conditions on how the weights of the model interact with each other for a given input. For example, in the next section, we will consider conditions of the form “the preactivation values of the hidden units in layer d, have magnitude larger than some small positive constant”. The idea is that when these conditions involving the weights and the input are satisfied, if we add noise to the weights, the output of the classifier for that input will provably suffer only little perturbation. We will more generally refer to each scalar quantity involved in these conditions, such as each of the pre-activation values, as an input-dependent property of the weights.\nWe will now formulate these input-dependent properties and the conditions on them, for a generic classifier, and in the next section, we will see how they can be instantiated in the case of deep networks. Consider a classifier for which we can define R different conditions, which when satisfied on a given input, will help us guarantee the classifier’s noise-resilience at that input i.e., bound the output perturbation under random parameter perturbations (to get an idea of what R corresponds to, in the case of deep networks, we will have a condition for each layer, and so R will scale with depth). In particular, let the rth condition be a bound involving a particular set of input-dependent properties of the weights denoted by {ρr,1(W,x, y), ρr,2(W,x, y),⋯,} – here, each element ρr,l(W,x, y) is a scalar value that depends on the weights and the input, just like pre-activation values4. Note that here the first subscript l is the index of the element in the set, and the second subscript r is the index of the set itself. Now for each of these properties, we will define a corresponding set of positive constants (that are independent ofW,x and y), denoted by {∆⋆r,1,∆ ⋆ r,2,⋯}, which we will use to specify our conditions. In particular, we say that the weightsW satisfy the rth condition on the input (x, y) if5:\n∀l, ρr,l(W,x, y) > ∆ ⋆ r,l (1)\nFor convenience, we also define an additional R+1th set to be the singleton set containing the margin of the classifier on the input: f (x;W) [y] −maxj≠y f (x;W) [j]. Note that if this term is positive (negative) then the classification is (in)correct. We will also denote the constant ∆⋆R+1,1 as γclass.\nORDERING OF THE SETS OF PROPERTIES We now impose a crucial constraint on how these sets of properties depend on each other. Roughly speaking, we want that for a given input, if the first r − 1 sets of properties approximately satisfy the condition in Equation 1, then the properties in the rth set are noise-resilient i.e., under random parameter perturbations, these properties do not suffer much perturbation. This kind of constraint would naturally hold for deep networks if we have chosen the properties carefully e.g., we will show that, for any given input, the perturbation in the pre-activation values of the dth layer is small as long as the absolute pre-activation values in the layers below d − 1 are large, and a few other norm-bounds on the lower layer weights are satisfied.\nWe formalize the above requirement by defining expressions ∆r,l(σ) that bound the perturbation in the properties ρr,l, in terms of the variance σ2 of the parameter perturbations. For any r ≤ R + 1 and for any (x, y), our framework requires the following to hold:\n4As we will see in the next section, most of these properties depend on only the unlabeled input x and not on y. But for the sake of convenience, we include y in the formulation of the input-dependent property, and use the word input to refer to x or (x, y) depending on the context\n5When we say ∀l below, we refer to the set of all possible indices l in the rth set, noting that different sets may have different cardinality.\nif ∀q < r,∀l, ρq,l(W,x, y) > 0 then\nPrU∼N (0,σ2I)[∀l ∣ρr,l(W + U ,x, y) − ρr,l(W,x, y)∣ > ∆r,l(σ)\n2 and\n∀q<r,∀l ∣ρq,l(W + U ,x, y) − ρq,l(W,x, y)∣< ∆q,l(σ)\n2 ] ≤\n1\n(R + 1) √ m . (2)\nLet us unpack the above constraint. First, although the above constraint must hold for all inputs (x, y), it effectively applies only to those inputs that satisfy the pre-condition of the if-then statement: namely, it applies only to inputs (x, y) that approximately satisfy the first r − 1 conditions in Equation 1 in that ρq,l(W,x, y) > 0 (instead of ρq,l(W,x, y) > ∆⋆q,l). Next, we discuss the second part of the above if-then statement which specifies a probability term that is required to be small for all such inputs. In words, the first event within the probability term above is the event that for a given random perturbation U , the properties involved in the rth condition suffer a large perturbation. The second is the event that the properties involved in the first r − 1 conditions do not suffer much perturbation; but, given that these r − 1 conditions already hold approximately, this second event implies that these conditions are still preserved approximately under perturbation. In summary, our constraint requires the following: for any input on which the first r − 1 conditions hold, there should be very few parameter perturbations that significantly perturb the rth set of properties while preserving the first r − 1 conditions. When we instantiate the framework, we have to derive closed form expressions for the perturbation bounds ∆r,l(σ) (in terms of only σ and the constants ∆⋆r,l). As we will see, for ReLU networks, we will choose the properties in a way that this constraint naturally falls into place in a way that the perturbation bounds ∆r,l(σ) do not grow with the product of spectral norms (Lemma E.1).\nTHEOREM STATEMENT In this setup, we have the following ‘margin-based’ generalization guarantee on the original network. That is, we bound the 0-1 test error of the network by a margin-based error on the training data. Our generalization guarantee, which scales linearly with the number of conditions R, holds under the setting that the training algorithm always finds weights such that on the training data, the conditions in Equation 1 is satisfied for all r = 1,⋯,R.\nTheorem 3.1. Let σ∗ be the maximum standard deviation of the Gaussian parameter perturbation such that the constraint in Equation 2 holds with ∆r,l(σ⋆) ≤ ∆⋆r,l ∀r ≤ R + 1 and ∀l. Then, for any δ > 0, with probability 1 − δ over the draw of samples S from Dm, for anyW we have that, ifW satisfies the conditions in Equation 1 for all r ≤ R and for all training examples (x, y) ∈ S, then\nPr(x,y)∼D [L0(f (x;W) , y)] ≤Pr(x,y)∼S [Lγclass(f (x;W) , y)]\n+ Õ ⎛ ⎜ ⎝ R\n¿ Á ÁÀ2KL(N (W, (σ⋆)2I)∥P ) + ln 2mR δ\nm − 1\n⎞ ⎟ ⎠\nThe crux of our proof (in Appendix D) lies in generalizing the conditions of Equation 1 satisfied on the training data to test data one after the other, by proving that they are noise-resilient on both training and test data. Crucially, after we generalize the first r − 1 conditions from training data to test data (i.e., on most test and training data, the r − 1 conditions are satisfied), we will have from Equation 2 that the rth set of properties are noise-resilient on both training and test data. Using the noise-resilience of the rth set of properties on test/train data, we can generalize even the rth condition to test data.\nWe emphasize a key, fundamental tool that we present in Theorem C.1 to convert a generic PACBayesian bound on a stochastic classifier, to a generalization bound on the deterministic classifier. Our technique is at a high level similar to approaches in London et al. (2016); McAllester (2003). In Section C.1, we argue how this technique is more powerful than other approaches in Neyshabur et al. (2018); Langford & Shawe-Taylor (2002); Herbrich & Graepel (2000) in leveraging the noiseresilience of a classifier. The high level argument is that, to convert the PAC-Bayesian bound, these latter works relied on a looser output perturbation bound, one that holds on all possible inputs, with high probability over all perturbations i.e., a bound on maxx ∥f (x;W) − f (x;W + U)∥∞ w.h.p over draws of U . In contrast, our technique relies on a subtly different but significantly tighter bound: a bound on the output perturbation that holds with high probability given an input i.e., a bound\non ∥f (x;W) − f (x;W + U)∥∞ w.h.p over draws of U for each x. When we do instantiate our framework as in the next section, this subtle difference is critical in being able to bound the output perturbation without suffering from a factor proportional to the product of the spectral norms of the weight matrices (which is the case in Neyshabur et al. (2018)).\n\n3. Bound on perturbation of `2 norm on the rows of the Jacobians d/d′.\nPrU[¬PERT-BOUND(W + U ,{ζ̂ ′d/d′} d d′=1,x) ∧\nPERT-BOUND(W + U , Ĉd−1,x) ∧ UNCHANGED-ACTSd−1(W + U ,x)] ≤ δ̂\n\n4 APPLICATION OF OUR FRAMEWORK TO RELU NETWORKS\nNOTATION. In this section, we apply our framework to feedforward fully connected ReLU networks of depth D (we care about D > 2) and width H (which we will assume is larger than the input dimensionality N , to simplify our proofs) and derive a generalization bound on the original network that does not scale with the product of spectral norms of the weight matrices. Let φ (⋅) denote the ReLU activation. We consider a network parameterized byW = (W1,W2,⋯,WD) such that the output of the network is computed as f (x;W) = WDφ (WD−1⋯φ (W1x)). We denote the value of the hth hidden unit on the dth layer before and after the activation by gd (x;W) [h] and fd (x;W) [h] respectively. We define Jd/d ′ (x;W) ∶= ∂gd (x;W)/∂gd ′ (x;W) to be the Jacobian of the pre-activations of layer d with respect to the pre-activations of layer d′ for d′ ≤ d (each row in this Jacobian corresponds to a unit in layer d). In short, we will call this, Jacobian d/d′. Let Z denote the random initialization of the network.\nInformally, we consider a setting where the learning algorithm satisfies the following conditions on the training data that make it noise-resilient on training data: a) the `2 norm of the hidden layers are all small, b) the pre-activation values are all sufficiently large in magnitude, c) the Jacobian of any layer with respect to a lower layer, has rows with a small `2 norm, and has a small spectral norm. We cast these conditions in the form of Equation 1 by appropriately defining the properties ρ’s and the margins ∆⋆’s in the general framework. We note that these properties are quite similar to those already explored in Arora et al. (2018); Neyshabur et al. (2017); we provide more intuition about these properties, and how we cast them in our framework in Appendix E.1.\nHaving defined these properties, we first prove in Lemma E.1 in Appendix E a guarantee equivalent to the abstract inequality in Equation 2. Essentially, we show that under random perturbations of the parameters, the perturbation in the output of the network and the perturbation in the input-dependent properties involved in (a), (b), (c) themselves can all be bounded in terms of each other. Crucially, these perturbation bounds do not grow with the spectral norms of the network.\nHaving instantiated the framework as above, we then instantiate the bound provided by the framework. Our generalization bound scales with the bounds on the properties in (a) and (c) above as satisfied on the training data, and with the reciprocal of the property in (b) i.e., the smallest absolute value of the pre-activations on the training data. Additionally, our bound has an explicit dependence on the depth of the network, which arises from the fact that we generalize R = O(D) conditions. Most importantly, our bound does not have a dependence on the product of the spectral norms of the weight matrices.\nTheorem 4.1. (shorter version; see Appendix F for the complete statement) For any margin γclass > 0, and any δ > 0, with probability 1 − δ over the draw of samples from Dm, for anyW , we have that:\nPr(x,y)∼D [L0(f (x;W) , y)] ≤Pr(x,y)∼S [Lγclass(f (x;W) , y)] + Õ (D √ ∥W −Z∥2F /((σ ⋆)2m))\nHere 1/σ⋆ equals Õ( √ Hmax{Blayer-`2 ,Bpreact,Boutput,Bjac-row-`2 ,Bjac-spec}), where\nBlayer-`2 ∶= O ⎛\n⎝ max 1≤d<D\n∑ d d′=1 ζ ⋆ d/d′α ⋆ d′−1\nα⋆d\n⎞ ⎠ ,Bpreact ∶= O ⎛ ⎝ max 1≤d<D\n∑ d d′=1 ζ ⋆ d/d′α ⋆ d′−1\n√ Hγ⋆d\n⎞\n⎠\nBoutput ∶= O ⎛\n⎝\n∑ D d=1 ζ ⋆ D/dα ⋆ d−1\n√ Hγclass\n⎞ ⎠ ,Bjac-row-`2 ∶= O ⎛ ⎝ max 1≤d′<d<D\nζ⋆d−1/d′ + ∥Wd∥2,∞∑ d−1 d′′=d′+1 ψ ⋆ d−1/d′′ζ ⋆ d′′−1/d′\nζ⋆ d/d′\n⎞\n⎠\nBjac-spec ∶= O ⎛\n⎝ max 1≤d′<d<D\nψ⋆d′′−1/d′ + ∥Wd∥2∑ d−1 d′′=d′+1 ψ ⋆ d−1/d′′ζ ⋆ d′′−1/d′\nψ⋆ d/d′\n⎞\n⎠\nwhere, the terms α⋆d, γ ⋆ d etc., are norm-bounds that hold on all training data (x, y) ∈ S as follows: α⋆d ≥ ∥f d (x;W)∥ (an upper bound on the `2 norm of each hidden layer output), γ⋆d ≤ minh ∣f d (x;W) [h]∣ (a lower bound on the absolute values of the pre-activations for each layer), ζ⋆d/d′ ≥ ∥J d/d′(x;W)∥\n2,∞ (an upper bound on the row `2 norms of the Jacobian for each\nlayer), ψ⋆d/d′ ≥ ∥J d/d′(x;W)∥\n2 (an upper bound on the spectral norm of the Jacobian for each\nlayer).\nIn Figure 1, we show how the terms in the bound vary for networks of varying depth with a small width of H = 40 on the MNIST dataset. We observe that Blayer-`2 ,Boutput,Bjac-row-`2 ,Bjac-spec typically lie in the range of [100,102] and scale with depth as ∝ 1.57D. In contrast, the equivalent term from Neyshabur et al. (2018) consisting of the product of spectral norms can be as large as 103 or 105 and scale with D more severely as 2.15D.\nThe bottleneck in our bound is Bpreact, which scales inversely with the magnitude of the smallest absolute pre-activation value of the network. In practice, this term can be arbitrarily large, even though it does not depend on the product of spectral norms/depth. This is because some hidden units can have arbitrarily small absolute pre-activation values – although this is true only for a small proportion of these units.\nTo give an idea of the typical, non-pathological magnitude of the pre-activation values, we plot two other variations of Bpreact: a) 5%-Bpreact which is calculated by ignoring 5% of the training datapoints with the smallest absolute pre-activation values and b) median-Bpreact which is calculated by ignoring half the hidden units in each layer with the smallest absolute pre-activation values for each input. We observe that median-Bpreact is quite small (of the order of 102), while 5%-Bpreact, while large (of the order of 104), is still orders of magnitude smaller than Bpreact.\nIn Figure 2 we show how our overall bound and existing product-of-spectral-norm-based bounds (Bartlett et al., 2017; Neyshabur et al., 2018) vary with depth. While our bound is orders of magnitude larger than prior bounds, the key point here is that our bound grows with depth as 1.57D while prior bounds grow with depth as 2.15D indicating that our bound should perform asymptotically better with respect to depth. Indeed, we verify that our bound obtains better values than the other existing bounds when D = 28 (see Figure 2 b). We also plot hypothetical variations of our bound replacing Bpreact with 5%-Bpreact (see “Ours-5%”) and median-Bpreact (see “Ours-Median”) both of which perform orders of magnitude better than our actual bound (note that these two hypothetical bounds do not actually hold good). In fact for larger depth, the bound with 5%-Bpreact performs better than all other bounds (including existing bounds). This indicates that the only bottleneck in our bound comes from the dependence on the smallest pre-activation magnitudes, and if this particular\ndependence is addressed, our bound has the potential to achieve tighter guarantees for even smaller D such as D = 8.\n(2018) maxx ∥x∥2D √ H∏Dd=1 ∥Wd∥2\nγclass ⋅\n√\n∑ D d=1 ∥Wd−Zd∥2F ∥Wd∥22 and Bartlett et al. (2017) maxx ∥x∥2∏ D d=1 ∥Wd∥2 γclass ⋅\n(∑ D d=1 ( ∥Wd−Zd∥2,1 ∥Wd∥ )\n2/3 ) 3/2 both of which have been modified to include distance from initialization\ninstead of distance from origin for a fair comparison. Observe the last two bounds have a plot with a larger slope than the other bounds indicating that they might potentially do worse for a sufficiently large D. Indeed, this can be observed from the plots on the right where we report the distribution of the logarithm of these bounds for D = 28 across 12 runs (although under training settings different from the experiments on the left; see Appendix F.3 for the exact details).\nWe refer the reader to Appendix F.3 for added discussion where we demonstrate how all the quantities in our bound vary with depth for H = 1280 (Figure 3, 4) and with width for D = 8,14 (Figures 5 and 6).\nFinally, as noted before, we emphasize that the dependence of our bound on the pre-activation values is a limitation in how we characterize noise-resilience through our conditions rather than a drawback in our general PAC-Bayesian framework itself. Specifically, using the assumed lower bound on the pre-activation magnitudes we can ensure that, under noise, the activation states of the units do not flip; then the noise propagates through the network in a tractable, “linear” manner. Improving this analysis is an important direction for future work. For example, one could modify our analysis to allow perturbations large enough to flip a small proportion of the activation states; one could potentially formulate such realistic conditions by drawing inspiration from the conditions in Neyshabur et al. (2017); Arora et al. (2018).\nHowever, we note that even though these prior approaches made more realistic assumptions about the magnitudes of the pre-activation values, the key limitation in these approaches is that even under our non-realistic assumption, their approaches would yield bounds only on stochastic/compressed networks. Generalizing noise-resilience from training data to test data is crucial to extending these bounds to the original network, which we accomplish.\n\n4. Bound on perturbation of spectral norm of the Jacobians d/d′.\nPrU[¬PERT-BOUND(W + U ,{ψ̂′d/d′} d d′=1,x) ∧\nPERT-BOUND(W + U , Ĉd−1,x) ∧ UNCHANGED-ACTSd−1(W + U ,x)] ≤ δ̂\nProof. For the most part of this discussion, we will consider a perturbed network where all the hidden units are frozen to be at the same activation state as they were at, before the perturbation. We will denote the weights of such a network byW[+U] and its output at the dth layer by fd (x;W[+U]). By having the activations states frozen, the Gaussian perturbations propagate linearly through the activations, effectively remaining as Gaussian perturbations; then, we can enjoy the well-established properties of the Gaussian even after they propagate.\nPERTURBATION BOUND ON THE `2 NORM OF LAYER d. We bound the change in the `2 norm of the dth layer’s output by applying a triangle inequality6 after splitting it into a sum of vectors. Each summand here (which we define as vd′ for each d′ ≤ d) is the difference in the dth layer output on introducing noise in weight matrix d′ after having introduced noise into all the first d′ − 1 weight matrices.\n∣∥fd (x;W[+Ud])∥ − ∥f d (x;W)∥∣ ≤ ∥fd (x;W[+Ud]) − f d (x;W)∥\nbecause the activations are ReLU, we can replace this with the perturbation of the pre-activation\n≤ ∥gd (x;W[+Ud]) − g d (x;W)∥\n≤ XXXXXXXXXXXXXXXXXXX d ∑ d′=1 ⎛ ⎜ ⎜ ⎜ ⎝ gd (x;W[+Ud′]) − g d (x;W[+Ud′−1]) ´¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¸¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¶ ∶=vd′ ⎞ ⎟ ⎟ ⎟ ⎠ XXXXXXXXXXXXXXXXXXX\n≤ d\n∑ d′=1\n∥vd′∥ = d\n∑ d′=1\n√\n∑ h\nv2d′,h (9)\nHere, vd′,h is the perturbation in the preactivation of hidden unit h on layer d′, brought about by perturbation of the d′th weight matrix in a network where only the first d′ − 1 weight matrices have already been perturbed.\nNow, for each h, we bound vd′,h in Equation 9. Since the activations have been frozen we can rewrite each vd′,h as the product of the hth row of the unperturbed network’s Jacobian d/d′ , followed by only the perturbation matrix Ud′ , and then the output of the layer d′ − 1. Concretely, we have7 8:\nvd′,h =\n1×Hd′ ³¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹·¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹µ Jd/d ′ (x;W)[h] Hd′×Hd′−1 ³·µ Ud′ Hd′−1×1 ³¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹·¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹µ fd ′−1\n(x;W[+Ud′−1]) ´¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¸¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¶\nspherical Gaussian\nWhat do these random variables vd′,h look like? Conditioned on Ud′−1, the second part of our expansion of vd′,h, namely, Ud′fd\n′−1 (x;W[+Ud′−1]) is a multivariate spherical Gaussian (see Lemma B.2) of the form N (0, σ2∥fd ′−1 (x;W[+Ud′−1]) ∥ 2I). As a result, conditioned on Ud′−1, vd′,h is a univariate Gaussian N (0, σ2∥Jd/d ′ (x;W)[h]∥2∥fd ′−1 (x;W[+Ud′−1])∥ 2).\nThen, we can apply a standard Gaussian tail bound (see Lemma B.1) to conclude that with probability 1 − δ̂/DH over the draws of Ud′ (conditioned on any Ud′−1), vd′,h is bounded as:\n∣vd′,h∣ ≤ σ ∥J d/d′ (x;W)[h]∥ ∥fd ′−1 (x;W[+Ud′−1])∥\n√\n2 ln 2DH\nδ̂ . (10)\nThen, by a union bound over all the hidden units on layer d, and for each d′, we have that with probability 1 − δ̂, Equation 9 is upper bounded as:\n6Specifically, for two vectors a,b, we have from triangle inequality that ∥b∥ ≤ ∥a∥ + ∥b − a∥ and ∥a∥ ≤ ∥b∥ + ∥a − b∥. As a result of this, we have: − ∥a − b∥ ≤ ∥a∥ − ∥b∥ ≤ ∥a − b∥. We use this inequality in our proof.\n7Below, we have used Hd to denote the number of units on the dth layer (and this equals H for the hidden units and K for the output layer).\n8Note that the succinct formula below holds good even for the corner case d′ = d, where the first Jacobian-row term becomes a vector with zeros on all but the hth entry and therefore only the hth row of the perturbation matrix Ud′ will participate in the expression of vd′,h.\n∑ d′\n√\n∑ h\nv2d′,h ≤ d\n∑ d′=1\nσ ∥Jd/d ′ (x;W)∥\nF ∥fd\n′−1 (x;W[+Ud′−1])∥\n√\n2 ln 2DH\nδ̂ . (11)\nUsing this we prove the probability bound in the lemma statement. To simplify notations, let us denote Ĉd−1⋃{ζ̂d/d′}dd′=1 by Ĉprev. Furthermore, we will drop redundant symbols in the arguments of the events we have defined. Then, recall that we want to upper bound the following probability (we ignore the argumentsW + U and x for brevity):\nPr [(¬PERT-BOUND ({α̂′d})) ∧ PERT-BOUND(Ĉprev) ∧ UNCHANGED-ACTSd−1]\nRecall that Equation 11 is a bound on the perturbation of the `2 norm of the dth layer’s output when the activation states are explicitly frozen. If the perturbation we randomly draw happens to satisfy UNCHANGED-ACTSd−1 then the bound in Equation 11 holds good even in the case where the activation states are not explicitly frozen. Furthermore, when PERT-BOUND(Ĉprev) holds, the bound in Equation 11 can be upper-bounded by α̂′d as defined in the lemma statement, because under PERT-BOUND(Ĉprev), the middle term in Equation 11 can be upper bounded using triangle inequality as ∥fd ′−1 (x;W[+Ud′−1])∥ ≤ ∥f d′−1 (x;W)∥ + α̂d′−1. Hence, the event above happens only for the perturbations for which Equation 11 fails and hence we have that the above probability term is upper bounded by δ̂.\nPERTURBATION BOUND ON THE PREACTIVATION VALUES OF LAYER d. Following the same analysis as above, the bound we are seeking here is essentially maxh∑dd′=1 ∣vd′,h∣. The bound follows similarly from Equation 10.\nPERTURBATION BOUND ON THE `2 NORM OF THE ROWS OF THE JACOBIAN d/d′. We split this term like we did in the previous subsection, and apply triangle equality as follows:\nmax h\n∣∥Jd/d ′ (x;W)[h]∥ − ∥Jd/d ′ (x;W[+Ud])[h]∥∣\n≤ max h\n∥Jd/d ′ (x;W)[h] − Jd/d ′ (x;W[+Ud])[h]∥\n≤ XXXXXXXXXXXXXXXXXXXXX d ∑ d′′=1 ⎛ ⎜ ⎜ ⎜ ⎜ ⎜ ⎝ Jd/d ′ (x;W[+Ud′′])[h] − J d/d′ (x;W[+Ud′′−1])[h] ´¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¸¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¶ ∶=yd′′ h ⎞ ⎟ ⎟ ⎟ ⎟ ⎟ ⎠ XXXXXXXXXXXXXXXXXXXXX\n≤ max h\nd\n∑ d′′=1\n∥yd ′′\nh ∥ = max h\nd\n∑ d′′=1\n√\n∑ h′\n(yd′′,h,h′) 2 (12)\nHere, we have defined yd ′′\nh to be the vector that corresponds to the difference in the hth row of the Jacobian d/d′ brought about by perturbing the d′′th weight matrix, given that the first d′′ − 1 matrices have already been perturbed. We use h to iterate over the units in the dth layer and h′ to iterate over the units in the d′th layer.\nNow, under the frozen activation states, when we perturb the weight matrices from 1 uptil d′, since these matrices are not involved in the Jacobian d/d′, fortunately, the Jacobian d/d′ is not perturbed (as the set of active weights in d/d′ are the same when we perturbW asW[+Ud′]). So, we will only need to bound yd′′,h,h′ for d′′ > d′.\nWhat does the distribution of yd′′,h,h′ look like for d′′ > d′? We can expand9 yd′′,h,h′ as the product of i) the hth row of the Jacobian d/d′′ ii) the perturbation matrix Ud′′ and iii) the h′th column of the Jacobian d′/d′′ − 1 for the perturbed network:\n9Again, note that the below succinct formula works even for corner cases like d′′ = d′ or d′′ = d.\nyd′′,h,h′ =\n1×Hd′′ ³¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹·¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹µ Jd/d ′′ (x;W)[h] Hd′′×Hd′′−1 ³·µ Ud′′ Hd′′−1×1 ³¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹·¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹µ Jd ′′−1/d′ (x;W[+Ud′′−1])[∶, h ′ ]\n´¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¸¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¶ spherical Gaussian\nConditioned on Ud′′−1, the second part of this expansion, namely, Ud′′Jd ′′−1/d′(x;W[+Ud′′−1])[∶ , h′] is a multivariate spherical Gaussian (see Lemma B.2) of the form N (0, σ2∥Jd ′′−1/d′(x;W[+Ud′′−1])[∶, h ′]∥2I). As a result, conditioned on Ud′′−1, yd′′,h,h′ is a univariate Gaussian N (0, σ2∥Jd/d ′′ (x;W)[h]∥2∥Jd ′′−1/d′(x;W[+Ud′′−1])[∶, h ′]∥2).\nThen, by applying a standard Gaussian tail bound we have that with probability 1 − δ̂ D2H2 over the draws of Ud′′ conditioned on Ud′′−1, each of these quantities is bounded as:\n∣yd′′,h,h′ ∣ ≤ σ∥J d/d′′ (x;W)[h]∥∥Jd ′′−1/d′ (x;W[+Ud′′−1])[∶, h ′ ]∥\n√\n2 ln D2H2\nδ̂ (13)\nWe simplify the bound on the right hand side a bit further so that it does not involve any Jacobian of layer d. Specifically, when d′′ < d, ∥Jd/d ′′ (x;W)[h]∥ can be written as the product of the spectral norm of the Jacobian d′ − 1/d′′ and the `2 norm of the hth row of Jacobian d − 1/d. Here, the latter can be upper bounded by the `2 norm of the hth row of Wd since the Jacobian (for a ReLU network) is essentially Wd but with some columns zerod out. When d = d′′, ∥Jd/d ′ (x;W)[h]∥ is essentially 1\nas the Jacobian is merely the identity matrix. Thus, we have:\n∣yd′′,h,h′ ∣ ≤ ⎧⎪⎪⎪ ⎨ ⎪⎪⎪⎩ σ ∥wdh∥ ∥J d−1/d′′(x;W)∥ 2 ∥Jd ′′−1/d′(x;W[+Ud′′−1])[∶, h ′]∥\n√ 4 ln DH\nδ̂ d′′ < d\nσ∥Jd ′′−1/d′(x;W[+Ud′′−1])[∶, h ′]∥ √\n4 ln DH δ̂\nd′′ = d\nBy a union bound on all d′′, we then get that with probability 1 − δ̂ D\nover the draws of Ud, we can upper bound Equation 12 as:\nmax h\nd\n∑ d′′=1\n√\n∑ h′\n(yd′′,h,h′) 2 ≤ σ ∥Jd−1/d ′ (x;W[+Ud′′−1])∥\nF\n√\n4 ln DH\nδ̂ +\nd−1 ∑\nd′′=d′+1 σmax h ∥wdh∥ ∥J\nd−1/d′′ (x;W)∥\n2 ∥Jd\n′′−1/d′ (x;W[+Ud′′−1])∥\nF\n√\n4 ln DH\nδ̂\nBy again applying a union bound for all d′, we get the above bound to hold simultaneously for all d′\nwith probability at least 1 − δ̂. Then, by a similar argument as in the case of the perturbation bound on the output of each layer, we get the result of the lemma.\nPERTURBATION BOUND ON THE SPECTRAL NORM OF THE JACOBIAN d/d′ . Again, we split this term and apply triangle equality as follows:\n∣∥Jd/d ′ (x;W)∥ 2 − ∥Jd/d ′ (x;W[+Ud])∥ 2 ∣ ≤ ∥Jd/d ′ (x;W) − Jd/d ′ (x;W[+Ud])∥\n2\n≤ XXXXXXXXXXXXXXXXXXX d ∑ d′′=1 ⎛ ⎜ ⎜ ⎜ ⎜ ⎝ Jd/d ′ (x;W[+Ud′′]) − J d/d′ (x;W[+Ud′′−1]) ´¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¸¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¶ ∶=Yd′′ ⎞ ⎟ ⎟ ⎟ ⎟ ⎠ XXXXXXXXXXXXXXXXXXX2\n≤ d\n∑ d′′=1\n∥Yd′′∥2 (14)\nHere, we have defined Yd′′ to be the matrix that corresponds to the difference in the Jacobian d/d′ brought about by perturbaing the the d′′th weight matrix, given that the first d′′ − 1 matrices have already been perturbed.\nAs argued before, under the frozen activation states, when we perturb the weight matrices from 1 uptil d′, since these matrices are not involved in the Jacobian d/d′, fortunately, the Jacobian d/d′ is not perturbed (as the set of active weights in d/d′ are the same when we perturbW asW[+Ud′]). So, we will only need to bound Yd′′ for d′′ > d′.\nRecall that we can exapnd Yd′′ for d′′ > d′, yd′′,h,h′ as the product of i) Jacobian d/d′′ ii) the perturbation matrix Ud′′ and iii) the Jacobian d′/d′′ − 1 for the perturbed network10:\nYd′′ =\nHd×Hd′′ ³¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹·¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹µ Jd/d ′′ (x;W) Hd′′×Hd′′−1 ³·µ Ud′′ Hd′′−1×Hd′ ³¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹·¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹µ Jd ′′−1/d′\n(x;W[+Ud′′−1]) ´¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¸¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¹¶\nspherical Gaussian\nNow, the spectral norm of Yd′′ is at most the products of the spectral norms of each of these three matrices. Using Lemma B.3, the spectral norm of the middle term Ud′′ can be bounded by σ √\n2H ln 2DH δ̂ with high probability 1 − δ̂ D over the draws of Ud′′ . 11\nWe will also decompose the spectral norm of the first term so that our final bound does not involve any Jacobian of the dth layer. When d′′ = d, this term has spectral norm 1 because the Jacobian d/d is essentially the identity matrix. When d′′ < d, we have that ∥Jd/d ′′ (x;W)∥\n2 ≤\n∥Jd/d−1(x;W)∥ 2 ∥Jd−1/d ′′ (x;W)∥ 2 . Furthermore, since, for a ReLU network, Jd/d−1(x;W) is effectively Wd with some columns zerod out, the spectral norm of the Jacobian is upper bounded by the spectral norm Wd.\nPutting all these together, we have that with probability 1 − δ̂ D over the draws of Ud′′ , the following holds good:\n∥Yd′′∥2 ≤ ⎧⎪⎪⎪ ⎨ ⎪⎪⎪⎩ σ ∥Wd∥2 ∥J d−1/d′′(x;W)∥ 2 ∥Jd ′′−1/d′(x;W[+Ud′′−1])∥ 2\n√ 2H ln 2DH\nδ̂ d′′ < d\nσ ∥Jd ′′−1/d′(x;W[+Ud′′−1])∥\n2\n√ 2H ln 2DH\nδ̂ d′′ = d\nBy a union bound, we then get that with probability 1 − δ̂ over the draws of Ud, we can upper bound Equation 14 as:\nd\n∑ d′′=1\n∥Yd′∥2 ≤σ ∥J d′′−1/d′\n(x;W[+Ud′′−1])∥ 2\n√\n2H ln 2DH\nδ̂\n+ σ d−1 ∑\nd′′=d′+1 ∥Wd∥2 ∥J\nd−1/d′′ (x;W)∥\n2 ∥Jd\n′′−1/d′ (x;W[+Ud′′−1])∥\n2\n√\n2H ln 2DH\nδ̂\nNote that the above bound simultaneously holds over all d′ (without the application of a union bound). Finally we get the result of the lemma by a similar argument as in the case of the perturbation bound on the output of each layer.\n10Again, note that the below succinct formula works even for corner cases like d′′ = d′ or d′′ = d. 11Although Lemma B.3 applies only to the case where Ud′′ is a H ×H matrix, it can be easily extended to the corner cases when d′′ = 1 or d′′ =D. When d′′ = 1, Ud′′ would be a H ×N matrix, where H > N ; one could imagine adding more random columns to this matrix, and applying Lemma B.3. Since adding columns does not reduce the spectral norm, the bound on the larger matrix would apply on the original matrix too. A similar argument would apply to d′′ =D, where the matrix would be K ×H .\n\n5 SUMMARY AND FUTURE WORK\nIn this work, we introduced a novel PAC-Bayesian framework for leveraging the noise-resilience of deep neural networks on training data, to derive a generalization bound on the original uncompressed, deterministic network. The main philosophy of our approach is to first generalize the noise-resilience from training data to test data using which we convert a PAC-Bayesian bound on a stochastic network to a standard margin-based generalization bound. We apply our approach to ReLU based networks and derive a bound that scales with terms that capture the interactions between the weight matrices better than the product of spectral norms.\nFor future work, the most important direction is that of removing the dependence on our strong assumption that the magnitude of the pre-activation values of the network are not too small on training data. More generally, a better understanding of the source of noise-resilience in deep ReLU networks would help in applying our framework more carefully in these settings, leading to tighter guarantees on the original network.\nACKNOWLEDGEMENTS. Vaishnavh Nagarajan was partially supported by a grant from the Bosch Center for AI.\n",
    "approval": true,
    "rationale": "This paper presents a PAC-Bayesian framework that bounds the generalization error of the learned model. While PAC-Bayesian bounds have been studied before, the focus of this paper is to study how different conditions in the network (e.g. behavior of activations) generalize from training set to the distribution. This is important since prior work have not been able to handle this issue properly and as a consequence, previous bounds are either on the networks with perturbed weights or with unrealistic assumptions on the behavior of the network for any input in the domain.\n\nI think the paper could have been written more clearly. I had a hard time following the arguments in the paper. For example, I had to start reading from the Appendix to understand what is going on and found the appendix more helpful than the main text. Moreover, the constraints should be discussed more clearly and verified through experiments.\n\nI see Constraint 2 as a major shortcoming of the paper. The promise of the paper was to avoid making assumptions on the input domain (one of the drawbacks in Neyshabur et al 2018) but the constraint 2 is on any input in the domain. In my view, this makes the result less interesting.\n\nFinally, as authors mention themselves, I think conditions in Theorem F.1 (the label should be 4.1 since it is in Section 4) could be improved with more work. More specifically, it seems that the condition on the pre-activation value can be improved by rebalancing using the positive homogeneity of ReLU activations.\n\nOverall, while I find the motivation and the approach interesting, I think this is not a complete piece of work and it can be improved significantly.\n\n===========\nUpdate: Authors have addressed my main concern, improved the presentation and added extra experiments that improve the quality of the paper.  I recommend accepting this paper. ",
    "rating": 8,
    "type": "positive"
  },
  {
    "title": "THE BREAK-EVEN POINT ON OPTIMIZATION TRAJEC- TORIES OF DEEP NEURAL NETWORKS",
    "abstract": "The early phase of training of deep neural networks is critical for their final performance. In this work, we study how the hyperparameters of stochastic gradient descent (SGD) used in the early phase of training affect the rest of the optimization trajectory. We argue for the existence of the “break-even\" point on this trajectory, beyond which the curvature of the loss surface and noise in the gradient are implicitly regularized by SGD. In particular, we demonstrate on multiple classification tasks that using a large learning rate in the initial phase of training reduces the variance of the gradient, and improves the conditioning of the covariance of gradients. These effects are beneficial from the optimization perspective and become visible after the break-even point. Complementing prior work, we also show that using a low learning rate results in bad conditioning of the loss surface even for a neural network with batch normalization layers. In short, our work shows that key properties of the loss surface are strongly influenced by SGD in the early phase of training. We argue that studying the impact of the identified effects on generalization is a promising future direction.",
    "text": "1 INTRODUCTION\nThe connection between optimization and generalization of deep neural networks (DNNs) is not fully understood. For instance, using a large initial learning rate often improves generalization, which can come at the expense of the initial training loss reduction (Goodfellow et al., 2016; Li et al., 2019; Jiang et al., 2020). In contrast, using batch normalization layers typically improves both generalization and convergence speed of deep neural networks (Luo et al., 2019; Bjorck et al., 2018). These simple examples illustrate limitations of our understanding of DNNs.\nUnderstanding the early phase of training has recently emerged as a promising avenue for studying the link between optimization and generalization of DNNs. It has been observed that applying regularization in the early phase of training is necessary to arrive at a well generalizing final solution (Keskar et al., 2017; Sagun et al., 2017; Achille et al., 2017). Another observed phenomenon is that the local shape of the loss surface changes rapidly in the beginning of training (LeCun et al., 2012; Keskar et al., 2017; Achille et al., 2017; Jastrzebski et al., 2018; Fort & Ganguli, 2019). Theoretical approaches to understanding deep networks also increasingly focus on the early part of the optimization trajectory (Li et al., 2019; Arora et al., 2019).\nIn this work, we study the dependence of the entire optimization trajectory on the early phase of training. We investigate noise in the mini-batch gradients using the covariance of gradients,1 and the local curvature of the loss surface using the Hessian. These two matrices capture important and\n∗Equal contribution. 1We define it as K = 1\nN ∑N i=1(gi − g)\nT (gi − g), where gi = g(xi, yi; θ) is the gradient of the training loss L with respect to θ on xi, N is the number of training examples, and g is the full-batch gradient.\ncomplementary aspects of optimization (Roux et al., 2008; Ghorbani et al., 2019) and generalization performance of DNNs (Jiang et al., 2020; Keskar et al., 2017; Bjorck et al., 2018; Fort et al., 2019). We include a more detailed discussion in Sec. 2.\nOur first contribution is a simplified model of the early part of the training trajectory of DNNs. Based on prior empirical work (Sagun et al., 2017), we assume that the local curvature of the loss surface (the spectral norm of the Hessian) increases or decreases monotonically along the optimization trajectory. Under this model, gradient descent reaches a point in the early phase of training at which it oscillates along the most curved direction of the loss surface. We call this point the break-even point and show empirical evidence of its existence in the training of actual DNNs.\nOur main contribution is to state and present empirical evidence for two conjectures about the dependence of the entire optimization trajectory on the early phase of training. Specifically, we conjecture that the hyperparameters of stochastic gradient descent (SGD) used before reaching the break-even point control: (1) the spectral norms of K and H, and (2) the conditioning of K and H. In particular, using a larger learning rate prior to reaching the break-even point reduces the spectral norm of K along the optimization trajectory (see Fig. 1 for an illustration of this phenomenon). Reducing the spectral norm of K decreases the variance of the mini-batch gradient, which has been linked to improved convergence speed (Johnson & Zhang, 2013).\nFinally, we apply our analysis to a network with batch normalization (BN) layers and find that our predictions are valid in this case as well. Delving deeper in this line of investigation, we show that using a large learning rate is necessary to reach better-conditioned (relatively to a network without BN layers) regions of the loss surface, which was previously attributed to BN alone (Bjorck et al., 2018; Ghorbani et al., 2019; Page, 2019).\n\n2 RELATED WORK\nImplicit regularization induced by the optimization method. The choice of the optimization method implicitly affects generalization performance of deep neural networks (Neyshabur, 2017). In particular, using a large initial learning rate is known to improve generalization (Goodfellow et al.,\n2016; Li et al., 2019). A classical approach to study these questions is to bound the generalization error using measures such as the norm of the parameters at the final minimum (Bartlett et al., 2017; Jiang et al., 2020).\nAn emerging approach is to study the properties of the whole optimization trajectory. Arora et al. (2019) suggest it is necessary to study the optimization trajectory to understand optimization and generalization of deep networks. In a related work, Erhan et al. (2010); Achille et al. (2017) show the existence of a critical period of learning. Erhan et al. (2010) argue that training, unless pretraining is used, is sensitive to shuffling of examples in the first epochs of training. Achille et al. (2017); Golatkar et al. (2019); Sagun et al. (2017); Keskar et al. (2017) demonstrate that adding regularization in the beginning of training affects the final generalization disproportionately more compared to doing so later. We continue research in this direction and study how the choice of hyperparameters in SGD in the early phase of training affects the optimization trajectory in terms of the covariance of gradients, and the Hessian.\nThe covariance of gradients and the Hessian. The Hessian quantifies the local curvature of the loss surface. Recent work has shown that the largest eigenvalues of H can grow quickly in the early phase of training (Keskar et al., 2017; Sagun et al., 2017; Fort & Scherlis, 2019; Jastrzebski et al., 2018). Keskar et al. (2017); Jastrzebski et al. (2017) studied the dependence of the Hessian (at the final minimum) on the optimization hyperparameters. The Hessian can be decomposed into two terms, where the dominant term (at least at the end of training) is the uncentered covariance of gradients G (Sagun et al., 2017; Papyan, 2019).\nThe covariance of gradients, which we denote by K, encapsulates the geometry and the magnitude of variation in gradients across different samples. The matrix K was related to the generalization error in Roux et al. (2008); Jiang et al. (2020). Closely related quantities, such as the cosine alignment between gradients computed on different examples, were recently shown to explain some aspects of deep networks generalization (Fort et al., 2019; Liu et al., 2020; He & Su, 2020). Zhang et al. (2019) argues that in DNNs the Hessian and the covariance of gradients are close in terms of the largest eigenvalues.\nLearning dynamics of deep neural networks. Our theoretical model is motivated by recent work on learning dynamics of neural networks (Goodfellow et al., 2014; Masters & Luschi, 2018; Wu et al., 2018; Yao et al., 2018; Xing et al., 2018; Jastrzebski et al., 2018; Lan et al., 2019). We are directly inspired by Xing et al. (2018) who show that for popular classification benchmarks, the cosine of the angle between consecutive optimization steps in SGD is negative. Similar observations can be found in Lan et al. (2019). Our theoretical analysis is inspired by Wu et al. (2018) who study how SGD selects the final minimum from a stability perspective. We apply their methodology to the early phase of training, and make predictions about the entire training trajectory.\n\n3 THE BREAK-EVEN POINT AND THE TWO CONJECTURES ABOUT SGD TRAJECTORY\nOur overall motivation is to better understand the connection between optimization and generalization of DNNs. In this section we study how the covariance of gradients (K) and the Hessian (H) depend on the early phase of training. We are inspired by recent empirical observations showing their importance for optimization and generalization of DNNs (see Sec. 2 for a detailed discussion).\nRecent work has shown that in the early phase of training the gradient norm (Goodfellow et al., 2016; Fort & Ganguli, 2019; Liu et al., 2020) and the local curvature of the loss surface (Jastrzebski et al., 2018; Fort & Ganguli, 2019) can rapidly increase. Informally speaking, one scenario we study here is when this initial growth is rapid enough to destabilize training. Inspired by Wu et al. (2018), we formalize this intuition using concepts from dynamical stability. Based on the developed analysis, we state two conjectures about the dependence of K and H on hyperparameters of SGD, which we investigate empirically in Sec. 4.\nDefinitions. We begin by introducing the notation. Let us denote the loss on an example (x, y) by L(x, y; θ), where θ is a D-dimensional parameter vector. The two key objects we study are the Hessian of the training loss (H), and the covariance of gradients K = 1N ∑N i=1(gi − g)T (gi − g),\nwhere gi = g(xi, yi; θ) is the gradient of L with respect to θ calculated on i-th example, N is the number of training examples, and g is the full-batch gradient. We denote the i-th normalized eigenvector and eigenvalue of a matrix A by eiA and λ i A. Both H and K are computed at a given θ, but we omit this dependence in the notation. Let t index steps of optimization, and let θ(t) denote the parameter vector at optimization step t.\nInspired by Wu et al. (2018) we introduce the following condition to quantify stability at a given θ(t). Let us denote the projection of parameters θ onto e1H by ψ = 〈θ, e1H〉. With a slight abuse of notation let g(ψ) = 〈g(θ), e1H〉. We say SGD is unstable along e1H at θ(t) if the norm of elements of sequence ψ(τ + 1) = ψ(τ)− ηg(ψ(τ)) diverges when τ →∞, where ψ(0) = θ(t). The sequence ψ(τ) represents optimization trajectory in which every step t′ > t is projected onto e1H .\nAssumptions. Based on recent empirical studies, we make the following assumptions.\n1. The loss surface projected onto e1H is a quadratic one-dimensional function of the form f(ψ) = ∑N i=1(ψ − ψ∗)2Hi. The same assumption was made in Wu et al. (2018), but\nfor all directions in the weight space. Alain et al. (2019) show empirically that the loss averaged over all training examples is well approximated by a quadratic function along e1H .\n2. The eigenvectors e1H and e 1 K are co-linear, i.e. e 1 H = ±e1K , and λ1K = αλ1H for some\nα ∈ R. This is inspired by the fact that the top eigenvalues of H can be well approximated using G (non-centered K) (Papyan, 2019; Sagun et al., 2017). Zhang et al. (2019) shows empirical evidence for co-linearity of the largest eigenvalues of K and H.\n3. If optimization is not stable along e1H at a given θ(t), λ 1 H decreases in the next step, and the\ndistance to the minimum along e1H increases in the next step. This is inspired by recent work showing training can escape a region with too large curvature compared to the learning rate (Zhu et al., 2018; Wu et al., 2018; Jastrzebski et al., 2018).\n4. The spectral norm of H, λ1H , increases during training and the distance to the minimum along e1H decreases, unless increasing λ 1 H would lead to entering a region where training\nis not stable along e1H . This is inspired by (Keskar et al., 2017; Goodfellow et al., 2016; Sagun et al., 2017; Jastrzebski et al., 2018; Fort & Scherlis, 2019; Fort & Ganguli, 2019) who show that in many settings λ1H or gradient norm increases in the beginning of training, while at the same time the overall training loss decreases.\nFinally, we also assume that S N , i.e. that the batch size is small compared to the number of training examples. These assumptions are only used to build a theoretical model for the early phase of training. Its main purpose is to make predictions about the training procedure that we test empirically in Sec. 4.\nReaching the break-even point earlier for a larger learning rate or a smaller batch size. Let us restrict ourselves to the case when training is initialized at θ(0) at which SGD is stable along e1H(0).\n2 We aim to show that the learning rate (η) and the batch size (S) determine H and K in our model, and conjecture that the same holds empirically for realistic neural networks.\nConsider two optimization trajectories for η1 and η2, where η1 > η2, that are initialized at the same θ0, where optimization is stable along e1H(t) and λ 1 H(t) > 0. Under Assumption 1 the loss surface\nalong e1H(t) can be expressed as f(ψ) = ∑N i=1(ψ − ψ∗)2Hi(t), where Hi(t) ∈ R. It can be shown that at any iteration t the necessary and sufficient condition for SGD to be stable along e1H(t) is:\n(1− ηλ1H(t))2 + s(t)2 η2(N − S) S(N − 1) ≤ 1, (1)\nwhere N is the training set size and s(t)2 = Var[Hi(t)] over the training examples. A proof can be found in (Wu et al., 2018). We call this point on the trajectory on which the LHS of Eq. 1 becomes equal to 1 for the first time the break-even point. By definition, there exists only a single break-even point on the training trajectory.\nUnder Assumption 3, λ1H(t) and λ 1 K(t) increase over time. If S = N , the break-even point is reached at λ1H(t) = 2 η . More generally, it can be shown that for η1, the break-even point is reached\n2We include a similar argument for the opposite case in App. B.\nfor a lower magnitude of λ1H(t) than for η2. The same reasoning can be repeated for S (in which case we assume N S). We state this formally and prove in App. B. Under Assumption 4, after passing the break-even point on the training trajectory, SGD does not enter regions where either λ1H or λ 1 K is larger than at the break-even point, as otherwise it would lead to increasing one of the terms in LHS of Eq. 1, and hence losing stability along e1H .\nTwo conjectures about real DNNs. Assuming that real DNNs reach the break-even point, we make the following two conjectures about their optimization trajectory.\nThe most direct implication of reaching the break-even point is that λ1K and λ 1 H at the break-even point depend on η and S, which we formalize as:\nConjecture 1 (Variance reduction effect of SGD). Along the SGD trajectory, the maximum attained values of λ1H and λ 1 K are smaller for a larger learning rate or a smaller batch size.\nWe refer to Conjecture 1 as variance reduction effect of SGD because reducing λ1K can be shown to reduce the L2 distance between the full-batch gradient, and the mini-batch gradient. We expect that similar effects exist for other optimization or regularization methods. We leave investigating them for future work.\nNext, we make another, stronger, conjecture. It is plausible to assume that reaching the break-even point affects to a lesser degree λiH and λ i K for i 6= 1 because increasing their values does not impact stability along e1H . Based on this we conjecture that:\nConjecture 2 (Pre-conditioning effect of SGD). Along the SGD trajectory, the maximum attained values of λ ∗ K\nλ1K and λ\n∗ H\nλ1H are larger for a larger learning rate or a smaller batch size, where λK ∗ and λH ∗ are the smallest non-zero eigenvalues of K and H, respectively. Furthermore, the maximum attained values of Tr(K) and Tr(H) are smaller for a larger learning rate or a smaller batch size.\nWe consider non-zero eigenvalues in the conjecture, because K has at most N − 1 non-zero eigenvalues, where N is the number of training points, which can be much smaller than D in overparametrized DNNs. Both conjectures are valid only for learning rates and batch sizes that guarantee that training converges.\nFrom the optimization perspective, the effects discussed above are desirable. Many papers in the optimization literature underline the importance of reducing the variance of the mini-batch gradient (Johnson & Zhang, 2013) and the conditioning of the covariance of gradients (Roux et al., 2008). There also exists a connection between these effects and generalization (Jiang et al., 2020), which we discuss towards the end of the paper.\n\n4 EXPERIMENTS\nIn this section we first analyse learning dynamics in the early phase of training. Next, we empirically investigate the two conjectures. In the final part we extend our analysis to a neural network with batch normalization layers.\nWe run experiments on the following datasets: CIFAR-10 (Krizhevsky, 2009), IMDB dataset (Maas et al., 2011), ImageNet (Deng et al., 2009), and MNLI (Williams et al., 2018). We apply to these datasets the following architectures: a vanilla CNN (SimpleCNN) following Keras example (Chollet et al., 2015), ResNet-32 (He et al., 2015), LSTM (Hochreiter & Schmidhuber, 1997), DenseNet (Huang et al., 2016), and BERT (Devlin et al., 2018). We also include experiments using a multi-layer perceptron trained on the FashionMNIST dataset (Xiao et al., 2017) in the Appendix. All experimental details are described in App. D.\nFollowing Dauphin et al. (2014); Alain et al. (2019), we estimate the top eigenvalues and eigenvectors of H on a small subset of the training set (e.g. 5% in the case of CIFAR-10) using the Lanczos algorithm (Lanczos, 1950). As computing the full eigenspace of K is infeasible for real DNNs, we compute the covariance using mini-batch gradients. In App. C we show empirically that (after normalization) this approximates well the largest eigenvalue, and we include other details on computing the eigenspaces.\n\n4.1 A CLOSER LOOK AT THE EARLY PHASE OF TRAINING\nFirst, we examine the learning dynamics in the early phase of training. Our goal is to verify some of the assumptions made in Sec. 3. We analyse the evolution of λ1H and λ 1 K when using η = 0.01 and η = 0.001 to train SimpleCNN on the CIFAR-10 dataset. We repeat this experiment 5 times using different random initializations of network parameters.\nVisualizing the break-even point. We visualize the early part of the optimization trajectory in Fig. 1. Following Erhan et al. (2010), we embed the test set predictions at each step of training of SimpleCNN using UMAP (McInnes et al., 2018). The background color indicates λ1K (left) and the training accuracy (right) at the iteration with the closest embedding in Euclidean distance.\nWe observe that the trajectory corresponding to the lower learning rate reaches regions of the loss surface characterized by larger λ1K , compared to regions reached at the same training accuracy in the second trajectory. Additionally, in Fig. 3 we plot the spectrum of K (left) and H (right) at the iterations when λK and λH respectively reach the highest values. We observe more outliers for the lower learning rate in the distributions of both λK and λH .\nAre λ1K and λ1H correlated in the beginning of training? The key assumption behind our theoretical model is that λ1K and λ 1 H are correlated, at least prior to reaching the break-even point. We confirm this in Fig. 2. The highest achieved λ1K and λ 1 H are larger for the smaller η. Additionally, we observe that after achieving the highest value of λ1H , further growth of λ 1 K does not translate to an increase of λ1H . This is expected as λ 1 H decays to 0 when the mean loss decays to 0 for cross entropy loss (Martens, 2016).\nDoes training become increasingly unstable in the early phase of training? According to Assumption 3, an increase of λ1K and λ 1 H translates into a decrease in stability, which we formalized as stability along e1H . Computing stability along e 1 H directly is computationally expensive. Instead, we measure a more tractable proxy. At each iteration we measure the loss on the training set before and\nafter taking the step, which we denote as ∆L (a positive value indicates a reduction of the training loss). In Fig. 2 we observe that training becomes increasingly unstable (∆L starts to take negative values) as λ1K reaches the maximum value.\nSummary. We have shown that the early phase of training is consistent with the assumptions made in our theoretical model. That is, λ1K and λ 1 H increase approximately proportionally to each other, which is also generally correlated with a decrease of a proxy of stability. Finally, we have shown qualitatively reaching the break-even point.\n\n4.2 THE VARIANCE REDUCTION AND THE PRE-CONDITIONING EFFECT OF SGD\nIn this section we test empirically Conjecture 1 and Conjecture 2. For each model we manually pick a suitable range of learning rates and batch sizes to ensure that the properties of K and H that we study have converged under a reasonable computational budget. We mainly focus on studying the covariance of gradients (K), and leave a closer investigation of the Hessian for future work. We use the batch size of 128 to compute K when we vary the batch size for training. When we vary the learning rate instead, we use the same batch size as the one used to train the model. App. C describes the remaining details on how we approximate the eigenspaces of K and H.\nWe summarize the results for SimpleCNN, ResNet-32, LSTM, BERT, and DenseNet in Fig. 4, Fig. 5, and Fig. 6. Curves are smoothed using moving average for clarity. Training curves and additional experiments are reported in App. E.\nTesting Conjecture 1. To test Conjecture 1, we examine the highest value of λ1K observed along the optimization trajectory. As visible in Fig. 4, using a higher η results in λ1K achieving a lower\nmaximum during training. Similarly, we observe that using a higher S in SGD leads to reaching a higher maximum value of λ1K . For instance, for SimpleCNN (top row of Fig. 4) we observe max(λ1K) = 0.68 and max(λ 1 K) = 3.30 for η = 0.1 and η = 0.01, respectively.\nTesting Conjecture 2. To test Conjecture 2, we compute the maximum value of λ∗K/λ1K along the optimization trajectory. It is visible in Fig. 4 that using a higher η results in reaching a larger maximum value of λ∗K/λ 1 K along the trajectory. For instance, in the case of SimpleCNN max(λ∗K/λ 1 K) = 0.37 and max(λ ∗ K/λ 1 K) = 0.24 for η = 0.1 and η = 0.01, respectively.\nA counter-intuitive effect of decreasing the batch size. Consistently with Conjecture 2, we observe that the maximum value of Tr(K) is smaller for the smaller batch size. In the case of SimpleCNN max(Tr(K)) = 5.56 and max(Tr(K)) = 10.86 for S = 10 and S = 100, respectively. Due to space constraints we report the effect of η and S on Tr(K) in other settings in App. E.\nThis effect is counter-intuitive because Tr(K) is proportional to the variance of the mini-batch gradient (see also App. C). Naturally, using a lower batch size generally increases the variance of the mini-batch gradient, and Tr(K). This apparent contradiction is explained by the fact that we measure Tr(K) using a different batch size (128) than the one used to train the model. Hence, decreasing the batch size both increases (due to approximating the gradient using fewer samples) and decreases (as predicted in Conjecture 2) the variance of the mini-batch gradient along the optimization trajectory.\nHow early in training is the break-even point reached? We find that λ1K and λ1H reach their highest values early in training, close to reaching 60% training accuracy on CIFAR-10, and 75% training accuracy on IMDB. The training and validation accuracies are reported for all the experiments in App. E. This suggests that the break-even point is reached early in training.\nThe Hessian. In the above, we have focused on the covariance of gradients. In Fig. 5 we report how λ1H depends on η and S for ResNet-32 and SimpleCNN. Consistently with prior work (Keskar et al., 2017; Jastrzebski et al., 2018), we observe that using a smaller η or using a larger S coincides with a larger maximum value of λ1H . For instance, for SimpleCNN we observe max(λ 1 H) = 26.27 and max(λ1H) = 211.17 for η = 0.1 and η = 0.01, respectively. We leave testing predictions made in Conjecture 2 about the Hessian for future work.\nLarger scale studies. Finally, we test the two conjectures in two larger scale settings: BERT fine-tuned on the MNLI dataset, and DenseNet trained on the ImageNet dataset. Due to memory constraints, we only vary the learning rate. We report results in Fig. 6. We observe that both conjectures hold in these two settings. It is worth noting that DenseNet uses batch normalization layers. In the next section we investigate closer batch-normalized networks.\nSummary. In this section we have shown evidence supporting the variance reduction (Conjecture 1) and the pre-conditioning effect (Conjecture 2) of SGD in a range of classification tasks. We also found that the above conclusions hold for MLP trained on the Fashion MNIST dataset, SGD with momentum, and SGD with learning rate decay. We include these results in App. E-G.\n\n4.3 IMPORTANCE OF LEARNING RATE FOR CONDITIONING IN NETWORKS WITH BATCH NORMALIZATION LAYERS\nThe loss surface of deep neural networks has been widely reported to be ill-conditioned (LeCun et al., 2012; Martens, 2016). Recently, Ghorbani et al. (2019); Page (2019) argued that the key reason behind the efficacy of batch normalization (Ioffe & Szegedy, 2015) is improving conditioning of the loss surface. In Conjecture 2 we suggest that using a high η (or a small S) results in improving the conditioning of K and H. A natural question that we investigate in this section is how the two phenomena are related. We study here the effect of learning rate, and report in App. H an analogous study for batch size.\nAre the two conjectures valid in networks with batch normalization layers? First, to investigate whether our conjectures hold in networks with batch normalization layers, we run similar experiments as in Sec. 4.2 with a SimpleCNN model with batch normalization layers inserted after each layer (SimpleCNN-BN), on the CIFAR-10 dataset. We test η ∈ {0.001, 0.01, 0.1, 1.0} (using η = 1.0 leads to divergence of SimpleCNN without BN). We summarize the results in Fig. 7. We observe that the evolution of λ∗K/λ 1 K and λ 1 K is consistent with both Conjecture 1 and Conjecture 2.\nA closer look at the early phase of training. To further corroborate that our analysis applies to networks with batch normalization layers, we study the early phase of training of SimpleCNN-BN, complementing the results in Sec. 4.1.\nWe observe in Fig. 7 (bottom) that training of SimpleCNN-BN starts in a region characterized by a relatively high λ1K . This is consistent with prior work showing that networks with batch normalization layers can exhibit gradient explosion in the first iteration (Yang et al., 2019). The value of λ1K then decays for all but the lowest η. This behavior is consistent with our theoretical model. We also track the norm of the scaling factor in the batch normalization layers, ‖γ‖, in the last layer of the network in Fig. 7 (bottom). It is visible that η = 1.0 and η = 0.1 initially decrease the value of ‖γ‖, which we hypothesize to be one of the mechanisms due to which high η steers optimization towards better conditioned regions of the loss surface in batch-normalized networks. Interestingly, this seems consistent with Luo et al. (2019) who argue that using mini-batch statistics in batch normalization acts as an implicit regularizer by reducing ‖γ‖.\nUsing batch normalization requires using a high learning rate. As our conjectures hold for SimpleCNN-BN, a natural question is if the loss surface can be ill-conditioned with a low learning rate even when batch normalization is used. Ghorbani et al. (2019) show that without batch normalization, mini-batch gradients are largely contained in the subspace spanned by the top eigenvectors of noncentered K. To answer this question we track ‖g‖/‖g5‖, where g denotes the mini-batch gradient, and g5 denotes the mini-batch gradient projected onto the top 5 eigenvectors of K. A value of ‖g‖/‖g5‖ close to 1 implies that the mini-batch gradient is mostly contained in the subspace spanned by the top 5 eigenvectors of K.\nWe compare two settings: SimpleCNN-BN optimized with η = 0.001, and SimpleCNN optimized with η = 0.01. We make three observations. First, the maximum and minimum values of ‖g‖/‖g5‖ are 1.90 (1.37) and 2.02 (1.09), respectively. Second, the maximum and minimum values of λ1K are 12.05 and 3.30, respectively. Finally, λ∗K/λ 1 K reaches 0.343 in the first setting, and 0.24 in the second setting. Comparing these differences to differences that are induced by using the highest η = 1.0 in SimpleCNN-BN, we can conclude that using a large learning rate is necessary to observe the effect of loss smoothing which was previously attributed to batch normalization alone (Ghorbani et al., 2019; Page, 2019; Bjorck et al., 2018). This might be directly related to the result that using a high learning rate is necessary to achieve good generalization when using batch normalization layers (Bjorck et al., 2018).\nSummary. We have shown that the effects of the learning rate predicted in Conjecture 1 and Conjecture 2 hold for a network with batch normalization layers, and that using a high learning rate is necessary in a network with batch normalization layers to improve conditioning of the loss surface, compared to conditioning of the loss surface in the same network without batch normalization layers.\n\n5 CONCLUSION\nBased on our theoretical model, we argued for the existence of the break-even point on the optimization trajectory induced by SGD. We presented evidence that hyperparameters used in the early phase of training control the spectral norm and the conditioning of K (a matrix describing noise in the mini-batch gradients) and H (a matrix describing local curvature of the loss surface) after reaching the break-even point. In particular, using a large initial learning rate steers training to better conditioned regions of the loss surface, which is beneficial from the optimization point of view.\nA natural direction for the future is connecting our observations to recent studies on the relation of measures, such as gradient variance, to the generalization of deep networks (Li et al., 2019; Jiang et al., 2020; Fort et al., 2019). Our work shows that the hyperparameters of SGD control these measures after the break-even point. Another interesting direction is to understand the connection between the existence of the break-even point and the existence of the critical learning period in training of DNNs (Achille et al., 2017).\n",
    "approval": true,
    "rationale": "The authors demonstrate that, during training, there is a point during the early phase of training that leads stochastic gradient descent (SGD) to a point where the covariance of the gradients (K) has a lower spectral norm (smaller first eigenvalue) and improved conditioning in K and the Hessian of the training loss (H).\n\nThe authors experiments seem to verify that learning rate and batch size do play a part in the spectral norm of K and the conditioning of K. My one issue is that, while effects on K produced by higher learning rates are supposed to be \"good\", the authors do not directly relate this back to model performance. From my years of experience training neural networks, I have seen many scenarios in which higher learning rates result in worse performance, even after reducing the learning rate. Can this be related back to the author's claims? Under what conditions does a higher learning rate lead to these effects on K and H and will it always lead to better model performance?\n\n\nOther comments:\nIn definitions, it says that the eigenvalue of matrix A is \\lambda_A^i, however, later in the 4th part of the assumptions, the spectral norm of H is referred to as \\lambda_1^H. Is there a difference here? Typo?\n\nThe last part of the definitions where \\Phi(\\tau) is introduced should have a formal definition for \\Phi(\\tau) as \\Phi is initially does not take any parameter \\tau.\n\nIn section 4.1, \"further growth of \\lambda_K^1 K does not translate into an increase of \\lambda_K^1\" \\lambda_K^1 is repeated. Typo?\n\n** After author response **\nChanging from weak accept to accept.\n\nThe authors have addressed my concerns about the paper.",
    "rating": 8,
    "type": "positive"
  },
  {
    "title": "WORKS BY EXPLOITING NUMERICAL PRECISION VARIABILITY",
    "abstract": "Tartan TRT a hardware accelerator for inference with Deep Neural Networks (DNNs) is presented and evaluated on Convolutional Neural Networks. TRT exploits the variable per layer precision requirements of DNNs to deliver execution time that is proportional to the precision p in bits used per layer for convolutional and fully-connected layers. Prior art has demonstrated an accelerator with the same execution performance only for convolutional layersJudd et al. (2016a;c). Experiments on image classification CNNs show that on average across all networks studied, TRT outperforms a state-of-the-art bit-parallel accelerator Chen et al. (2014b) by 1.90× without any loss in accuracy while it is 1.17× more energy efficient. TRT requires no network retraining while it enables trading off accuracy for additional improvements in execution performance and energy efficiency. For example, if a 1% relative loss in accuracy is acceptable, TRT is on average 2.04× faster and 1.25× more energy efficient than a conventional bitparallel accelerator. A Tartan configuration that processes 2-bits at time, requires less area than the 1-bit configuration, improves efficiency to 1.24× over the bitparallel baseline while being 73% faster for convolutional layers and 60% faster for fully-connected layers is also presented.",
    "text": "1 INTRODUCTION\nIt is only recently that commodity computing hardware in the form of graphics processors delivered the performance necessary for practical, large scale Deep Neural Network applications Krizhevsky et al. (2012). At the same time, the end of Dennard Scaling in semiconductor technology Esmaeilzadeh et al. (2011) makes it difficult to deliver further advances in hardware performance using existing general purpose designs. It seems that further advances in DNN sophistication would have to rely mostly on algorithmic and in general innovations at the software level which can be helped by innovations in hardware design. Accordingly, hardware DNN accelerators have emerged. The DianNao accelerator family was the first to use a wide single-instruction single-data (SISD) architecture to process up to 4K operations in parallel on a single chip Chen et al. (2014a;b) outperforming graphics processors by two orders of magnitude. Development in hardware accelerators has since proceeded in two directions: either toward more general purpose accelerators that can support more machine learning algorithms while keeping performance mostly on par with DaDianNao (DaDN) Chen et al. (2014b), or toward further specialization of specific layers or classes of DNNs with the goal of outperforming DaDN in execution time and/or energy efficiency, e.g., Han et al. (2016); Albericio et al. (2016a); Judd et al. (2016a); Chen, Yu-Hsin and Krishna, Tushar and Emer, Joel and Sze, Vivienne (2016); Reagen et al. (2016). This work is along the second direction. Section 5 reviews several other accelerator designs.\nWhile DaDN’s functional units process 16-bit fixed-point values, DNNs exhibit varying precision requirements across and within layers, e.g., Judd et al. (2015). Accordingly, it is possible to use\nshorter, per layer representations for activations and/or weights. However, with existing bit-parallel functional units doing so does not translate into a performance nor an energy advantage as the values are expanded into the native hardware precision inside the unit.\nThis work presents Tartan (TRT), a massively parallel hardware accelerator whose execution time for fully-connected and convolutional layers scales with the precision p used to represent the input values. TRT uses hybrid bit-serial/bit-parallel functional units and exploits the abundant parallelism of typical DNN layers with the goal of exceeding DaDN’s execution time performance and energy efficiency. Ideally Tartan can improve execution time by 16p where p is the precision used for the activations in convolutional layers, and for the activations and weights in fully-connected layers. Every bit of precision that can be eliminated ideally reduces execution time and increases energy efficiency. TRT builds upon the Stripes (STR) accelerator Judd et al. (2016c;a) which improves execution time and energy efficiency only for convolutional layers.\nThis work evaluates TRT on a set of convolutional neural networks (CNNs) for image classification. On average TRT reduces inference time by 1.61×, 1.91× and 1.90× over DaDN for the fullyconnected, the convolutional, and all layers respectively. Energy efficiency compared to DaDN with TRT is 0.92×, 1.18× and 1.17× respectively. TRT enables trading off accuracy for improving execution time and energy efficiency. For example, on average for the fully-connected layers, accepting a 1% loss in accuracy improves performance to 1.73× and energy efficiency to 1.00× compared to DaDN.\nThe rest of this document is organized as follows: Section 2 illustrates the key concepts behind TRT via an example. Section 3 reviews the DaDN architecture and presents an equivalent Tartan configuration. Section 4 presents the experimental results. Section 5 reviews related work and discusses the limitations of this study and the potential challenges with TRT . Section 6 concludes.\n2 Tartan: A SIMPLIFIED EXAMPLE\nThis section illustrates at a high-level the TRT design by showing how it would process two purposely trivial cases: 1) a fully-connected layer (FCL) with a single input activation producing two output activations, and 2) a convolutional layer (CVL) with two input activations and one singleweight filter producing two output activations. The per layer calculations are:\nFully − Connected : Convolutional : f1 = w1 × a c1 = w × a1 f2 = w2 × a c2 = w × a2\nWhere f1, f2, c1 and c2 are output activations, w1, w2, and w are weights, and a1, a2 and a are input activations. For clarity all values are assumed to be represented in 2 bits of precision.\n\n2.1 CONVENTIONAL BIT-PARALLEL PROCESSING\nFigure 2.1a shows a bit-parallel processing engine representative of DaDN. Every cycle, the engine can calculate the product of two 2-bit inputs, i (weight) and v (activation) and accumulate or store it into the output register OR. Parts (b) and (c) of the figure show how this unit can calculate the example CVL over two cycles. In part (b) and during cycle 0, the unit accepts along the v input bits 0 and 1 of a1 (noted as a1/0 and a1/1 respectively on the figure), and along i bits 0 and 1 of w and produces both bits of output c1. Similarly, during cycle 1 (part (c)), the unit processes a2 and w to produce c2. In total, over two cycles, the engine produced two 2b × 2b products. Processing the example FCL also takes two cycles: In the first cycle w1 and a produce f1, and in the second cycle w2 and a produce f2. This process is not shown in the interest of space.\n2.2 Tartan’S APPROACH\nFigure 2 shows how a TRT-like engine would process the example CVL. Figure 2a shows the engine’s structure which comprises two subunits. The two subunits accept each one bit of an activation per cycle through inputs v0 and v1 respectively and as before, there is a common 2-bit weight input (i1, i0). In total, the number of input bits is 4, identical to the bit-parallel engine.\nEach subunit contains three 2-bit registers: a shift-register AR, a parallel load register BR, and an parallel load output register OR. Each cycle each subunit can calculate the product of its single bit vi input with BR which it can write or accumulate into its OR. There is no bit-parallel multiplier since the subunits process a single activation bit per cycle. Instead, two AND gates, a shift-and-add functional unit, and OR form a shift-and-add multiplier/accumulator. Each AR can load a single bit per cycle from one of the i wires, and BR can be parallel loaded from AR or from the i wires.\nConvolutional Layer: Figure 2b through Figure 2d show how the CVL is processed. The figures abstract away the unit details showing only the register contents. As Figure 2b shows, during cycle 1, the w synapse is loaded in parallel to the BRs of both subunits via the i1 and i0 inputs. During cycle 2, bits 0 of a1 and of a2 are sent via the v0 and v1 inputs respectively to the first and second subunit. The subunits calculate concurrently a1/0 × w and a2/0 × w and accumulate these results into their ORs. Finally, in cycle 3, bit 1 of a1 and a2 appear respectively on v0 and v1. The subunits calculate respectively a1/1 × w and a2/1 × w accumulating the final output activations c1 and c2 into their ORs.\nIn total it took 3 cycles to process the layer. However, at the end of the third cycle, another w could have been loaded into the BRs (the i are idle) allowing a new set of outputs to commence computation during cycle 4. That is loading a new weight can be hidden during the processing of the current output activation for all but the first time. In the steady state, when the input activations are represented in two bits, this engine will be producing two 2b × 2b terms every two cycles thus matching the bandwidth of the bit-parallel engine.\nIf the activations a1 and a2 could be represented in just one bit, then this engine would be producing two output activations per cycle, twice the bandwidth of the bit-parallel engine. The latter is incapable of exploiting the reduced precision. In general, if the bit-parallel hardware was using Pbase bits to represent the activations while only Pa bits were enough, TRT would outperform the bit-parallel engine by PbasePTRT .\nFully-Connected Layer: Figure 3 shows how a TRT-like unit would process the example FCL. As Figure 3a shows, in cycle 1, bit 1 of w1 and of w2 appear respectively on lines i1 and i0. The left subunit’s AR is connected to i1 while the right subunit’s AR is connected to i0. The ARs shift in the corresponding bits into their least significant bit sign-extending to the vacant position (shown as a 0 bit on the example). During cycle 2, as Figure 3b shows, bits 0 of w1 and of w2 appear on the respective i lines and the respective ARs shift them in. At the end of the cycle, the left subunit’s AR contains the full 2-bit w1 and the right subunit’s AR the full 2-bit w2. In cycle 3, Figure 3c shows that the contents of AR are copied to BR in each subunit. From the next cycle, calculating the products can now proceed similarly to what was done for the CVL. In this case, however, each BR contains a different weight whereas in the CVL all BRs held the same w value. The shift capability of the ARs coupled with the different i wire per subunit connection allowed us to load a different weight bit-serially over two cycles. Figure 3d and Figure 3e show cycles 4 and 5 respectively. During cycle 4, bit 0 of a1 appears on both v inputs and is multiplied with the BR in each subunit. In cycle 5, bit 1 of a1 appears on both v inputs and the subunits complete the calculation of f1 and f2. It takes two cycles to produce the two 2b× 2b products once the correct inputs appear into the BRs. While in our example no additional inputs nor outputs are shown, it would have been possible to overlap the loading of a new set of w inputs into the ARs while processing the current weights stored into the BRs. That is the loading into ARs, copying into BRs, and the bit-serial multiplication of the BRs with the activations is a 3-stage pipeline where each stage can take multiple cycles. In general, assuming that both activations and weights are represented using 2 bits, this engine would match the performance of the bit-parallel engine in the steady state. When both set of inputs i and v can be represented with fewer bits, 1 in this case, the engine would produce two terms per cycle, twice the bandwidth of the bit-parallel engine of the previous section.\nSummary: In general, if Pbase the precision of the bit-parallel engine, and PLa and PLw the precisions that can be used respectively for activations and weights for layer L, a TRT engine can ideally outperform an equivalent bit parallel engine by Pbase\nPLa for CVLs, and by Pbase max(PLa ,P L w ) for FCLs. This example used the simplest TRT engine configuration. Since typical layers exhibit massive parallelism, TRT can be configured with many more subunits while exploiting weight reuse for CVLs and activation reuse for FCLs. The next section describes the baseline state-of-the-art DNNs accelerator and presents an equivalent TRT configuration.\n3 Tartan ARCHITECTURE\nThis work presents TRT as a modification of the state-of-the-art DaDianNao accelerator. Accordingly, Section 3.1 reviews DaDN’s design and how it can process FCLs and CVLs. For clarity, in what follows the term brick refers to a set of 16 elements of a 3D activation or weight array1 input which are contiguous along the i dimension, e.g., a(x, y, i)...a(x, y, i+ 15). Bricks will be denoted by their origin element with a B subscript, e.g., aB(x, y, i). The size of a brick is a design parameter.\n\n3.1 BASELINE SYSTEM: DADIANNAO\nTRT is demonstrated as a modification of the DaDianNao accelerator (DaDN) proposed by Chen et al. (2014b). Figure 4a shows a DaDN tile which processes 16 filters concurrently calculating 16 activation and weight products per filter for a total of 256 products per cycle. Each cycle the tile accepts 16 weights per filter for total of 256 synapses and 16 input activations. The tile multiplies each weight with only one activation whereas each activation is multiplied with 16 weights, one per filter. The tile reduces the 16 products into a single partial output activation per filter, for a total of 16 partial output activations for the tile. Each DaDN chip comprises 16 such tiles, each processing a different set of 16 filters per cycle. Accordingly, each cycle, the whole chip processes 16 activations and 256× 16 = 4K weights producing 16× 16 = 256 partial output activations, 16 per tile. Internally, each tile has: 1) a synapse buffer (SB) that provides 256 weights per cycle one per weight lane, 2) an input neuron buffer (NBin) which provides 16 activations per cycle through 16 neuron lanes, and 3) a neuron output buffer (NBout) which accepts 16 partial output activations per cycle. In the tile’s datapath each activation lane is paired with 16 weight lanes one from each filter. Each synapse and neuron lane pair feeds a multiplier, and an adder tree per filter lane reduces the 16 per filter products into a partial sum. In all, the filter lanes produce each a partial sum per cycle, for a\n1An FCL can be thought of as a CVL where the input activation array has unit x and y dimensions, and there are as many filters as output activations, and where the filter dimenions are identical to the input activation array.\ntotal of 16 partial output activations per Once a full window is processed, the 16 resulting sums, are fed through a non-linear activation function, f , to produce the 16 final output activations. The multiplications and reductions needed per cycle are implemented via 256 multipliers one per weight lane and sixteen 17-input (16 products plus the partial sum from NBout) adder trees one per filter lane.\nFigure 5a shows an overview of the DaDN chip. There are 16 processing tiles connected via an interconnect to a shared central eDRAM Neuron Memory (NM). DaDN’s main goal was minimizing off-chip bandwidth while maximizing on-chip compute utilization. To avoid fetching weights from off-chip, DaDN uses a 2MB eDRAM Synapse Buffer (SB) for weights per tile for a total of 32MB eDRAM. All inter-layer activation outputs except for the initial input and the final output are stored in NM which is connected via a broadcast interconnect to the 16 Input Neuron Buffers (NBin) buffers. All values are 16-bit fixed-point, hence a 256-bit wide interconnect can broadcast a full activation brick in one step. Off-chip accesses are needed only for reading: 1) the input image, 2) the weight once per layer, and 3) for writing the final output.\nProcessing starts by reading from external memory the first layer’s filter weights, and the input image. The weights are distributed over the SBs and the input is stored into NM. Each cycle an input activation brick is broadcast to all units. Each units reads 16 weight bricks from its SB and produces a partial output activation brick which it stores in its NBout. Once computed, the output activations are stored through NBout to NM and then fed back through the NBins when processing the next layer. Loading the next set of weights from external memory can be overlapped with the processing of the current layer as necessary.\n3.2 Tartan\nAs Section 2 explained, TRT processes activations bit-serially multiplying a single activation bit with a full weight per cycle. Each DaDN tile multiplies 16 16-bit activations with 256 weights each cycle. To match DaDN’s computation bandwidth, TRT needs to multiply 256 1-bit activations with 256 weights per cycle. Figure 4b shows the TRT tile. It comprises 256 Serial Inner-Product Units (SIPs) organized in a 16×16 grid. Similar to DaDN each SIP multiplies 16 weights with 16 activations and reduces these products into a partial output activation. Unlike DaDN, each SIP accepts 16 single-bit activation inputs. Each SIP has two registers, each a vector of 16 16-bit subregisters: 1) the Serial Weight Register (SWR), and 2) the Weight Register (WR). These correspond to AR and BR of the example of Section 2. NBout remains as in DaDN, however, it is distributed along the SIPs as shown.\nConvolutional Layers: Processing starts by reading in parallel 256 weights from the SB as in DaDN, and loading the 16 per SIP row weights in parallel to all SWRs in the row. Over the next PLa cycles, the weights are multiplied by the bits of an input activation brick per column. TRT exploits weight reuse across 16 windows sending a different input activation brick to each column. For example, for a CVL with a stride of 4 a TRT tile will processes 16 activation bricks aB(x, y, i), aB(x+ 4, y, i) through aB(x+ 63, y, i) in parallel a bit per cycle. Assuming that the tile processes filters fi though fi+15, after PLa cycles it would produce the following partial output activations: oB(x/4, y/4, fi), through oB(x/4 + 15, y/4, fi), that is 16 contiguous on the x dimension output activation bricks. Whereas DaDN would process 16 activations bricks over 16 cycles, TRT processes them concurrently but bit-serially over PLa cycles. If P L a is less than 16, TRT will outperform DaDN by 16/PLa , and when P L a is 16, TRT will match DaDN’s performance.\nFully-Connected Layers: Processing starts by loading bit-serially and in parallel over PLw cycles, 4K weights into the SWRs. Each SWR per row gets a different set of 16 weights as each subregister is connected to one out of the 256 wires of the SB output bus for the SIP row. Once the weights have been loaded, the SWRs are copied to the SWs and multiplication with the input activations can then proceed bit-serially over PLa cycles. Assuming that there are enough output activations so that a different output activation can be assigned to each SIP, the same input activation brick can be broadcast to all SIP columns. For example, for an FCL a TRT tile will process one activation brick aB(i) bit-serially to produce 16 output activation bricks oB(i) through oB(i×16) one per SIP column. Loading the next set of weights can be done in parallel with processing the current set, thus execution time is constrained by PLmax = max(P L a , P L w ). Thus, a TRT tile produces 256 partial\noutput activations every PLmax cycles, a speedup of 16/Pmax over DaDN since a DaDN tile always needs 16 cycles to do the same.\nFor TRT to be fully utilized an FCL must have at least 4K output activations. Some of the networks studied have a layer with as little as 2K output activations. To avoid underutilization, the SIPs along each row are cascaded into a daisy-chain, where the output of one can feed into an input of the next via a multiplexer. This way, the computation of an output activation can be sliced over the SIPs along the same row. In this case, each SIP processes only a portion of the input activations resulting into several partial output activations along the SIPs on the same row. Over the next np cycles, where np the number of slices used, the np partial outputs can be reduced into the final output activation. The user can chose any number of slices up to 16, so that TRT can be fully utilized even with fullyconnected layers of just 256 outputs. For example, in NeuralTalk Karpathy & Li (2014) the smallest layers can have 600 outputs or fewer.\nOther Layers: TRT like DaDN can process the additional layers needed by the studied networks. For this purpose the tile includes additional hardware support for max pooling similar to DaDN. An activation function unit is present at the output of NBout in order to apply nonlinear activations before the output neurons are written back to NM.\n\n3.3 SIP AND OTHER COMPONENTS\nSIP: Bit-Serial Inner-Product Units: Figure 6 shows TRT’s Bit-Serial Inner-Product Unit (SIP). Each SIP multiplies 16 activations by 16 weights to produce an output activation. Each SIP has two registers, a Serial Weight Register (SWR) and a Weight Registers (WR), each containing 16 16-bit subregisters. Each SWR subregister is a shift register with a single bit connection to one of the weight bus wires that is used to read weights bit-serially for FCLs. Each WR subregister can be parallel loaded from either the weight bus or the corresponding SWR subregister, to process CVLs or FCLs respectively. Each SIP includes 256 2-input AND gates that multiply the weights in the WR with the incoming activation bits, and a 16 × 16b adder tree that sums the partial products. A final adder plus a shifter accumulate the adder tree results into an output register. In each SIP, a multiplexer at the first input of the adder tree implements the cascade mode supporting slicing the output activation computation along the SIPs of a single row. To support signed 2’s complement neurons, the SIP can subtract the weight corresponding to the most significant bit (MSB) from the partial sum when the MSB is 1. This is done with negation blocks for each weight before the adder tree. Each SIP also includes a comparator (max) to support max pooling layers.\nDispatcher and Reducers: Figure 5b shows an overview of the full TRT system. As in DaDN there is a central NM and 16 tiles. A Dispatcher unit is tasked with reading input activations from NM always performing eDRAM-friendly wide accesses. It transposes each activation and communicates each a bit a time over the global interconnect. For CVLs the dispatcher has to maintain a pool of multiple activation bricks, each from different window, which may require fetching multiple rows from NM. However, since a new set of windows is only needed every PLa cycles, the dispatcher can keep up for the layers studied. For FCLs one activation brick is sufficient. A Reducer per title is tasked with collecting the output activations and writing them to NM. Since output activations take multiple cycles to produce, there is sufficient bandwidth to sustain all 16 tiles.\n\n3.4 PROCESSING SEVERAL BITS AT ONCE\nIn order to improve TRT’s area and power efficiency, the number of bits processed at once can be parameterized. In this case, the weights are multiplied with several activation bits at once, and the multiplication results are partially shifted before they are inserted into their corresponding adder tree.\nIn order to load the weights on time, the SWR subregister has to be modified so it can load several bits in parallel, and shift that number of positions every cycle. The negation block (for 2’s complement support) will operate only over the most significant product result.\nThe chief advantage of such a design is that less SIPs are needed in order to achieve the same throughput – for example, processing 2 bits at once allows reducing the number of columns from 16 to 8. Although the total number of bus wires is similar, the distance they have to cover is significantly reduced. Likewise, the total number of adders required stays similar, but they are clustered closer together.\nA drawback of this design is the limitation to precisions that are exact multiples of the number of bits processed at once.\n\n4 EVALUATION\nThis section evaluates TRT’s performance, energy and area and explores the trade-off between accuracy and performance comparing to DaDN.\n\n4.1 METHODOLOGY\nNumerical Representation Requirements Analysis: The per layer precision profiles are found via the methodology of Judd et al. Judd et al. (2015). Caffe Jia et al. (2014) was used to measure how reducing the precision of each FCL affects the network’s overall top-1 prediction accuracy over 5000 images. The network definitions and pre-trained synaptic weights are taken from the Caffe Model Zoo Jia (2015). Since TRT’s performance for FCLs is bound by the maximum of the weight and activation precisions, our exploration was limited to the cases where both are the same. The search procedure is a gradient descent where a given layer’s precision is iteratively decremented one bit at a time, until the network’s accuracy drops. For weights, the fixed point numbers are set to represent values between -1 and 1. For activations, the number of fractional bits is fixed to a previouslydetermined value known not to hurt accuracy, as per Judd et al. (2015). While both activations and weights use the same number of bits, their precisions and ranges differ.\nPerformance, Area and Energy: DaDN, STR and TRT were modeled using the same methodology for consistency. A custom cycle-accurate simulator models execution time. Computation was scheduled as described by Judd et al. (2016a) to maximize energy efficiency for DaDN. The logic components of the both systems were synthesized with the Synopsys Design Compiler Synopsys for a TSMC 65nm library to report power and area. The circuit is clocked at 980 MHz. The NBin and NBout SRAM buffers were modelled using CACTI Muralimanohar & Balasubramonian. The eDRAM area and energy were modelled with Destiny Poremba et al. (2015).\n\n4.2 RESULTS\nFully-Connected Layer Precisions: Table 1 reports the per layer precisions for the CVLs and FCLs of the networks studied along with the speedup over DaDN that would be ideally possible. The discussion in this section focuses solely on FCLs. The precisions that can be used vary from 8 up to 10 bits vs. the 16 bits DaDN uses. The ideal speedup ranges from 63% to 66% with no accuracy loss. Additional exploration of the precision space may yield even shorter precisions without sacrificing accuracy. Modest additional improvements are possible with a loss of 1% in accuracy.\nExecution Time: Table 2 reports TRT’s performance and energy efficiency relative to DaDN for the precision profiles in Table 1 separately for the fully-connected layers, for the convolutional layers,\n\n4.3 TWO-BIT AT ONCE PERFORMANCE EVALUATION\nWe evaluate the performance for a multi-bit design as described in section 3.4, where 2 bits are processed every cycle in as half as many total SIPs. The precisions used are the same as indicated in Table 1 for 100% accuracy, rounded up to the next multiple of two. The results are shown in Table 4. The 2-bit TRT always improves performance compared to DaDN as the “vs. DaDN” columns show. Compared to the 1-bit TRT performance is slightly lower however given that the area of the 2-bit TRT is much lower, this can be a good trade-off. Overall, there are two forces at work that shape performance relative to the 1-bit TRT . There is performance potential lost due to rounding all precisions to an even number, and there is performance benefit by requiring less parallelism. The time needed to serially load the first bundle of weights is also reduced. In VGG 19 the performance benefit due to the lower parallelism requirement outweights the performance loss due to precision rounding. In all other cases, the reverse is true.\nA hardware synthesis and layout of both DaDN and TRT’s 2-bit variant using TSMC 65nm typical case libraries shows that the total area overhead can be as low as 24.9%, with an improved energy efficiency in fully connected layers of 1.24× on average.\n\n5 RELATED WORK AND LIMITATIONS OF THIS WORK\nThe recent success of Deep Learning has led to several proposals for hardware acceleration of DNNs. This section reviews some of these recent efforts. However, specialized hardware designs for neural networks is a field with a relatively long history. Relevant to TRT , bit-serial processing hardware for neural networks has been proposed several decades ago, e.g., Svensson & Nordstrom (1990); Murray et al. (1988). While the performance of these designs scales with precision it would be lower than that of an equivalently configured bit-parallel engine. For example, Svensson & Nordstrom (1990) uses an interesting bit-serial multiplier which requires O(4 × p) cycles, where p the precision in bits. Furthermore, as semiconductor technology has progressed the number of resources that can be\nput on chip and the trade offs (e.g., relative speed of memory vs. transistors vs. wires) are today vastly different facilitating different designs. However, truly bit-serial processing such as that used in the aforementioned proposals needs to be revisited with today’s technology constraints due to its potentially high compute density (compute bandwidth delivered per area).\nIn general, hardware acceleration for DNNs has recently progressed in two directions: 1) considering more general purpose accelerators that can support additional machine learing algorithms, and 2) considering further improvements primarily for convolutional neural networks and the two most dominant in terms of execution time layer types: convolutional and fully-connected. In the first category there are accelerators such as Cambricon Liu et al. (2016) and Cambricon-X Zhang et al. (2016). While targeting support for more machine learning algorithms is desirable, work on further optimizing performance for specific algorithms such as TRT is valuable and needs to be pursued as it will affect such general purpose accelerators.\nTRT is closely related to Stripes Judd et al. (2016c;a) whose execution time scales with precision but only for CVLs. STR does not improve performance for FCLs. TRT improves upon STR by enabling: 1) performance improvements for FCLs, and 2) slicing the activation computation across multiple SIPs thus preventing underutilization for layers with fewer than 4K outputs. Pragmatic uses a similar in spirit organization to STR but its performance on CVLs depends only on the number of activation bits that are 1 Albericio et al. (2016b). It should be possible to apply the TRT extensions to Pragmatic, however, performance in FCLs will still be dictated by weight precision. The area and energy overheads would need to be amortized by a commensurate performance improvement.\nThe Efficient Inference Engine (EIE) uses synapse pruning, weight compression, zero activation elimination, and network retraining to drastically reduce the amount of computation and data communication when processing fully-connected layers Han et al. (2016). An appropriately configured EIE will outperform TRT for FCLs, provided that the network is pruned and retrained. However, the two approaches attack a different component of FCL processing and there should be synergy between them. Specifically, EIE currently does not exploit the per layer precision variability of DNNs and relies on retraining the network. It would be interesting to study how EIE would benefit from a TRT-like compute engine where EIE’s data compression and pruning is used to create vectors of weights and activations to be processed in parallel. EIE uses single-lane units whereas TRT uses a coarser-grain lane arrangement and thus would be prone to more imbalance. A middle ground may be able to offer some performance improvement while compensating for cross-lane imbalance.\nEyeriss uses a systolic array like organization and gates off computations for zero activations Chen, Yu-Hsin and Krishna, Tushar and Emer, Joel and Sze, Vivienne (2016) and targets primarily highenergy efficiency. An actual prototype has been built and is in full operation. Cnvlutin is a SIMD accelerator that skips on-the-fly ineffectual activations such as those that are zero or close to zero Albericio et al. (2016a). Minerva is a DNN hardware generator which also takes advantage of zero activations and that targets high-energy efficiency Reagen et al. (2016). Layer fusion can further reduce off-chip communication and create additional parallelism Alwani et al. (2016). As multiple layers are processed concurrently, a straightforward combination with TRT would use the maximum of the precisions when layers are fused.\nGoogle’s Tensor Processing Unit uses quantization to represent values using 8 bits Jouppi (2016) to support TensorFlow Abadi et al. (2015). As Table 1 shows, some layers can use lower than 8 bits of precision which suggests that even with quantization it may be possible to use fewer levels and to potentially benefit from an engine such as TRT .\nLimitations: As in DaDN this work assumed that each layer fits on-chip. However, as networks evolve it is likely that they will increase in size thus requiring multiple TRT nodes as was suggested in DaDN. However, some newer networks tend to use more but smaller layers. Regardless, it would be desirable to reduce the area cost of TRT most of which is due to the eDRAM buffers. We have not explored this possibility in this work. Proteus Judd et al. (2016b) is directly compatible with TRT and can reduce memory footprint by about 60% for both convolutional and fully-connected layers. Ideally, compression, quantization and pruning similar in spirit to EIE Han et al. (2016) would be used to reduce computation, communication and footprint. General memory compresion Mittal & Vetter (2016) techniques offer additional opportunities for reducing footprint and communication.\nWe evaluated TRT only on CNNs for image classification. Other network architectures are important and the layer configurations and their relative importance varies. TRT enables performance\nimprovements for two of the most dominant layer types. We have also provided some preliminary evidence that TRT works well for NeuralTalk LSTM Karpathy & Li (2014). Moreover, by enabling output activation computation slicing it can accommodate relatively small layers as well.\nApplying some of the concepts that underlie the TRT design to other more general purpose accelerators such as Cambricon Liu et al. (2016) or graphics processors would certainly be more preferable than a dedicated accelerator in most application scenarios. However, these techniques are best first investigated into specific designs and then can be generalized appropriately.\nWe have evaluated TRT only for inference only. Using an engine whose performance scales with precision would provide another degree of freedom for network training as well. However, TRT needs to be modified accordingly to support all the operations necessary during training and the training algorithms need to be modified to take advantage of precision adjustments.\nThis section commented only on related work on digital hardware accelerators for DNNs. Advances at the algorithmic level would impact TRT as well or may even render it obsolete. For example, work on using binary weights Courbariaux et al. (2015) would obviate the need for an accelerator whose performance scales with weight precision. Investigating TRT’s interaction with other network types and architectures and other machine learning algorithms is left for future work.\n\n6 CONCLUSION\nThis work presented Tartan an accelerator for inference with Deep Learning Networks whose performance scales inversely linearly with the number of bits used to represent values in fully-connected and convolutional layers. TRT also enables on-the-fly accuracy vs. performance and energy efficiency trade offs and its benefits were demonstrated over a set of popular image classification networks. The new key ideas in TRT are: 1) Supporting both the bit-parallel and the bit-serial loading of weights into processing units to facilitate the processing of either convolutional or fullyconnected layers, and 2) cascading the adder trees of various subunits (SIPs) to enable slicing the output computation thus reducing or eliminating cross-lane imbalance for relatively small layers.\nTRT opens up a new direction for research in inference and training by enabling precision adjustments to translate into performance and energy savings. These precisions adjustments can be done statically prior to execution or dynamically during execution. While we demonstrated TRT for inference only, we believe that TRT , especially if combined with Pragmatic, opens up a new direction for research in training as well. For systems level research and development, TRT with its ability to trade off accuracy for performance and energy efficiency enables a new degree of adaptivity for operating systems and applications.\n",
    "approval": false,
    "rationale": "I do not feel very qualified to review this paper. I studied digital logic back in university, that was it. I think the work deserves a reviewer with far more sophisticated background in this area. It certainly seems useful. My advice is also to submit it another venue.",
    "rating": 4,
    "type": "negative"
  },
  {
    "title": "ITERATIVE TEMPORAL DIFFERENCING WITH FIXED RANDOM FEEDBACK ALIGNMENT SUPPORT SPIKE- TIME DEPENDENT PLASTICITY IN VANILLA BACKPROP- AGATION FOR DEEP LEARNING",
    "abstract": "In vanilla backpropagation (VBP), activation function matters considerably in terms of non-linearity and differentiability. Vanishing gradient has been an important problem related to the bad choice of activation function in deep learning (DL). This work shows that a differentiable activation function is not necessary any more for error backpropagation. The derivative of the activation function can be replaced by an iterative temporal differencing (ITD) using fixed random feedback weight alignment (FBA). Using FBA with ITD, we can transform the VBP into a more biologically plausible approach for learning deep neural network architectures. We don’t claim that ITD works completely the same as the spike-time dependent plasticity (STDP) in our brain but this work can be a step toward the integration of STDP-based error backpropagation in deep learning.",
    "text": "1 INTRODUCTION\nVBP was proposed around 1987 Rumelhart et al. (1985). Almost at the same time, biologicallyinspired convolutional networks was also introduced as well using VBP LeCun et al. (1989). Deep learning (DL) was introduced as an approach to learn deep neural network architecture using VBP LeCun et al. (1989; 2015); Krizhevsky et al. (2012). Extremely deep networks learning reached 152 layers of representation with residual and highway networks He et al. (2016); Srivastava et al. (2015). Deep reinforcement learning was successfully implemented and applied which was mimicking the dopamine effect in our brain for self-supervised and unsupervised learning Silver et al. (2016); Mnih et al. (2015; 2013). Hierarchical convolutional neural network have been biologically inspired by our visual cortex Hubel & Wiesel (1959); Fukushima (1988; 1975); Yamins & DiCarlo (2016).\nGeoff Hinton in 1988 proposed recirculation in VBP Hinton & McClelland (1988) which does not require the derivative of the activation function. The recirculation-based backprop is the main inspiration behind our work, an iterative temporal differencing in VBP. He gave a lecture about this approach again in NIPS 2007 Hinton (2007), and recently gave a similar lecture in Standford in 2014 and 2017 to reject the four arguments against the biological foundation of backprop. In his latest related lecture in Standford, he explains the main four arguments by neuroscientists on why VBP is not biologically or neurologically feasible 1.\nThe discovery of fixed random synaptic feedback weights alignments (FBA) in error backpropagation for deep learning started a new quest of finding the biological version of VBP Lillicrap et al. (2016) since it solves the symmetrical synaptic weights problem in backprop. Recently, spiketime dependent plasticity was the important issue with backprop. One of the works in this direction, highly inspired from Hinton’s recirculation idea Hinton & McClelland (1988), is deep learning using segregated dendrites Guergiuev et al. (2016). Apical dendrites as the segregated synaptic feedback are claimed to be capable of modeling STDP into the backprop successfully Guergiuev et al. (2016).\n",
    "approval": false,
    "rationale": "The paper falls far short of the standard expected of an ICLR submission. \n\nThe paper has little to no content. There are large sections of blank page throughout. The algorithm, iterative temporal differencing, is introduced in a figure -- there is no formal description. The experiments are only performed on MNIST. The subfigures are not labeled. The paper over-uses acronyms; sentences like “In this figure, VBP, VBP with FBA, and ITD using FBA for VBP…” are painful to read. \n\n\n",
    "rating": 2,
    "type": "negative"
  }
]
