"""Run PETER threshold experiment.

Instead of using a fixed K for retrieving citations and semantic papers, we use a fixed
threshold, then we check the accuracy and some statistics on the number of retrieved
papers and the label accuracy.
"""

import json
import shutil
from collections.abc import Iterable
from dataclasses import dataclass
from pathlib import Path
from typing import Annotated

import numpy as np
import rich
import typer
from rich.table import Table

from paper import gpt
from paper import peerread as pr
from paper.gpt.evaluate_paper_graph import (
    GRAPH_EVAL_USER_PROMPTS,
    GRAPH_EXTRACT_USER_PROMPTS,
)
from paper.util import cli, metrics
from paper.util.cmd import run, title
from paper.util.serde import load_data

app = typer.Typer(
    context_settings={"help_option_names": ["-h", "--help"]},
    add_completion=False,
    rich_markup_mode="rich",
    pretty_exceptions_show_locals=False,
    no_args_is_help=True,
)


def _run(path: Path, *cmd: object) -> None:
    """Run command only if `path` does not already exist.

    Args:
        path: Path to check. The command won't run if it exists. This should be the path
            where a file or directory generated by the command will be.
        *cmd: The command and arguments to run.
    """
    if path.exists():
        print(f"{path} already exists.")
        return

    print(f"{path} does not exist. Running command.")
    run(*cmd)


@app.command()
def main(
    output_dir: Annotated[
        Path,
        typer.Option(
            "--output",
            help="Base directory for all outputs",
            writable=True,
            file_okay=False,
        ),
    ],
    graph_dir: Annotated[
        Path,
        typer.Option(
            "--graph",
            help="Directory containing the Peter graph",
            exists=True,
            file_okay=False,
        ),
    ],
    peerread_ann: Annotated[
        Path,
        typer.Option(
            "--ann", help="Path to PeerRead annotations", exists=True, dir_okay=False
        ),
    ],
    eval_prompt: Annotated[
        str,
        typer.Option(
            help="The user prompt to use for paper evaluation.",
            click_type=cli.Choice(GRAPH_EVAL_USER_PROMPTS),
        ),
    ] = "full-graph",
    graph_prompt: Annotated[
        str,
        typer.Option(
            help="The user prompt to use for graph extraction.",
            click_type=cli.Choice(GRAPH_EXTRACT_USER_PROMPTS),
        ),
    ] = "full",
    citation: Annotated[float, typer.Option(help="Citation threshold")] = 0.8,
    semantic: Annotated[float, typer.Option(help="Semantic threshold")] = 0.8,
    limit: Annotated[int, typer.Option(help="Number of papers")] = 10,
    force: Annotated[bool, typer.Option(help="Ignore previous results if set")] = False,
    model: Annotated[str, typer.Option(help="GPT model to use.")] = "gpt-4o-mini",
) -> None:
    """Run the complete paper workflow pipeline.

    This script runs three steps in sequence:
    1. Peter PeerRead threshold
    2. GPT PeterSum
    3. GPT Eval Graph

    Each step's output is used as input for the next step.
    If --clean is not specified, steps will be skipped if their output already exists.
    """
    # Create a subdirectory name based on parameters
    output_dir = output_dir / (
        f"cit{citation}_sem{semantic}_lim{limit}_mod{model}"
        f"_eval{eval_prompt}_graph{graph_prompt}"
    )

    if force and output_dir.exists():
        typer.echo(f"Removing existing directory: {output_dir}")
        shutil.rmtree(output_dir)

    output_dir.mkdir(parents=True, exist_ok=True)
    typer.echo(f"Using output directory: {output_dir}")

    # Step 1: Run Peter PeerRead threshold
    title("Running Peter PeerRead threshold")

    peter_output = output_dir / "peerread_with_peter.json"
    _run(
        peter_output,
        "paper",
        "peter",
        "peerread-threshold",
        "--graph-dir",
        graph_dir,
        "--peerread-ann",
        peerread_ann,
        "--output",
        peter_output,
        "--num-papers",
        limit,
        "--citation",
        citation,
        "--semantic",
        semantic,
    )

    # Step 2: Run GPT PeterSum
    title("Running GPT PeterSum")

    summary_dir = output_dir / "summary"
    summary_output = summary_dir / "result.json"
    _run(
        summary_output,
        "paper",
        "gpt",
        "petersum",
        "run",
        "--ann-graph",
        peter_output,
        "--output",
        summary_dir,
        "--limit",
        "0",
        "--model",
        model,
    )

    # Step 3: Run GPT Eval Graph
    eval_dir = output_dir / "eval"
    eval_output = eval_dir / "result.json"
    title("Running GPT Eval Graph")

    _run(
        eval_output,
        "paper",
        "gpt",
        "eval",
        "graph",
        "run",
        "--papers",
        summary_output,
        "--output",
        eval_dir,
        "--limit",
        "0",
        "--eval-prompt",
        eval_prompt,
        "--graph-prompt",
        graph_prompt,
        "--model",
        model,
    )

    title("Result")
    typer.echo(f"Results saved to {eval_dir}")
    metrics_file = eval_dir / "metrics.json"
    metrics = json.loads(metrics_file.read_bytes())
    typer.echo(f"Accuracy: {metrics['accuracy']}")

    typer.echo("Statistics:")
    data = gpt.PromptResult.unwrap(
        load_data(eval_output, gpt.PromptResult[gpt.GraphResult])
    )
    rich.print(_get_statistics(data))


def _get_statistics(data: Iterable[gpt.GraphResult]) -> Table:
    """Calculate statistics and return them as a Rich table."""
    stats = _calculate_stats(_count_related(item) for item in data)

    table = Table(title="Statistics")
    table.add_column("Field", style="cyan", no_wrap=True)
    table.add_column("Corr", style="magenta", justify="right")
    table.add_column("Min", style="green", justify="right")
    table.add_column("25", style="yellow", justify="right")
    table.add_column("Median", style="blue", justify="right")
    table.add_column("75", style="yellow", justify="right")
    table.add_column("Max", style="green", justify="right")

    for field, values in stats.items():
        corr = f"{values.correlation:.3f}" if values.correlation is not None else "NaN"
        min_val = str(values.min)
        q1_val = f"{values.q1:.1f}"
        median_val = f"{values.median:.1f}"
        q3_val = f"{values.q3:.1f}"
        max_val = str(values.max)

        table.add_row(field, corr, min_val, q1_val, median_val, q3_val, max_val)

    return table


@dataclass(frozen=True, kw_only=True)
class Counts:
    """Various counts and derived counts from related papers."""

    label: int

    related: int
    semantic: int
    citation: int
    semantic_positive: int
    citation_positive: int
    semantic_negative: int
    citation_negative: int
    positive: int
    negative: int


def _count_related(item: gpt.GraphResult) -> Counts:
    related = item.related
    if related is None:
        raise ValueError("Invalid graph result. Does not contain related papers.")

    ctx = pr.ContextPolarity
    src = gpt.RelatedPaperSource

    semantic_positive = _filter_related(related, ctx.POSITIVE, src.SEMANTIC)
    semantic_negative = _filter_related(related, ctx.NEGATIVE, src.SEMANTIC)
    citation_positive = _filter_related(related, ctx.POSITIVE, src.CITATIONS)
    citation_negative = _filter_related(related, ctx.NEGATIVE, src.CITATIONS)

    return Counts(
        label=item.paper.label,
        related=len(related),
        semantic=semantic_positive + semantic_negative,
        citation=citation_positive + citation_negative,
        semantic_positive=semantic_positive,
        citation_positive=citation_positive,
        semantic_negative=semantic_negative,
        citation_negative=citation_negative,
        positive=semantic_positive + citation_positive,
        negative=semantic_negative + citation_negative,
    )


def _filter_related(
    related: Iterable[gpt.PaperRelatedSummarised],
    polarity: pr.ContextPolarity,
    source: gpt.RelatedPaperSource,
) -> int:
    return sum(1 for r in related if r.polarity is polarity and r.source is source)


@dataclass(frozen=True, kw_only=True)
class FieldStats:
    """Descriptive statistics and correlation for a field."""

    correlation: float | None
    min: int
    q1: float
    median: float
    q3: float
    max: int


def _calculate_stats(counts: Iterable[Counts]) -> dict[str, FieldStats]:
    """Calculate statistics for each count field against the label field."""
    count_list = list(counts)  # Avoid iterating multiple times
    labels = [count.label for count in count_list]

    field_names = [
        "related",
        "semantic",
        "citation",
        "semantic_positive",
        "citation_positive",
        "semantic_negative",
        "citation_negative",
        "positive",
        "negative",
    ]

    all_stats: dict[str, FieldStats] = {}

    for field in field_names:
        values = [getattr(count, field) for count in count_list]
        correlation = metrics.pearson_correlation(labels, values)

        # Calculate descriptive statistics using numpy
        np_values = np.array(values)
        min_val = int(np.min(np_values))
        q1_val = float(np.percentile(np_values, 25))
        median_val = float(np.median(np_values))
        q3_val = float(np.percentile(np_values, 75))
        max_val = int(np.max(np_values))

        all_stats[field] = FieldStats(
            correlation=correlation,
            min=min_val,
            q1=q1_val,
            median=median_val,
            q3=q3_val,
            max=max_val,
        )

    return all_stats


if __name__ == "__main__":
    app()
