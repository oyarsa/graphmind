"""Run PETER threshold experiment.

Instead of using a fixed K for retrieving citations and semantic papers, we use a fixed
threshold, then we check the accuracy and some statistics on the number of retrieved
papers and the label accuracy.
"""

import enum
import json
import shutil
from collections.abc import Callable, Iterable
from dataclasses import dataclass
from pathlib import Path
from typing import Annotated

import numpy as np
import rich
import typer
from rich.table import Table

from paper import gpt
from paper import peerread as pr
from paper import related_papers as rp
from paper.gpt.evaluate_paper_graph import (
    GRAPH_EVAL_USER_PROMPTS,
    GRAPH_EXTRACT_USER_PROMPTS,
)
from paper.util import cli, metrics
from paper.util.cmd import run, title
from paper.util.serde import load_data

app = typer.Typer(
    context_settings={"help_option_names": ["-h", "--help"]},
    add_completion=False,
    rich_markup_mode="rich",
    pretty_exceptions_show_locals=False,
    no_args_is_help=True,
)


def _run(path: Path, *cmd: object) -> None:
    """Run command only if `path` does not already exist.

    Args:
        path: Path to check. The command won't run if it exists. This should be the path
            where a file or directory generated by the command will be.
        *cmd: The command and arguments to run.
    """
    if path.exists():
        print(f"{path} already exists.")
        return

    print(f"{path} does not exist. Running command.")
    run(*cmd)


def run_peter_threshold(
    peter_output: Path,
    graph_dir: Path,
    peerread_ann: Path,
    limit: int,
    citation: float,
    semantic: float,
    retrieved_k: int,
) -> None:
    """Run PETER threshold calculation on PeerRead data.

    Args:
        peter_output: Path where the output will be saved.
        graph_dir: Directory containing the PETER graph.
        peerread_ann: Path to PeerRead annotations.
        limit: Number of papers to process.
        citation: Citation threshold.
        semantic: Semantic threshold.
        retrieved_k: Number of semantic neighbours to retrieve before applying threshold.
    """
    _run(
        peter_output,
        "paper",
        "peter",
        "peerread-threshold",
        "--graph-dir",
        graph_dir,
        "--peerread-ann",
        peerread_ann,
        "--output",
        peter_output,
        "--num-papers",
        limit,
        "--citation",
        citation,
        "--semantic",
        semantic,
        "--retrieved-k",
        retrieved_k,
    )


def run_petersum(
    summary_output: Path,
    summary_dir: Path,
    peter_output: Path,
    model: str,
) -> None:
    """Run GPT PeterSum on PETER results.

    Args:
        summary_output: Path where the GPT PeterSum results will be saved.
        summary_dir: Directory where all summary outputs will be saved.
        peter_output: Path to the PETER threshold results.
        model: GPT model to use.
    """
    _run(
        summary_output,
        "paper",
        "gpt",
        "petersum",
        "run",
        "--ann-graph",
        peter_output,
        "--output",
        summary_dir,
        "--limit",
        "0",
        "--model",
        model,
    )


def run_eval_graph(
    eval_output: Path,
    eval_dir: Path,
    summary_output: Path,
    eval_prompt: str,
    graph_prompt: str,
    model: str,
) -> None:
    """Run GPT Eval Graph on summarized papers.

    Args:
        eval_output: Path where the GPT Eval Graph results will be saved.
        eval_dir: Directory where all evaluation outputs will be saved.
        summary_output: Path to the GPT PeterSum results.
        eval_prompt: The user prompt to use for paper evaluation.
        graph_prompt: The user prompt to use for graph extraction.
        model: GPT model to use.
    """
    _run(
        eval_output,
        "paper",
        "gpt",
        "eval",
        "graph",
        "run",
        "--papers",
        summary_output,
        "--output",
        eval_dir,
        "--limit",
        "0",
        "--eval-prompt",
        eval_prompt,
        "--graph-prompt",
        graph_prompt,
        "--model",
        model,
    )


@app.command(no_args_is_help=True)
def full(
    output_dir: Annotated[
        Path,
        typer.Option(
            "--output",
            help="Base directory for all outputs",
            writable=True,
            file_okay=False,
        ),
    ],
    graph_dir: Annotated[
        Path,
        typer.Option(
            "--graph",
            help="Directory containing the Peter graph",
            exists=True,
            file_okay=False,
        ),
    ],
    peerread_ann: Annotated[
        Path,
        typer.Option(
            "--ann", help="Path to PeerRead annotations", exists=True, dir_okay=False
        ),
    ],
    eval_prompt: Annotated[
        str,
        typer.Option(
            help="The user prompt to use for paper evaluation.",
            click_type=cli.Choice(GRAPH_EVAL_USER_PROMPTS),
        ),
    ] = "full-graph",
    graph_prompt: Annotated[
        str,
        typer.Option(
            help="The user prompt to use for graph extraction.",
            click_type=cli.Choice(GRAPH_EXTRACT_USER_PROMPTS),
        ),
    ] = "full",
    citation: Annotated[float, typer.Option(help="Citation threshold")] = 0.8,
    semantic: Annotated[float, typer.Option(help="Semantic threshold")] = 0.8,
    limit: Annotated[int, typer.Option(help="Number of papers")] = 10,
    retrieved_k: Annotated[
        int,
        typer.Option(
            help="Number of semantic neighbours to retrieve before applying threshold."
        ),
    ] = 100,
    force: Annotated[bool, typer.Option(help="Ignore previous results if set")] = False,
    model: Annotated[str, typer.Option(help="GPT model to use.")] = "gpt-4o-mini",
) -> None:
    """Run the complete paper workflow pipeline.

    This script runs three steps in sequence:
    1. Peter PeerRead threshold
    2. GPT PeterSum
    3. GPT Eval Graph

    Each step's output is used as input for the next step.
    If --clean is not specified, steps will be skipped if their output already exists.
    """
    # Create a subdirectory name based on parameters
    output_dir = output_dir / (
        f"cit{citation}_sem{semantic}_lim{limit}_mod{model}"
        f"_eval{eval_prompt}_graph{graph_prompt}"
    )

    if force and output_dir.exists():
        typer.echo(f"Removing existing directory: {output_dir}")
        shutil.rmtree(output_dir)

    output_dir.mkdir(parents=True, exist_ok=True)
    typer.echo(f"Using output directory: {output_dir}")

    # Step 1: Run Peter PeerRead threshold
    title("Running Peter PeerRead threshold")
    peter_output = output_dir / "peerread_with_peter.json"
    run_peter_threshold(
        peter_output, graph_dir, peerread_ann, limit, citation, semantic, retrieved_k
    )

    # Step 2: Run GPT PeterSum
    title("Running GPT PeterSum")
    summary_dir = output_dir / "summary"
    summary_output = summary_dir / "result.json"
    run_petersum(summary_output, summary_dir, peter_output, model)

    # Step 3: Run GPT Eval Graph
    eval_dir = output_dir / "eval"
    eval_output = eval_dir / "result.json"
    title("Running GPT Eval Graph")
    run_eval_graph(
        eval_output, eval_dir, summary_output, eval_prompt, graph_prompt, model
    )

    title("Result")
    typer.echo(f"Results saved to {eval_dir}")
    metrics_file = eval_dir / "metrics.json"
    metrics = json.loads(metrics_file.read_bytes())
    typer.echo(f"Accuracy: {metrics['accuracy']}")

    typer.echo("Statistics:")
    data = gpt.PromptResult.unwrap(
        load_data(eval_output, gpt.PromptResult[gpt.GraphResult])
    )
    for what in WhatLabel:
        rich.print(_get_statistics(data, what, _count_related_summarised))


@app.command(no_args_is_help=True)
def graph(
    output_dir: Annotated[
        Path,
        typer.Option(
            "--output",
            help="Base directory for outputs",
            writable=True,
            file_okay=False,
        ),
    ],
    graph_dir: Annotated[
        Path,
        typer.Option(
            "--graph",
            help="Directory containing the Peter graph",
            exists=True,
            file_okay=False,
        ),
    ],
    peerread_ann: Annotated[
        Path,
        typer.Option(
            "--ann", help="Path to PeerRead annotations", exists=True, dir_okay=False
        ),
    ],
    citation: Annotated[float, typer.Option(help="Citation threshold")] = 0.8,
    semantic: Annotated[float, typer.Option(help="Semantic threshold")] = 0.8,
    retrieved_k: Annotated[
        int,
        typer.Option(
            help="Number of semantic neighbours to retrieve before applying threshold."
        ),
    ] = 100,
    limit: Annotated[int, typer.Option(help="Number of papers")] = 10,
    force: Annotated[bool, typer.Option(help="Ignore previous results if set")] = False,
) -> None:
    """Run only the PETER graph build and generate correlations for true labels.

    This script runs only the initial PETER graph build to generate correlation statistics
    for true labels, without involving GPT processing.
    """
    # Create a subdirectory name based on parameters
    output_dir = output_dir / f"graph_only_cit{citation}_sem{semantic}_lim{limit}"

    if force and output_dir.exists():
        typer.echo(f"Removing existing directory: {output_dir}")
        shutil.rmtree(output_dir)

    output_dir.mkdir(parents=True, exist_ok=True)
    typer.echo(f"Using output directory: {output_dir}")

    title("Running Peter PeerRead threshold")
    peter_output = output_dir / "peerread_with_peter.json"
    run_peter_threshold(
        peter_output, graph_dir, peerread_ann, limit, citation, semantic, retrieved_k
    )

    title("Processing Graph Results")
    results = load_data(peter_output, rp.PaperResult)

    title("Result")
    rich.print(_get_statistics(results, WhatLabel.TRUE, _count_related_peter))


@dataclass(frozen=True, kw_only=True)
class Counts:
    """Various counts and derived counts from related papers."""

    y_pred: int
    y_true: int

    related: int
    semantic: int
    citation: int
    semantic_positive: int
    citation_positive: int
    semantic_negative: int
    citation_negative: int
    positive: int
    negative: int


class WhatLabel(enum.Enum):
    """What label from `Counts` to use for statistics."""

    TRUE = enum.auto()
    PRED = enum.auto()


def _count_related_summarised(item: gpt.GraphResult) -> Counts:
    """Count semantic/citation positive/negative from GPT PETER summarisation results.

    Raises:
        ValueError: if `item` has a None `related` field. This can happen for legacy
            data files from before the field was added.
    """
    related = item.related
    if related is None:
        raise ValueError("Invalid graph result. Must have related papers.")

    ctx = pr.ContextPolarity
    src = gpt.RelatedPaperSource

    semantic_positive = _filter_related(related, ctx.POSITIVE, src.SEMANTIC)
    semantic_negative = _filter_related(related, ctx.NEGATIVE, src.SEMANTIC)
    citation_positive = _filter_related(related, ctx.POSITIVE, src.CITATIONS)
    citation_negative = _filter_related(related, ctx.NEGATIVE, src.CITATIONS)

    return Counts(
        y_pred=item.paper.y_pred,
        y_true=item.paper.y_true,
        related=len(related),
        semantic=semantic_positive + semantic_negative,
        citation=citation_positive + citation_negative,
        semantic_positive=semantic_positive,
        citation_positive=citation_positive,
        semantic_negative=semantic_negative,
        citation_negative=citation_negative,
        positive=semantic_positive + citation_positive,
        negative=semantic_negative + citation_negative,
    )


def _filter_related(
    related: Iterable[gpt.PaperRelatedSummarised],
    polarity: pr.ContextPolarity,
    source: gpt.RelatedPaperSource,
) -> int:
    return sum(1 for r in related if r.polarity is polarity and r.source is source)


def _count_related_peter(item: rp.PaperResult) -> Counts:
    """Count semantic/citation positive/negative from direct PETER query results."""
    semantic_positive = len(item.results.semantic_positive)
    semantic_negative = len(item.results.semantic_negative)
    citation_positive = len(item.results.citations_positive)
    citation_negative = len(item.results.citations_negative)

    return Counts(
        y_pred=-1,
        y_true=item.paper.paper.label,
        related=semantic_positive
        + semantic_negative
        + citation_negative
        + citation_positive,
        semantic=semantic_positive + semantic_negative,
        citation=citation_positive + citation_negative,
        semantic_positive=semantic_positive,
        citation_positive=citation_positive,
        semantic_negative=semantic_negative,
        citation_negative=citation_negative,
        positive=semantic_positive + citation_positive,
        negative=semantic_negative + citation_negative,
    )


@dataclass(frozen=True, kw_only=True)
class FieldStats:
    """Descriptive statistics and correlation for a field."""

    correlation: float | None
    min: int
    q1: float
    median: float
    q3: float
    max: int


def _get_statistics[T](
    data: Iterable[T], what: WhatLabel, count_fn: Callable[[T], Counts]
) -> Table:
    """Calculate statistics and return them as a Rich table."""
    stats = _calculate_stats(map(count_fn, data), what)

    table = Table(title=f"Statistics - {what}")
    table.add_column("Field", style="cyan", no_wrap=True)
    table.add_column("Corr", style="magenta", justify="right")
    table.add_column("Min", style="green", justify="right")
    table.add_column("25", style="yellow", justify="right")
    table.add_column("Median", style="blue", justify="right")
    table.add_column("75", style="yellow", justify="right")
    table.add_column("Max", style="green", justify="right")

    for field, values in stats.items():
        corr = f"{values.correlation:.3f}" if values.correlation is not None else "NaN"
        min_val = str(values.min)
        q1_val = f"{values.q1:.1f}"
        median_val = f"{values.median:.1f}"
        q3_val = f"{values.q3:.1f}"
        max_val = str(values.max)

        table.add_row(field, corr, min_val, q1_val, median_val, q3_val, max_val)

    return table


def _calculate_stats(
    counts: Iterable[Counts], what: WhatLabel
) -> dict[str, FieldStats]:
    """Calculate statistics for each count field against the label field."""
    count_list = list(counts)  # Avoid iterating multiple times
    labels = [c.y_true if what is WhatLabel.TRUE else c.y_pred for c in count_list]

    field_names = [
        "related",
        "semantic",
        "citation",
        "semantic_positive",
        "citation_positive",
        "semantic_negative",
        "citation_negative",
        "positive",
        "negative",
    ]

    all_stats: dict[str, FieldStats] = {}

    for field in field_names:
        values = [getattr(count, field) for count in count_list]
        correlation = metrics.pearson_correlation(labels, values)

        # Calculate descriptive statistics using numpy
        np_values = np.array(values)
        min_val = int(np.min(np_values))
        q1_val = float(np.percentile(np_values, 25))
        median_val = float(np.median(np_values))
        q3_val = float(np.percentile(np_values, 75))
        max_val = int(np.max(np_values))

        all_stats[field] = FieldStats(
            correlation=correlation,
            min=min_val,
            q1=q1_val,
            median=median_val,
            q3=q3_val,
            max=max_val,
        )

    return all_stats


if __name__ == "__main__":
    app()
