"""Run PETER threshold experiment.

Instead of using a fixed K for retrieving citations and semantic papers, we use a fixed
threshold, then we check the accuracy and some statistics on the number of retrieved
papers and the label accuracy.
"""

import json
import shutil
from pathlib import Path
from typing import Annotated

import typer

from paper import gpt
from paper.gpt.evaluate_paper_graph import (
    GRAPH_EVAL_USER_PROMPTS,
    GRAPH_EXTRACT_USER_PROMPTS,
)
from paper.util import cli
from paper.util.cmd import run, title
from paper.util.serde import load_data

app = typer.Typer(
    context_settings={"help_option_names": ["-h", "--help"]},
    add_completion=False,
    rich_markup_mode="rich",
    pretty_exceptions_show_locals=False,
    no_args_is_help=True,
)


def _run(path: Path, *cmd: object) -> None:
    """Run command only if `path` does not already exist.

    Args:
        path: Path to check. The command won't run if it exists. This should be the path
            where a file or directory generated by the command will be.
        *cmd: The command and arguments to run.
    """
    if path.exists():
        print(f"{path} already exists.")
        return

    print(f"{path} does not exist. Running command.")
    run(*cmd)


@app.command()
def main(
    output_dir: Annotated[
        Path,
        typer.Option(
            "--output",
            help="Base directory for all outputs",
            writable=True,
            file_okay=False,
        ),
    ],
    graph_dir: Annotated[
        Path,
        typer.Option(
            "--graph",
            help="Directory containing the Peter graph",
            exists=True,
            file_okay=False,
        ),
    ],
    peerread_ann: Annotated[
        Path,
        typer.Option(
            "--ann", help="Path to PeerRead annotations", exists=True, dir_okay=False
        ),
    ],
    eval_prompt: Annotated[
        str,
        typer.Option(
            help="The user prompt to use for paper evaluation.",
            click_type=cli.Choice(GRAPH_EVAL_USER_PROMPTS),
        ),
    ] = "full-graph",
    graph_prompt: Annotated[
        str,
        typer.Option(
            help="The user prompt to use for graph extraction.",
            click_type=cli.Choice(GRAPH_EXTRACT_USER_PROMPTS),
        ),
    ] = "full",
    citation: Annotated[float, typer.Option(help="Citation threshold")] = 0.8,
    semantic: Annotated[float, typer.Option(help="Semantic threshold")] = 0.8,
    limit: Annotated[int, typer.Option(help="Number of papers")] = 10,
    force: Annotated[bool, typer.Option(help="Ignore previous results if set")] = False,
    model: Annotated[str, typer.Option(help="GPT model to use.")] = "gpt-4o-mini",
) -> None:
    """Run the complete paper workflow pipeline.

    This script runs three steps in sequence:
    1. Peter PeerRead threshold
    2. GPT PeterSum
    3. GPT Eval Graph

    Each step's output is used as input for the next step.
    If --clean is not specified, steps will be skipped if their output already exists.
    """
    if force:
        shutil.rmtree(output_dir, ignore_errors=True)
    output_dir.mkdir(parents=True, exist_ok=True)

    # Step 1: Run Peter PeerRead threshold
    title("Running Peter PeerRead threshold")

    peter_output = output_dir / "peerread_with_peter.json"
    _run(
        peter_output,
        "paper",
        "peter",
        "peerread-threshold",
        "--graph-dir",
        graph_dir,
        "--peerread-ann",
        peerread_ann,
        "--output",
        peter_output,
        "--num-papers",
        limit,
        "--citation",
        citation,
        "--semantic",
        semantic,
    )

    # Step 2: Run GPT PeterSum
    title("Running GPT PeterSum")

    summary_dir = output_dir / "summary"
    summary_output = summary_dir / "result.json"
    _run(
        summary_output,
        "paper",
        "gpt",
        "petersum",
        "run",
        "--ann-graph",
        peter_output,
        "--output",
        summary_dir,
        "--limit",
        "0",
        "--model",
        model,
    )

    # Step 3: Run GPT Eval Graph
    eval_dir = output_dir / "eval"
    eval_output = eval_dir / "result.json"
    title("Running GPT Eval Graph")

    _run(
        eval_output,
        "paper",
        "gpt",
        "eval",
        "graph",
        "run",
        "--papers",
        summary_output,
        "--output",
        eval_dir,
        "--limit",
        "0",
        "--eval-prompt",
        eval_prompt,
        "--graph-prompt",
        graph_prompt,
        "--model",
        model,
    )

    title("Result")
    typer.echo(f"Results saved to {eval_dir}")
    metrics_file = eval_dir / "metrics.json"
    metrics = json.loads(metrics_file.read_bytes())
    typer.echo(f"Accuracy: {metrics['accuracy']}")
    typer.echo(f"Statistics: {_get_statistics(eval_output)}")


def _get_statistics(eval_output: Path) -> str:
    data = load_data(eval_output, gpt.PromptResult[gpt.GraphResult])
    return str(len(data))


if __name__ == "__main__":
    app()
