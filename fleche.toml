[project]
name = "graphmind"

[remote]
host = "create"
base_path = "/scratch/users/${SSH_USER}/fleche"

[env]
PATH = "/users/${SSH_USER}/.cargo/bin:${PATH}"
HF_HOME = "/scratch/prj/inf_llmcache/hf_cache/"
UV_CACHE_DIR = "/scratch/users/${SSH_USER}/uv_cache/"
UV_PROJECT_ENVIRONMENT = "/scratch/users/${SSH_USER}/fleche/${PROJECT}/.venv"
PYTHONUNBUFFERED = "1"

[slurm]
partition = "interruptible_gpu"
constraint = "(a100|a40|a30|v100|rtx3090)"
gpus = 1
exclude = "erc-hpc-comp054"  # MIG-partitioned A100 with only 4.75GB slices

# Quick CUDA check
[jobs.cuda_check]
command = """
echo "Hello from $HOSTNAME"

uv run --extra cuda --extra baselines python -c '
import torch
print(f"CUDA available: {torch.cuda.is_available()}")
print(f"Device count: {torch.cuda.device_count()}")
if torch.cuda.is_available():
    print(f"Device name: {torch.cuda.get_device_name(0)}")
    print(f"CUDA version: {torch.version.cuda}")
'
"""

# Training: fleche run train --env DATASET=orc --env CONFIG=llama_cls
# Add --env NUM_EXAMPLES=5 to limit dataset size for testing
[jobs.train]
command = """
echo "Hello from $HOSTNAME"

uv run --extra cuda --extra baselines \
    paper baselines sft train \
    --train output/baselines/llama_data/${DATASET}_train.json.zst \
    --dev output/baselines/llama_data/${DATASET}_dev.json.zst \
    --test output/baselines/llama_data/${DATASET}_test.json.zst \
    --output output/baselines/llama_${DATASET}_${CONFIG}${SEED:+_seed${SEED}} \
    --config src/paper/baselines/sft_config/${CONFIG}.toml \
    ${NUM_EXAMPLES:+--num-examples $NUM_EXAMPLES} \
    ${SEED:+--seed $SEED}
"""
inputs = ["output/baselines/llama_data/"]
outputs = ["output/baselines/llama_${DATASET}_${CONFIG}${SEED:+_seed${SEED}}/"]
env = { DATASET = "orc", CONFIG = "llama_cls" }

# Inference: fleche run infer --env DATASET=orc --env CONFIG=llama_cls
# Add --env NUM_EXAMPLES=5 to limit dataset size for testing
[jobs.infer]
command = """
echo "Hello from $HOSTNAME"

uv run --extra cuda --extra baselines \
    paper baselines sft infer \
    --model output/baselines/llama_${DATASET}_${CONFIG}/final_model \
    --input output/baselines/llama_data/${DATASET}_test.json.zst \
    --output output/baselines/llama_${DATASET}_${CONFIG}_infer \
    ${NUM_EXAMPLES:+--num-examples $NUM_EXAMPLES}
"""
inputs = ["output/baselines/llama_data/"]
outputs = ["output/baselines/llama_${DATASET}_${CONFIG}_infer/"]
env = { DATASET = "orc", CONFIG = "llama_cls" }

# Generative SFT Training: fleche run train_gen --env DATASET=orc --env CONFIG=llama_gen_basic
# Add --env NUM_EXAMPLES=5 to limit dataset size for testing
[jobs.train_gen]
command = """
echo "Hello from $HOSTNAME"

uv run --extra cuda --extra baselines \
    paper baselines sft-gen train \
    --train output/baselines/llama_data/${DATASET}_train.json.zst \
    --dev output/baselines/llama_data/${DATASET}_dev.json.zst \
    --test output/baselines/llama_data/${DATASET}_test.json.zst \
    --output output/baselines/llama_gen_${DATASET}_${CONFIG}${SEED:+_seed${SEED}} \
    --config src/paper/baselines/sft_config/${CONFIG}.toml \
    ${NUM_EXAMPLES:+--num-examples $NUM_EXAMPLES} \
    ${SEED:+--seed $SEED}
"""
inputs = ["output/baselines/llama_data/"]
outputs = ["output/baselines/llama_gen_${DATASET}_${CONFIG}${SEED:+_seed${SEED}}/"]
env = { DATASET = "orc", CONFIG = "llama_gen_basic" }

# Generative SFT Inference: fleche run infer_gen --env DATASET=orc --env CONFIG=llama_gen_basic
# Add --env NUM_EXAMPLES=5 to limit dataset size for testing
[jobs.infer_gen]
command = """
echo "Hello from $HOSTNAME"

uv run --extra cuda --extra baselines \
    paper baselines sft-gen infer \
    --model output/baselines/llama_gen_${DATASET}_${CONFIG}/final_model \
    --input output/baselines/llama_data/${DATASET}_test.json.zst \
    --output output/baselines/llama_gen_${DATASET}_${CONFIG}_infer \
    ${NUM_EXAMPLES:+--num-examples $NUM_EXAMPLES}
"""
inputs = ["output/baselines/llama_data/"]
outputs = ["output/baselines/llama_gen_${DATASET}_${CONFIG}_infer/"]
env = { DATASET = "orc", CONFIG = "llama_gen_basic" }

# Graph-enriched Generative SFT Training: fleche run train_gen_graph --env DATASET=orc --env CONFIG=llama_gen_graph
# Requires preprocessed data with graph context (run sft_gen_preprocess locally first)
# Add --env NUM_EXAMPLES=5 to limit dataset size for testing
[jobs.train_gen_graph]
command = """
echo "Hello from $HOSTNAME"

uv run --extra cuda --extra baselines \
    python -m paper.baselines.sft_gen_graph train \
    --train output/baselines/llama_data/${DATASET}_train_graph_enriched.json.zst \
    --dev output/baselines/llama_data/${DATASET}_dev_graph_enriched.json.zst \
    --test output/baselines/llama_data/${DATASET}_test_graph_enriched.json.zst \
    --output output/baselines/llama_gen_graph_${DATASET}_${CONFIG}${SEED:+_seed${SEED}} \
    --config src/paper/baselines/sft_config/${CONFIG}.toml \
    ${NUM_EXAMPLES:+--num-examples $NUM_EXAMPLES} \
    ${SEED:+--seed $SEED}
"""
inputs = ["output/baselines/llama_data/"]
outputs = ["output/baselines/llama_gen_graph_${DATASET}_${CONFIG}${SEED:+_seed${SEED}}/"]
env = { DATASET = "orc", CONFIG = "llama_gen_graph" }

# Graph-enriched Generative SFT Inference: fleche run infer_gen_graph --env DATASET=orc --env CONFIG=llama_gen_graph
[jobs.infer_gen_graph]
command = """
echo "Hello from $HOSTNAME"

uv run --extra cuda --extra baselines \
    python -m paper.baselines.sft_gen_graph infer \
    --model output/baselines/llama_gen_graph_${DATASET}_${CONFIG}/final_model \
    --input output/baselines/llama_data/${DATASET}_test_graph_enriched.json.zst \
    --output output/baselines/llama_gen_graph_${DATASET}_${CONFIG}_infer \
    ${NUM_EXAMPLES:+--num-examples $NUM_EXAMPLES}
"""
inputs = ["output/baselines/llama_data/"]
outputs = ["output/baselines/llama_gen_graph_${DATASET}_${CONFIG}_infer/"]
env = { DATASET = "orc", CONFIG = "llama_gen_graph" }

# ==============================================================================
# GPT Ablation Experiments (run locally)
# ==============================================================================
# Standard prompts: sans, related, norel-graph, semantic-only, full-graph-structured
# Add --env SOURCES=citations or --env SOURCES=semantic for source filtering
# Add --env RUNS=1 for quick test, --env LIMIT=10 to limit papers

# GPT ORC: fleche run gpt_orc --env PROMPT=sans
[jobs.gpt_orc]
host = "local"
command = """
uv run paper gpt eval graph experiment \
    --papers output/venus5/split/dev_100_balanced.json.zst \
    --output output/eval_orc/ablation_${PROMPT}${SOURCES:+_${SOURCES}} \
    --model ${MODEL} \
    --limit ${LIMIT} \
    --runs ${RUNS} \
    --eval-prompt ${PROMPT} \
    --demos orc_balanced_4 \
    --seed 42 \
    ${SOURCES:+--sources ${SOURCES}}
"""
env = { PROMPT = "full-graph-structured", MODEL = "gpt-4o-mini", RUNS = "5", LIMIT = "100" }

# GPT PeerRead: fleche run gpt_peerread --env PROMPT=sans
[jobs.gpt_peerread]
host = "local"
command = """
uv run paper gpt eval graph experiment \
    --papers output/new_peerread/peter_summarised/balanced_68.json.zst \
    --output output/eval_peerread/ablation_${PROMPT}${SOURCES:+_${SOURCES}} \
    --model ${MODEL} \
    --limit ${LIMIT} \
    --runs ${RUNS} \
    --eval-prompt ${PROMPT} \
    --demos peerread_balanced_4 \
    --seed 42 \
    ${SOURCES:+--sources ${SOURCES}}
"""
env = { PROMPT = "full-graph-structured", MODEL = "gpt-4o-mini", RUNS = "5", LIMIT = "68" }

# Quick test jobs (single paper, single run)
# GPT ORC Test: fleche run gpt_orc_test --env PROMPT=sans
[jobs.gpt_orc_test]
host = "local"
command = """
uv run paper gpt eval graph run \
    --papers output/venus5/split/dev_100_balanced.json.zst \
    --output /tmp/gpt_test_orc_${PROMPT} \
    --model ${MODEL} \
    --limit 1 \
    --eval-prompt ${PROMPT} \
    --demos orc_balanced_4 \
    --seed 42 \
    --n-evaluations 1 \
    ${SOURCES:+--sources ${SOURCES}}
"""
env = { PROMPT = "full-graph-structured", MODEL = "gpt-4o-mini" }

# GPT PeerRead Test: fleche run gpt_peerread_test --env PROMPT=sans
[jobs.gpt_peerread_test]
host = "local"
command = """
uv run paper gpt eval graph run \
    --papers output/new_peerread/peter_summarised/balanced_68.json.zst \
    --output /tmp/gpt_test_peerread_${PROMPT} \
    --model ${MODEL} \
    --limit 1 \
    --eval-prompt ${PROMPT} \
    --demos peerread_balanced_4 \
    --seed 42 \
    --n-evaluations 1 \
    ${SOURCES:+--sources ${SOURCES}}
"""
env = { PROMPT = "full-graph-structured", MODEL = "gpt-4o-mini" }
