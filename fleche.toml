[project]
name = "graphmind"

[remote]
host = "create"
base_path = "/scratch/users/k21220155/fleche"

[env]
PATH = "/users/k21220155/.cargo/bin:${PATH}"
HF_HOME = "/scratch/users/k21220155/hf_cache/"
UV_CACHE_DIR = "/scratch/users/k21220155/uv_cache/"
PYTHONUNBUFFERED = "1"

[slurm]
partition = "interruptible_gpu"
constraint = "(a100|a40|a30|v100|rtx3090)"
gpus = 1

# Quick CUDA check
[jobs.cuda_check]
command = """
echo "Hello from $HOSTNAME"

uv run --extra cuda --extra baselines python -c '
import torch
print(f"CUDA available: {torch.cuda.is_available()}")
print(f"Device count: {torch.cuda.device_count()}")
if torch.cuda.is_available():
    print(f"Device name: {torch.cuda.get_device_name(0)}")
    print(f"CUDA version: {torch.version.cuda}")
'
"""

# Training: fleche run train --env DATASET=orc --env CONFIG=llama_basic
[jobs.train]
command = """
echo "Hello from $HOSTNAME"

uv run --extra cuda --extra baselines \
    paper baselines sft train \
    --train output/baselines/llama_data/${DATASET}_train.json.zst \
    --dev output/baselines/llama_data/${DATASET}_dev.json.zst \
    --test output/baselines/llama_data/${DATASET}_test.json.zst \
    --output output/baselines/llama_${DATASET}_${CONFIG} \
    --config src/paper/baselines/sft_config/${CONFIG}.toml
"""
inputs = ["output/baselines/llama_data/"]
outputs = ["output/baselines/"]
env = {
    DATASET = "orc",
    CONFIG = "llama_basic",
}

# Inference: fleche run infer --env DATASET=orc --env CONFIG=llama_basic
[jobs.infer]
command = """
echo "Hello from $HOSTNAME"

uv run --extra cuda --extra baselines \
    paper baselines sft infer \
    --model output/baselines/llama_${DATASET}_${CONFIG}/final_model \
    --input output/baselines/llama_data/${DATASET}_test.json.zst \
    --output output/baselines/llama_${DATASET}_${CONFIG}_infer
"""
inputs = ["output/baselines/llama_data/"]
outputs = ["output/baselines/"]
env = {
    DATASET = "orc",
    CONFIG = "llama_basic",
}
