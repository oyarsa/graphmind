[project]
name = "graphmind"

[remote]
host = "create"
base_path = "/scratch/users/k21220155/fleche"

[env]
PATH = "/users/k21220155/.cargo/bin:${PATH}"
HF_HOME = "/scratch/prj/inf_llmcache/scratch_tmp/hf_cache/"
UV_CACHE_DIR = "/scratch/users/k21220155/uv_cache/"
UV_PROJECT_ENVIRONMENT = "/scratch/users/k21220155/fleche/graphmind/.venv"
PYTHONUNBUFFERED = "1"

[slurm]
partition = "interruptible_gpu"
constraint = "(a100|a40|a30|v100|rtx3090)"
gpus = 1
exclude = "erc-hpc-comp054"  # MIG-partitioned A100 with only 4.75GB slices

# Quick CUDA check
[jobs.cuda_check]
command = """
echo "Hello from $HOSTNAME"

uv run --extra cuda --extra baselines python -c '
import torch
print(f"CUDA available: {torch.cuda.is_available()}")
print(f"Device count: {torch.cuda.device_count()}")
if torch.cuda.is_available():
    print(f"Device name: {torch.cuda.get_device_name(0)}")
    print(f"CUDA version: {torch.version.cuda}")
'
"""

# Training: fleche run train --env DATASET=orc --env CONFIG=llama_basic
# Add --env NUM_EXAMPLES=5 to limit dataset size for testing
[jobs.train]
command = """
echo "Hello from $HOSTNAME"

uv run --extra cuda --extra baselines \
    paper baselines sft train \
    --train output/baselines/llama_data/${DATASET}_train.json.zst \
    --dev output/baselines/llama_data/${DATASET}_dev.json.zst \
    --test output/baselines/llama_data/${DATASET}_test.json.zst \
    --output output/baselines/llama_${DATASET}_${CONFIG}${SEED:+_seed${SEED}} \
    --config src/paper/baselines/sft_config/${CONFIG}.toml \
    ${NUM_EXAMPLES:+--num-examples $NUM_EXAMPLES} \
    ${SEED:+--seed $SEED}
"""
inputs = ["output/baselines/llama_data/"]
outputs = ["output/baselines/"]
env = { DATASET = "orc", CONFIG = "llama_basic" }

# Inference: fleche run infer --env DATASET=orc --env CONFIG=llama_basic
# Add --env NUM_EXAMPLES=5 to limit dataset size for testing
[jobs.infer]
command = """
echo "Hello from $HOSTNAME"

uv run --extra cuda --extra baselines \
    paper baselines sft infer \
    --model output/baselines/llama_${DATASET}_${CONFIG}/final_model \
    --input output/baselines/llama_data/${DATASET}_test.json.zst \
    --output output/baselines/llama_${DATASET}_${CONFIG}_infer \
    ${NUM_EXAMPLES:+--num-examples $NUM_EXAMPLES}
"""
inputs = ["output/baselines/llama_data/"]
outputs = ["output/baselines/"]
env = { DATASET = "orc", CONFIG = "llama_basic" }

# Generative SFT Training: fleche run train_gen --env DATASET=peerread --env CONFIG=llama_peerread_gen
# Add --env NUM_EXAMPLES=5 to limit dataset size for testing
[jobs.train_gen]
command = """
echo "Hello from $HOSTNAME"

uv run --extra cuda --extra baselines \
    paper baselines sft-gen train \
    --train output/baselines/llama_data/${DATASET}_train.json.zst \
    --dev output/baselines/llama_data/${DATASET}_dev.json.zst \
    --test output/baselines/llama_data/${DATASET}_test.json.zst \
    --output output/baselines/llama_gen_${DATASET}_${CONFIG}${SEED:+_seed${SEED}} \
    --config src/paper/baselines/sft_config/${CONFIG}.toml \
    ${NUM_EXAMPLES:+--num-examples $NUM_EXAMPLES} \
    ${SEED:+--seed $SEED}
"""
inputs = ["output/baselines/llama_data/"]
outputs = ["output/baselines/"]
env = { DATASET = "peerread", CONFIG = "llama_peerread_gen" }

# Generative SFT Inference: fleche run infer_gen --env DATASET=peerread --env CONFIG=llama_peerread_gen
# Add --env NUM_EXAMPLES=5 to limit dataset size for testing
[jobs.infer_gen]
command = """
echo "Hello from $HOSTNAME"

uv run --extra cuda --extra baselines \
    paper baselines sft-gen infer \
    --model output/baselines/llama_gen_${DATASET}_${CONFIG}/final_model \
    --input output/baselines/llama_data/${DATASET}_test.json.zst \
    --output output/baselines/llama_gen_${DATASET}_${CONFIG}_infer \
    ${NUM_EXAMPLES:+--num-examples $NUM_EXAMPLES}
"""
inputs = ["output/baselines/llama_data/"]
outputs = ["output/baselines/"]
env = { DATASET = "peerread", CONFIG = "llama_peerread_gen" }
