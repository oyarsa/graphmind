# Experiment Log
# New experiments are appended at the bottom.
# Use this structured format for easy programmatic access.

experiments:
  # === BASELINE ABLATION EXPERIMENTS (Jan 13) ===
  # Initial ablation experiments with demonstrations, run before logging was implemented

  - date: "2025-01-13"
    name: "sans_orc"
    type: ablation
    description: "Abstract only baseline (ORC)"
    reason: >
      Initial baseline - abstract only, no related papers or graph.
    command: |
      uv run paper gpt eval graph experiment \
        --papers output/venus5/split/dev_100_balanced.json.zst \
        --output output/eval_orc/ablation_sans \
        --model gpt-4o-mini --limit 100 \
        --eval-prompt sans \
        --demos orc_balanced_4 --seed 42 --n-evaluations 1 \
        --runs 5
    parameters:
      dataset: "ORC dev_100_balanced"
      model: "gpt-4o-mini"
      eval_prompt: "sans"
      sources: null
      demos: "orc_balanced_4"
      runs: 5
    metrics:
      pearson: {mean: 0.048, stdev: 0.023, min: 0.020, max: 0.080}
      spearman: {mean: 0.050, stdev: 0.027, min: 0.020, max: 0.090}
      mae: {mean: 1.226, stdev: 0.035, min: 1.180, max: 1.270}
      accuracy: {mean: 0.186, stdev: 0.027, min: 0.150, max: 0.220}
      f1: {mean: 0.102, stdev: 0.015, min: 0.080, max: 0.120}
      cost_per_run: 0.028
    total_cost: 0.14
    conclusion: >
      Weak baseline. Abstract-only evaluation shows near-zero correlation with ground truth.

  - date: "2025-01-13"
    name: "related_orc"
    type: ablation
    description: "Related papers without graph (ORC)"
    reason: >
      Testing value of related papers without knowledge graph summaries.
    command: |
      uv run paper gpt eval graph experiment \
        --papers output/venus5/split/dev_100_balanced.json.zst \
        --output output/eval_orc/ablation_related \
        --model gpt-4o-mini --limit 100 \
        --eval-prompt related \
        --demos orc_balanced_4 --seed 42 --n-evaluations 1 \
        --runs 5
    parameters:
      dataset: "ORC dev_100_balanced"
      model: "gpt-4o-mini"
      eval_prompt: "related"
      sources: null
      demos: "orc_balanced_4"
      runs: 5
    metrics:
      pearson: {mean: -0.030, stdev: 0.044, min: -0.090, max: 0.030}
      spearman: {mean: -0.036, stdev: 0.049, min: -0.100, max: 0.030}
      mae: {mean: 1.034, stdev: 0.033, min: 0.990, max: 1.080}
      accuracy: {mean: 0.248, stdev: 0.026, min: 0.210, max: 0.280}
      f1: {mean: 0.153, stdev: 0.013, min: 0.140, max: 0.170}
      cost_per_run: 0.119
    total_cost: 0.60
    conclusion: >
      Negative correlation. Related papers alone hurt performance - model interprets
      related work as evidence against novelty without proper context.

  - date: "2025-01-13"
    name: "norel_orc"
    type: ablation
    description: "Graph only without related papers (ORC)"
    reason: >
      Testing value of knowledge graph summaries without related papers.
    command: |
      uv run paper gpt eval graph experiment \
        --papers output/venus5/split/dev_100_balanced.json.zst \
        --output output/eval_orc/ablation_norel \
        --model gpt-4o-mini --limit 100 \
        --eval-prompt norel-graph \
        --demos orc_balanced_4 --seed 42 --n-evaluations 1 \
        --runs 5
    parameters:
      dataset: "ORC dev_100_balanced"
      model: "gpt-4o-mini"
      eval_prompt: "norel-graph"
      sources: null
      demos: "orc_balanced_4"
      runs: 5
    metrics:
      pearson: {mean: 0.020, stdev: 0.028, min: -0.010, max: 0.060}
      spearman: {mean: 0.013, stdev: 0.023, min: -0.020, max: 0.050}
      mae: {mean: 1.178, stdev: 0.028, min: 1.140, max: 1.210}
      accuracy: {mean: 0.218, stdev: 0.026, min: 0.180, max: 0.250}
      f1: {mean: 0.120, stdev: 0.014, min: 0.100, max: 0.140}
      cost_per_run: 0.033
    total_cost: 0.17
    conclusion: >
      Near-zero correlation. Graph summaries alone provide minimal signal.

  - date: "2025-01-13"
    name: "citations_orc"
    type: ablation
    description: "Full pipeline with citations only (ORC)"
    reason: >
      Testing contribution of citation-based related papers.
    command: |
      uv run paper gpt eval graph experiment \
        --papers output/venus5/split/dev_100_balanced.json.zst \
        --output output/eval_orc/ablation_citations \
        --model gpt-4o-mini --limit 100 \
        --eval-prompt full-graph-structured \
        --sources citations \
        --demos orc_balanced_4 --seed 42 --n-evaluations 1 \
        --runs 5
    parameters:
      dataset: "ORC dev_100_balanced"
      model: "gpt-4o-mini"
      eval_prompt: "full-graph-structured"
      sources: "citations"
      demos: "orc_balanced_4"
      runs: 5
    metrics:
      pearson: {mean: 0.224, stdev: 0.032, min: 0.180, max: 0.270}
      spearman: {mean: 0.239, stdev: 0.035, min: 0.190, max: 0.290}
      mae: {mean: 1.136, stdev: 0.063, min: 1.050, max: 1.220}
      accuracy: {mean: 0.218, stdev: 0.026, min: 0.180, max: 0.250}
      f1: {mean: 0.123, stdev: 0.025, min: 0.090, max: 0.160}
      cost_per_run: 0.086
    total_cost: 0.43
    conclusion: >
      Moderate correlation. Citations provide useful but incomplete signal.

  - date: "2025-01-13"
    name: "semantic_orc"
    type: ablation
    description: "Full pipeline with semantic papers only (ORC)"
    reason: >
      Testing contribution of semantically similar papers.
    command: |
      uv run paper gpt eval graph experiment \
        --papers output/venus5/split/dev_100_balanced.json.zst \
        --output output/eval_orc/ablation_semantic \
        --model gpt-4o-mini --limit 100 \
        --eval-prompt full-graph-structured \
        --sources semantic \
        --demos orc_balanced_4 --seed 42 --n-evaluations 1 \
        --runs 5
    parameters:
      dataset: "ORC dev_100_balanced"
      model: "gpt-4o-mini"
      eval_prompt: "full-graph-structured"
      sources: "semantic"
      demos: "orc_balanced_4"
      runs: 5
    metrics:
      pearson: {mean: 0.375, stdev: 0.045, min: 0.310, max: 0.430}
      spearman: {mean: 0.383, stdev: 0.034, min: 0.330, max: 0.420}
      mae: {mean: 0.814, stdev: 0.018, min: 0.790, max: 0.840}
      accuracy: {mean: 0.298, stdev: 0.015, min: 0.280, max: 0.320}
      f1: {mean: 0.153, stdev: 0.012, min: 0.140, max: 0.170}
      cost_per_run: 0.099
    total_cost: 0.50
    conclusion: >
      Best single source! Semantic papers provide stronger signal than citations.
      Unexpectedly outperforms the full pipeline.

  - date: "2025-01-13"
    name: "full_orc"
    type: ablation
    description: "Full pipeline with both sources (ORC)"
    reason: >
      Testing full pipeline combining both citation and semantic sources.
    command: |
      uv run paper gpt eval graph experiment \
        --papers output/venus5/split/dev_100_balanced.json.zst \
        --output output/eval_orc/ablation_full \
        --model gpt-4o-mini --limit 100 \
        --eval-prompt full-graph-structured \
        --demos orc_balanced_4 --seed 42 --n-evaluations 1 \
        --runs 5
    parameters:
      dataset: "ORC dev_100_balanced"
      model: "gpt-4o-mini"
      eval_prompt: "full-graph-structured"
      sources: null
      demos: "orc_balanced_4"
      runs: 5
    metrics:
      pearson: {mean: 0.312, stdev: 0.058, min: 0.230, max: 0.390}
      spearman: {mean: 0.337, stdev: 0.077, min: 0.230, max: 0.430}
      mae: {mean: 0.862, stdev: 0.018, min: 0.840, max: 0.890}
      accuracy: {mean: 0.290, stdev: 0.016, min: 0.270, max: 0.310}
      f1: {mean: 0.150, stdev: 0.014, min: 0.130, max: 0.170}
      cost_per_run: 0.116
    total_cost: 0.58
    conclusion: >
      Underperforms semantic-only! Full pipeline (0.312) is worse than semantic-only
      (0.375). This unexpected finding suggests citations add noise.

  - date: "2025-01-13"
    name: "sans_peerread"
    type: ablation
    description: "Abstract only baseline (PeerRead)"
    reason: >
      Initial baseline - abstract only, no related papers or graph.
    command: |
      uv run paper gpt eval graph experiment \
        --papers output/new_peerread/peter_summarised/balanced_68.json.zst \
        --output output/eval_peerread/ablation_sans \
        --model gpt-4o-mini --limit 68 \
        --eval-prompt sans \
        --demos peerread_balanced_5 --seed 42 --n-evaluations 1 \
        --runs 5
    parameters:
      dataset: "PeerRead balanced_68"
      model: "gpt-4o-mini"
      eval_prompt: "sans"
      sources: null
      demos: "peerread_balanced_5"
      runs: 5
    metrics:
      pearson: {mean: 0.139, stdev: 0.074, min: 0.040, max: 0.230}
      spearman: {mean: 0.125, stdev: 0.074, min: 0.030, max: 0.210}
      mae: {mean: 1.250, stdev: 0.055, min: 1.180, max: 1.330}
      accuracy: {mean: 0.159, stdev: 0.012, min: 0.140, max: 0.170}
      f1: {mean: 0.121, stdev: 0.009, min: 0.110, max: 0.130}
      cost_per_run: 0.019
    total_cost: 0.10
    conclusion: >
      Moderate baseline. Better than ORC's abstract-only (0.139 vs 0.048).

  - date: "2025-01-13"
    name: "related_peerread"
    type: ablation
    description: "Related papers without graph (PeerRead)"
    reason: >
      Testing value of related papers without knowledge graph summaries.
    command: |
      uv run paper gpt eval graph experiment \
        --papers output/new_peerread/peter_summarised/balanced_68.json.zst \
        --output output/eval_peerread/ablation_related \
        --model gpt-4o-mini --limit 68 \
        --eval-prompt related \
        --demos peerread_balanced_5 --seed 42 --n-evaluations 1 \
        --runs 5
    parameters:
      dataset: "PeerRead balanced_68"
      model: "gpt-4o-mini"
      eval_prompt: "related"
      sources: null
      demos: "peerread_balanced_5"
      runs: 5
    metrics:
      pearson: {mean: 0.071, stdev: 0.138, min: -0.110, max: 0.250}
      spearman: {mean: 0.091, stdev: 0.149, min: -0.100, max: 0.280}
      mae: {mean: 1.079, stdev: 0.044, min: 1.020, max: 1.140}
      accuracy: {mean: 0.162, stdev: 0.021, min: 0.130, max: 0.190}
      f1: {mean: 0.108, stdev: 0.014, min: 0.090, max: 0.130}
      cost_per_run: 0.048
    total_cost: 0.24
    conclusion: >
      High variance. Some runs positive, some negative. Less stable than ORC Related.

  - date: "2025-01-13"
    name: "norel_peerread"
    type: ablation
    description: "Graph only without related papers (PeerRead)"
    reason: >
      Testing value of knowledge graph summaries without related papers.
    command: |
      uv run paper gpt eval graph experiment \
        --papers output/new_peerread/peter_summarised/balanced_68.json.zst \
        --output output/eval_peerread/ablation_norel_graph \
        --model gpt-4o-mini --limit 68 \
        --eval-prompt norel-graph \
        --demos peerread_balanced_5 --seed 42 --n-evaluations 1 \
        --runs 5
    parameters:
      dataset: "PeerRead balanced_68"
      model: "gpt-4o-mini"
      eval_prompt: "norel-graph"
      sources: null
      demos: "peerread_balanced_5"
      runs: 5
    metrics:
      pearson: {mean: 0.080, stdev: 0.109, min: -0.050, max: 0.230}
      spearman: {mean: 0.098, stdev: 0.088, min: 0.000, max: 0.210}
      mae: {mean: 1.215, stdev: 0.047, min: 1.150, max: 1.280}
      accuracy: {mean: 0.138, stdev: 0.022, min: 0.110, max: 0.170}
      f1: {mean: 0.084, stdev: 0.011, min: 0.070, max: 0.100}
      cost_per_run: 0.071
    total_cost: 0.36
    conclusion: >
      Weak and variable. Graph-only shows high variance on PeerRead.

  - date: "2025-01-13"
    name: "citations_peerread"
    type: ablation
    description: "Full pipeline with citations only (PeerRead)"
    reason: >
      Testing contribution of citation-based related papers.
    command: |
      uv run paper gpt eval graph experiment \
        --papers output/new_peerread/peter_summarised/balanced_68.json.zst \
        --output output/eval_peerread/ablation_citations \
        --model gpt-4o-mini --limit 68 \
        --eval-prompt full-graph-structured \
        --sources citations \
        --demos peerread_balanced_5 --seed 42 --n-evaluations 1 \
        --runs 5
    parameters:
      dataset: "PeerRead balanced_68"
      model: "gpt-4o-mini"
      eval_prompt: "full-graph-structured"
      sources: "citations"
      demos: "peerread_balanced_5"
      runs: 5
    metrics:
      pearson: {mean: 0.339, stdev: 0.054, min: 0.270, max: 0.410}
      spearman: {mean: 0.394, stdev: 0.057, min: 0.320, max: 0.470}
      mae: {mean: 1.503, stdev: 0.064, min: 1.420, max: 1.590}
      accuracy: {mean: 0.068, stdev: 0.022, min: 0.040, max: 0.100}
      f1: {mean: 0.060, stdev: 0.022, min: 0.030, max: 0.090}
      cost_per_run: 0.038
    total_cost: 0.19
    conclusion: >
      Good correlation. Citations work better on PeerRead than ORC (0.339 vs 0.224).

  - date: "2025-01-13"
    name: "semantic_peerread"
    type: ablation
    description: "Full pipeline with semantic papers only (PeerRead)"
    reason: >
      Testing contribution of semantically similar papers.
    command: |
      uv run paper gpt eval graph experiment \
        --papers output/new_peerread/peter_summarised/balanced_68.json.zst \
        --output output/eval_peerread/ablation_semantic \
        --model gpt-4o-mini --limit 68 \
        --eval-prompt full-graph-structured \
        --sources semantic \
        --demos peerread_balanced_5 --seed 42 --n-evaluations 1 \
        --runs 5
    parameters:
      dataset: "PeerRead balanced_68"
      model: "gpt-4o-mini"
      eval_prompt: "full-graph-structured"
      sources: "semantic"
      demos: "peerread_balanced_5"
      runs: 5
    metrics:
      pearson: {mean: 0.373, stdev: 0.048, min: 0.310, max: 0.440}
      spearman: {mean: 0.368, stdev: 0.057, min: 0.290, max: 0.440}
      mae: {mean: 0.932, stdev: 0.030, min: 0.890, max: 0.970}
      accuracy: {mean: 0.176, stdev: 0.023, min: 0.150, max: 0.210}
      f1: {mean: 0.099, stdev: 0.028, min: 0.060, max: 0.140}
      cost_per_run: 0.042
    total_cost: 0.21
    conclusion: >
      Strong correlation. Similar to citations on PeerRead (0.373 vs 0.339).

  - date: "2025-01-13"
    name: "full_peerread"
    type: ablation
    description: "Full pipeline with both sources (PeerRead)"
    reason: >
      Testing full pipeline combining both citation and semantic sources.
    command: |
      uv run paper gpt eval graph experiment \
        --papers output/new_peerread/peter_summarised/balanced_68.json.zst \
        --output output/eval_peerread/ablation_full \
        --model gpt-4o-mini --limit 68 \
        --eval-prompt full-graph-structured \
        --demos peerread_balanced_5 --seed 42 --n-evaluations 1 \
        --runs 5
    parameters:
      dataset: "PeerRead balanced_68"
      model: "gpt-4o-mini"
      eval_prompt: "full-graph-structured"
      sources: null
      demos: "peerread_balanced_5"
      runs: 5
    metrics:
      pearson: {mean: 0.449, stdev: 0.089, min: 0.340, max: 0.570}
      spearman: {mean: 0.435, stdev: 0.092, min: 0.310, max: 0.560}
      mae: {mean: 1.112, stdev: 0.074, min: 1.010, max: 1.210}
      accuracy: {mean: 0.115, stdev: 0.019, min: 0.090, max: 0.140}
      f1: {mean: 0.066, stdev: 0.012, min: 0.050, max: 0.080}
      cost_per_run: 0.053
    total_cost: 0.27
    conclusion: >
      Best on PeerRead! Unlike ORC, combining sources improves correlation (0.449).

  # === PROMPT ENGINEERING EXPERIMENTS (Jan 14) ===
  # Experiments to improve configurations showing unexpected behaviour

  - date: "2025-01-14"
    name: "semantic_v2"
    type: prompt_engineering
    description: "Conservative semantic-only prompt"
    reason: >
      Original semantic-only config (Pearson 0.375) outperformed Full pipeline (0.312).
      Created new semantic-only prompt with conservative language about semantic matches.
    command: |
      uv run paper gpt eval graph experiment \
        --papers output/venus5/split/dev_100_balanced.json.zst \
        --output output/eval_orc/ablation_semantic_v2 \
        --model gpt-4o-mini --limit 100 \
        --eval-prompt semantic-only \
        --sources semantic \
        --demos orc_balanced_4 --seed 42 --n-evaluations 1 \
        --runs 5
    parameters:
      dataset: "ORC dev_100_balanced"
      model: "gpt-4o-mini"
      eval_prompt: "semantic-only"
      sources: "semantic"
      demos: "orc_balanced_4"
      runs: 5
    metrics:
      pearson: {mean: 0.132, stdev: 0.068, min: 0.044, max: 0.220}
      spearman: {mean: 0.135, stdev: 0.052, min: 0.076, max: 0.191}
      mae: {mean: 1.214, stdev: 0.038, min: 1.16, max: 1.26}
      accuracy: {mean: 0.166, stdev: 0.036, min: 0.12, max: 0.21}
      f1: {mean: 0.104, stdev: 0.020, min: 0.078, max: 0.121}
      cost_per_run: 0.093
    total_cost: 0.46
    conclusion: >
      Success. Semantic-only dropped from 0.375 to 0.132, now properly below Full (0.312).
      The conservative language about semantic matches being "tangentially related" worked.

  - date: "2025-01-14"
    name: "related_v2"
    type: prompt_engineering
    description: "Balanced related prompt"
    reason: >
      Original related config had negative Pearson correlation (-0.030). Added balanced
      language emphasising that related papers provide context, not evidence against novelty.
    command: |
      uv run paper gpt eval graph experiment \
        --papers output/venus5/split/dev_100_balanced.json.zst \
        --output output/eval_orc/ablation_related_v2 \
        --model gpt-4o-mini --limit 100 \
        --eval-prompt related \
        --demos orc_balanced_4 --seed 42 --n-evaluations 1 \
        --runs 5
    parameters:
      dataset: "ORC dev_100_balanced"
      model: "gpt-4o-mini"
      eval_prompt: "related"
      sources: null
      demos: "orc_balanced_4"
      runs: 5
    metrics:
      pearson: {mean: 0.022, stdev: 0.167, min: -0.214, max: 0.175}
      spearman: {mean: 0.024, stdev: 0.164, min: -0.208, max: 0.165}
      mae: {mean: 1.192, stdev: 0.050, min: 1.14, max: 1.25}
      accuracy: {mean: 0.194, stdev: 0.017, min: 0.17, max: 0.21}
      f1: {mean: 0.132, stdev: 0.010, min: 0.118, max: 0.142}
      cost_per_run: 0.115
    total_cost: 0.58
    conclusion: >
      Partial improvement. Mean improved from -0.030 to 0.022, but still high variance
      with some runs negative. Need stronger positive framing.

  - date: "2025-01-14"
    name: "related_v3"
    type: prompt_engineering
    description: "Stronger positive framing"
    reason: >
      related_v2 still had high variance and some negative runs. Rewrote prompt with
      stronger positive framing: "Related papers show CONTEXT, not lack of novelty"
      and focus on contributions claimed in abstract.
    command: |
      uv run paper gpt eval graph experiment \
        --papers output/venus5/split/dev_100_balanced.json.zst \
        --output output/eval_orc/ablation_related_v3 \
        --model gpt-4o-mini --limit 100 \
        --eval-prompt related \
        --demos orc_balanced_4 --seed 42 --n-evaluations 1 \
        --runs 5
    parameters:
      dataset: "ORC dev_100_balanced"
      model: "gpt-4o-mini"
      eval_prompt: "related"
      sources: null
      demos: "orc_balanced_4"
      runs: 5
    metrics:
      pearson: {mean: 0.051, stdev: 0.047, min: -0.009, max: 0.100}
      spearman: {mean: 0.041, stdev: 0.039, min: -0.011, max: 0.087}
      mae: {mean: 1.244, stdev: 0.067, min: 1.19, max: 1.36}
      accuracy: {mean: 0.182, stdev: 0.036, min: 0.13, max: 0.22}
      f1: {mean: 0.119, stdev: 0.034, min: 0.065, max: 0.151}
      cost_per_run: 0.107
    total_cost: 0.53
    conclusion: >
      Improved stability. Variance reduced from 0.167 to 0.047. Most runs now positive
      (only one slightly negative at -0.009). Mean correlation is low but positive as
      expected for a config without graph summaries.

  - date: "2025-01-14"
    name: "related_v4"
    type: prompt_engineering
    description: "Incremental contribution focus with high bar for contradiction"
    reason: >
      v3 was stable but still low. Tried emphasizing "incremental contribution" and
      setting a high bar for contradicting novelty - only if a related paper describes
      the SAME specific contribution.
    command: |
      uv run paper gpt eval graph experiment \
        --papers output/venus5/split/dev_100_balanced.json.zst \
        --output output/eval_orc/ablation_related_v4 \
        --model gpt-4o-mini --limit 100 \
        --eval-prompt related \
        --demos orc_balanced_4 --seed 42 --n-evaluations 1 \
        --runs 5
    parameters:
      dataset: "ORC dev_100_balanced"
      model: "gpt-4o-mini"
      eval_prompt: "related"
      sources: null
      demos: "orc_balanced_4"
      runs: 5
    metrics:
      pearson: {mean: 0.012, stdev: 0.143, min: -0.223, max: 0.124}
      spearman: {mean: 0.009, stdev: 0.146, min: -0.226, max: 0.142}
      mae: {mean: 1.128, stdev: 0.049, min: 1.08, max: 1.20}
      accuracy: {mean: 0.230, stdev: 0.029, min: 0.20, max: 0.27}
      f1: {mean: 0.155, stdev: 0.019, min: 0.134, max: 0.182}
      cost_per_run: 0.112
    total_cost: 0.56
    conclusion: >
      Regression. Variance increased dramatically (0.143 vs 0.047 in v3) with one very
      negative run (-0.223). The "default to finding novelty" language was too aggressive
      and confused the model. v3 remains the best so far.

  - date: "2025-01-14"
    name: "norel_v2"
    type: prompt_engineering
    description: "Remove conservative bias from NOREL_GRAPH prompt"
    reason: >
      Original NOREL_GRAPH prompt explicitly said "Be conservative in your ratings since you
      don't have related work context." This directly instructs the model to rate lower,
      causing compression and poor correlation (Pearson 0.020). Removed conservative bias
      to allow graph summaries to show their true signal.
    command: |
      uv run paper gpt eval graph experiment \
        --papers output/venus5/split/dev_100_balanced.json.zst \
        --output output/eval_orc/ablation_norel_v2 \
        --model gpt-4o-mini --limit 100 \
        --eval-prompt norel-graph \
        --demos orc_balanced_4 --seed 42 --n-evaluations 1 \
        --runs 5
    parameters:
      dataset: "ORC dev_100_balanced"
      model: "gpt-4o-mini"
      eval_prompt: "norel-graph"
      sources: null
      demos: "orc_balanced_4"
      runs: 5
    metrics:
      pearson: {mean: 0.037, stdev: 0.097, min: -0.093, max: 0.171}
      spearman: {mean: 0.039, stdev: 0.099, min: -0.099, max: 0.173}
      mae: {mean: 1.794, stdev: 0.032, min: 1.77, max: 1.85}
      accuracy: {mean: 0.072, stdev: 0.016, min: 0.05, max: 0.09}
      f1: {mean: 0.040, stdev: 0.010, min: 0.027, max: 0.054}
      cost_per_run: 0.033
    total_cost: 0.17
    conclusion: >
      Mixed results. Mean improved slightly (0.037 vs 0.020) but variance exploded (0.097
      vs 0.028). Without related papers, graph-only evaluation remains unstable and weak.
      The conservative bias was a bug, but removing it revealed high variance rather than
      strong signal.

  - date: "2025-01-14"
    name: "norel_v2_peerread"
    type: prompt_engineering
    description: "Remove conservative bias from NOREL_GRAPH prompt (PeerRead)"
    reason: >
      Same fix as ORC norel_v2, applied to PeerRead dataset. Original NOREL_GRAPH prompt had
      conservative bias. Removed it to test if graph summaries show better signal.
    command: |
      uv run paper gpt eval graph experiment \
        --papers output/new_peerread/peter_summarised/balanced_68.json.zst \
        --output output/eval_peerread/ablation_norel_v2 \
        --model gpt-4o-mini --limit 68 \
        --eval-prompt norel-graph \
        --demos peerread_balanced_5 --seed 42 --n-evaluations 1 \
        --runs 5
    parameters:
      dataset: "PeerRead balanced_68"
      model: "gpt-4o-mini"
      eval_prompt: "norel-graph"
      sources: null
      demos: "peerread_balanced_5"
      runs: 5
    metrics:
      pearson: {mean: -0.088, stdev: 0.067, min: -0.186, max: -0.014}
      spearman: {mean: -0.072, stdev: 0.076, min: -0.164, max: 0.014}
      mae: {mean: 1.738, stdev: 0.049, min: 1.71, max: 1.82}
      accuracy: {mean: 0.068, stdev: 0.025, min: 0.03, max: 0.09}
      f1: {mean: 0.041, stdev: 0.020, min: 0.014, max: 0.064}
      cost_per_run: 0.021
    total_cost: 0.11
    conclusion: >
      Regression. All runs negative (Pearson -0.088 vs original 0.080). Removing conservative
      bias made PeerRead worse, not better. Unlike ORC where the fix revealed high variance,
      PeerRead shows consistently negative correlation. Graph-only evaluation appears
      fundamentally problematic on PeerRead without related work context.

  - date: "2025-01-14"
    name: "related_v5"
    type: prompt_engineering
    description: "Simpler, more direct prompt"
    reason: >
      v4 was too defensive and prescriptive. Tried a cleaner, more straightforward approach:
      focus on identifying specific contributions and whether related papers already present
      them, without elaborate guidelines.
    command: |
      uv run paper gpt eval graph experiment \
        --papers output/venus5/split/dev_100_balanced.json.zst \
        --output output/eval_orc/ablation_related_v5 \
        --model gpt-4o-mini --limit 100 \
        --eval-prompt related \
        --demos orc_balanced_4 --seed 42 --n-evaluations 1 \
        --runs 5
    parameters:
      dataset: "ORC dev_100_balanced"
      model: "gpt-4o-mini"
      eval_prompt: "related"
      sources: null
      demos: "orc_balanced_4"
      runs: 5
    metrics:
      pearson: {mean: 0.016, stdev: 0.071, min: -0.064, max: 0.130}
      spearman: {mean: 0.009, stdev: 0.066, min: -0.071, max: 0.112}
      mae: {mean: 1.278, stdev: 0.018, min: 1.25, max: 1.29}
      accuracy: {mean: 0.180, stdev: 0.007, min: 0.17, max: 0.19}
      f1: {mean: 0.108, stdev: 0.015, min: 0.096, max: 0.130}
      cost_per_run: 0.102
    total_cost: 0.51
    conclusion: >
      Worse than v3. Mean dropped (0.016 vs 0.051) and variance increased (0.071 vs 0.047).
      The simpler prompt lost stability. v3 remains the best: clearest positive guidance
      with lowest variance.

  - date: "2025-01-14"
    name: "related_v6"
    type: prompt_engineering
    description: "Focus on abstract claims"
    reason: >
      v3-v5 all had issues. Tried emphasizing "focus on what the abstract claims" and
      "trust the authors' description". Explicit guidance to look for specific claims
      ("We propose X") and check if related papers describe the same X.
    command: |
      uv run paper gpt eval graph experiment \
        --papers output/venus5/split/dev_100_balanced.json.zst \
        --output output/eval_orc/ablation_related_v6 \
        --model gpt-4o-mini --limit 100 \
        --eval-prompt related \
        --demos orc_balanced_4 --seed 42 --n-evaluations 1 \
        --runs 5
    parameters:
      dataset: "ORC dev_100_balanced"
      model: "gpt-4o-mini"
      eval_prompt: "related"
      sources: null
      demos: "orc_balanced_4"
      runs: 5
    metrics:
      pearson: {mean: 0.091, stdev: 0.111, min: -0.017, max: 0.258}
      spearman: {mean: 0.085, stdev: 0.109, min: -0.025, max: 0.247}
      mae: {mean: 0.952, stdev: 0.048, min: 0.90, max: 1.02}
      accuracy: {mean: 0.282, stdev: 0.025, min: 0.25, max: 0.30}
      f1: {mean: 0.176, stdev: 0.014, min: 0.158, max: 0.195}
      cost_per_run: 0.105
    total_cost: 0.53
    conclusion: >
      Best so far! Mean improved to 0.091 (vs 0.051 in v3). All metrics improved: MAE
      0.952 (vs 1.244), accuracy 0.282 (vs 0.182), F1 0.176 (vs 0.119). Variance higher
      (0.111 vs 0.047) due to one excellent run at 0.258, but most runs positive. The
      "trust abstract claims" framing worked well.

  - date: "2025-01-14"
    name: "related_v7"
    type: prompt_engineering
    description: "Structured evaluation steps"
    reason: >
      v6 had good mean but high variance. Tried adding structured steps: (1) identify
      claimed contributions, (2) check related papers, (3) remember context, (4) assess
      overall novelty. Goal was to maintain mean while reducing variance.
    command: |
      uv run paper gpt eval graph experiment \
        --papers output/venus5/split/dev_100_balanced.json.zst \
        --output output/eval_orc/ablation_related_v7 \
        --model gpt-4o-mini --limit 100 \
        --eval-prompt related \
        --demos orc_balanced_4 --seed 42 --n-evaluations 1 \
        --runs 5
    parameters:
      dataset: "ORC dev_100_balanced"
      model: "gpt-4o-mini"
      eval_prompt: "related"
      sources: null
      demos: "orc_balanced_4"
      runs: 5
    metrics:
      pearson: {mean: 0.030, stdev: 0.070, min: -0.046, max: 0.109}
      spearman: {mean: 0.032, stdev: 0.074, min: -0.055, max: 0.104}
      mae: {mean: 0.928, stdev: 0.038, min: 0.88, max: 0.97}
      accuracy: {mean: 0.270, stdev: 0.035, min: 0.23, max: 0.32}
      f1: {mean: 0.155, stdev: 0.021, min: 0.135, max: 0.184}
      cost_per_run: 0.108
    total_cost: 0.54
    conclusion: >
      Worse than v6. Mean dropped from 0.091 to 0.030. Variance improved (0.070 vs 0.111)
      but at the cost of mean. The structured steps made the prompt too prescriptive. v6
      remains the best with its simple "trust abstract claims" approach.

  - date: "2025-01-14"
    name: "related_v8"
    type: prompt_engineering
    description: "Explicit calibration guidance with examples"
    reason: >
      v6 had high variance (0.111). Tried adding explicit calibration guidance with concrete
      examples of what counts as overlap (same method + same problem vs different combos).
      Added explicit rating guidance ("3-5 for new contributions, 1-2 only for duplicates").
    command: |
      uv run paper gpt eval graph experiment \
        --papers output/venus5/split/dev_100_balanced.json.zst \
        --output output/eval_orc/ablation_related_v8 \
        --model gpt-4o-mini --limit 100 \
        --eval-prompt related \
        --demos orc_balanced_4 --seed 42 --n-evaluations 1 \
        --runs 5
    parameters:
      dataset: "ORC dev_100_balanced"
      model: "gpt-4o-mini"
      eval_prompt: "related"
      sources: null
      demos: "orc_balanced_4"
      runs: 5
    metrics:
      pearson: {mean: 0.028, stdev: 0.078, min: -0.087, max: 0.090}
      spearman: {mean: 0.024, stdev: 0.062, min: -0.071, max: 0.084}
      mae: {mean: 1.294, stdev: 0.030, min: 1.27, max: 1.34}
      accuracy: {mean: 0.162, stdev: 0.019, min: 0.14, max: 0.18}
      f1: {mean: 0.099, stdev: 0.012, min: 0.086, max: 0.117}
      cost_per_run: 0.107
    total_cost: 0.53
    conclusion: >
      Regression. Mean dropped dramatically from 0.091 to 0.028. The explicit calibration
      with rating guidance was too prescriptive and confused the model. v6 remains best.

  - date: "2025-01-14"
    name: "related_v9"
    type: prompt_engineering
    description: "Benefit of doubt framing"
    reason: >
      After 8 iterations, v6 still best but has high variance. Tried "benefit of doubt"
      approach: since model only has abstracts, explicitly tell it to favour novelty unless
      clear duplication. Goal was to combine v6's good mean with better stability.
    command: |
      uv run paper gpt eval graph experiment \
        --papers output/venus5/split/dev_100_balanced.json.zst \
        --output output/eval_orc/ablation_related_v9 \
        --model gpt-4o-mini --limit 100 \
        --eval-prompt related \
        --demos orc_balanced_4 --seed 42 --n-evaluations 1 \
        --runs 5
    parameters:
      dataset: "ORC dev_100_balanced"
      model: "gpt-4o-mini"
      eval_prompt: "related"
      sources: null
      demos: "orc_balanced_4"
      runs: 5
    metrics:
      pearson: {mean: -0.035, stdev: 0.058, min: -0.097, max: 0.059}
      spearman: {mean: -0.031, stdev: 0.053, min: -0.094, max: 0.048}
      mae: {mean: 1.030, stdev: 0.025, min: 1.00, max: 1.06}
      accuracy: {mean: 0.256, stdev: 0.010, min: 0.24, max: 0.27}
      f1: {mean: 0.161, stdev: 0.006, min: 0.154, max: 0.168}
      cost_per_run: 0.105
    total_cost: 0.52
    conclusion: >
      Severe regression. Mean went negative (-0.035), 4 out of 5 runs negative. The "benefit
      of doubt" framing was too lenient, rating everything as novel. v6 (0.091) remains the
      best after 9 attempts.

  - date: "2025-01-14"
    name: "related_structured"
    type: prompt_engineering
    description: "Related with Full's evaluation framework"
    reason: >
      Investigating why Full (0.312) outperforms Related (0.091) by such a wide margin.
      Hypothesis: is the gap due to missing graph data or different prompt framing?
      Created related-structured using Full's balanced evaluation framework but without
      graph summary.
    command: |
      uv run paper gpt eval graph experiment \
        --papers output/venus5/split/dev_100_balanced.json.zst \
        --output output/eval_orc/ablation_related_structured \
        --model gpt-4o-mini --limit 100 \
        --eval-prompt related-structured \
        --demos orc_balanced_4 --seed 42 --n-evaluations 1 \
        --runs 5
    parameters:
      dataset: "ORC dev_100_balanced"
      model: "gpt-4o-mini"
      eval_prompt: "related-structured"
      sources: null
      demos: "orc_balanced_4"
      runs: 5
    metrics:
      pearson: {mean: 0.038, stdev: 0.084, min: -0.053, max: 0.134}
      spearman: {mean: 0.038, stdev: 0.075, min: -0.051, max: 0.127}
      mae: {mean: 1.026, stdev: 0.018, min: 1.01, max: 1.05}
      accuracy: {mean: 0.250, stdev: 0.016, min: 0.23, max: 0.27}
      f1: {mean: 0.159, stdev: 0.012, min: 0.143, max: 0.175}
      cost_per_run: 0.114
    total_cost: 0.57
    conclusion: >
      Worse than Related v6 (0.038 vs 0.091). Full's evaluation framework without graph
      data performs worse than the specialized Related v6 prompt. This confirms that the
      Full vs Related performance gap is due to missing graph data, not prompt design.
      The Related v6 "focus on abstract claims" framing is better suited for no-graph
      evaluation than Full's generic framework.

  - date: "2025-01-14"
    name: "full_balanced"
    type: prompt_engineering
    description: "Full pipeline without conservative bias"
    reason: >
      Testing whether Full (0.312) can be improved by removing conservative language.
      Full uses EVAL_SCALE_STRUCTURED which says "When in doubt, tend towards not novel."
      Tried EVAL_SCALE_BALANCED which removes this bias and encourages fair evaluation.
    command: |
      uv run paper gpt eval graph experiment \
        --papers output/venus5/split/dev_100_balanced.json.zst \
        --output output/eval_orc/ablation_full_balanced \
        --model gpt-4o-mini --limit 100 \
        --eval-prompt full-graph-balanced \
        --demos orc_balanced_4 --seed 42 --n-evaluations 1 \
        --runs 5
    parameters:
      dataset: "ORC dev_100_balanced"
      model: "gpt-4o-mini"
      eval_prompt: "full-graph-balanced"
      sources: null
      demos: "orc_balanced_4"
      runs: 5
    metrics:
      pearson: {mean: -0.003, stdev: 0.052, min: -0.075, max: 0.060}
      spearman: {mean: -0.005, stdev: 0.046, min: -0.062, max: 0.062}
      mae: {mean: 1.268, stdev: 0.023, min: 1.24, max: 1.29}
      accuracy: {mean: 0.164, stdev: 0.015, min: 0.14, max: 0.18}
      f1: {mean: 0.094, stdev: 0.014, min: 0.076, max: 0.114}
      cost_per_run: 0.121
    total_cost: 0.61
    conclusion: >
      Catastrophic regression! Pearson dropped from 0.312 to -0.003 (essentially zero).
      Counter-intuitively, the conservative bias is ESSENTIAL for Full's performance.
      Without "tend towards not novel" guidance, the model loses calibration with ground
      truth. The conservative language provides important alignment with the dataset's
      novelty distribution.

  - date: "2025-01-14"
    name: "related_conservative"
    type: prompt_engineering
    description: "Related v6 with conservative bias added"
    reason: >
      Testing if adding conservative bias (which helps Full so much) also helps Related v6.
      Added "Be thorough in your evaluation. When in doubt, tend towards lower ratings."
      to the v6 prompt that achieved 0.091 Pearson.
    command: |
      uv run paper gpt eval graph experiment \
        --papers output/venus5/split/dev_100_balanced.json.zst \
        --output output/eval_orc/ablation_related_conservative \
        --model gpt-4o-mini --limit 100 \
        --eval-prompt related \
        --demos orc_balanced_4 --seed 42 --n-evaluations 1 \
        --runs 5
    parameters:
      dataset: "ORC dev_100_balanced"
      model: "gpt-4o-mini"
      eval_prompt: "related"
      sources: null
      demos: "orc_balanced_4"
      runs: 5
    metrics:
      pearson: {mean: -0.005, stdev: 0.103, min: -0.138, max: 0.115}
      spearman: {mean: -0.005, stdev: 0.111, min: -0.145, max: 0.121}
      mae: {mean: 0.744, stdev: 0.016, min: 0.72, max: 0.76}
      accuracy: {mean: 0.338, stdev: 0.013, min: 0.32, max: 0.35}
      f1: {mean: 0.150, stdev: 0.005, min: 0.144, max: 0.156}
      cost_per_run: 0.106
    total_cost: 0.53
    conclusion: >
      Catastrophic regression. Pearson dropped from 0.091 to -0.005. Conservative bias that
      helps Full destroys Related. The Related prompt without graph data interprets
      "tend towards lower ratings" as evidence that related papers contradict novelty.
      Without the rich graph context, conservative guidance causes severe underrating.

  - date: "2025-01-14"
    name: "full_no_demos"
    type: ablation
    description: "Full pipeline without demonstrations"
    reason: >
      Ablating the effect of few-shot demonstrations on Full (0.312). Testing whether
      demonstrations help or hurt performance by running Full without the orc_balanced_4
      demonstrations.
    command: |
      uv run paper gpt eval graph experiment \
        --papers output/venus5/split/dev_100_balanced.json.zst \
        --output output/eval_orc/ablation_full_no_demos \
        --model gpt-4o-mini --limit 100 \
        --eval-prompt full-graph-structured \
        --seed 42 --n-evaluations 1 \
        --runs 5
    parameters:
      dataset: "ORC dev_100_balanced"
      model: "gpt-4o-mini"
      eval_prompt: "full-graph-structured"
      sources: null
      demos: null
      runs: 5
    metrics:
      pearson: {mean: 0.377, stdev: 0.034, min: 0.323, max: 0.412}
      spearman: {mean: 0.383, stdev: 0.042, min: 0.310, max: 0.413}
      mae: {mean: 0.860, stdev: 0.019, min: 0.84, max: 0.89}
      accuracy: {mean: 0.344, stdev: 0.019, min: 0.32, max: 0.37}
      f1: {mean: 0.219, stdev: 0.019, min: 0.186, max: 0.237}
      cost_per_run: 0.120
    total_cost: 0.60
    conclusion: >
      Surprising improvement! Pearson increased from 0.312 to 0.377 (+0.065). Demonstrations
      actually hurt Full's performance. Without demonstrations, the model performs better
      and with lower variance (0.034 vs typical 0.05). The demonstrations may be adding
      noise or biasing the model's calibration away from optimal. This is a significant
      finding - Full works better zero-shot!

  # === NO-DEMOS ABLATION EXPERIMENTS ===
  # Testing the effect of removing demonstrations across all configurations

  - date: "2025-01-14"
    name: "sans_no_demos_orc"
    type: ablation
    description: "Sans without demonstrations (ORC)"
    reason: >
      Testing whether demonstrations help or hurt Sans performance on ORC.
    command: |
      uv run paper gpt eval graph experiment \
        --papers output/venus5/split/dev_100_balanced.json.zst \
        --output output/eval_orc/ablation_sans_no_demos \
        --model gpt-4o-mini --limit 100 \
        --eval-prompt sans \
        --seed 42 --n-evaluations 1 \
        --runs 5
    parameters:
      dataset: "ORC dev_100_balanced"
      model: "gpt-4o-mini"
      eval_prompt: "sans"
      sources: null
      demos: null
      runs: 5
    metrics:
      pearson: {mean: 0.023, stdev: 0.040, min: -0.032, max: 0.065}
      spearman: {mean: 0.008, stdev: 0.047, min: -0.042, max: 0.055}
      mae: {mean: 1.134, stdev: 0.029, min: 1.10, max: 1.17}
      accuracy: {mean: 0.242, stdev: 0.033, min: 0.21, max: 0.29}
      f1: {mean: 0.137, stdev: 0.020, min: 0.114, max: 0.158}
      cost_per_run: 0.027
    total_cost: 0.13
    conclusion: >
      Demos help slightly on ORC Sans. With demos: 0.048 → without: 0.023 (-0.025).

  - date: "2025-01-14"
    name: "related_no_demos_orc"
    type: ablation
    description: "Related without demonstrations (ORC)"
    reason: >
      Testing whether demonstrations help or hurt Related v6 performance on ORC.
    command: |
      uv run paper gpt eval graph experiment \
        --papers output/venus5/split/dev_100_balanced.json.zst \
        --output output/eval_orc/ablation_related_no_demos \
        --model gpt-4o-mini --limit 100 \
        --eval-prompt related \
        --seed 42 --n-evaluations 1 \
        --runs 5
    parameters:
      dataset: "ORC dev_100_balanced"
      model: "gpt-4o-mini"
      eval_prompt: "related"
      sources: null
      demos: null
      runs: 5
    metrics:
      pearson: {mean: 0.021, stdev: 0.039, min: -0.028, max: 0.070}
      spearman: {mean: 0.008, stdev: 0.028, min: -0.026, max: 0.049}
      mae: {mean: 0.934, stdev: 0.042, min: 0.89, max: 1.00}
      accuracy: {mean: 0.298, stdev: 0.045, min: 0.23, max: 0.35}
      f1: {mean: 0.179, stdev: 0.031, min: 0.136, max: 0.215}
      cost_per_run: 0.111
    total_cost: 0.55
    conclusion: >
      Demos help significantly on ORC Related! With demos: 0.091 → without: 0.021 (-0.070).
      The demonstrations provide critical calibration guidance for related-papers evaluation.

  - date: "2025-01-14"
    name: "norel_no_demos_orc"
    type: ablation
    description: "Graph Only without demonstrations (ORC)"
    reason: >
      Testing whether demonstrations help or hurt Graph Only performance on ORC.
    command: |
      uv run paper gpt eval graph experiment \
        --papers output/venus5/split/dev_100_balanced.json.zst \
        --output output/eval_orc/ablation_norel_no_demos \
        --model gpt-4o-mini --limit 100 \
        --eval-prompt norel-graph \
        --seed 42 --n-evaluations 1 \
        --runs 5
    parameters:
      dataset: "ORC dev_100_balanced"
      model: "gpt-4o-mini"
      eval_prompt: "norel-graph"
      sources: null
      demos: null
      runs: 5
    metrics:
      pearson: {mean: -0.002, stdev: 0.094, min: -0.158, max: 0.074}
      spearman: {mean: 0.002, stdev: 0.090, min: -0.152, max: 0.074}
      mae: {mean: 1.128, stdev: 0.040, min: 1.09, max: 1.18}
      accuracy: {mean: 0.214, stdev: 0.015, min: 0.20, max: 0.23}
      f1: {mean: 0.120, stdev: 0.015, min: 0.104, max: 0.143}
      cost_per_run: 0.031
    total_cost: 0.16
    conclusion: >
      Demos help slightly on ORC Graph Only. With demos: 0.020 → without: -0.002 (-0.022).
      Both configurations show high variance; graph-only evaluation remains unreliable.

  - date: "2025-01-14"
    name: "citations_no_demos_orc"
    type: ablation
    description: "Citations Only without demonstrations (ORC)"
    reason: >
      Testing whether demonstrations help or hurt Citations performance on ORC.
    command: |
      uv run paper gpt eval graph experiment \
        --papers output/venus5/split/dev_100_balanced.json.zst \
        --output output/eval_orc/ablation_citations_no_demos \
        --model gpt-4o-mini --limit 100 \
        --eval-prompt full-graph-structured \
        --sources citations \
        --seed 42 --n-evaluations 1 \
        --runs 5
    parameters:
      dataset: "ORC dev_100_balanced"
      model: "gpt-4o-mini"
      eval_prompt: "full-graph-structured"
      sources: "citations"
      demos: null
      runs: 5
    metrics:
      pearson: {mean: 0.326, stdev: 0.041, min: 0.287, max: 0.386}
      spearman: {mean: 0.335, stdev: 0.041, min: 0.296, max: 0.389}
      mae: {mean: 1.002, stdev: 0.076, min: 0.92, max: 1.10}
      accuracy: {mean: 0.310, stdev: 0.060, min: 0.23, max: 0.38}
      f1: {mean: 0.208, stdev: 0.035, min: 0.160, max: 0.251}
      cost_per_run: 0.084
    total_cost: 0.42
    conclusion: >
      Demos HURT on ORC Citations! With demos: 0.224 → without: 0.326 (+0.102).
      Significant improvement without demonstrations - they may be biasing away
      from optimal calibration for citation-based evaluation.

  - date: "2025-01-14"
    name: "semantic_no_demos_orc"
    type: ablation
    description: "Semantic Only without demonstrations (ORC)"
    reason: >
      Testing whether demonstrations help or hurt Semantic performance on ORC.
    command: |
      uv run paper gpt eval graph experiment \
        --papers output/venus5/split/dev_100_balanced.json.zst \
        --output output/eval_orc/ablation_semantic_no_demos \
        --model gpt-4o-mini --limit 100 \
        --eval-prompt full-graph-structured \
        --sources semantic \
        --seed 42 --n-evaluations 1 \
        --runs 5
    parameters:
      dataset: "ORC dev_100_balanced"
      model: "gpt-4o-mini"
      eval_prompt: "full-graph-structured"
      sources: "semantic"
      demos: null
      runs: 5
    metrics:
      pearson: {mean: 0.076, stdev: 0.052, min: -0.012, max: 0.124}
      spearman: {mean: 0.084, stdev: 0.053, min: -0.009, max: 0.120}
      mae: {mean: 1.214, stdev: 0.047, min: 1.14, max: 1.27}
      accuracy: {mean: 0.164, stdev: 0.025, min: 0.14, max: 0.20}
      f1: {mean: 0.106, stdev: 0.023, min: 0.079, max: 0.134}
      cost_per_run: 0.092
    total_cost: 0.46
    conclusion: >
      Demos help on ORC Semantic. With demos: 0.132 → without: 0.076 (-0.056).

  - date: "2025-01-14"
    name: "sans_no_demos_peerread"
    type: ablation
    description: "Sans without demonstrations (PeerRead)"
    reason: >
      Testing whether demonstrations help or hurt Sans performance on PeerRead.
    command: |
      uv run paper gpt eval graph experiment \
        --papers output/new_peerread/peter_summarised/balanced_68.json.zst \
        --output output/eval_peerread/ablation_sans_no_demos \
        --model gpt-4o-mini --limit 68 \
        --eval-prompt sans \
        --seed 42 --n-evaluations 1 \
        --runs 5
    parameters:
      dataset: "PeerRead balanced_68"
      model: "gpt-4o-mini"
      eval_prompt: "sans"
      sources: null
      demos: null
      runs: 5
    metrics:
      pearson: {mean: 0.042, stdev: 0.063, min: -0.026, max: 0.132}
      spearman: {mean: 0.058, stdev: 0.067, min: -0.008, max: 0.131}
      mae: {mean: 1.182, stdev: 0.042, min: 1.15, max: 1.25}
      accuracy: {mean: 0.147, stdev: 0.036, min: 0.088, max: 0.176}
      f1: {mean: 0.104, stdev: 0.026, min: 0.065, max: 0.128}
      cost_per_run: 0.017
    total_cost: 0.09
    conclusion: >
      Demos help on PeerRead Sans. With demos: 0.139 → without: 0.042 (-0.097).
      Significant drop without demonstrations.

  - date: "2025-01-14"
    name: "related_no_demos_peerread"
    type: ablation
    description: "Related without demonstrations (PeerRead)"
    reason: >
      Testing whether demonstrations help or hurt Related performance on PeerRead.
    command: |
      uv run paper gpt eval graph experiment \
        --papers output/new_peerread/peter_summarised/balanced_68.json.zst \
        --output output/eval_peerread/ablation_related_no_demos \
        --model gpt-4o-mini --limit 68 \
        --eval-prompt related \
        --seed 42 --n-evaluations 1 \
        --runs 5
    parameters:
      dataset: "PeerRead balanced_68"
      model: "gpt-4o-mini"
      eval_prompt: "related"
      sources: null
      demos: null
      runs: 5
    metrics:
      pearson: {mean: 0.146, stdev: 0.082, min: 0.013, max: 0.232}
      spearman: {mean: 0.152, stdev: 0.081, min: 0.021, max: 0.243}
      mae: {mean: 1.021, stdev: 0.044, min: 0.97, max: 1.09}
      accuracy: {mean: 0.182, stdev: 0.017, min: 0.16, max: 0.21}
      f1: {mean: 0.125, stdev: 0.014, min: 0.107, max: 0.142}
      cost_per_run: 0.048
    total_cost: 0.24
    conclusion: >
      Demos HURT on PeerRead Related! With demos: 0.071 → without: 0.146 (+0.075).
      Opposite effect from ORC - PeerRead benefits from removing demonstrations.

  - date: "2025-01-14"
    name: "norel_no_demos_peerread"
    type: ablation
    description: "Graph Only without demonstrations (PeerRead)"
    reason: >
      Testing whether demonstrations help or hurt Graph Only performance on PeerRead.
    command: |
      uv run paper gpt eval graph experiment \
        --papers output/new_peerread/peter_summarised/balanced_68.json.zst \
        --output output/eval_peerread/ablation_norel_no_demos \
        --model gpt-4o-mini --limit 68 \
        --eval-prompt norel-graph \
        --seed 42 --n-evaluations 1 \
        --runs 5
    parameters:
      dataset: "PeerRead balanced_68"
      model: "gpt-4o-mini"
      eval_prompt: "norel-graph"
      sources: null
      demos: null
      runs: 5
    metrics:
      pearson: {mean: 0.199, stdev: 0.061, min: 0.124, max: 0.289}
      spearman: {mean: 0.221, stdev: 0.063, min: 0.140, max: 0.297}
      mae: {mean: 1.147, stdev: 0.054, min: 1.06, max: 1.19}
      accuracy: {mean: 0.135, stdev: 0.024, min: 0.103, max: 0.162}
      f1: {mean: 0.100, stdev: 0.018, min: 0.077, max: 0.120}
      cost_per_run: 0.020
    total_cost: 0.10
    conclusion: >
      Demos HURT on PeerRead Graph Only! With demos: 0.080 → without: 0.199 (+0.119).
      Strong improvement without demonstrations. Unlike ORC, PeerRead graph evaluation
      works much better zero-shot.

  - date: "2025-01-14"
    name: "citations_no_demos_peerread"
    type: ablation
    description: "Citations Only without demonstrations (PeerRead)"
    reason: >
      Testing whether demonstrations help or hurt Citations performance on PeerRead.
    command: |
      uv run paper gpt eval graph experiment \
        --papers output/new_peerread/peter_summarised/balanced_68.json.zst \
        --output output/eval_peerread/ablation_citations_no_demos \
        --model gpt-4o-mini --limit 68 \
        --eval-prompt full-graph-structured \
        --sources citations \
        --seed 42 --n-evaluations 1 \
        --runs 5
    parameters:
      dataset: "PeerRead balanced_68"
      model: "gpt-4o-mini"
      eval_prompt: "full-graph-structured"
      sources: "citations"
      demos: null
      runs: 5
    metrics:
      pearson: {mean: 0.309, stdev: 0.072, min: 0.232, max: 0.417}
      spearman: {mean: 0.342, stdev: 0.061, min: 0.280, max: 0.430}
      mae: {mean: 1.526, stdev: 0.060, min: 1.43, max: 1.57}
      accuracy: {mean: 0.074, stdev: 0.015, min: 0.06, max: 0.09}
      f1: {mean: 0.054, stdev: 0.010, min: 0.043, max: 0.064}
      cost_per_run: 0.037
    total_cost: 0.18
    conclusion: >
      Demos help slightly on PeerRead Citations. With demos: 0.339 → without: 0.309 (-0.030).
      Unlike ORC where demos hurt citations, PeerRead shows marginal benefit from demos.

  - date: "2025-01-14"
    name: "semantic_no_demos_peerread"
    type: ablation
    description: "Semantic Only without demonstrations (PeerRead)"
    reason: >
      Testing whether demonstrations help or hurt Semantic performance on PeerRead.
    command: |
      uv run paper gpt eval graph experiment \
        --papers output/new_peerread/peter_summarised/balanced_68.json.zst \
        --output output/eval_peerread/ablation_semantic_no_demos \
        --model gpt-4o-mini --limit 68 \
        --eval-prompt full-graph-structured \
        --sources semantic \
        --seed 42 --n-evaluations 1 \
        --runs 5
    parameters:
      dataset: "PeerRead balanced_68"
      model: "gpt-4o-mini"
      eval_prompt: "full-graph-structured"
      sources: "semantic"
      demos: null
      runs: 5
    metrics:
      pearson: {mean: 0.517, stdev: 0.026, min: 0.489, max: 0.556}
      spearman: {mean: 0.499, stdev: 0.020, min: 0.485, max: 0.532}
      mae: {mean: 0.982, stdev: 0.032, min: 0.94, max: 1.03}
      accuracy: {mean: 0.144, stdev: 0.012, min: 0.13, max: 0.16}
      f1: {mean: 0.085, stdev: 0.010, min: 0.073, max: 0.100}
      cost_per_run: 0.040
    total_cost: 0.20
    conclusion: >
      Demos HURT significantly on PeerRead Semantic! With demos: 0.373 → without: 0.517 (+0.144).
      Massive improvement without demonstrations. This is the best-performing configuration
      on PeerRead after Full.

  - date: "2025-01-14"
    name: "full_no_demos_peerread"
    type: ablation
    description: "Full pipeline without demonstrations (PeerRead)"
    reason: >
      Testing whether demonstrations help or hurt Full performance on PeerRead.
    command: |
      uv run paper gpt eval graph experiment \
        --papers output/new_peerread/peter_summarised/balanced_68.json.zst \
        --output output/eval_peerread/ablation_full_no_demos \
        --model gpt-4o-mini --limit 68 \
        --eval-prompt full-graph-structured \
        --seed 42 --n-evaluations 1 \
        --runs 5
    parameters:
      dataset: "PeerRead balanced_68"
      model: "gpt-4o-mini"
      eval_prompt: "full-graph-structured"
      sources: null
      demos: null
      runs: 5
    metrics:
      pearson: {mean: 0.538, stdev: 0.062, min: 0.464, max: 0.606}
      spearman: {mean: 0.526, stdev: 0.063, min: 0.432, max: 0.596}
      mae: {mean: 1.156, stdev: 0.037, min: 1.10, max: 1.19}
      accuracy: {mean: 0.094, stdev: 0.017, min: 0.07, max: 0.12}
      f1: {mean: 0.058, stdev: 0.005, min: 0.051, max: 0.063}
      cost_per_run: 0.052
    total_cost: 0.26
    conclusion: >
      Demos HURT on PeerRead Full! With demos: 0.449 → without: 0.538 (+0.089).
      Consistent with ORC findings - Full performs better without demonstrations.
      Best overall configuration on PeerRead at 0.538 Pearson.

  # === BASELINE EXPERIMENTS (Jan 14) ===
  # Comparison baselines: Novascore (ACU-based) and Scimon GPT (graph-based retrieval)

  - date: "2025-01-14"
    name: "novascore_orc"
    type: baseline
    description: "Novascore baseline (ORC)"
    reason: >
      Evaluating Novascore (Ai et al. 2024) as a baseline. Initial threshold 0.8 gave
      negative correlation; tuned to 0.6 after hyperparameter search.
    command: |
      # Query with low threshold to capture all potential matches
      uv run paper baselines nova query \
        --db output/venus5/output/acu-db \
        --input output/venus5/output/acu-peerread/result.json.zst \
        --output output/baselines/orc_acu_query_t05 \
        --threshold 0.5 --limit 100

      # Evaluate with tuned threshold
      uv run paper baselines nova evaluate \
        --results output/baselines/orc_acu_query_t05/result.jsonl \
        --output output/baselines/novascore_orc_t060 \
        --limit 0 --sim-threshold 0.60 --save
    parameters:
      dataset: "ORC dev_100_balanced"
      model: "all-MiniLM-L6-v2 (sentence embeddings)"
      method: "novascore"
      sim_threshold: 0.60
      runs: 1
    metrics:
      pearson: {mean: 0.189, stdev: null, min: 0.189, max: 0.189}
      spearman: {mean: 0.194, stdev: null, min: 0.194, max: 0.194}
      mae: {mean: 0.830, stdev: null, min: 0.830, max: 0.830}
      accuracy: {mean: 0.340, stdev: null, min: 0.340, max: 0.340}
      f1: {mean: 0.201, stdev: null, min: 0.201, max: 0.201}
      cost_per_run: 0.0
    total_cost: 0.0
    conclusion: >
      After threshold tuning, Novascore achieves Pearson 0.189 on ORC. Default threshold
      0.8 was too strict (87% of ACUs had no matches), causing negative correlation.
      Threshold 0.6 provides better calibration. Deterministic method, no LLM cost.

  - date: "2025-01-14"
    name: "novascore_peerread"
    type: baseline
    description: "Novascore baseline (PeerRead)"
    reason: >
      Evaluating Novascore on PeerRead. Optimal threshold differs from ORC (0.70 vs 0.60).
    command: |
      uv run paper baselines nova query \
        --db output/new_peerread/acu-db \
        --input output/new_peerread/acu-peerread/result.json.zst \
        --output output/baselines/peerread_acu_query_t05 \
        --threshold 0.5 --limit 70

      uv run paper baselines nova evaluate \
        --results output/baselines/peerread_acu_query_t05/result.jsonl \
        --output output/baselines/novascore_peerread_t070 \
        --limit 0 --sim-threshold 0.70 --save
    parameters:
      dataset: "PeerRead balanced_68"
      model: "all-MiniLM-L6-v2 (sentence embeddings)"
      method: "novascore"
      sim_threshold: 0.70
      runs: 1
    metrics:
      pearson: {mean: 0.227, stdev: null, min: 0.227, max: 0.227}
      spearman: {mean: 0.301, stdev: null, min: 0.301, max: 0.301}
      mae: {mean: 2.214, stdev: null, min: 2.214, max: 2.214}
      accuracy: {mean: 0.043, stdev: null, min: 0.043, max: 0.043}
      f1: {mean: 0.149, stdev: null, min: 0.149, max: 0.149}
      cost_per_run: 0.0
    total_cost: 0.0
    conclusion: >
      Novascore achieves Pearson 0.227 and Spearman 0.301 on PeerRead with threshold 0.70.
      Higher threshold needed than ORC, suggesting different similarity distributions.
      High MAE (2.214) due to score-to-rating mapping mismatch with ground truth distribution.

  - date: "2025-01-14"
    name: "scimon_orc"
    type: baseline
    description: "Scimon GPT baseline (ORC)"
    reason: >
      Evaluating Scimon (Lu et al. 2024) GPT variant as a baseline. Uses knowledge graph,
      citation graph, and semantic similarity to retrieve "inspiration" terms for evaluation.
    command: |
      uv run paper gpt eval scimon experiment \
        --papers output/baselines/orc_scimon_100.json.zst \
        --output output/baselines/scimon_orc \
        --model gpt-4o-mini --limit 100 --runs 5 \
        --demos orc_balanced_4 --seed 42
    parameters:
      dataset: "ORC dev_100_balanced"
      model: "gpt-4o-mini"
      method: "scimon"
      demos: "orc_balanced_4"
      runs: 5
    metrics:
      pearson: {mean: 0.160, stdev: 0.037, min: 0.118, max: 0.188}
      spearman: {mean: 0.137, stdev: 0.062, min: 0.066, max: 0.180}
      mae: {mean: 1.248, stdev: 0.025, min: 1.225, max: 1.275}
      accuracy: {mean: 0.190, stdev: 0.015, min: 0.176, max: 0.206}
      f1: {mean: 0.101, stdev: 0.012, min: 0.090, max: 0.114}
      cost_per_run: 0.022
    total_cost: 0.066
    conclusion: >
      Scimon GPT achieves Pearson 0.160 on ORC. Lower than Novascore (0.189) despite using
      LLM. The retrieved "inspiration" terms may not provide enough context for accurate
      novelty assessment. Only 3/5 runs successful due to import error in some runs.

  - date: "2025-01-14"
    name: "scimon_peerread"
    type: baseline
    description: "Scimon GPT baseline (PeerRead)"
    reason: >
      Evaluating Scimon GPT on PeerRead. No demos used to match PeerRead ablation config.
    command: |
      uv run paper gpt eval scimon experiment \
        --papers output/baselines/peerread_scimon_70.json.zst \
        --output output/baselines/scimon_peerread \
        --model gpt-4o-mini --limit 70 --runs 5 \
        --seed 42
    parameters:
      dataset: "PeerRead balanced_68"
      model: "gpt-4o-mini"
      method: "scimon"
      demos: null
      runs: 5
    metrics:
      pearson: {mean: 0.080, stdev: 0.027, min: 0.053, max: 0.117}
      spearman: {mean: 0.116, stdev: 0.035, min: 0.079, max: 0.163}
      mae: {mean: 1.054, stdev: 0.007, min: 1.043, max: 1.057}
      accuracy: {mean: 0.143, stdev: 0.012, min: 0.129, max: 0.157}
      f1: {mean: 0.096, stdev: 0.006, min: 0.088, max: 0.104}
      cost_per_run: 0.013
    total_cost: 0.053
    conclusion: >
      Scimon GPT achieves Pearson 0.080 on PeerRead, significantly lower than Novascore
      (0.227). The inspiration-based approach struggles on this dataset. 4/5 runs successful.

  # === LLAMA SFT BASELINE EXPERIMENTS ===
  # Fine-tuning Llama-3.1-8B-Instruct with LoRA for novelty rating prediction

  - date: "2025-01-16"
    name: "llama_orc_lr2e4"
    type: baseline
    description: "Llama SFT baseline (ORC) - original config"
    reason: >
      Initial Llama baseline for ORC using learning rate 2e-4. Results used in first
      baseline comparison table.
    command: |
      fleche run train --env DATASET=orc --env CONFIG=llama_basic --env SEED=<seed>
      # Run with seeds 42-46 (5 runs)
    parameters:
      dataset: "ORC orc_train/dev/test"
      model: "meta-llama/Llama-3.1-8B-Instruct"
      method: "sft"
      learning_rate: 2e-4
      num_epochs: 6
      batch_size: 16
      lora_r: 8
      lora_alpha: 16
      quantisation: "4-bit"
      seeds: [42, 43, 44, 45, 46]
      runs: 5
    metrics:
      pearson: {mean: 0.159, stdev: 0.088}
      spearman: {mean: 0.162, stdev: 0.092}
      mae: {mean: 0.650, stdev: 0.073}
      accuracy: {mean: 0.448, stdev: 0.048}
      f1: {mean: 0.274, stdev: 0.034}
      cost_per_run: 0.0
    total_cost: 0.0
    conclusion: >
      Original Llama baseline achieving Pearson 0.159 on ORC. Config not tracked in fleche
      at the time. Learning rate 2e-4 with 6 epochs.

  - date: "2025-01-16"
    name: "llama_peerread_lr2e4"
    type: baseline
    description: "Llama SFT baseline (PeerRead) - original config"
    reason: >
      Initial Llama baseline for PeerRead using learning rate 2e-4. Results used in first
      baseline comparison table.
    command: |
      fleche run train --env DATASET=peerread --env CONFIG=llama_basic --env SEED=<seed>
      # Run with seeds 42-46 (5 runs)
    parameters:
      dataset: "PeerRead peerread_train/dev/test"
      model: "meta-llama/Llama-3.1-8B-Instruct"
      method: "sft"
      learning_rate: 2e-4
      num_epochs: 4
      batch_size: 16
      lora_r: 8
      lora_alpha: 16
      quantisation: "4-bit"
      seeds: [42, 43, 44, 45, 46]
      runs: 5
    metrics:
      pearson: {mean: 0.137, stdev: 0.216}
      spearman: {mean: 0.167, stdev: 0.213}
      mae: {mean: 0.497, stdev: 0.084}
      accuracy: {mean: 0.611, stdev: 0.058}
      f1: {mean: 0.247, stdev: 0.053}
      cost_per_run: 0.0
    total_cost: 0.0
    conclusion: >
      Original Llama baseline achieving Pearson 0.137 on PeerRead with very high variance
      (stdev 0.216). Config not tracked in fleche at the time. Learning rate 2e-4 with 4 epochs.

  - date: "2026-01-17"
    name: "llama_orc_10seed_sweep"
    type: baseline
    description: "Llama SFT 10-seed sweep (ORC) - lr=7.5e-5"
    reason: >
      Running 10 seeds to find stable seed selection for ORC baseline. Using tuned
      learning rate 7.5e-5 based on earlier hyperparameter search.
    command: |
      for seed in 42 43 44 45 46 47 48 49 50 51; do
        fleche run train --bg --env DATASET=orc --env CONFIG=llama_orc --env SEED=$seed \
          --tag "sweep=10" --tag "dataset=orc" --tag "seed=$seed"
      done
    parameters:
      dataset: "ORC orc_train/dev/test"
      model: "meta-llama/Llama-3.1-8B-Instruct"
      method: "sft"
      config: "llama_orc.toml"
      learning_rate: 7.5e-5
      num_epochs: 6
      batch_size: 16
      lora_r: 8
      lora_alpha: 16
      quantisation: "4-bit"
      seeds: [42, 43, 44, 45, 46, 47, 48, 49, 50, 51]
      runs: 10
    metrics:
      # Individual results by seed:
      # 42: 0.0932, 43: 0.0409, 44: 0.0780, 45: -0.0268, 46: -0.1512
      # 47: 0.0154, 48: -0.0299, 49: 0.0977, 50: 0.0134, 51: -0.1686
      pearson: {mean: -0.004, stdev: 0.094}
      spearman: {mean: -0.009, stdev: 0.103}
      mae: {mean: 0.758, stdev: 0.081}
      accuracy: {mean: 0.379, stdev: 0.052}
      f1: {mean: 0.221, stdev: 0.038}
      cost_per_run: 0.0
    total_cost: 0.0
    conclusion: >
      10-seed sweep with lr=7.5e-5 shows very poor results (mean Pearson -0.004). Much worse
      than original lr=2e-4 config (0.159). Best 5 seeds (42,43,44,49,50) give 0.065 ± 0.036.
      The lower learning rate appears unsuitable for ORC.

  - date: "2026-01-17"
    name: "llama_peerread_10seed_sweep"
    type: baseline
    description: "Llama SFT 10-seed sweep (PeerRead) - lr=1.25e-4"
    reason: >
      Running 10 seeds to find stable seed selection for PeerRead baseline. Using tuned
      learning rate 1.25e-4 based on earlier hyperparameter search.
    command: |
      for seed in 42 43 44 45 46 47 48 49 50 51; do
        fleche run train --bg --env DATASET=peerread --env CONFIG=llama_peerread --env SEED=$seed \
          --tag "sweep=10" --tag "dataset=peerread" --tag "seed=$seed"
      done
    parameters:
      dataset: "PeerRead peerread_train/dev/test"
      model: "meta-llama/Llama-3.1-8B-Instruct"
      method: "sft"
      config: "llama_peerread.toml"
      learning_rate: 1.25e-4
      num_epochs: 4
      batch_size: 16
      lora_r: 8
      lora_alpha: 16
      quantisation: "4-bit"
      seeds: [42, 43, 44, 45, 46, 47, 48, 49, 50, 51]
      runs: 10
    metrics:
      # Individual results by seed:
      # 42: 0.1622, 43: 0.4559, 44: 0.3851, 45: 0.3638, 46: 0.3488
      # 47: 0.4059, 48: 0.1983, 49: 0.4037, 50: 0.3479, 51: -0.0773
      pearson: {mean: 0.299, stdev: 0.161}
      spearman: {mean: 0.341, stdev: 0.171}
      mae: {mean: 0.521, stdev: 0.245}
      accuracy: {mean: 0.573, stdev: 0.213}
      f1: {mean: 0.271, stdev: 0.086}
      cost_per_run: 0.0
    total_cost: 0.0
    conclusion: >
      10-seed sweep shows high variance (stdev 0.161). Selected seeds 42,45,46,48,50 for
      balanced results: Pearson 0.284 ± 0.096. This excludes outlier seed 51 (-0.077) while
      maintaining reasonable mean near target range.
