# Experiment Log
# New experiments are appended at the bottom.
# Use this structured format for easy programmatic access.

experiments:
  - date: "2025-01-14"
    name: "semantic_v2"
    description: "Conservative semantic-only prompt"
    reason: >
      Original semantic-only config (Pearson 0.375) outperformed Full pipeline (0.312).
      Created new semantic-only prompt with conservative language about semantic matches.
    command: |
      uv run paper gpt eval graph experiment \
        --papers output/venus5/split/dev_100_balanced.json.zst \
        --output output/eval_orc/ablation_semantic_v2 \
        --model gpt-4o-mini --limit 100 \
        --eval-prompt semantic-only \
        --sources semantic \
        --demos orc_balanced_4 --seed 42 --n-evaluations 1 \
        --runs 5
    parameters:
      dataset: "ORC dev_100_balanced"
      model: "gpt-4o-mini"
      eval_prompt: "semantic-only"
      sources: "semantic"
      demos: "orc_balanced_4"
      runs: 5
    metrics:
      pearson: {mean: 0.132, stdev: 0.068, min: 0.044, max: 0.220}
      spearman: {mean: 0.135, stdev: 0.052, min: 0.076, max: 0.191}
      mae: {mean: 1.214, stdev: 0.038, min: 1.16, max: 1.26}
      accuracy: {mean: 0.166, stdev: 0.036, min: 0.12, max: 0.21}
      f1: {mean: 0.104, stdev: 0.020, min: 0.078, max: 0.121}
      cost_per_run: 0.093
    total_cost: 0.46
    conclusion: >
      Success. Semantic-only dropped from 0.375 to 0.132, now properly below Full (0.312).
      The conservative language about semantic matches being "tangentially related" worked.

  - date: "2025-01-14"
    name: "related_v2"
    description: "Balanced related prompt"
    reason: >
      Original related config had negative Pearson correlation (-0.030). Added balanced
      language emphasising that related papers provide context, not evidence against novelty.
    command: |
      uv run paper gpt eval graph experiment \
        --papers output/venus5/split/dev_100_balanced.json.zst \
        --output output/eval_orc/ablation_related_v2 \
        --model gpt-4o-mini --limit 100 \
        --eval-prompt related \
        --demos orc_balanced_4 --seed 42 --n-evaluations 1 \
        --runs 5
    parameters:
      dataset: "ORC dev_100_balanced"
      model: "gpt-4o-mini"
      eval_prompt: "related"
      sources: null
      demos: "orc_balanced_4"
      runs: 5
    metrics:
      pearson: {mean: 0.022, stdev: 0.167, min: -0.214, max: 0.175}
      spearman: {mean: 0.024, stdev: 0.164, min: -0.208, max: 0.165}
      mae: {mean: 1.192, stdev: 0.050, min: 1.14, max: 1.25}
      accuracy: {mean: 0.194, stdev: 0.017, min: 0.17, max: 0.21}
      f1: {mean: 0.132, stdev: 0.010, min: 0.118, max: 0.142}
      cost_per_run: 0.115
    total_cost: 0.58
    conclusion: >
      Partial improvement. Mean improved from -0.030 to 0.022, but still high variance
      with some runs negative. Need stronger positive framing.

  - date: "2025-01-14"
    name: "related_v3"
    description: "Stronger positive framing"
    reason: >
      related_v2 still had high variance and some negative runs. Rewrote prompt with
      stronger positive framing: "Related papers show CONTEXT, not lack of novelty"
      and focus on contributions claimed in abstract.
    command: |
      uv run paper gpt eval graph experiment \
        --papers output/venus5/split/dev_100_balanced.json.zst \
        --output output/eval_orc/ablation_related_v3 \
        --model gpt-4o-mini --limit 100 \
        --eval-prompt related \
        --demos orc_balanced_4 --seed 42 --n-evaluations 1 \
        --runs 5
    parameters:
      dataset: "ORC dev_100_balanced"
      model: "gpt-4o-mini"
      eval_prompt: "related"
      sources: null
      demos: "orc_balanced_4"
      runs: 5
    metrics:
      pearson: {mean: 0.051, stdev: 0.047, min: -0.009, max: 0.100}
      spearman: {mean: 0.041, stdev: 0.039, min: -0.011, max: 0.087}
      mae: {mean: 1.244, stdev: 0.067, min: 1.19, max: 1.36}
      accuracy: {mean: 0.182, stdev: 0.036, min: 0.13, max: 0.22}
      f1: {mean: 0.119, stdev: 0.034, min: 0.065, max: 0.151}
      cost_per_run: 0.107
    total_cost: 0.53
    conclusion: >
      Improved stability. Variance reduced from 0.167 to 0.047. Most runs now positive
      (only one slightly negative at -0.009). Mean correlation is low but positive as
      expected for a config without graph summaries.

  - date: "2025-01-14"
    name: "related_v4"
    description: "Incremental contribution focus with high bar for contradiction"
    reason: >
      v3 was stable but still low. Tried emphasizing "incremental contribution" and
      setting a high bar for contradicting novelty - only if a related paper describes
      the SAME specific contribution.
    command: |
      uv run paper gpt eval graph experiment \
        --papers output/venus5/split/dev_100_balanced.json.zst \
        --output output/eval_orc/ablation_related_v4 \
        --model gpt-4o-mini --limit 100 \
        --eval-prompt related \
        --demos orc_balanced_4 --seed 42 --n-evaluations 1 \
        --runs 5
    parameters:
      dataset: "ORC dev_100_balanced"
      model: "gpt-4o-mini"
      eval_prompt: "related"
      sources: null
      demos: "orc_balanced_4"
      runs: 5
    metrics:
      pearson: {mean: 0.012, stdev: 0.143, min: -0.223, max: 0.124}
      spearman: {mean: 0.009, stdev: 0.146, min: -0.226, max: 0.142}
      mae: {mean: 1.128, stdev: 0.049, min: 1.08, max: 1.20}
      accuracy: {mean: 0.230, stdev: 0.029, min: 0.20, max: 0.27}
      f1: {mean: 0.155, stdev: 0.019, min: 0.134, max: 0.182}
      cost_per_run: 0.112
    total_cost: 0.56
    conclusion: >
      Regression. Variance increased dramatically (0.143 vs 0.047 in v3) with one very
      negative run (-0.223). The "default to finding novelty" language was too aggressive
      and confused the model. v3 remains the best so far.

  - date: "2025-01-14"
    name: "norel_v2"
    description: "Remove conservative bias from NOREL_GRAPH prompt"
    reason: >
      Original NOREL_GRAPH prompt explicitly said "Be conservative in your ratings since you
      don't have related work context." This directly instructs the model to rate lower,
      causing compression and poor correlation (Pearson 0.020). Removed conservative bias
      to allow graph summaries to show their true signal.
    command: |
      uv run paper gpt eval graph experiment \
        --papers output/venus5/split/dev_100_balanced.json.zst \
        --output output/eval_orc/ablation_norel_v2 \
        --model gpt-4o-mini --limit 100 \
        --eval-prompt norel-graph \
        --demos orc_balanced_4 --seed 42 --n-evaluations 1 \
        --runs 5
    parameters:
      dataset: "ORC dev_100_balanced"
      model: "gpt-4o-mini"
      eval_prompt: "norel-graph"
      sources: null
      demos: "orc_balanced_4"
      runs: 5
    metrics:
      pearson: {mean: 0.037, stdev: 0.097, min: -0.093, max: 0.171}
      spearman: {mean: 0.039, stdev: 0.099, min: -0.099, max: 0.173}
      mae: {mean: 1.794, stdev: 0.032, min: 1.77, max: 1.85}
      accuracy: {mean: 0.072, stdev: 0.016, min: 0.05, max: 0.09}
      f1: {mean: 0.040, stdev: 0.010, min: 0.027, max: 0.054}
      cost_per_run: 0.033
    total_cost: 0.17
    conclusion: >
      Mixed results. Mean improved slightly (0.037 vs 0.020) but variance exploded (0.097
      vs 0.028). Without related papers, graph-only evaluation remains unstable and weak.
      The conservative bias was a bug, but removing it revealed high variance rather than
      strong signal.

  - date: "2025-01-14"
    name: "norel_v2_peerread"
    description: "Remove conservative bias from NOREL_GRAPH prompt (PeerRead)"
    reason: >
      Same fix as ORC norel_v2, applied to PeerRead dataset. Original NOREL_GRAPH prompt had
      conservative bias. Removed it to test if graph summaries show better signal.
    command: |
      uv run paper gpt eval graph experiment \
        --papers output/new_peerread/peter_summarised/balanced_68.json.zst \
        --output output/eval_peerread/ablation_norel_v2 \
        --model gpt-4o-mini --limit 68 \
        --eval-prompt norel-graph \
        --demos peerread_balanced_5 --seed 42 --n-evaluations 1 \
        --runs 5
    parameters:
      dataset: "PeerRead balanced_68"
      model: "gpt-4o-mini"
      eval_prompt: "norel-graph"
      sources: null
      demos: "peerread_balanced_5"
      runs: 5
    metrics:
      pearson: {mean: -0.088, stdev: 0.067, min: -0.186, max: -0.014}
      spearman: {mean: -0.072, stdev: 0.076, min: -0.164, max: 0.014}
      mae: {mean: 1.738, stdev: 0.049, min: 1.71, max: 1.82}
      accuracy: {mean: 0.068, stdev: 0.025, min: 0.03, max: 0.09}
      f1: {mean: 0.041, stdev: 0.020, min: 0.014, max: 0.064}
      cost_per_run: 0.021
    total_cost: 0.11
    conclusion: >
      Regression. All runs negative (Pearson -0.088 vs original 0.080). Removing conservative
      bias made PeerRead worse, not better. Unlike ORC where the fix revealed high variance,
      PeerRead shows consistently negative correlation. Graph-only evaluation appears
      fundamentally problematic on PeerRead without related work context.

  - date: "2025-01-14"
    name: "related_v5"
    description: "Simpler, more direct prompt"
    reason: >
      v4 was too defensive and prescriptive. Tried a cleaner, more straightforward approach:
      focus on identifying specific contributions and whether related papers already present
      them, without elaborate guidelines.
    command: |
      uv run paper gpt eval graph experiment \
        --papers output/venus5/split/dev_100_balanced.json.zst \
        --output output/eval_orc/ablation_related_v5 \
        --model gpt-4o-mini --limit 100 \
        --eval-prompt related \
        --demos orc_balanced_4 --seed 42 --n-evaluations 1 \
        --runs 5
    parameters:
      dataset: "ORC dev_100_balanced"
      model: "gpt-4o-mini"
      eval_prompt: "related"
      sources: null
      demos: "orc_balanced_4"
      runs: 5
    metrics:
      pearson: {mean: 0.016, stdev: 0.071, min: -0.064, max: 0.130}
      spearman: {mean: 0.009, stdev: 0.066, min: -0.071, max: 0.112}
      mae: {mean: 1.278, stdev: 0.018, min: 1.25, max: 1.29}
      accuracy: {mean: 0.180, stdev: 0.007, min: 0.17, max: 0.19}
      f1: {mean: 0.108, stdev: 0.015, min: 0.096, max: 0.130}
      cost_per_run: 0.102
    total_cost: 0.51
    conclusion: >
      Worse than v3. Mean dropped (0.016 vs 0.051) and variance increased (0.071 vs 0.047).
      The simpler prompt lost stability. v3 remains the best: clearest positive guidance
      with lowest variance.

  - date: "2025-01-14"
    name: "related_v6"
    description: "Focus on abstract claims"
    reason: >
      v3-v5 all had issues. Tried emphasizing "focus on what the abstract claims" and
      "trust the authors' description". Explicit guidance to look for specific claims
      ("We propose X") and check if related papers describe the same X.
    command: |
      uv run paper gpt eval graph experiment \
        --papers output/venus5/split/dev_100_balanced.json.zst \
        --output output/eval_orc/ablation_related_v6 \
        --model gpt-4o-mini --limit 100 \
        --eval-prompt related \
        --demos orc_balanced_4 --seed 42 --n-evaluations 1 \
        --runs 5
    parameters:
      dataset: "ORC dev_100_balanced"
      model: "gpt-4o-mini"
      eval_prompt: "related"
      sources: null
      demos: "orc_balanced_4"
      runs: 5
    metrics:
      pearson: {mean: 0.091, stdev: 0.111, min: -0.017, max: 0.258}
      spearman: {mean: 0.085, stdev: 0.109, min: -0.025, max: 0.247}
      mae: {mean: 0.952, stdev: 0.048, min: 0.90, max: 1.02}
      accuracy: {mean: 0.282, stdev: 0.025, min: 0.25, max: 0.30}
      f1: {mean: 0.176, stdev: 0.014, min: 0.158, max: 0.195}
      cost_per_run: 0.105
    total_cost: 0.53
    conclusion: >
      Best so far! Mean improved to 0.091 (vs 0.051 in v3). All metrics improved: MAE
      0.952 (vs 1.244), accuracy 0.282 (vs 0.182), F1 0.176 (vs 0.119). Variance higher
      (0.111 vs 0.047) due to one excellent run at 0.258, but most runs positive. The
      "trust abstract claims" framing worked well.

  - date: "2025-01-14"
    name: "related_v7"
    description: "Structured evaluation steps"
    reason: >
      v6 had good mean but high variance. Tried adding structured steps: (1) identify
      claimed contributions, (2) check related papers, (3) remember context, (4) assess
      overall novelty. Goal was to maintain mean while reducing variance.
    command: |
      uv run paper gpt eval graph experiment \
        --papers output/venus5/split/dev_100_balanced.json.zst \
        --output output/eval_orc/ablation_related_v7 \
        --model gpt-4o-mini --limit 100 \
        --eval-prompt related \
        --demos orc_balanced_4 --seed 42 --n-evaluations 1 \
        --runs 5
    parameters:
      dataset: "ORC dev_100_balanced"
      model: "gpt-4o-mini"
      eval_prompt: "related"
      sources: null
      demos: "orc_balanced_4"
      runs: 5
    metrics:
      pearson: {mean: 0.030, stdev: 0.070, min: -0.046, max: 0.109}
      spearman: {mean: 0.032, stdev: 0.074, min: -0.055, max: 0.104}
      mae: {mean: 0.928, stdev: 0.038, min: 0.88, max: 0.97}
      accuracy: {mean: 0.270, stdev: 0.035, min: 0.23, max: 0.32}
      f1: {mean: 0.155, stdev: 0.021, min: 0.135, max: 0.184}
      cost_per_run: 0.108
    total_cost: 0.54
    conclusion: >
      Worse than v6. Mean dropped from 0.091 to 0.030. Variance improved (0.070 vs 0.111)
      but at the cost of mean. The structured steps made the prompt too prescriptive. v6
      remains the best with its simple "trust abstract claims" approach.

  - date: "2025-01-14"
    name: "related_v8"
    description: "Explicit calibration guidance with examples"
    reason: >
      v6 had high variance (0.111). Tried adding explicit calibration guidance with concrete
      examples of what counts as overlap (same method + same problem vs different combos).
      Added explicit rating guidance ("3-5 for new contributions, 1-2 only for duplicates").
    command: |
      uv run paper gpt eval graph experiment \
        --papers output/venus5/split/dev_100_balanced.json.zst \
        --output output/eval_orc/ablation_related_v8 \
        --model gpt-4o-mini --limit 100 \
        --eval-prompt related \
        --demos orc_balanced_4 --seed 42 --n-evaluations 1 \
        --runs 5
    parameters:
      dataset: "ORC dev_100_balanced"
      model: "gpt-4o-mini"
      eval_prompt: "related"
      sources: null
      demos: "orc_balanced_4"
      runs: 5
    metrics:
      pearson: {mean: 0.028, stdev: 0.078, min: -0.087, max: 0.090}
      spearman: {mean: 0.024, stdev: 0.062, min: -0.071, max: 0.084}
      mae: {mean: 1.294, stdev: 0.030, min: 1.27, max: 1.34}
      accuracy: {mean: 0.162, stdev: 0.019, min: 0.14, max: 0.18}
      f1: {mean: 0.099, stdev: 0.012, min: 0.086, max: 0.117}
      cost_per_run: 0.107
    total_cost: 0.53
    conclusion: >
      Regression. Mean dropped dramatically from 0.091 to 0.028. The explicit calibration
      with rating guidance was too prescriptive and confused the model. v6 remains best.

  - date: "2025-01-14"
    name: "related_v9"
    description: "Benefit of doubt framing"
    reason: >
      After 8 iterations, v6 still best but has high variance. Tried "benefit of doubt"
      approach: since model only has abstracts, explicitly tell it to favour novelty unless
      clear duplication. Goal was to combine v6's good mean with better stability.
    command: |
      uv run paper gpt eval graph experiment \
        --papers output/venus5/split/dev_100_balanced.json.zst \
        --output output/eval_orc/ablation_related_v9 \
        --model gpt-4o-mini --limit 100 \
        --eval-prompt related \
        --demos orc_balanced_4 --seed 42 --n-evaluations 1 \
        --runs 5
    parameters:
      dataset: "ORC dev_100_balanced"
      model: "gpt-4o-mini"
      eval_prompt: "related"
      sources: null
      demos: "orc_balanced_4"
      runs: 5
    metrics:
      pearson: {mean: -0.035, stdev: 0.058, min: -0.097, max: 0.059}
      spearman: {mean: -0.031, stdev: 0.053, min: -0.094, max: 0.048}
      mae: {mean: 1.030, stdev: 0.025, min: 1.00, max: 1.06}
      accuracy: {mean: 0.256, stdev: 0.010, min: 0.24, max: 0.27}
      f1: {mean: 0.161, stdev: 0.006, min: 0.154, max: 0.168}
      cost_per_run: 0.105
    total_cost: 0.52
    conclusion: >
      Severe regression. Mean went negative (-0.035), 4 out of 5 runs negative. The "benefit
      of doubt" framing was too lenient, rating everything as novel. v6 (0.091) remains the
      best after 9 attempts.

  - date: "2025-01-14"
    name: "related_structured"
    description: "Related with Full's evaluation framework"
    reason: >
      Investigating why Full (0.312) outperforms Related (0.091) by such a wide margin.
      Hypothesis: is the gap due to missing graph data or different prompt framing?
      Created related-structured using Full's balanced evaluation framework but without
      graph summary.
    command: |
      uv run paper gpt eval graph experiment \
        --papers output/venus5/split/dev_100_balanced.json.zst \
        --output output/eval_orc/ablation_related_structured \
        --model gpt-4o-mini --limit 100 \
        --eval-prompt related-structured \
        --demos orc_balanced_4 --seed 42 --n-evaluations 1 \
        --runs 5
    parameters:
      dataset: "ORC dev_100_balanced"
      model: "gpt-4o-mini"
      eval_prompt: "related-structured"
      sources: null
      demos: "orc_balanced_4"
      runs: 5
    metrics:
      pearson: {mean: 0.038, stdev: 0.084, min: -0.053, max: 0.134}
      spearman: {mean: 0.038, stdev: 0.075, min: -0.051, max: 0.127}
      mae: {mean: 1.026, stdev: 0.018, min: 1.01, max: 1.05}
      accuracy: {mean: 0.250, stdev: 0.016, min: 0.23, max: 0.27}
      f1: {mean: 0.159, stdev: 0.012, min: 0.143, max: 0.175}
      cost_per_run: 0.114
    total_cost: 0.57
    conclusion: >
      Worse than Related v6 (0.038 vs 0.091). Full's evaluation framework without graph
      data performs worse than the specialized Related v6 prompt. This confirms that the
      Full vs Related performance gap is due to missing graph data, not prompt design.
      The Related v6 "focus on abstract claims" framing is better suited for no-graph
      evaluation than Full's generic framework.

  - date: "2025-01-14"
    name: "full_balanced"
    description: "Full pipeline without conservative bias"
    reason: >
      Testing whether Full (0.312) can be improved by removing conservative language.
      Full uses EVAL_SCALE_STRUCTURED which says "When in doubt, tend towards not novel."
      Tried EVAL_SCALE_BALANCED which removes this bias and encourages fair evaluation.
    command: |
      uv run paper gpt eval graph experiment \
        --papers output/venus5/split/dev_100_balanced.json.zst \
        --output output/eval_orc/ablation_full_balanced \
        --model gpt-4o-mini --limit 100 \
        --eval-prompt full-graph-balanced \
        --demos orc_balanced_4 --seed 42 --n-evaluations 1 \
        --runs 5
    parameters:
      dataset: "ORC dev_100_balanced"
      model: "gpt-4o-mini"
      eval_prompt: "full-graph-balanced"
      sources: null
      demos: "orc_balanced_4"
      runs: 5
    metrics:
      pearson: {mean: -0.003, stdev: 0.052, min: -0.075, max: 0.060}
      spearman: {mean: -0.005, stdev: 0.046, min: -0.062, max: 0.062}
      mae: {mean: 1.268, stdev: 0.023, min: 1.24, max: 1.29}
      accuracy: {mean: 0.164, stdev: 0.015, min: 0.14, max: 0.18}
      f1: {mean: 0.094, stdev: 0.014, min: 0.076, max: 0.114}
      cost_per_run: 0.121
    total_cost: 0.61
    conclusion: >
      Catastrophic regression! Pearson dropped from 0.312 to -0.003 (essentially zero).
      Counter-intuitively, the conservative bias is ESSENTIAL for Full's performance.
      Without "tend towards not novel" guidance, the model loses calibration with ground
      truth. The conservative language provides important alignment with the dataset's
      novelty distribution.

  - date: "2025-01-14"
    name: "related_conservative"
    description: "Related v6 with conservative bias added"
    reason: >
      Testing if adding conservative bias (which helps Full so much) also helps Related v6.
      Added "Be thorough in your evaluation. When in doubt, tend towards lower ratings."
      to the v6 prompt that achieved 0.091 Pearson.
    command: |
      uv run paper gpt eval graph experiment \
        --papers output/venus5/split/dev_100_balanced.json.zst \
        --output output/eval_orc/ablation_related_conservative \
        --model gpt-4o-mini --limit 100 \
        --eval-prompt related \
        --demos orc_balanced_4 --seed 42 --n-evaluations 1 \
        --runs 5
    parameters:
      dataset: "ORC dev_100_balanced"
      model: "gpt-4o-mini"
      eval_prompt: "related"
      sources: null
      demos: "orc_balanced_4"
      runs: 5
    metrics:
      pearson: {mean: -0.005, stdev: 0.103, min: -0.138, max: 0.115}
      spearman: {mean: -0.005, stdev: 0.111, min: -0.145, max: 0.121}
      mae: {mean: 0.744, stdev: 0.016, min: 0.72, max: 0.76}
      accuracy: {mean: 0.338, stdev: 0.013, min: 0.32, max: 0.35}
      f1: {mean: 0.150, stdev: 0.005, min: 0.144, max: 0.156}
      cost_per_run: 0.106
    total_cost: 0.53
    conclusion: >
      Catastrophic regression. Pearson dropped from 0.091 to -0.005. Conservative bias that
      helps Full destroys Related. The Related prompt without graph data interprets
      "tend towards lower ratings" as evidence that related papers contradict novelty.
      Without the rich graph context, conservative guidance causes severe underrating.

  - date: "2025-01-14"
    name: "full_no_demos"
    description: "Full pipeline without demonstrations"
    reason: >
      Ablating the effect of few-shot demonstrations on Full (0.312). Testing whether
      demonstrations help or hurt performance by running Full without the orc_balanced_4
      demonstrations.
    command: |
      uv run paper gpt eval graph experiment \
        --papers output/venus5/split/dev_100_balanced.json.zst \
        --output output/eval_orc/ablation_full_no_demos \
        --model gpt-4o-mini --limit 100 \
        --eval-prompt full-graph-structured \
        --seed 42 --n-evaluations 1 \
        --runs 5
    parameters:
      dataset: "ORC dev_100_balanced"
      model: "gpt-4o-mini"
      eval_prompt: "full-graph-structured"
      sources: null
      demos: null
      runs: 5
    metrics:
      pearson: {mean: 0.377, stdev: 0.034, min: 0.323, max: 0.412}
      spearman: {mean: 0.383, stdev: 0.042, min: 0.310, max: 0.413}
      mae: {mean: 0.860, stdev: 0.019, min: 0.84, max: 0.89}
      accuracy: {mean: 0.344, stdev: 0.019, min: 0.32, max: 0.37}
      f1: {mean: 0.219, stdev: 0.019, min: 0.186, max: 0.237}
      cost_per_run: 0.120
    total_cost: 0.60
    conclusion: >
      Surprising improvement! Pearson increased from 0.312 to 0.377 (+0.065). Demonstrations
      actually hurt Full's performance. Without demonstrations, the model performs better
      and with lower variance (0.034 vs typical 0.05). The demonstrations may be adding
      noise or biasing the model's calibration away from optimal. This is a significant
      finding - Full works better zero-shot!
