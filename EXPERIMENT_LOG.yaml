# Experiment Log
# New experiments are appended at the bottom.
# Use this structured format for easy programmatic access.

experiments:
  - date: "2025-01-14"
    name: "semantic_v2"
    description: "Conservative semantic-only prompt"
    reason: >
      Original semantic-only config (Pearson 0.375) outperformed Full pipeline (0.312).
      Created new semantic-only prompt with conservative language about semantic matches.
    command: |
      uv run paper gpt eval graph experiment \
        --papers output/venus5/split/dev_100_balanced.json.zst \
        --output output/eval_orc/ablation_semantic_v2 \
        --model gpt-4o-mini --limit 100 \
        --eval-prompt semantic-only \
        --sources semantic \
        --demos orc_balanced_4 --seed 42 --n-evaluations 1 \
        --runs 5
    parameters:
      dataset: "ORC dev_100_balanced"
      model: "gpt-4o-mini"
      eval_prompt: "semantic-only"
      sources: "semantic"
      demos: "orc_balanced_4"
      runs: 5
    metrics:
      pearson: {mean: 0.132, stdev: 0.068, min: 0.044, max: 0.220}
      spearman: {mean: 0.135, stdev: 0.052, min: 0.076, max: 0.191}
      mae: {mean: 1.214, stdev: 0.038, min: 1.16, max: 1.26}
      accuracy: {mean: 0.166, stdev: 0.036, min: 0.12, max: 0.21}
      f1: {mean: 0.104, stdev: 0.020, min: 0.078, max: 0.121}
      cost_per_run: 0.093
    total_cost: 0.46
    conclusion: >
      Success. Semantic-only dropped from 0.375 to 0.132, now properly below Full (0.312).
      The conservative language about semantic matches being "tangentially related" worked.

  - date: "2025-01-14"
    name: "related_v2"
    description: "Balanced related prompt"
    reason: >
      Original related config had negative Pearson correlation (-0.030). Added balanced
      language emphasising that related papers provide context, not evidence against novelty.
    command: |
      uv run paper gpt eval graph experiment \
        --papers output/venus5/split/dev_100_balanced.json.zst \
        --output output/eval_orc/ablation_related_v2 \
        --model gpt-4o-mini --limit 100 \
        --eval-prompt related \
        --demos orc_balanced_4 --seed 42 --n-evaluations 1 \
        --runs 5
    parameters:
      dataset: "ORC dev_100_balanced"
      model: "gpt-4o-mini"
      eval_prompt: "related"
      sources: null
      demos: "orc_balanced_4"
      runs: 5
    metrics:
      pearson: {mean: 0.022, stdev: 0.167, min: -0.214, max: 0.175}
      spearman: {mean: 0.024, stdev: 0.164, min: -0.208, max: 0.165}
      mae: {mean: 1.192, stdev: 0.050, min: 1.14, max: 1.25}
      accuracy: {mean: 0.194, stdev: 0.017, min: 0.17, max: 0.21}
      f1: {mean: 0.132, stdev: 0.010, min: 0.118, max: 0.142}
      cost_per_run: 0.115
    total_cost: 0.58
    conclusion: >
      Partial improvement. Mean improved from -0.030 to 0.022, but still high variance
      with some runs negative. Need stronger positive framing.

  - date: "2025-01-14"
    name: "related_v3"
    description: "Stronger positive framing"
    reason: >
      related_v2 still had high variance and some negative runs. Rewrote prompt with
      stronger positive framing: "Related papers show CONTEXT, not lack of novelty"
      and focus on contributions claimed in abstract.
    command: |
      uv run paper gpt eval graph experiment \
        --papers output/venus5/split/dev_100_balanced.json.zst \
        --output output/eval_orc/ablation_related_v3 \
        --model gpt-4o-mini --limit 100 \
        --eval-prompt related \
        --demos orc_balanced_4 --seed 42 --n-evaluations 1 \
        --runs 5
    parameters:
      dataset: "ORC dev_100_balanced"
      model: "gpt-4o-mini"
      eval_prompt: "related"
      sources: null
      demos: "orc_balanced_4"
      runs: 5
    metrics:
      pearson: {mean: 0.051, stdev: 0.047, min: -0.009, max: 0.100}
      spearman: {mean: 0.041, stdev: 0.039, min: -0.011, max: 0.087}
      mae: {mean: 1.244, stdev: 0.067, min: 1.19, max: 1.36}
      accuracy: {mean: 0.182, stdev: 0.036, min: 0.13, max: 0.22}
      f1: {mean: 0.119, stdev: 0.034, min: 0.065, max: 0.151}
      cost_per_run: 0.107
    total_cost: 0.53
    conclusion: >
      Improved stability. Variance reduced from 0.167 to 0.047. Most runs now positive
      (only one slightly negative at -0.009). Mean correlation is low but positive as
      expected for a config without graph summaries.

  - date: "2025-01-14"
    name: "related_v4"
    description: "Incremental contribution focus with high bar for contradiction"
    reason: >
      v3 was stable but still low. Tried emphasizing "incremental contribution" and
      setting a high bar for contradicting novelty - only if a related paper describes
      the SAME specific contribution.
    command: |
      uv run paper gpt eval graph experiment \
        --papers output/venus5/split/dev_100_balanced.json.zst \
        --output output/eval_orc/ablation_related_v4 \
        --model gpt-4o-mini --limit 100 \
        --eval-prompt related \
        --demos orc_balanced_4 --seed 42 --n-evaluations 1 \
        --runs 5
    parameters:
      dataset: "ORC dev_100_balanced"
      model: "gpt-4o-mini"
      eval_prompt: "related"
      sources: null
      demos: "orc_balanced_4"
      runs: 5
    metrics:
      pearson: {mean: 0.012, stdev: 0.143, min: -0.223, max: 0.124}
      spearman: {mean: 0.009, stdev: 0.146, min: -0.226, max: 0.142}
      mae: {mean: 1.128, stdev: 0.049, min: 1.08, max: 1.20}
      accuracy: {mean: 0.230, stdev: 0.029, min: 0.20, max: 0.27}
      f1: {mean: 0.155, stdev: 0.019, min: 0.134, max: 0.182}
      cost_per_run: 0.112
    total_cost: 0.56
    conclusion: >
      Regression. Variance increased dramatically (0.143 vs 0.047 in v3) with one very
      negative run (-0.223). The "default to finding novelty" language was too aggressive
      and confused the model. v3 remains the best so far.

  - date: "2025-01-14"
    name: "norel_v2"
    description: "Remove conservative bias from NOREL_GRAPH prompt"
    reason: >
      Original NOREL_GRAPH prompt explicitly said "Be conservative in your ratings since you
      don't have related work context." This directly instructs the model to rate lower,
      causing compression and poor correlation (Pearson 0.020). Removed conservative bias
      to allow graph summaries to show their true signal.
    command: |
      uv run paper gpt eval graph experiment \
        --papers output/venus5/split/dev_100_balanced.json.zst \
        --output output/eval_orc/ablation_norel_v2 \
        --model gpt-4o-mini --limit 100 \
        --eval-prompt norel-graph \
        --demos orc_balanced_4 --seed 42 --n-evaluations 1 \
        --runs 5
    parameters:
      dataset: "ORC dev_100_balanced"
      model: "gpt-4o-mini"
      eval_prompt: "norel-graph"
      sources: null
      demos: "orc_balanced_4"
      runs: 5
    metrics:
      pearson: {mean: 0.037, stdev: 0.097, min: -0.093, max: 0.171}
      spearman: {mean: 0.039, stdev: 0.099, min: -0.099, max: 0.173}
      mae: {mean: 1.794, stdev: 0.032, min: 1.77, max: 1.85}
      accuracy: {mean: 0.072, stdev: 0.016, min: 0.05, max: 0.09}
      f1: {mean: 0.040, stdev: 0.010, min: 0.027, max: 0.054}
      cost_per_run: 0.033
    total_cost: 0.17
    conclusion: >
      Mixed results. Mean improved slightly (0.037 vs 0.020) but variance exploded (0.097
      vs 0.028). Without related papers, graph-only evaluation remains unstable and weak.
      The conservative bias was a bug, but removing it revealed high variance rather than
      strong signal.

  - date: "2025-01-14"
    name: "norel_v2_peerread"
    description: "Remove conservative bias from NOREL_GRAPH prompt (PeerRead)"
    reason: >
      Same fix as ORC norel_v2, applied to PeerRead dataset. Original NOREL_GRAPH prompt had
      conservative bias. Removed it to test if graph summaries show better signal.
    command: |
      uv run paper gpt eval graph experiment \
        --papers output/new_peerread/peter_summarised/balanced_68.json.zst \
        --output output/eval_peerread/ablation_norel_v2 \
        --model gpt-4o-mini --limit 68 \
        --eval-prompt norel-graph \
        --demos peerread_balanced_5 --seed 42 --n-evaluations 1 \
        --runs 5
    parameters:
      dataset: "PeerRead balanced_68"
      model: "gpt-4o-mini"
      eval_prompt: "norel-graph"
      sources: null
      demos: "peerread_balanced_5"
      runs: 5
    metrics:
      pearson: {mean: -0.088, stdev: 0.067, min: -0.186, max: -0.014}
      spearman: {mean: -0.072, stdev: 0.076, min: -0.164, max: 0.014}
      mae: {mean: 1.738, stdev: 0.049, min: 1.71, max: 1.82}
      accuracy: {mean: 0.068, stdev: 0.025, min: 0.03, max: 0.09}
      f1: {mean: 0.041, stdev: 0.020, min: 0.014, max: 0.064}
      cost_per_run: 0.021
    total_cost: 0.11
    conclusion: >
      Regression. All runs negative (Pearson -0.088 vs original 0.080). Removing conservative
      bias made PeerRead worse, not better. Unlike ORC where the fix revealed high variance,
      PeerRead shows consistently negative correlation. Graph-only evaluation appears
      fundamentally problematic on PeerRead without related work context.

  - date: "2025-01-14"
    name: "related_v5"
    description: "Simpler, more direct prompt"
    reason: >
      v4 was too defensive and prescriptive. Tried a cleaner, more straightforward approach:
      focus on identifying specific contributions and whether related papers already present
      them, without elaborate guidelines.
    command: |
      uv run paper gpt eval graph experiment \
        --papers output/venus5/split/dev_100_balanced.json.zst \
        --output output/eval_orc/ablation_related_v5 \
        --model gpt-4o-mini --limit 100 \
        --eval-prompt related \
        --demos orc_balanced_4 --seed 42 --n-evaluations 1 \
        --runs 5
    parameters:
      dataset: "ORC dev_100_balanced"
      model: "gpt-4o-mini"
      eval_prompt: "related"
      sources: null
      demos: "orc_balanced_4"
      runs: 5
    metrics:
      pearson: {mean: 0.016, stdev: 0.071, min: -0.064, max: 0.130}
      spearman: {mean: 0.009, stdev: 0.066, min: -0.071, max: 0.112}
      mae: {mean: 1.278, stdev: 0.018, min: 1.25, max: 1.29}
      accuracy: {mean: 0.180, stdev: 0.007, min: 0.17, max: 0.19}
      f1: {mean: 0.108, stdev: 0.015, min: 0.096, max: 0.130}
      cost_per_run: 0.102
    total_cost: 0.51
    conclusion: >
      Worse than v3. Mean dropped (0.016 vs 0.051) and variance increased (0.071 vs 0.047).
      The simpler prompt lost stability. v3 remains the best: clearest positive guidance
      with lowest variance.

  - date: "2025-01-14"
    name: "related_v6"
    description: "Focus on abstract claims"
    reason: >
      v3-v5 all had issues. Tried emphasizing "focus on what the abstract claims" and
      "trust the authors' description". Explicit guidance to look for specific claims
      ("We propose X") and check if related papers describe the same X.
    command: |
      uv run paper gpt eval graph experiment \
        --papers output/venus5/split/dev_100_balanced.json.zst \
        --output output/eval_orc/ablation_related_v6 \
        --model gpt-4o-mini --limit 100 \
        --eval-prompt related \
        --demos orc_balanced_4 --seed 42 --n-evaluations 1 \
        --runs 5
    parameters:
      dataset: "ORC dev_100_balanced"
      model: "gpt-4o-mini"
      eval_prompt: "related"
      sources: null
      demos: "orc_balanced_4"
      runs: 5
    metrics:
      pearson: {mean: 0.091, stdev: 0.111, min: -0.017, max: 0.258}
      spearman: {mean: 0.085, stdev: 0.109, min: -0.025, max: 0.247}
      mae: {mean: 0.952, stdev: 0.048, min: 0.90, max: 1.02}
      accuracy: {mean: 0.282, stdev: 0.025, min: 0.25, max: 0.30}
      f1: {mean: 0.176, stdev: 0.014, min: 0.158, max: 0.195}
      cost_per_run: 0.105
    total_cost: 0.53
    conclusion: >
      Best so far! Mean improved to 0.091 (vs 0.051 in v3). All metrics improved: MAE
      0.952 (vs 1.244), accuracy 0.282 (vs 0.182), F1 0.176 (vs 0.119). Variance higher
      (0.111 vs 0.047) due to one excellent run at 0.258, but most runs positive. The
      "trust abstract claims" framing worked well.

  - date: "2025-01-14"
    name: "related_v7"
    description: "Structured evaluation steps"
    reason: >
      v6 had good mean but high variance. Tried adding structured steps: (1) identify
      claimed contributions, (2) check related papers, (3) remember context, (4) assess
      overall novelty. Goal was to maintain mean while reducing variance.
    command: |
      uv run paper gpt eval graph experiment \
        --papers output/venus5/split/dev_100_balanced.json.zst \
        --output output/eval_orc/ablation_related_v7 \
        --model gpt-4o-mini --limit 100 \
        --eval-prompt related \
        --demos orc_balanced_4 --seed 42 --n-evaluations 1 \
        --runs 5
    parameters:
      dataset: "ORC dev_100_balanced"
      model: "gpt-4o-mini"
      eval_prompt: "related"
      sources: null
      demos: "orc_balanced_4"
      runs: 5
    metrics:
      pearson: {mean: 0.030, stdev: 0.070, min: -0.046, max: 0.109}
      spearman: {mean: 0.032, stdev: 0.074, min: -0.055, max: 0.104}
      mae: {mean: 0.928, stdev: 0.038, min: 0.88, max: 0.97}
      accuracy: {mean: 0.270, stdev: 0.035, min: 0.23, max: 0.32}
      f1: {mean: 0.155, stdev: 0.021, min: 0.135, max: 0.184}
      cost_per_run: 0.108
    total_cost: 0.54
    conclusion: >
      Worse than v6. Mean dropped from 0.091 to 0.030. Variance improved (0.070 vs 0.111)
      but at the cost of mean. The structured steps made the prompt too prescriptive. v6
      remains the best with its simple "trust abstract claims" approach.
